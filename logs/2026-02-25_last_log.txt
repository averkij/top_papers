[25.02.2026 04:17] Read previous papers.
[25.02.2026 04:17] Generating top page (month).
[25.02.2026 04:17] Writing top page (month).
[25.02.2026 05:57] Read previous papers.
[25.02.2026 05:57] Get feed.
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21193
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21015
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20739
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14337
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16990
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21198
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20945
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16603
[25.02.2026 05:57] Extract page data from URL. URL: https://huggingface.co/papers/2602.21204
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21201
[25.02.2026 05:57] Extract page data from URL. URL: https://huggingface.co/papers/2602.21185
[25.02.2026 05:57] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20424
[25.02.2026 05:57] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.02.2026 05:57] No deleted papers detected.
[25.02.2026 05:57] Downloading and parsing papers (pdf, html). Total: 12.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.21193.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.21193.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.21193.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.21015.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.21015.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.21015.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.20739.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.20739.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.20739.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.14337.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.14337.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.14337.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.16990.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.16990.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.16990.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.21198.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.21198.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.21198.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.20945.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.20945.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.20945.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.16603.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.16603.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.16603.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.21204.
[25.02.2026 05:57] Downloading paper 2602.21204 from https://arxiv.org/pdf/2602.21204v1...
[25.02.2026 05:57] Extracting affiliations from text.
[25.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Test-Time Training with KV Binding Is Secretly Linear Attention Junchen Liu * 1 2 3 Sven Elflein 1 2 3 Or Litany 1 4 Zan Gojcic 1 Ruilong Li * 1 6 2 0 2 4 2 ] . [ 1 4 0 2 1 2 . 2 0 6 2 : r a "
[25.02.2026 05:57] Response: ```python
[]
```
[25.02.2026 05:57] Extracting affiliations from text.
[25.02.2026 05:57] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Test-Time Training with KV Binding Is Secretly Linear Attention Junchen Liu * 1 2 3 Sven Elflein 1 2 3 Or Litany 1 4 Zan Gojcic 1 Ruilong Li * 1 6 2 0 2 4 2 ] . [ 1 4 0 2 1 2 . 2 0 6 2 : r aTest-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as form of online meta-learning that memorizes keyvalue mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that broad class of TTT architectures can be expressed as form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides systematic reduction of diverse TTT variants to standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity. 1. Introduction Test-Time Training (TTT) has emerged as powerful paradigm for dynamic model adaptation. Initially introduced to address distribution shift by updating model parameters on unlabeled test inputs, TTT has since evolved into distinct architectural primitive (Sun et al., 2020). Recent work has increasingly framed TTT as an alternative to standard softmax attention in transformers, offering favorable properties like linear-time compute and constant memory usage during autoregressive inference (Zhang et al., 2025; Behrouz et al., 2024). Among the various TTT formulations, this paper focuses on TTT with KV binding (Sun et al., 2025; Zhang et al., 2025; Han et al., 2025; Behrouz et al., 2024), which optimizes self-supervised key-value association objective in the inner loop, as opposed to end-to-end methods that backpropagate from the final task loss (Tandon et al., 2025). The prevailing interpretation of TTT casts it as 1NVIDIA 2University of Toronto 3Vector Institute 4Technion. Correspondence to: Ruilong Li <ruilongl@nvidia.com>. Preprint. February 25, 2026. 1 form of online meta-learning or memorization (Sun et al., 2025; Finn et al., 2017; Metz et al., 2018), where the inner loop dynamically constructs temporary key-value (KV) map by optimizing neural network (e.g., an MLP) on previously observed tokens. The subsequent inference step is viewed as querying this stored knowledge. This perspective has led to increased complexity in recent architectural designs, motivating the use of sophisticated optimizers, normalization schemes, and deep inner-loop networks (Zhang et al., 2025; Han et al., 2025; Behrouz et al., 2024; 2025a; Dalal et al., 2025), all explicitly designed to improve the fidelity of this memorization. The Memorization Paradox. Yet, this interpretation of TTT as test-time memorization can be directly contradicted by empirical evidence. If TTT would truly function by explicitly learning and retrieving keyvalue associations, its behavior would conform to basic principles of memory formation and optimization dynamics. Contrary to this expectation, we identify systematic anomalies that directly contradict this hypothesis: Distributional Asymmetry. Unlike standard attention, in which queries and keys share the same semantic space, converged TTT models exhibit significant distributional mismatch between queries and keys. Replacing queries with keys. Replacing queries with keys for TTT models has negligible effect on the task performance, suggesting that queries do not play functional retrieval role as in standard attention. Optimization vs. Performance. Counterintuitively, improvements in the inner loop, which can be interpreted as stronger memorization, do not guarantee better downstream performance. The Gradient Ascent Anomaly. Most strikingly, we find that replacing inner-loop gradient descent with gradient ascent always preserves, and in some cases even improves, task performance. These observations collectively challenge the prevailing view of TTT as an online meta-learning or key-value memorization mechanism. TTT is Secretly Linear Attention. Motivated by these observations, and by prior work showing that TTT reduces to linear attention in the restricted case of single linear innerTest-Time Training with KV Binding Is Secretly Linear Attention loop layer with zero initialization (Sun et al., 2025), we revisit the mathematical formulation of TTT. We show analytically that even TTT variants with complex fast-weight parameterizations (including multi-layer MLPs and momentum) can be equivalently rewritten as form of learned linear attention operator (Katharopoulos et al., 2020). Under this unified view, the inner loop does not perform meta learning in the conventional sense. Instead, it induces structured, history-dependent mixing of query, key, and value vectors. This perspective resolves the empirical paradoxes identified above: gradient ascent preserves performance because sign inversions are absorbed into the learned value projection, and distributional symmetry between queries and keys is unnecessary because the mechanism operates as feature mixer rather than similaritybased retrieval system. Practical Implications. Unmasking TTT as Linear Attention is not just theoretical exercise; it unlocks significant practical benefits. By adopting this perspective, we: Simplify. We show that many components introduced in prior TTT architectures, such as weight normalization and momentum, are often redundant. Parallelize. We derive fully parallel form of TTT that achieves up to 4.0 inference throughput on attention calculation while maintaining performance. Generalize. We provide systematic reduction of diverse TTT variants to common linear attention form, suggesting that TTT is best understood as flexible learned linear attention mechanism with enhanced representational capacity which opens up an increased design space. 2. Related Work 2.1. Linear Attention Recurrent Neural Networks (RNNs) have recently gained renewed interest as efficient alternatives to standard Transformers (Vaswani et al., 2017). The introduction of linear attention (Katharopoulos et al., 2020) has accelerated advances in RNN-based architectures, leading to the incorporation of various mechanisms such as token-dependent decay factors (Gu et al., 2021; Smith et al., 2023; Orvieto et al., 2023; Peng et"
[25.02.2026 05:57] Mistral response. {"id": "402a37af702f403896d942857b9f5082", "created": 1771999064, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1460, "total_tokens": 1489, "completion_tokens": 29, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"NVIDIA\",\n    \"University of Toronto\",\n    \"Vector Institute\",\n    \"Technion\"\n]\n```"}}]}
[25.02.2026 05:57] Response: ```python
[
    "NVIDIA",
    "University of Toronto",
    "Vector Institute",
    "Technion"
]
```
[25.02.2026 05:57] Deleting PDF ./assets/pdf/2602.21204.pdf.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.21201.
[25.02.2026 05:57] Extra JSON file exists (./assets/json/2602.21201.json), skip PDF parsing.
[25.02.2026 05:57] Paper image links file exists (./assets/img_data/2602.21201.json), skip HTML parsing.
[25.02.2026 05:57] Success.
[25.02.2026 05:57] Downloading and parsing paper https://huggingface.co/papers/2602.21185.
[25.02.2026 05:57] Downloading paper 2602.21185 from https://arxiv.org/pdf/2602.21185v1...
[25.02.2026 05:58] Extracting affiliations from text.
[25.02.2026 05:58] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 4 2 ] . [ 1 5 8 1 1 2 . 2 0 6 2 : r Published as conference paper at ICLR THE DIFFUSION DUALITY, CHAPTER II: Œ®-SAMPLERS AND EFFICIENT CURRICULUM Justin Deschenaux1 Caglar Gulcehre1,2 Subham Sekhar Sahoo3 1EPFL, Lausanne, Switzerland 2Microsoft AI 3Cornell Tech, NY "
[25.02.2026 05:58] Response: ```python
["EPFL, Lausanne, Switzerland", "Microsoft AI", "Cornell Tech, NY"]
```
[25.02.2026 05:58] Deleting PDF ./assets/pdf/2602.21185.pdf.
[25.02.2026 05:58] Success.
[25.02.2026 05:58] Downloading and parsing paper https://huggingface.co/papers/2602.20424.
[25.02.2026 05:58] Extra JSON file exists (./assets/json/2602.20424.json), skip PDF parsing.
[25.02.2026 05:58] Paper image links file exists (./assets/img_data/2602.20424.json), skip HTML parsing.
[25.02.2026 05:58] Success.
[25.02.2026 05:58] Enriching papers with extra data.
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 0. Abstract Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.  					AI-generated summary Despite rapid recent progress in the te...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 1. Abstract Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.  					AI-generated summary Understanding...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 2. Abstract PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.  					AI-generated summary Reinforcement learning for agentic multimodal models often suffers from interaction collapse, wh...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 3. Abstract LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.  					AI-generated summary Recent advances in AI-assisted programming have empowered agents to ...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 4. Abstract A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.  					AI-generated summary Most recomm...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 5. Abstract Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.  					AI-generated summary Embodied LLMs endow robots with high-level task reasoning, but they cannot ...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 6. Abstract Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.  					AI-generated summary Large Language Models (LLMs) consistently benefit from scaled Chai...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 7. Abstract FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.  					AI-generated summary The...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 8. Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  					AI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of onlin...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 9. Abstract  We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to ...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 10. Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  					AI-generated summary Uniform-state discrete diffusion models excel at few-step...
[25.02.2026 05:58] ********************************************************************************
[25.02.2026 05:58] Abstract 11. Abstract AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.  					AI-generated summary Real-world requests to AI agents are fundamen...
[25.02.2026 05:58] Read previous papers.
[25.02.2026 05:58] Generating reviews via LLM API.
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#open_source", "#agents", "#benchmark", "#long_context", "#training", "#synthetic", "#dataset", "#data"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–æ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö: –∫–∞–∫ –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –∫–æ–º–∞–Ω–¥–æ–≤–∞—Ç—å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#robotics", "#benchmark", "#3d", "#cv"], "emoji": "üß©", "ru": {"title": "–§–∏–∑–∏–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ CHAIN –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –ø–æ–Ω–∏–º–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–∏
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#open_source", "#agents", "#multimodal", "#rl", "#reasoning", "#video", "#training", "#optimization"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è PyVision-RL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è 
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#agents", "#plp", "#long_context", "#benchmark"], "emoji": "üõ†Ô∏è", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –¥–ª–∏–Ω–Ω–æ–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LongCLI-Bench ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è
[25.02.2026 05:58] Using data from previous issue: {"categories": [], "emoji": "üìà", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Conv-FinRe ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–∫—Ü–∏—è–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#agents", "#robotics", "#dataset", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–∞—à–∏–Ω—É —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Reflective Test-Time Planning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π —Ä–æ–±–æ—Ç–∞ –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#rl", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ RL", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) —á–µ—Ä–µ–∑ –æ–±
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–†–∞—Å—Ü–µ–ø–ª–µ–Ω–∏—é –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –æ—Ç —á–∞—Å—Ç–æ—Ç—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äî –ø–æ–±–µ–¥–∞ –Ω–∞–¥ –æ—á–µ—Ä–µ–¥–Ω–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π", "desc": "FlowPrefill ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –æ—á–µ—Ä–µ–¥–∏ –∑–∞
[25.02.2026 05:58] Querying the API.
[25.02.2026 05:58] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  					AI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.
[25.02.2026 05:58] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ test-time training (–æ–±—É—á–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è) –∫–∞–∫ –∏–∑—É—á–µ–Ω–Ω–æ–µ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∞ –Ω–µ –∫–∞–∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —à–∏—Ä–æ–∫–∏–π –∫–ª–∞—Å—Å TTT –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –º–æ–∂–Ω–æ –≤—ã—Ä–∞–∑–∏—Ç—å —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–∞—Ç–æ—Ä learned linear attention, —á—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç —Ä–∞–Ω–µ–µ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π. –≠—Ç–∞ –Ω–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ø—Ä–æ—Å—Ç–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–ª—É—á—à–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ TTT –≤–∞—Ä–∏–∞–Ω—Ç—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç test-time training –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è.",
  "emoji": "‚ö°",
  "title": "Test-time –æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è"
}
```
[25.02.2026 05:58] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  					AI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity."

[25.02.2026 05:58] Response: ```python
["ARCHITECTURE", "TRAINING", "INFERENCE"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes a novel reinterpretation of test-time training (TTT) as learned linear attention, which represents a new architectural perspective and simplification of existing TTT architectures.

- **TRAINING**: The paper discusses test-time training methodologies and how TTT architectures can be reformulated, which directly relates to training approaches.

- **INFERENCE**: The paper mentions "fully parallel formulations that preserve performance while improving efficiency," which relates to optimizing model deployment and inference efficiency.
[25.02.2026 05:58] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING", "INFERENCE"]


**Justification:**

- **ARCHITECTURE**: The paper proposes a novel reinterpretation of test-time training (TTT) as learned linear attention, which represents a new architectural perspective and simplification of existing TTT architectures.

- **TRAINING**: The paper discusses test-time training methodologies and how TTT architectures can be reformulated, which directly relates to training approaches.

- **INFERENCE**: The paper mentions "fully parallel formulations that preserve performance while improving efficiency," which relates to optimizing model deployment and inference efficiency.
[25.02.2026 05:58] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  					AI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity."

[25.02.2026 05:58] Response: ```python
["INTERPRETABILITY", "OPTIMIZATION"]
```

**Justification:**

1. **INTERPRETABILITY**: The paper analyzes and explains model behavior by reinterpreting test-time training (TTT). It reveals phenomena that contradict existing interpretations and provides a new perspective on how TTT architectures work, which is core to understanding model behavior.

2. **OPTIMIZATION**: The paper discusses architectural simplifications, improved efficiency, and parallel formulations that preserve performance while improving efficiency - all of which are optimization-related contributions to training and inference methods.
[25.02.2026 05:58] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "OPTIMIZATION"]


**Justification:**

1. **INTERPRETABILITY**: The paper analyzes and explains model behavior by reinterpreting test-time training (TTT). It reveals phenomena that contradict existing interpretations and provides a new perspective on how TTT architectures work, which is core to understanding model behavior.

2. **OPTIMIZATION**: The paper discusses architectural simplifications, improved efficiency, and parallel formulations that preserve performance while improving efficiency - all of which are optimization-related contributions to training and inference methods.
[25.02.2026 05:58] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper reinterprets test-time training (TTT) as a learned linear attention mechanism instead of a memorization process. The authors argue that TTT can be better understood through the lens of sequence modeling, which reveals inconsistencies in the traditional view of TTT as merely memorizing key-value pairs. By framing TTT in this way, the paper highlights architectural simplifications and efficiency improvements that can be achieved. Ultimately, this new perspective enhances the representational capacity of TTT architectures, making them more effective and easier to implement.","title":"Reframing Test-Time Training: From Memorization to Learned Linear Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper reinterprets test-time training (TTT) as a learned linear attention mechanism instead of a memorization process. The authors argue that TTT can be better understood through the lens of sequence modeling, which reveals inconsistencies in the traditional view of TTT as merely memorizing key-value pairs. By framing TTT in this way, the paper highlights architectural simplifications and efficiency improvements that can be achieved. Ultimately, this new perspective enhances the representational capacity of TTT architectures, making them more effective and easier to implement.', title='Reframing Test-Time Training: From Memorization to Learned Linear Attention'))
[25.02.2026 05:58] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ÊñáÈáçÊñ∞Ëß£Èáä‰∫ÜÊµãËØïÊó∂ËÆ≠ÁªÉÔºàTTTÔºâÔºåÂ∞ÜÂÖ∂ËßÜ‰∏∫‰∏ÄÁßçÂ≠¶‰π†ÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõÔºåËÄåÈùûÁÆÄÂçïÁöÑËÆ∞ÂøÜ„ÄÇÊàë‰ª¨ÂèëÁé∞TTTÁöÑÂ§öÁßçÁé∞Ë±°‰∏éËÆ∞ÂøÜÁöÑËß£ÈáäÁõ∏ÁüõÁõæÔºåÂõ†Ê≠§ÈáçÊñ∞ÂÆ°ËßÜ‰∫ÜTTTÁöÑÂÖ¨Âºè„ÄÇÈÄöËøáËøôÁßçÊñ∞ÁöÑËßÜËßíÔºåÊàë‰ª¨ËÉΩÂ§üÁÆÄÂåñÊû∂ÊûÑÔºåÊèêÈ´òÊïàÁéáÔºåÂπ∂Â∞ÜÂ§öÁßçTTTÂèò‰ΩìÂΩíÁ∫≥‰∏∫Ê†áÂáÜÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõÂΩ¢Âºè„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÁ†îÁ©∂ÁªìÊûúË°®ÊòéTTTÊòØ‰∏ÄÁßçÂÖ∑ÊúâÂ¢ûÂº∫Ë°®Á§∫ËÉΩÂäõÁöÑÂ≠¶‰π†Á∫øÊÄßÊ≥®ÊÑèÂäõÔºåËÄåÈùû‰ªÖ‰ªÖÊòØÊµãËØïÊó∂ÁöÑËÆ∞ÂøÜ„ÄÇ","title":"ÊµãËØïÊó∂ËÆ≠ÁªÉÔºöÂ≠¶‰π†ÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÈáçÊñ∞Ëß£Èáä‰∫ÜÊµãËØïÊó∂ËÆ≠ÁªÉÔºàTTTÔºâÔºåÂ∞ÜÂÖ∂ËßÜ‰∏∫‰∏ÄÁßçÂ≠¶‰π†ÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõÔºåËÄåÈùûÁÆÄÂçïÁöÑËÆ∞ÂøÜ„ÄÇÊàë‰ª¨ÂèëÁé∞TTTÁöÑÂ§öÁßçÁé∞Ë±°‰∏éËÆ∞ÂøÜÁöÑËß£ÈáäÁõ∏ÁüõÁõæÔºåÂõ†Ê≠§ÈáçÊñ∞ÂÆ°ËßÜ‰∫ÜTTTÁöÑÂÖ¨Âºè„ÄÇÈÄöËøáËøôÁßçÊñ∞ÁöÑËßÜËßíÔºåÊàë‰ª¨ËÉΩÂ§üÁÆÄÂåñÊû∂ÊûÑÔºåÊèêÈ´òÊïàÁéáÔºåÂπ∂Â∞ÜÂ§öÁßçTTTÂèò‰ΩìÂΩíÁ∫≥‰∏∫Ê†áÂáÜÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõÂΩ¢Âºè„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåÁ†îÁ©∂ÁªìÊûúË°®ÊòéTTTÊòØ‰∏ÄÁßçÂÖ∑ÊúâÂ¢ûÂº∫Ë°®Á§∫ËÉΩÂäõÁöÑÂ≠¶‰π†Á∫øÊÄßÊ≥®ÊÑèÂäõÔºåËÄåÈùû‰ªÖ‰ªÖÊòØÊµãËØïÊó∂ÁöÑËÆ∞ÂøÜ„ÄÇ', title='ÊµãËØïÊó∂ËÆ≠ÁªÉÔºöÂ≠¶‰π†ÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõ'))
[25.02.2026 05:58] Using data from previous issue: {"categories": [], "emoji": "üßÆ", "ru": {"title": "–ú–∞—à–∏–Ω—ã –º–æ–≥—É—Ç –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–µ–æ—Ä–µ–º—ã: –∞–≥–µ–Ω—Ç Aletheia —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ Aletheia, —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Gemini 3 Deep Think, –Ω–∞ –∫–æ–Ω–∫—É—Ä—Å–µ FirstProof. –ê–≥–µ–Ω—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Ä–µ—à–∏–ª 6 –∏–∑ 10
[25.02.2026 05:58] Querying the API.
[25.02.2026 05:58] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  					AI-generated summary Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2
[25.02.2026 05:58] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã Predictor-Corrector –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–æ–±—â–∞—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–∞–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—É–º–∞. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å PC-—Å—ç–º–ø–ª–µ—Ä–∞–º–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Ç–∞–∫ –∏ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –¥–æ—Å—Ç–∏–≥–∞—è –ª—É—á—à–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ FID-–æ—Ü–µ–Ω–æ–∫. –ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —É–ª—É—á—à–∞—Ç—å—Å—è —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —á–∏—Å–ª–∞ —à–∞–≥–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ –ø–∞–º—è—Ç–∏ –∫—É—Ä–∏–∫—É–ª—É–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –≥–∞—É—Å—Å–æ–≤–æ–π —Ä–µ–ª–∞–∫—Å–∞—Ü–∏–µ–π, —Å–æ–∫—Ä–∞—Ç–∏–≤ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 25% –∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ 33%.",
  "emoji": "üîÑ",
  "title": "Predictor-Corrector —Å—ç–º–ø–ª–µ—Ä—ã –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏"
}
```
[25.02.2026 05:58] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  					AI-generated summary Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2"

[25.02.2026 05:58] Response: ```python
["ARCHITECTURE", "TRAINING", "DATASET"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes a novel family of Predictor-Corrector (PC) samplers for discrete diffusion models, which represents a new neural architecture/component for generation.

- **TRAINING**: The paper develops "a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33%", which directly addresses improving model training methods.

- **DATASET**: The paper evaluates on datasets including OpenWebText, LM1B, and CIFAR10, and these are established benchmarks/datasets used for evaluation purposes.
[25.02.2026 05:58] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING", "DATASET"]


**Justification:**

- **ARCHITECTURE**: The paper proposes a novel family of Predictor-Corrector (PC) samplers for discrete diffusion models, which represents a new neural architecture/component for generation.

- **TRAINING**: The paper develops "a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33%", which directly addresses improving model training methods.

- **DATASET**: The paper evaluates on datasets including OpenWebText, LM1B, and CIFAR10, and these are established benchmarks/datasets used for evaluation purposes.
[25.02.2026 05:58] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  					AI-generated summary Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2"

[25.02.2026 05:58] Response: ```python
['DIFFUSION', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[25.02.2026 05:58] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents a new approach to discrete diffusion models using Predictor-Corrector (PC) samplers, which enhance the quality and efficiency of generative tasks in language and image modeling. Unlike traditional methods, these PC samplers improve performance with more sampling steps, challenging the reliance on masked diffusion techniques. The authors demonstrate that their models achieve lower generative perplexity and better FID/IS scores, indicating superior output quality. Additionally, they introduce a memory-efficient training curriculum that significantly reduces training time and memory usage while maintaining performance.","title":"Revolutionizing Diffusion Models with Predictor-Corrector Samplers"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new approach to discrete diffusion models using Predictor-Corrector (PC) samplers, which enhance the quality and efficiency of generative tasks in language and image modeling. Unlike traditional methods, these PC samplers improve performance with more sampling steps, challenging the reliance on masked diffusion techniques. The authors demonstrate that their models achieve lower generative perplexity and better FID/IS scores, indicating superior output quality. Additionally, they introduce a memory-efficient training curriculum that significantly reduces training time and memory usage while maintaining performance.', title='Revolutionizing Diffusion Models with Predictor-Corrector Samplers'))
[25.02.2026 05:58] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ¶ªÊï£Êâ©Êï£Ê®°ÂûãÔºå‰ΩøÁî®È¢ÑÊµã-Ê†°Ê≠£ÈááÊ†∑Âô®Êù•ÊèêÈ´òÁîüÊàêË¥®ÈáèÂíåÊïàÁéá„ÄÇËøôÁßçÊñπÊ≥ïÊåëÊàò‰∫Ü‰º†ÁªüÁöÑÊé©ËîΩÊâ©Êï£Âú®ËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÁöÑÂøÖË¶ÅÊÄßÔºåÂ∞§ÂÖ∂Âú®Â∞ëÊ≠•ÁîüÊàêÂíåÂºïÂØºÊñπÈù¢Ë°®Áé∞‰ºòÂºÇ„ÄÇ‰∏é‰º†ÁªüÁöÑÁ•ñÂÖàÈááÊ†∑Âô®Áõ∏ÊØîÔºåÈ¢ÑÊµã-Ê†°Ê≠£ÈááÊ†∑Âô®Âú®ÁîüÊàêËøáÁ®ã‰∏≠ËÉΩÂ§üÊåÅÁª≠ÊîπËøõÔºåÂ∞§ÂÖ∂Âú®Â§ÑÁêÜËØ≠Ë®ÄÂíåÂõæÂÉèÂª∫Ê®°Êó∂ÊïàÊûúÊòæËëó„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÜÖÂ≠òÈ´òÊïàÁöÑËÆ≠ÁªÉËØæÁ®ãÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉÊó∂Èó¥ÂíåÂÜÖÂ≠òÊ∂àËÄóÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÁîüÊàêÊÄßËÉΩ„ÄÇ","title":"È¢ÑÊµã-Ê†°Ê≠£ÈááÊ†∑Âô®ÔºöÊâ©Êï£Ê®°ÂûãÁöÑÊñ∞Êú™Êù•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ¶ªÊï£Êâ©Êï£Ê®°ÂûãÔºå‰ΩøÁî®È¢ÑÊµã-Ê†°Ê≠£ÈááÊ†∑Âô®Êù•ÊèêÈ´òÁîüÊàêË¥®ÈáèÂíåÊïàÁéá„ÄÇËøôÁßçÊñπÊ≥ïÊåëÊàò‰∫Ü‰º†ÁªüÁöÑÊé©ËîΩÊâ©Êï£Âú®ËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÁöÑÂøÖË¶ÅÊÄßÔºåÂ∞§ÂÖ∂Âú®Â∞ëÊ≠•ÁîüÊàêÂíåÂºïÂØºÊñπÈù¢Ë°®Áé∞‰ºòÂºÇ„ÄÇ‰∏é‰º†ÁªüÁöÑÁ•ñÂÖàÈááÊ†∑Âô®Áõ∏ÊØîÔºåÈ¢ÑÊµã-Ê†°Ê≠£ÈááÊ†∑Âô®Âú®ÁîüÊàêËøáÁ®ã‰∏≠ËÉΩÂ§üÊåÅÁª≠ÊîπËøõÔºåÂ∞§ÂÖ∂Âú®Â§ÑÁêÜËØ≠Ë®ÄÂíåÂõæÂÉèÂª∫Ê®°Êó∂ÊïàÊûúÊòæËëó„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÜÖÂ≠òÈ´òÊïàÁöÑËÆ≠ÁªÉËØæÁ®ãÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉÊó∂Èó¥ÂíåÂÜÖÂ≠òÊ∂àËÄóÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÁîüÊàêÊÄßËÉΩ„ÄÇ', title='È¢ÑÊµã-Ê†°Ê≠£ÈááÊ†∑Âô®ÔºöÊâ©Êï£Ê®°ÂûãÁöÑÊñ∞Êú™Êù•'))
[25.02.2026 05:58] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#alignment", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é: –æ—Ü–µ–Ω–∫–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ
[25.02.2026 05:58] Renaming data file.
[25.02.2026 05:58] Renaming previous data. hf_papers.json to ./d/2026-02-25.json
[25.02.2026 05:58] Saving new data file.
[25.02.2026 05:58] Generating page.
[25.02.2026 05:58] Renaming previous page.
[25.02.2026 05:58] Renaming previous data. index.html to ./d/2026-02-25.html
[25.02.2026 05:58] Writing result.
[25.02.2026 05:58] Renaming log file.
[25.02.2026 05:58] Renaming previous data. log.txt to ./logs/2026-02-25_last_log.txt
