[25.02.2026 21:30] Read previous papers.
[25.02.2026 21:30] Generating top page (month).
[25.02.2026 21:30] Writing top page (month).
[25.02.2026 22:25] Read previous papers.
[25.02.2026 22:25] Get feed.
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21193
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12192
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20739
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21015
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21204
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21202
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20951
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14337
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18940
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16990
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20309
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16745
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21198
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21185
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20731
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20424
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19633
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16932
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16813
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21201
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21196
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20945
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18998
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20792
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20743
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19020
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18735
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16603
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21053
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21042
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20903
[25.02.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20540
[25.02.2026 22:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.02.2026 22:25] No deleted papers detected.
[25.02.2026 22:25] Downloading and parsing papers (pdf, html). Total: 32.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21193.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21193.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21193.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.12192.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.12192.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.12192.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20739.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20739.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20739.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21015.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21015.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21015.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21204.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21204.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21204.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21202.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21202.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21202.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20951.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20951.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20951.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.14337.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.14337.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.14337.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.18940.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.18940.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.18940.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.16990.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.16990.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.16990.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20309.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20309.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20309.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.16745.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.16745.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.16745.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21198.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21198.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21198.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21185.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21185.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21185.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20731.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20731.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20731.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20424.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20424.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20424.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.19633.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.19633.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.19633.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.16932.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.16932.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.16932.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.16813.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.16813.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.16813.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21201.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21201.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21201.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21196.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21196.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21196.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20945.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20945.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20945.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.18998.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.18998.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.18998.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20792.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20792.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20792.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20743.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20743.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20743.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.19020.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.19020.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.19020.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.18735.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.18735.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.18735.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.16603.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.16603.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.16603.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21053.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21053.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21053.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.21042.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.21042.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.21042.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20903.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20903.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20903.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.20540.
[25.02.2026 22:25] Extra JSON file exists (./assets/json/2602.20540.json), skip PDF parsing.
[25.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.20540.json), skip HTML parsing.
[25.02.2026 22:25] Success.
[25.02.2026 22:25] Enriching papers with extra data.
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 0. Abstract Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.  					AI-generated summary Despite rapid recent progress in the te...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 1. Abstract A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks.  					AI-generated summary Built upon the existing analysis of retrieval heads in large language models, we p...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 2. Abstract PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.  					AI-generated summary Reinforcement learning for agentic multimodal models often suffers from interaction collapse, wh...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 3. Abstract Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.  					AI-generated summary Understanding...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 4. Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  					AI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of onlin...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 5. Abstract Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.  					AI-generated summary We study efficient ...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 6. Abstract ArtiAgent automates the creation of real-artifact image pairs using three agents for perception, synthesis, and curation in diffusion transformers.  					AI-generated summary Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromis...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 7. Abstract LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.  					AI-generated summary Recent advances in AI-assisted programming have empowered agents to ...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 8. Abstract Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 9. Abstract A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.  					AI-generated summary Most recomm...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 10. Abstract QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency.  					AI-generated ...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 11. Abstract Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.  					AI-generated summary Test-time scaling can improve model performan...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 12. Abstract Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.  					AI-generated summary Embodied LLMs endow robots with high-level task reasoning, but they cannot ...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 13. Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  					AI-generated summary Uniform-state discrete diffusion models excel at few-step...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 14. Abstract COMiT framework learns structured discrete visual tokens through iterative encoding and flow-matching decoding, improving object-centric representation and compositional generalization.  					AI-generated summary Discrete image tokenizers have emerged as a key component of modern vision and...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 15. Abstract AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.  					AI-generated summary Real-world requests to AI agents are fundamen...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 16. Abstract TAPE framework improves language model agent performance in complex environments through enhanced planning and constrained execution strategies.  					AI-generated summary Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interaction...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 17. Abstract Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.  					AI-generated summary Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 18. Abstract Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.  					AI-generated summary Language models based on discrete diffusion have a...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 19. Abstract  We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to ...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 20. Abstract UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed.  					AI-generated summary Efficiently processing long sequences with Transf...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 21. Abstract Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.  					AI-generated summary Large Language Models (LLMs) consistently benefit from scaled Chai...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 22. Abstract General AgentBench evaluates large language model agents across multiple domains and scaling methods, revealing performance degradation and fundamental limitations in sequential and parallel scaling approaches.  					AI-generated summary LLM agents are increasingly expected to function as g...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 23. Abstract A biomechanics-aware keypoint simulation framework and the first open dataset, SIMSPINE, provide anatomically consistent 3D spinal annotations for natural full-body motions, enabling data-driven learning of vertebral kinematics and improving spine motion estimation accuracy.  					AI-genera...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 24. Abstract Adaptive text anonymization framework automatically adjusts anonymization strategies based on privacy-utility requirements using prompt optimization for language models across diverse domains and constraints.  					AI-generated summary Anonymizing textual documents is a highly context-sensi...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 25. Abstract Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks.  					AI-generated summary Detecting LLM training data is generally framed as a membe...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 26. Abstract LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement.  					AI-generated summary This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages th...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 27. Abstract FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.  					AI-generated summary The...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 28. Abstract A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.  					AI-generated summary Large Vision-Language Models (VLMs)...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 29. Abstract OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings.  					AI-generated summary Optical character recognition (OCR) has advanced rap...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 30. Abstract TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity.  					AI-generated summary Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation...
[25.02.2026 22:25] ********************************************************************************
[25.02.2026 22:25] Abstract 31. Abstract A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals.  					AI-generated summary Import container dwell...
[25.02.2026 22:25] Read previous papers.
[25.02.2026 22:25] Generating reviews via LLM API.
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#open_source", "#agents", "#benchmark", "#long_context", "#training", "#synthetic", "#dataset", "#data"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–æ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö: –∫–∞–∫ –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –∫–æ–º–∞–Ω–¥–æ–≤–∞—Ç—å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#small_models", "#training", "#rag", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–õ—ë–≥–∫–æ–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏–∑ –≤—ã–±—Ä–∞–Ω
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#open_source", "#agents", "#multimodal", "#rl", "#reasoning", "#video", "#training", "#optimization"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è PyVision-RL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è 
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#robotics", "#benchmark", "#3d", "#cv"], "emoji": "üß©", "ru": {"title": "–§–∏–∑–∏–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ CHAIN –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –ø–æ–Ω–∏–º–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–∏
[25.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "Test-time –æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ test-time training (–æ–±—É—á–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è) –∫–∞–∫ –∏–∑—É—á–µ–Ω–Ω–æ–µ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∞ –Ω–µ –∫–∞–∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —à–∏—Ä–æ–∫–∏–π –∫–ª–∞—Å—Å TTT
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#inference", "#open_source", "#benchmark", "#multimodal", "#rag", "#optimization"], "emoji": "üóúÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#open_source", "#synthetic", "#dataset", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω ArtiAgent ‚Äî —Å–∏—Å—Ç–µ–º–∞ –∏–∑ —Ç—Ä—ë—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#agents", "#plp", "#long_context", "#benchmark"], "emoji": "üõ†Ô∏è", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –¥–ª–∏–Ω–Ω–æ–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LongCLI-Bench ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è
[25.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å—Ç–∏–Ω—ã –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç—á—ë—Ç–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö Deep Research Agents. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –±–µ–≥–ª–æ—Å—Ç–∏ —Ç–µ
[25.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üìà", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Conv-FinRe ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–∫—Ü–∏—è–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#open_source", "#robotics", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "QuantVLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ—Å—Ç–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π vision-language-action, –∫–æ—Ç–æ
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PETS ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#agents", "#robotics", "#dataset", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–∞—à–∏–Ω—É —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Reflective Test-Time Planning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π —Ä–æ–±–æ—Ç–∞ –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#open_source"], "emoji": "üîÑ", "ru": {"title": "Predictor-Corrector —Å—ç–º–ø–ª–µ—Ä—ã –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã Predictor-Corrector –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#reasoning", "#architecture", "#cv", "#training"], "emoji": "üß©", "ru": {"title": "–ö–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—ä–µ–∫—Ç-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "COMiT - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ç–æ
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#alignment", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é: –æ—Ü–µ–Ω–∫–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ
[25.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ TAPE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö —Å –∂–µ—Å—Ç–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ—Å—Ç—å —Ä–µ—à–µ
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#plp"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ RankEvolve ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –ø–æ–º–æ—â—å—é LLM, 
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#open_source", "#training", "#optimization"], "emoji": "üåä", "ru": {"title": "–ü–æ—Ç–æ–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–¥–∏–ª–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å —è–∑—ã–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ç–æ–∫–æ–≤ (FLM), –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –¥–µ–Ω–æ
[25.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üßÆ", "ru": {"title": "–ú–∞—à–∏–Ω—ã –º–æ–≥—É—Ç –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–µ–æ—Ä–µ–º—ã: –∞–≥–µ–Ω—Ç Aletheia —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ Aletheia, —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Gemini 3 Deep Think, –Ω–∞ –∫–æ–Ω–∫—É—Ä—Å–µ FirstProof. –ê–≥–µ–Ω—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Ä–µ—à–∏–ª 6 –∏–∑ 10
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#inference", "#long_context", "#architecture", "#training", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "UPipe ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#rl", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ RL", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) —á–µ—Ä–µ–∑ –æ–±
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#plp", "#agents", "#open_source", "#reasoning"], "emoji": "üß©", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –Ω–∞ LLM: –∫–æ–≥–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω General AgentBench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö agent–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#3d", "#cv"], "emoji": "ü¶¥", "ru": {"title": "–ê–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è 3D –æ—Ü–µ–Ω–∫–∞ –¥–≤–∏–∂–µ–Ω–∏–π –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–∞ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–∞ —Å —É—á—ë—Ç–æ–º –±–∏–æ–º–µ—Ö–∞–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤
[25.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#security", "#training", "#rl", "#leakage"], "emoji": "üîç", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —É—Å–∏–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—è–≤–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Active Data Reconstructi
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#3d"], "emoji": "üß©", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "LaS-Comp –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ 3D –º–æ–¥–µ–ª–∏ —Å –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º.
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–†–∞—Å—Ü–µ–ø–ª–µ–Ω–∏—é –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –æ—Ç —á–∞—Å—Ç–æ—Ç—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äî –ø–æ–±–µ–¥–∞ –Ω–∞–¥ –æ—á–µ—Ä–µ–¥–Ω–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π", "desc": "FlowPrefill ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –æ—á–µ—Ä–µ–¥–∏ –∑–∞
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#multimodal", "#interpretability", "#open_source", "#benchmark", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#multilingual", "#small_models", "#transfer_learning", "#low_resource", "#optimization", "#open_source", "#cv", "#training"], "emoji": "üß©", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –ø–∏—Å—å–º–µ–Ω–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é", "desc": "OmniOCR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#dataset", "#rl"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "TextPecker ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
[25.02.2026 22:25] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üö¢", "ru": {"title": "Generative AI —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –≤ –ø–æ—Ä—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ, –æ–±—ä–µ–¥–∏–Ω—è—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π AI
[25.02.2026 22:25] Renaming data file.
[25.02.2026 22:25] Renaming previous data. hf_papers.json to ./d/2026-02-25.json
[25.02.2026 22:25] Saving new data file.
[25.02.2026 22:25] Generating page.
[25.02.2026 22:25] Renaming previous page.
[25.02.2026 22:25] Renaming previous data. index.html to ./d/2026-02-25.html
[25.02.2026 22:25] Writing result.
[25.02.2026 22:25] Renaming log file.
[25.02.2026 22:25] Renaming previous data. log.txt to ./logs/2026-02-25_last_log.txt
