[25.02.2026 19:00] Read previous papers.
[25.02.2026 19:00] Generating top page (month).
[25.02.2026 19:00] Writing top page (month).
[25.02.2026 19:46] Read previous papers.
[25.02.2026 19:46] Get feed.
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21193
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12192
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20739
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21015
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21204
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21202
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14337
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20951
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18940
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16990
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20309
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21198
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21185
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20731
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19633
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16813
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16745
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21201
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21196
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20424
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20945
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20792
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20743
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18998
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18735
[25.02.2026 19:46] Extract page data from URL. URL: https://huggingface.co/papers/2602.16932
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16603
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21053
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21042
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20903
[25.02.2026 19:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20540
[25.02.2026 19:46] Extract page data from URL. URL: https://huggingface.co/papers/2602.19020
[25.02.2026 19:46] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.02.2026 19:46] No deleted papers detected.
[25.02.2026 19:46] Downloading and parsing papers (pdf, html). Total: 32.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21193.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21193.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21193.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.12192.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.12192.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.12192.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20739.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20739.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20739.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21015.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21015.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21015.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21204.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21204.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21204.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21202.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21202.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21202.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.14337.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.14337.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.14337.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20951.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20951.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20951.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.18940.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.18940.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.18940.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.16990.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.16990.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.16990.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20309.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20309.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20309.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21198.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21198.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21198.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21185.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21185.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21185.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20731.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20731.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20731.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.19633.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.19633.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.19633.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.16813.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.16813.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.16813.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.16745.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.16745.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.16745.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21201.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21201.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21201.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21196.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21196.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21196.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20424.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20424.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20424.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20945.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20945.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20945.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20792.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20792.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20792.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20743.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20743.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20743.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.18998.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.18998.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.18998.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.18735.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.18735.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.18735.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.16932.
[25.02.2026 19:46] Downloading paper 2602.16932 from https://arxiv.org/pdf/2602.16932v1...
[25.02.2026 19:46] Extracting affiliations from text.
[25.02.2026 19:46] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 1 ] . [ 1 2 3 9 6 1 . 2 0 6 2 : r RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution Jinming Nian Santa Clara University Santa Clara, CA, USA jnian@scu.edu Dae Hoon Park Walmart Global Tech Sunnyvale, CA, USA dae.hoon.park@walmart.com Fangchen Li Independent Researcher Bothell, WA, USA fangchen.li@outlook.com Yi Fang Santa Clara University Santa Clara, CA, USA yfang@scu.edu Abstract Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is practical path towards automatic discovery of novel ranking algorithms. Code is available here. CCS Concepts Information systems Retrieval models and ranking. Keywords Lexical Retrieval, Evolutionary Search, LLM-as-optimizer, Automated Algorithm Discovery 1BEIR: ArguAna, FiQA, NFCorpus, SciFact, SciDocs, TREC-COVID. BRIGHT: Biology, Earth Science, Economics, Pony, StackOverflow, TheoremQA. Figure 1: Combined score over evolution steps for two seed programs. The combined score is the optimization target, defined as 0.8Avg Recall@100+0.2Avg nDCG@10, averaged across 12 IR datasets1. represent candidate solutions as executable code"
[25.02.2026 19:46] Response: ```python
[
    "Santa Clara University",
    "Walmart Global Tech",
    "Independent Researcher"
]
```
[25.02.2026 19:46] Deleting PDF ./assets/pdf/2602.16932.pdf.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.16603.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.16603.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.16603.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21053.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21053.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21053.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.21042.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.21042.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.21042.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20903.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20903.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20903.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.20540.
[25.02.2026 19:46] Extra JSON file exists (./assets/json/2602.20540.json), skip PDF parsing.
[25.02.2026 19:46] Paper image links file exists (./assets/img_data/2602.20540.json), skip HTML parsing.
[25.02.2026 19:46] Success.
[25.02.2026 19:46] Downloading and parsing paper https://huggingface.co/papers/2602.19020.
[25.02.2026 19:46] Downloading paper 2602.19020 from https://arxiv.org/pdf/2602.19020v1...
[25.02.2026 19:46] Extracting affiliations from text.
[25.02.2026 19:46] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Junjie Oscar Yin 1 John X. Morris 2 Vitaly Shmatikov 2 Sewon Min 3 4 Hannaneh Hajishirzi 1 4 6 2 0 2 2 2 ] . [ 1 0 2 0 9 1 . 2 0 6 2 : r a "
[25.02.2026 19:46] Response: ```python
[]
```
[25.02.2026 19:46] Extracting affiliations from text.
[25.02.2026 19:46] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Junjie Oscar Yin 1 John X. Morris 2 Vitaly Shmatikov 2 Sewon Min 3 4 Hannaneh Hajishirzi 1 4 6 2 0 2 2 2 ] . [ 1 0 2 0 9 1 . 2 0 6 2 : r aDetecting LLM training data is generally framed as membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), family of MIA that actively induces model to reconstruct given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, posttraining, and distillation data, with an average improvement of 10.7% over the previous runner-up. In particular, ADRA+ improves over Min-K%++ by 18.8% on BookMIA for pre-training detection and by 7.6% on AIME for post-training detection. 1. Introduction Modern language models are known to memorize and regurgitate training data, raising concerns about copyright, privacy, and data contamination (Carlini et al., 2021; Ahmed et al., 2026). Membership inference attacks (MIAs) are methods that detect whether given input was present in the training data (Shokri et al., 2017), and have shown some success when applied to LLMs (Shi et al., 2023). 1University of Washington 2Cornell University 3UC Berkeley 4Allen Institute for Artificial Intelligence. Correspondence to: Junjie Oscar Yin <osey@cs.washington.edu>. Preprint. February 24, 2026. Figure 1. Active Data Reconstruction Attack. Language model generates reconstructions from candidate prefix and is rewarded via contrastive objective. Members become easier to reconstruct than non-members over RL training, improving MIA performance. Prior MIAs query the target model without modifying its weights, passively extracting signals exposed through the output layersuch as loss (Shokri et al., 2017; Yeom et al., 2018; Carlini et al., 2021), log-probabilities (Shi et al., 2023; Zhang et al., 2024b), or text generations (Hallinan et al., 2025). We hypothesize that model weights encode latent membership signal that passive methods fail to reliably expose. If such signal exists, it could be actively elicited through model weight updates. We introduce Active Data Reconstruction Attack (ADRA), family of MIAs that uses Reinforcement Learning (RL) with reconstruction rewards to elicit latent membership signals. Because model weights encode traces of training data, RL training exploits the difference in member and non-member reconstructability for detection. Our approach is motivated by recent findings that RL sharpens behaviors already encoded in weights (Yue et al., 2025; Wang et al., 2025) while transferring minimal new information (Schulman & Lab, 2025), suggesting RL is well-suited to surface latent reconstruction signal. Learning to Detect Language Model Training Data via Active Reconstruction Figure 1 illustrates our attack: given candidate prefix, the model generates continuations (reconstructions) and is rewarded for producing generations similar to the candidate suffix. To effectively leverage RL for MIA, we design reconstruction metrics and contrastive rewards that match generations against pool containing the true suffix and negative distractors. This yields two algorithms: ADRA, which rewards the best match among the pool, and ADRA+, which adaptively adjusts how often the true suffix should be included during matching based on prior derived from loss-based scores. Empirically, we run comprehensive experiments spanning pre-training, post-training, and distillation settings across 6 open-weight LLMs. To better match frontier models knowledge cutoffs and heavier post-training, we construct 6 new MIA datasets and evaluate on 2 established benchmarks. We find that ADRA and ADRA+ consistently outperform prior MIAs across all settings, with an average improvement of 10.7% over the previous runner-up. For pre-training data detection, on the challenging WikiMIA2024 Hard benchmarkwhere most baselines hover below randomADRA+ reaches 60.6% AUROC, outperforming Min-K%++ by 10%. For post-training, under controlled contamination on AIME, ADRA+ achieves 85.9% AUROC, improving over N-Sampling by 13.2% and Min-K%++ by 7.6%. For distillation, on prefix-matched distillation traces, ADRA attains near-perfect membership inference, reaching 98.4% under Deepseek-R1 distillation. Ablations confirm that RL-based optimization, reconstruction-reward design, and the contrastive objective are critical to these gains. Our results suggest that model weights encode more about training data than fixed outputs reveal. RL training help surface this latent information, enabling stronger reconstruction and membership inference. Codebase, models, and datasets are available at https://github.com/oseyosey/MIA-RL. 2. Background Membership inference (Shokri et al., 2017) aims to determine if datapoint was present in the training data of model Œ∏. We focus on the specific problem of detecting training data from LLMs, which is known to be difficult (Duan et al., 2024). Notation. Let pŒ∏ be the target LLM with parameters Œ∏, trained on dataset Dtrain. Given candidate document Dcand, write = (c, y) where is the observed prefix and is the held-out suffix. Membership inference is the binary classification problem of determining whether was drawn from Dtrain or not. Define scoring function (x; Œ∏) that maps each candidate to scalar membership score; MIAs predict member iff (x; Œ∏) œµ for threshold œµ. Loss-based Methods. Most prior MIAs score candidate by the models token-level log-probabilities on x, with the intuition that models are likely to have lower loss on sequences that have been seen during training: Loss (Yeom et al., 2018): (x) = L(x; Œ∏). R-Loss (Carlini et al., 2021): calibrate by reference model Œ∏ref , = L(x; Œ∏) L(x; Œ∏ref ). Zlib Entropy (Carlini et al., 2021): normalize by the zlib compress"
[25.02.2026 19:47] Mistral response. {"id": "6152fca69e584e14ad7868d9e126c59e", "created": 1772048819, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1579, "total_tokens": 1611, "completion_tokens": 32, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of Washington\",\n    \"Cornell University\",\n    \"UC Berkeley\",\n    \"Allen Institute for Artificial Intelligence\"\n]\n```"}}]}
[25.02.2026 19:47] Response: ```python
[
    "University of Washington",
    "Cornell University",
    "UC Berkeley",
    "Allen Institute for Artificial Intelligence"
]
```
[25.02.2026 19:47] Deleting PDF ./assets/pdf/2602.19020.pdf.
[25.02.2026 19:47] Success.
[25.02.2026 19:47] Enriching papers with extra data.
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 0. Abstract Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.  					AI-generated summary Despite rapid recent progress in the te...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 1. Abstract A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks.  					AI-generated summary Built upon the existing analysis of retrieval heads in large language models, we p...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 2. Abstract PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.  					AI-generated summary Reinforcement learning for agentic multimodal models often suffers from interaction collapse, wh...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 3. Abstract Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.  					AI-generated summary Understanding...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 4. Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  					AI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of onlin...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 5. Abstract Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.  					AI-generated summary We study efficient ...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 6. Abstract LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.  					AI-generated summary Recent advances in AI-assisted programming have empowered agents to ...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 7. Abstract ArtiAgent automates the creation of real-artifact image pairs using three agents for perception, synthesis, and curation in diffusion transformers.  					AI-generated summary Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromis...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 8. Abstract Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 9. Abstract A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.  					AI-generated summary Most recomm...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 10. Abstract QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency.  					AI-generated ...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 11. Abstract Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.  					AI-generated summary Embodied LLMs endow robots with high-level task reasoning, but they cannot ...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 12. Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  					AI-generated summary Uniform-state discrete diffusion models excel at few-step...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 13. Abstract COMiT framework learns structured discrete visual tokens through iterative encoding and flow-matching decoding, improving object-centric representation and compositional generalization.  					AI-generated summary Discrete image tokenizers have emerged as a key component of modern vision and...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 14. Abstract TAPE framework improves language model agent performance in complex environments through enhanced planning and constrained execution strategies.  					AI-generated summary Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interaction...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 15. Abstract Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.  					AI-generated summary Language models based on discrete diffusion have a...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 16. Abstract Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.  					AI-generated summary Test-time scaling can improve model performan...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 17. Abstract  We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to ...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 18. Abstract UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed.  					AI-generated summary Efficiently processing long sequences with Transf...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 19. Abstract AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.  					AI-generated summary Real-world requests to AI agents are fundamen...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 20. Abstract Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.  					AI-generated summary Large Language Models (LLMs) consistently benefit from scaled Chai...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 21. Abstract A biomechanics-aware keypoint simulation framework and the first open dataset, SIMSPINE, provide anatomically consistent 3D spinal annotations for natural full-body motions, enabling data-driven learning of vertebral kinematics and improving spine motion estimation accuracy.  					AI-genera...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 22. Abstract Adaptive text anonymization framework automatically adjusts anonymization strategies based on privacy-utility requirements using prompt optimization for language models across diverse domains and constraints.  					AI-generated summary Anonymizing textual documents is a highly context-sensi...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 23. Abstract General AgentBench evaluates large language model agents across multiple domains and scaling methods, revealing performance degradation and fundamental limitations in sequential and parallel scaling approaches.  					AI-generated summary LLM agents are increasingly expected to function as g...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 24. Abstract LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement.  					AI-generated summary This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages th...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 25. Abstract Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.  					AI-generated summary Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 26. Abstract FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.  					AI-generated summary The...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 27. Abstract A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.  					AI-generated summary Large Vision-Language Models (VLMs)...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 28. Abstract OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings.  					AI-generated summary Optical character recognition (OCR) has advanced rap...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 29. Abstract TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity.  					AI-generated summary Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 30. Abstract A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals.  					AI-generated summary Import container dwell...
[25.02.2026 19:47] ********************************************************************************
[25.02.2026 19:47] Abstract 31. Abstract Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks.  					AI-generated summary Detecting LLM training data is generally framed as a membe...
[25.02.2026 19:47] Read previous papers.
[25.02.2026 19:47] Generating reviews via LLM API.
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#open_source", "#agents", "#benchmark", "#long_context", "#training", "#synthetic", "#dataset", "#data"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–æ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö: –∫–∞–∫ –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –∫–æ–º–∞–Ω–¥–æ–≤–∞—Ç—å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#small_models", "#training", "#rag", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–õ—ë–≥–∫–æ–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏–∑ –≤—ã–±—Ä–∞–Ω
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#open_source", "#agents", "#multimodal", "#rl", "#reasoning", "#video", "#training", "#optimization"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è PyVision-RL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è 
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#robotics", "#benchmark", "#3d", "#cv"], "emoji": "üß©", "ru": {"title": "–§–∏–∑–∏–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ CHAIN –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –ø–æ–Ω–∏–º–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–∏
[25.02.2026 19:47] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "Test-time –æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ test-time training (–æ–±—É—á–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è) –∫–∞–∫ –∏–∑—É—á–µ–Ω–Ω–æ–µ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∞ –Ω–µ –∫–∞–∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —à–∏—Ä–æ–∫–∏–π –∫–ª–∞—Å—Å TTT
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#inference", "#open_source", "#benchmark", "#multimodal", "#rag", "#optimization"], "emoji": "üóúÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#agents", "#plp", "#long_context", "#benchmark"], "emoji": "üõ†Ô∏è", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –¥–ª–∏–Ω–Ω–æ–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LongCLI-Bench ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#open_source", "#synthetic", "#dataset", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω ArtiAgent ‚Äî —Å–∏—Å—Ç–µ–º–∞ –∏–∑ —Ç—Ä—ë—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º
[25.02.2026 19:47] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å—Ç–∏–Ω—ã –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç—á—ë—Ç–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö Deep Research Agents. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –±–µ–≥–ª–æ—Å—Ç–∏ —Ç–µ
[25.02.2026 19:47] Using data from previous issue: {"categories": [], "emoji": "üìà", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Conv-FinRe ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–∫—Ü–∏—è–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#open_source", "#robotics", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "QuantVLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ—Å—Ç–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π vision-language-action, –∫–æ—Ç–æ
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#agents", "#robotics", "#dataset", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–∞—à–∏–Ω—É —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Reflective Test-Time Planning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π —Ä–æ–±–æ—Ç–∞ –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#open_source"], "emoji": "üîÑ", "ru": {"title": "Predictor-Corrector —Å—ç–º–ø–ª–µ—Ä—ã –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã Predictor-Corrector –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#reasoning", "#architecture", "#cv", "#training"], "emoji": "üß©", "ru": {"title": "–ö–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—ä–µ–∫—Ç-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "COMiT - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ç–æ
[25.02.2026 19:47] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ TAPE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö —Å –∂–µ—Å—Ç–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ—Å—Ç—å —Ä–µ—à–µ
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#open_source", "#training", "#optimization"], "emoji": "üåä", "ru": {"title": "–ü–æ—Ç–æ–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–¥–∏–ª–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å —è–∑—ã–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ç–æ–∫–æ–≤ (FLM), –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –¥–µ–Ω–æ
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PETS ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç
[25.02.2026 19:47] Using data from previous issue: {"categories": [], "emoji": "üßÆ", "ru": {"title": "–ú–∞—à–∏–Ω—ã –º–æ–≥—É—Ç –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–µ–æ—Ä–µ–º—ã: –∞–≥–µ–Ω—Ç Aletheia —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ Aletheia, —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Gemini 3 Deep Think, –Ω–∞ –∫–æ–Ω–∫—É—Ä—Å–µ FirstProof. –ê–≥–µ–Ω—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Ä–µ—à–∏–ª 6 –∏–∑ 10
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#inference", "#long_context", "#architecture", "#training", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "UPipe ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#alignment", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é: –æ—Ü–µ–Ω–∫–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#rl", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ RL", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) —á–µ—Ä–µ–∑ –æ–±
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#3d", "#cv"], "emoji": "ü¶¥", "ru": {"title": "–ê–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è 3D –æ—Ü–µ–Ω–∫–∞ –¥–≤–∏–∂–µ–Ω–∏–π –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–∞ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–∞ —Å —É—á—ë—Ç–æ–º –±–∏–æ–º–µ—Ö–∞–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤
[25.02.2026 19:47] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#benchmark", "#plp", "#agents", "#open_source", "#reasoning"], "emoji": "üß©", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –Ω–∞ LLM: –∫–æ–≥–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω General AgentBench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö agent–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#3d"], "emoji": "üß©", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "LaS-Comp –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ 3D –º–æ–¥–µ–ª–∏ —Å –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º.
[25.02.2026 19:47] Querying the API.
[25.02.2026 19:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.  					AI-generated summary Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.
[25.02.2026 19:47] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ RankEvolve ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –ø–æ–º–æ—â—å—é LLM, —É–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö –æ—Ü–µ–Ω–∏–≤–∞—Ç–µ–ª–µ–º –∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º. –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ —Ä–æ–ª—å –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–∏–¥–µ –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ –∫–æ–¥–∞ –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –º—É—Ç–∏—Ä—É—é—Ç, —Ä–µ–∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –∏ –æ—Ç–±–∏—Ä–∞—é—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ –Ω–∞ 12 –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –ù–∞—á–∏–Ω–∞—è —Å –±–∞–∑–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ BM25 –∏ query likelihood —Å –¥–∏—Ä–∏–∫–ª–µ–µ–≤—Å–∫–∏–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º, —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ–±–µ—â–∞—é—â—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö BEIR, BRIGHT –∏ TREC DL, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.",
  "emoji": "üß¨",
  "title": "–≠–≤–æ–ª—é—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
}
```
[25.02.2026 19:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.  					AI-generated summary Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms."

[25.02.2026 19:47] Response: ```python
["BENCHMARK", "PLP", "DATASET"]
```
[25.02.2026 19:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.  					AI-generated summary Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms."

[25.02.2026 19:47] Response: ```python
['OPTIMIZATION', 'TRANSFER_LEARNING']
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on automatically discovering improved lexical retrieval algorithms through evolutionary search and program evolution techniques, which is fundamentally about optimizing ranking algorithms.

- **TRANSFER_LEARNING**: The paper explicitly demonstrates that "evolved algorithms...show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20," indicating knowledge transfer across different datasets and domains.
[25.02.2026 19:47] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "TRANSFER_LEARNING"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on automatically discovering improved lexical retrieval algorithms through evolutionary search and program evolution techniques, which is fundamentally about optimizing ranking algorithms.

- **TRANSFER_LEARNING**: The paper explicitly demonstrates that "evolved algorithms...show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20," indicating knowledge transfer across different datasets and domains.
[25.02.2026 19:47] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents RankEvolve, a method that uses large language models (LLMs) and evolutionary search to create better retrieval algorithms for information retrieval tasks. Instead of relying solely on human intuition and parameter tuning, RankEvolve automatically evolves algorithms by mutating and recombining existing ones, starting with BM25 and query likelihood with Dirichlet smoothing. The process is guided by an evaluator that assesses the performance of these algorithms across multiple datasets. The results indicate that this approach can lead to the discovery of novel and effective ranking algorithms that perform well on established benchmarks.","title":"Automating Algorithm Discovery with Evolutionary Search"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents RankEvolve, a method that uses large language models (LLMs) and evolutionary search to create better retrieval algorithms for information retrieval tasks. Instead of relying solely on human intuition and parameter tuning, RankEvolve automatically evolves algorithms by mutating and recombining existing ones, starting with BM25 and query likelihood with Dirichlet smoothing. The process is guided by an evaluator that assesses the performance of these algorithms across multiple datasets. The results indicate that this approach can lead to the discovery of novel and effective ranking algorithms that perform well on established benchmarks.', title='Automating Algorithm Discovery with Evolutionary Search'))
[25.02.2026 19:47] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåËøõÂåñÊêúÁ¥¢ÊäÄÊúØÔºåËá™Âä®ÂèëÁé∞ÊîπËøõÁöÑËØçÊ±áÊ£ÄÁ¥¢ÁÆóÊ≥ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜRankEvolveÔºåËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éAlphaEvolveÁöÑÁ®ãÂ∫èËøõÂåñÊ°ÜÊû∂ÔºåËÉΩÂ§üÈÄöËøáÂØπÂÄôÈÄâÊéíÂêçÁÆóÊ≥ïËøõË°åÂèòÂºÇ„ÄÅÈáçÁªÑÂíåÈÄâÊã©ÔºåÊù•‰ºòÂåñÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇRankEvolve‰ªéBM25ÂíåÂ∏¶ÊúâDirichletÂπ≥ÊªëÁöÑÊü•ËØ¢‰ººÁÑ∂Ëøô‰∏§‰∏™Âü∫Á°ÄÁ®ãÂ∫èÂºÄÂßãÔºåÁªèËøáËøõÂåñÁîüÊàêÁöÑÊñ∞ÁÆóÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåËØÑ‰º∞ËÄÖÂºïÂØºÁöÑLLMÁ®ãÂ∫èËøõÂåñÊòØËá™Âä®ÂèëÁé∞Êñ∞ÊéíÂêçÁÆóÊ≥ïÁöÑÊúâÊïàÈÄîÂæÑ„ÄÇ","title":"Ëá™Âä®ÂèëÁé∞Êñ∞ÊéíÂêçÁÆóÊ≥ïÁöÑËøõÂåñ‰πãË∑Ø"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåËøõÂåñÊêúÁ¥¢ÊäÄÊúØÔºåËá™Âä®ÂèëÁé∞ÊîπËøõÁöÑËØçÊ±áÊ£ÄÁ¥¢ÁÆóÊ≥ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜRankEvolveÔºåËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éAlphaEvolveÁöÑÁ®ãÂ∫èËøõÂåñÊ°ÜÊû∂ÔºåËÉΩÂ§üÈÄöËøáÂØπÂÄôÈÄâÊéíÂêçÁÆóÊ≥ïËøõË°åÂèòÂºÇ„ÄÅÈáçÁªÑÂíåÈÄâÊã©ÔºåÊù•‰ºòÂåñÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇRankEvolve‰ªéBM25ÂíåÂ∏¶ÊúâDirichletÂπ≥ÊªëÁöÑÊü•ËØ¢‰ººÁÑ∂Ëøô‰∏§‰∏™Âü∫Á°ÄÁ®ãÂ∫èÂºÄÂßãÔºåÁªèËøáËøõÂåñÁîüÊàêÁöÑÊñ∞ÁÆóÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåËØÑ‰º∞ËÄÖÂºïÂØºÁöÑLLMÁ®ãÂ∫èËøõÂåñÊòØËá™Âä®ÂèëÁé∞Êñ∞ÊéíÂêçÁÆóÊ≥ïÁöÑÊúâÊïàÈÄîÂæÑ„ÄÇ', title='Ëá™Âä®ÂèëÁé∞Êñ∞ÊéíÂêçÁÆóÊ≥ïÁöÑËøõÂåñ‰πãË∑Ø'))
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–†–∞—Å—Ü–µ–ø–ª–µ–Ω–∏—é –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –æ—Ç —á–∞—Å—Ç–æ—Ç—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äî –ø–æ–±–µ–¥–∞ –Ω–∞–¥ –æ—á–µ—Ä–µ–¥–Ω–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π", "desc": "FlowPrefill ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –æ—á–µ—Ä–µ–¥–∏ –∑–∞
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#multimodal", "#interpretability", "#open_source", "#benchmark", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#multilingual", "#small_models", "#transfer_learning", "#low_resource", "#optimization", "#open_source", "#cv", "#training"], "emoji": "üß©", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –ø–∏—Å—å–º–µ–Ω–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é", "desc": "OmniOCR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#dataset", "#rl"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "TextPecker ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
[25.02.2026 19:47] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üö¢", "ru": {"title": "Generative AI —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –≤ –ø–æ—Ä—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ, –æ–±—ä–µ–¥–∏–Ω—è—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π AI
[25.02.2026 19:47] Querying the API.
[25.02.2026 19:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks.  					AI-generated summary Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\% over the previous runner-up. In particular, \MethodPlus~improves over Min-K\%++ by 18.8\% on BookMIA for pre-training detection and by 7.6\% on AIME for post-training detection.
[25.02.2026 19:47] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—è–≤–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Active Data Reconstruction Attack (ADRA), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reinforcement learning –≤–º–µ—Å—Ç–æ –ø–∞—Å—Å–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –ê–≤—Ç–æ—Ä—ã –≥–∏–ø–æ—Ç–µ–∑–∏—Ä—É—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –ª—É—á—à–µ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª—å—é, —á–µ–º –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∞ –Ω–µ –≤–∏–¥–µ–ª–∞, –∏ —ç—Ç–∞ —Ä–∞–∑–Ω–∏—Ü–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è membership inference. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç on-policy RL –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–±—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ fine-tuning —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ADRA –∏ –µ–≥–æ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ADRA+ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å —É–ª—É—á—à–µ–Ω–∏–µ–º –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 10.7%.",
  "emoji": "üîç",
  "title": "–ê–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —É—Å–∏–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
}
```
[25.02.2026 19:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks.  					AI-generated summary Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\% over the previous runner-up. In particular, \MethodPlus~improves over Min-K\%++ by 18.8\% on BookMIA for pre-training detection and by 7.6\% on AIME for post-training detection."

[25.02.2026 19:47] Response: ```python
["RL", "TRAINING"]
```
[25.02.2026 19:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks.  					AI-generated summary Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\% over the previous runner-up. In particular, \MethodPlus~improves over Min-K\%++ by 18.8\% on BookMIA for pre-training detection and by 7.6\% on AIME for post-training detection."

[25.02.2026 19:47] Response: ```python
['SECURITY', 'LEAKAGE']
```
[25.02.2026 19:47] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"The Active Data Reconstruction Attack (ADRA) is a novel approach to membership inference attacks (MIA) that utilizes reinforcement learning to identify training data by assessing how well a model can reconstruct specific texts. Unlike traditional MIAs that passively analyze fixed model outputs, ADRA actively engages the model to enhance its ability to reconstruct training data, leveraging the hypothesis that training data is more easily reconstructible than non-training data. By employing on-policy reinforcement learning, ADRA fine-tunes a policy to optimize data reconstruction, using specially designed metrics and rewards to improve detection accuracy. Experimental results demonstrate that ADRA and its adaptive variant ADRA+ significantly outperform existing methods, achieving an average improvement of 10.7% in detecting various types of training data.","title":"Reinforcement Learning for Enhanced Membership Inference"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The Active Data Reconstruction Attack (ADRA) is a novel approach to membership inference attacks (MIA) that utilizes reinforcement learning to identify training data by assessing how well a model can reconstruct specific texts. Unlike traditional MIAs that passively analyze fixed model outputs, ADRA actively engages the model to enhance its ability to reconstruct training data, leveraging the hypothesis that training data is more easily reconstructible than non-training data. By employing on-policy reinforcement learning, ADRA fine-tunes a policy to optimize data reconstruction, using specially designed metrics and rewards to improve detection accuracy. Experimental results demonstrate that ADRA and its adaptive variant ADRA+ significantly outperform existing methods, achieving an average improvement of 10.7% in detecting various types of training data.', title='Reinforcement Learning for Enhanced Membership Inference'))
[25.02.2026 19:47] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"‰∏ªÂä®Êï∞ÊçÆÈáçÂª∫ÊîªÂáªÔºàADRAÔºâÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Êù•ËØÜÂà´ËÆ≠ÁªÉÊï∞ÊçÆÔºåÈÄöËøáÊµãÈáèÊ®°ÂûãË°å‰∏∫ÁöÑÈáçÂª∫ËÉΩÂäõÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊàêÂëòÊé®Êñ≠ÊîªÂáª„ÄÇ‰∏é‰º†ÁªüÁöÑË¢´Âä®ÊàêÂëòÊé®Êñ≠ÊîªÂáª‰∏çÂêåÔºåADRA‰∏ªÂä®ËØ±ÂØºÊ®°ÂûãÈáçÂª∫ÁªôÂÆöÊñáÊú¨„ÄÇÊàë‰ª¨ÂÅáËÆæËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈáçÂª∫ËÉΩÂäõÈ´ò‰∫éÈùûÊàêÂëòÊï∞ÊçÆÔºåËøô‰∏ÄÂ∑ÆÂºÇÂèØ‰ª•Áî®‰∫éÊàêÂëòÊé®Êñ≠„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåADRAÂèäÂÖ∂Ëá™ÈÄÇÂ∫îÂèò‰ΩìÂú®Ê£ÄÊµãÈ¢ÑËÆ≠ÁªÉ„ÄÅÂêéËÆ≠ÁªÉÂíåËí∏È¶èÊï∞ÊçÆÊñπÈù¢Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü10.7%„ÄÇ","title":"‰∏ªÂä®ÈáçÂª∫ÔºåÁ≤æÂáÜÊé®Êñ≠ÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='‰∏ªÂä®Êï∞ÊçÆÈáçÂª∫ÊîªÂáªÔºàADRAÔºâÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Êù•ËØÜÂà´ËÆ≠ÁªÉÊï∞ÊçÆÔºåÈÄöËøáÊµãÈáèÊ®°ÂûãË°å‰∏∫ÁöÑÈáçÂª∫ËÉΩÂäõÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊàêÂëòÊé®Êñ≠ÊîªÂáª„ÄÇ‰∏é‰º†ÁªüÁöÑË¢´Âä®ÊàêÂëòÊé®Êñ≠ÊîªÂáª‰∏çÂêåÔºåADRA‰∏ªÂä®ËØ±ÂØºÊ®°ÂûãÈáçÂª∫ÁªôÂÆöÊñáÊú¨„ÄÇÊàë‰ª¨ÂÅáËÆæËÆ≠ÁªÉÊï∞ÊçÆÁöÑÈáçÂª∫ËÉΩÂäõÈ´ò‰∫éÈùûÊàêÂëòÊï∞ÊçÆÔºåËøô‰∏ÄÂ∑ÆÂºÇÂèØ‰ª•Áî®‰∫éÊàêÂëòÊé®Êñ≠„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåADRAÂèäÂÖ∂Ëá™ÈÄÇÂ∫îÂèò‰ΩìÂú®Ê£ÄÊµãÈ¢ÑËÆ≠ÁªÉ„ÄÅÂêéËÆ≠ÁªÉÂíåËí∏È¶èÊï∞ÊçÆÊñπÈù¢Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü10.7%„ÄÇ', title='‰∏ªÂä®ÈáçÂª∫ÔºåÁ≤æÂáÜÊé®Êñ≠ÔºÅ'))
[25.02.2026 19:47] Renaming data file.
[25.02.2026 19:47] Renaming previous data. hf_papers.json to ./d/2026-02-25.json
[25.02.2026 19:47] Saving new data file.
[25.02.2026 19:47] Generating page.
[25.02.2026 19:47] Renaming previous page.
[25.02.2026 19:47] Renaming previous data. index.html to ./d/2026-02-25.html
[25.02.2026 19:47] Writing result.
[25.02.2026 19:47] Renaming log file.
[25.02.2026 19:47] Renaming previous data. log.txt to ./logs/2026-02-25_last_log.txt
