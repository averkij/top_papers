[28.05.2025 05:13] Read previous papers.
[28.05.2025 05:13] Generating top page (month).
[28.05.2025 05:13] Writing top page (month).
[28.05.2025 06:17] Read previous papers.
[28.05.2025 06:17] Get feed.
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.21505
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.21334
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.20561
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.19973
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.16901
[28.05.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.16673
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.19377
[28.05.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 06:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 06:17] No deleted papers detected.
[28.05.2025 06:17] Downloading and parsing papers (pdf, html). Total: 34.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21505.
[28.05.2025 06:17] Downloading paper 2505.21505 from http://arxiv.org/pdf/2505.21505v1...
[28.05.2025 06:17] Extracting affiliations from text.
[28.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 5 0 5 1 2 . 5 0 5 2 : r How does Alignment Enhance LLMs Multilingual Capabilities? Language Neurons Perspective Shimao Zhang1* Zhejian Lai1* Xiang Liu1* Shuaijie She1 Xiao Liu2 Yeyun Gong2 Shujian Huang1 Jiajun Chen1 1 National Key Laboratory for Novel Software Technology, Nanjing University 2 Microsoft Research Asia {smzhang,laizj,liuxiang,shesj}@smail.nju.edu.cn {xiao.liu.msrasia,yegong}@microsoft.com {huangsj,chenjj}@nju.edu.cn "
[28.05.2025 06:17] Response: ```python
[
    "National Key Laboratory for Novel Software Technology, Nanjing University",
    "Microsoft Research Asia"
]
```
[28.05.2025 06:17] Deleting PDF ./assets/pdf/2505.21505.pdf.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21496.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21496.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21500.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21500.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21334.
[28.05.2025 06:17] Downloading paper 2505.21334 from http://arxiv.org/pdf/2505.21334v1...
[28.05.2025 06:17] Extracting affiliations from text.
[28.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 4 3 3 1 2 . 5 0 5 2 : r HoliTom : Holistic Token Merging for Fast Video Large Language Models Kele Shao1,2, Keda Tao2, Can Qin3, Haoxuan You4, Yang Sui5, Huan Wang2, 1Zhejiang University 2Westlake University 3Salesforce AI Research 4Columbia University 5Rice University https://github.com/cokeshao/HoliTom Figure 1: Left: We introduce HoliTom, training-free holistic token merge method for fast video LLMs. Its key innovation lies in its global, redundancy-aware outer-LLM spatio-temporal compression and robust, token similarity-based inner-LLM compression. Right: The Efficiency/Performance trade-off curve of multiple training-free methods on four widely used video understanding benchmarks: MVBench, EgoSchema, LongVideoBench, and VideoMME. Our method, HoliTom, surpasses the SoTA approaches by maintaining 99.1% average performance while reducing FLOPs to 6.9%. "
[28.05.2025 06:17] Response: ```python
["Zhejiang University", "Westlake University", "Salesforce AI Research", "Columbia University", "Rice University"]
```
[28.05.2025 06:17] Deleting PDF ./assets/pdf/2505.21334.pdf.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.20561.
[28.05.2025 06:17] Downloading paper 2505.20561 from http://arxiv.org/pdf/2505.20561v1...
[28.05.2025 06:17] Extracting affiliations from text.
[28.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-5-28 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Shenao Zhang1, Yaqing Wang2, Yinxiao Liu2, Tianqi Liu2, Peter Grabowski3, Eugene Ie3, Zhaoran Wang1, Yunxuan Li3 1Northwestern University, 2Google DeepMind, 3Google 5 2 0 2 6 2 ] . [ 1 1 6 5 0 2 . 5 0 5 2 : r Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL. 1. Introduction Large Language Models (LLMs) have demonstrated impressive reasoning abilities, such as in solving complex math problems. key factor driving this progress is the use of Chain-of-Thought (CoT) reasoning [50], where the model engages in intermediate deliberation before producing an answer. Building on this, recent advances have employed"
[28.05.2025 06:17] Response: ```python
["Northwestern University", "Google DeepMind", "Google"]
```
[28.05.2025 06:17] Deleting PDF ./assets/pdf/2505.20561.pdf.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.19973.
[28.05.2025 06:17] Downloading paper 2505.19973 from http://arxiv.org/pdf/2505.19973v1...
[28.05.2025 06:17] Extracting affiliations from text.
[28.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 3 7 9 9 1 . 5 0 5 2 : r DFIR-Metric: Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response Bilel Cherif1 , Tamas Bisztray2 , Richard A. Dubniczky3 , Aaesha Aldahmani1 , Saeed Alshehhi1 , and Norbert Tihanyi1,3((cid:66)) 1 Technology Innovation Institute, Abu Dhabi, UAE {bilel.cherif,saeed.alshehhi,aaesha.aldahmani,norbert.tihanyi}@tii.ae 2 University of Oslo, Oslo, Norway tamasbi@ifi.uio.no 3 Eötvös Loránd University, Budapest, Hungary {dubniczky,ntihanyi}@inf.elte.hu Abstract. Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, benchmark with three components: (1) Knowledge Assessment: set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric. Keywords: Digital Forensics Incident Response LLM Benchmarking Sinc"
[28.05.2025 06:17] Response: ```python
[
    "Technology Innovation Institute, Abu Dhabi, UAE",
    "University of Oslo, Oslo, Norway",
    "Eötvös Loránd University, Budapest, Hungary"
]
```
[28.05.2025 06:17] Deleting PDF ./assets/pdf/2505.19973.pdf.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.16901.
[28.05.2025 06:17] Downloading paper 2505.16901 from http://arxiv.org/pdf/2505.16901v1...
[28.05.2025 06:17] Extracting affiliations from text.
[28.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 1 0 9 6 1 . 5 0 5 2 : r CODE GRAPH MODEL (CGM): GRAPH-INTEGRATED LARGE LANGUAGE MODEL FOR REPOSITORY-LEVEL SOFTWARE ENGINEERING TASKS Hongyuan Tao *1, Ying Zhang *1,2, Zhenhao Tang *1, Hongen Peng1, Xukun Zhu1,3, Bingchang Liu1, Yingguang Yang1, Ziyin Zhang1,4, Zhaogui Xu1, Haipeng Zhang2, Linchao Zhu3, Rui Wang4, Hang Yu 1, Jianguo Li 1, Peng Di 1 1Ant Group, Hangzhou, China 2ShanghaiTech University, Shanghai, China 3Zhejiang University, Hangzhou, China 4Shanghai Jiaotong University, Shanghai, China "
[28.05.2025 06:17] Response: ```python
[
    "Ant Group, Hangzhou, China",
    "ShanghaiTech University, Shanghai, China",
    "Zhejiang University, Hangzhou, China",
    "Shanghai Jiaotong University, Shanghai, China"
]
```
[28.05.2025 06:17] Deleting PDF ./assets/pdf/2505.16901.pdf.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.16673.
[28.05.2025 06:17] Downloading paper 2505.16673 from http://arxiv.org/pdf/2505.16673v1...
[28.05.2025 06:17] Extracting affiliations from text.
[28.05.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 3 7 6 6 1 . 5 0 5 2 : r R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO Huanjin Yao2,3*, Qixiang Yin4*, Jingyi Zhang1, Min Yang2, Yibo Wang3, Wenhao Wu5 Fei Su4, Li Shen1, Minghui Qiu2, Dacheng Tao1, Jiaxing Huang1(cid:12) 1Nanyang Technological University 2ByteDance 4Beijing University of Posts and Telecommunications Equal Contribution (cid:12) Corresponding Author 3Tsinghua University 5The University of Sydney "
[28.05.2025 06:17] Response: ```python
[
    "Nanyang Technological University",
    "ByteDance",
    "Beijing University of Posts and Telecommunications",
    "Tsinghua University",
    "The University of Sydney"
]
```
[28.05.2025 06:17] Deleting PDF ./assets/pdf/2505.16673.pdf.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 06:17] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 06:17] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 06:17] Success.
[28.05.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2505.19377.
[28.05.2025 06:17] Downloading paper 2505.19377 from http://arxiv.org/pdf/2505.19377v1...
[28.05.2025 06:18] Extracting affiliations from text.
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 7 7 3 9 1 . 5 0 5 2 : r a Zichong Meng, Zeyu Han, Xiaogang Peng, Yiming Xie, Huaizu Jiang Northeastern University {meng.zic, han.zeyu, peng.xiaog, xie.yim, h.jiang}@northeastern.edu https://neu-vi.github.io/ACMDM/ Figure 1: Absolute coordinates make motion generation easy. Here we show that our model produces motion of higher fidelity, has better controllability, and reports promising results of generating SMPL-H meshes directly. "
[28.05.2025 06:18] Response: ```python
["Northeastern University"]
```
[28.05.2025 06:18] Deleting PDF ./assets/pdf/2505.19377.pdf.
[28.05.2025 06:18] Success.
[28.05.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 06:18] Extra JSON file exists (./assets/json/2505.16340.json), skip PDF parsing.
[28.05.2025 06:18] Paper image links file exists (./assets/img_data/2505.16340.json), skip HTML parsing.
[28.05.2025 06:18] Success.
[28.05.2025 06:18] Enriching papers with extra data.
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 0. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 1. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 2. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 3. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 4. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 5. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 6. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 7. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 8. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 9. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 10. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 11. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 12. The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual ...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 13. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 14. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 15. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 16. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 17. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 18. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 19. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 20. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 21. HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at vi...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 22. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 23. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 24. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 25. BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) ha...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 26. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 27. DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital ...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 28. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 29. Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown pro...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 30. Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 31. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 32. Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-a...
[28.05.2025 06:18] ********************************************************************************
[28.05.2025 06:18] Abstract 33. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 06:18] Read previous papers.
[28.05.2025 06:18] Generating reviews via LLM API.
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "🎨", "ru": {"title": "Универсальная согласованность стиля в генерации изображений", "desc": "OmniConsistency - это универсальный плагин для улучшения согласованности стилизации в задачах преобразования изобра
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "🖼️", "ru": {"title": "Автоматическая генерация научных постеров: от статьи к визуализации", "desc": "Эта статья представляет первый эталонный тест и набор метрик для генерации академических постеров, сопос
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "🧠", "ru": {"title": "Раскрывая пробелы в логике искусственного интеллекта", "desc": "MME-Reasoning - это новый комплексный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) к логическому рассуждению.
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "🎥", "ru": {"title": "VerIPO: Улучшение рассуждений видео-LLM с помощью верификатора", "desc": "Статья представляет метод VerIPO для улучшения способностей видео-LLM к рассуждениям. Метод использует Verif
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "🕵️", "ru": {"title": "Шерлок Холмс для ИИ: новый вызов в понимании видео", "desc": "Video-Holmes - это новый бенчмарк для оценки способностей мультимодальных языковых моделей к сложным рассуждениям на основе видео. Он ис
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "🧩", "ru": {"title": "GraLoRA: Гранулярная низкоранговая адаптация для эффективной настройки генеративных моделей", "desc": "Статья представляет новый метод адаптации моделей машинного обучения под названием Granular L
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "🎥", "ru": {"title": "Ограничения мультимодальных моделей в задаче OCR на видео", "desc": "Мультимодальные большие языковые модели (MLLM) показывают невысокую точность в задаче оптического распознавания символов
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "🧠", "ru": {"title": "rStar-Coder: прорыв в обучении языковых моделей рассуждениям о коде", "desc": "Исследователи представили rStar-Coder - крупномасштабный датасет для улучшения способностей языковых моделей (
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "🎬", "ru": {"title": "OpenS2V-Nexus: Революция в генерации видео на основе заданного содержания", "desc": "Статья представляет OpenS2V-Nexus - инфраструктуру для генерации видео на основе заданного содержания
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "🧠", "ru": {"title": "MetaMind: Искусственный интеллект с человеческим социальным пониманием", "desc": "MetaMind - это многоагентная система, улучшающая способность больших языковых моделей выполнять задачи
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "🎞️", "ru": {"title": "Семантическая оптимизация для быстрой и качественной генерации видео", "desc": "SVG2 - это фреймворк для улучшения эффективности и качества генерации видео без дополнительного обучения. Он использу
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "🧠", "ru": {"title": "MMMR: Новый стандарт оценки мультимодального мышления ИИ", "desc": "Статья представляет новый бенчмарк MMMR для оценки мультимодального рассуждения в крупных языковых моделях (MLLM). MMMR включает набор данных
[28.05.2025 06:18] Querying the API.
[28.05.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  					AI-generated summary 				 Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.
[28.05.2025 06:18] Response: {
  "desc": "Исследование предлагает алгоритм более точной идентификации нейронов для обнаружения языково-специфичных и языково-агностических нейронов в больших языковых моделях (LLM). Авторы анализируют влияние этого подхода на многоязычное выравнивание и возможности моделей. Исследование рассматривает многоязычное понимание, общее семантическое рассуждение, многоязычное преобразование выходных данных и вывод в пространстве словаря. Работа предоставляет эмпирические результаты и ценные выводы для лучшего понимания многоязычного выравнивания и многоязычных возможностей LLM.",
  "emoji": "🌐",
  "title": "Раскрытие тайн многоязычности в нейронах LLM"
}
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  					AI-generated summary 				 Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs."

[28.05.2025 06:18] Response: ```python
['MULTILINGUAL']
```
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  					AI-generated summary 				 Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs."

[28.05.2025 06:18] Response: ```python
['ALIGNMENT', 'LOW_RESOURCE']
```
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This research introduces a new algorithm for identifying neurons in large language models (LLMs) that are specific to certain languages as well as those that are language-agnostic. It explores how these neurons contribute to the model\'s ability to understand and generate text in multiple languages, enhancing multilingual alignment. The study categorizes the internal processes of LLMs into four key areas: understanding multiple languages, reasoning in a shared semantic space, transforming outputs across languages, and managing vocabulary. By analyzing the behavior of different types of neurons, the research provides insights into how LLMs can better support low-resource languages through learned multilingual capabilities.","title":"Enhancing Multilingual Capabilities in LLMs through Neuron Identification"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This research introduces a new algorithm for identifying neurons in large language models (LLMs) that are specific to certain languages as well as those that are language-agnostic. It explores how these neurons contribute to the model's ability to understand and generate text in multiple languages, enhancing multilingual alignment. The study categorizes the internal processes of LLMs into four key areas: understanding multiple languages, reasoning in a shared semantic space, transforming outputs across languages, and managing vocabulary. By analyzing the behavior of different types of neurons, the research provides insights into how LLMs can better support low-resource languages through learned multilingual capabilities.", title='Enhancing Multilingual Capabilities in LLMs through Neuron Identification'))
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究提出了一种更细粒度的神经元识别算法，用于检测大型语言模型（LLMs）中的语言特定神经元和语言无关神经元。我们分析了多语言理解、共享语义推理、多语言输出转换和词汇空间输出等方面对多语言对齐和能力的影响。研究表明，存在在处理不同语言时选择性激活的语言特定神经元，这为深入理解LLMs在多语言场景中的机制提供了新视角。通过对不同类型神经元的系统分析，我们为更好地理解LLMs的多语言对齐和能力提供了实证结果和有价值的见解。","title":"细粒度神经元识别，提升多语言能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究提出了一种更细粒度的神经元识别算法，用于检测大型语言模型（LLMs）中的语言特定神经元和语言无关神经元。我们分析了多语言理解、共享语义推理、多语言输出转换和词汇空间输出等方面对多语言对齐和能力的影响。研究表明，存在在处理不同语言时选择性激活的语言特定神经元，这为深入理解LLMs在多语言场景中的机制提供了新视角。通过对不同类型神经元的系统分析，我们为更好地理解LLMs的多语言对齐和能力提供了实证结果和有价值的见解。', title='细粒度神经元识别，提升多语言能力'))
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#synthetic", "#open_source", "#dataset", "#agents", "#training", "#data"], "emoji": "🧞", "ru": {"title": "UI-Genie: самообучающийся ИИ-помощник для графических интерфейсов", "desc": "UI-Genie - это самосовершенствующаяся система для агентов графическог
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "👁️", "ru": {"title": "ACTIVE-O3: Наделение MLLM активным восприятием для эффективного принятия решений", "desc": "Статья представляет ACTIVE-O3 - фреймворк обучения с подкреплением для наделения 
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "🖼️", "ru": {"title": "ImgEdit: прорыв в редактировании изображений с помощью ИИ", "desc": "Исследователи представили ImgEdit - крупномасштабный набор данных для редактирования изображений, содержащи
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#3d", "#games"], "emoji": "🧠", "ru": {"title": "Новый рубеж в пространственном интеллекте моделей компьютерного зрения", "desc": "Статья представляет новый бенчмарк ViewSpatial-Bench для оценки способностей моделей компьютерного зрения к пространст
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "🎬", "ru": {"title": "Управляемая генерация видео: новый уровень контроля над объектами в кадре", "desc": "Статья посвящена улучшению контролируемости, временной согласованности и детализации в ген
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "🔬", "ru": {"title": "SeePhys: выявление ограничений визуального мышления LLM в физике", "desc": "SeePhys - это новый мультимодальный бенчмарк для оценки способностей больших языковых моделей (LLM) в обл
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "🎯", "ru": {"title": "Точное управление языковыми моделями через атомарные компоненты знаний", "desc": "Статья представляет новый метод под названием Steering Target Atoms (STA) для точного контроля над генерацией
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "🖼️", "ru": {"title": "Эффективная генерация изображений от общего к частному", "desc": "DetailFlow - это новый метод генерации изображений, использующий авторегрессионный подход с последовательным уточнением 
[28.05.2025 06:18] Querying the API.
[28.05.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.
[28.05.2025 06:18] Response: {
  "desc": "HoliTom - это новый фреймворк для эффективного сжатия токенов в видео-LLM моделях. Он сочетает внешнюю обрезку LLM через глобальную временную сегментацию с внутренним объединением токенов на основе сходства. Такой подход позволяет значительно снизить вычислительные затраты без ущерба для производительности. В результате удается сократить количество операций с плавающей запятой до 6,9% от исходного, сохранив 99,1% производительности.",
  "emoji": "🎬",
  "title": "HoliTom: Революционное сжатие токенов для эффективных видео-LLM"
}
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference."

[28.05.2025 06:18] Response: ```python
["INFERENCE", "VIDEO"]
```
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference."

[28.05.2025 06:18] Response: ```python
["OPTIMIZATION"]
```
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HoliTom is a novel framework designed to enhance the efficiency of video large language models (LLMs) by reducing computational redundancy in video tokens. It combines outer-LLM pruning, which segments video data globally to identify and eliminate redundant tokens, with inner-LLM token merging based on similarity to further optimize performance. This dual approach allows for a significant reduction in visual tokens by over 90%, while still maintaining high performance levels, achieving 99.1% of the original output. The method also improves processing speed, reducing computational costs to just 6.9% of FLOPs and accelerating decoding throughput by 1.32 times, making it a practical solution for efficient video LLM inference.","title":"HoliTom: Efficient Video LLMs through Smart Token Pruning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HoliTom is a novel framework designed to enhance the efficiency of video large language models (LLMs) by reducing computational redundancy in video tokens. It combines outer-LLM pruning, which segments video data globally to identify and eliminate redundant tokens, with inner-LLM token merging based on similarity to further optimize performance. This dual approach allows for a significant reduction in visual tokens by over 90%, while still maintaining high performance levels, achieving 99.1% of the original output. The method also improves processing speed, reducing computational costs to just 6.9% of FLOPs and accelerating decoding throughput by 1.32 times, making it a practical solution for efficient video LLM inference.', title='HoliTom: Efficient Video LLMs through Smart Token Pruning'))
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HoliTom是一种新颖的训练无关的整体令牌合并框架，旨在通过全球时间分割进行外部LLM剪枝，显著减少视频大语言模型（视频LLM）的计算效率问题。该方法结合了外部LLM剪枝和内部LLM基于令牌相似性的合并，能够在不牺牲性能的情况下，减少视觉令牌超过90%。通过这种方式，HoliTom有效缓解了LLM的计算负担，同时保持了99.1%的原始性能。评估结果显示，该方法在计算成本和解码速度上均有显著提升，展示了其在高效视频LLM推理中的实际应用价值。","title":"HoliTom：高效视频LLM的全新剪枝策略"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HoliTom是一种新颖的训练无关的整体令牌合并框架，旨在通过全球时间分割进行外部LLM剪枝，显著减少视频大语言模型（视频LLM）的计算效率问题。该方法结合了外部LLM剪枝和内部LLM基于令牌相似性的合并，能够在不牺牲性能的情况下，减少视觉令牌超过90%。通过这种方式，HoliTom有效缓解了LLM的计算负担，同时保持了99.1%的原始性能。评估结果显示，该方法在计算成本和解码速度上均有显著提升，展示了其在高效视频LLM推理中的实际应用价值。', title='HoliTom：高效视频LLM的全新剪枝策略'))
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "🎞️", "ru": {"title": "Ускорение генерации длинных видео с помощью распределенного вывода", "desc": "Статья представляет новую стратегию распределенного вывода для видео-диффузионных моделей на основе Diffusion Transfor
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#audio"], "emoji": "🎙️", "ru": {"title": "SoloSpeech: генеративное извлечение целевой речи нового поколения", "desc": "SoloSpeech - это новый генеративный подход к извлечению целевой речи из смеси голосов. Он использует каскадный пайплайн, включающий сжатие, извлечение, реконструкци
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "🎞️", "ru": {"title": "Симметричное внедрение граничных кадров для улучшенного синтеза видео", "desc": "Статья представляет новый подход к синтезу промежуточных видеокадров между заданными начальным и ко
[28.05.2025 06:18] Querying the API.
[28.05.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.
[28.05.2025 06:18] Response: {
  "desc": "BARL - это новая система Байесовского адаптивного обучения с подкреплением для улучшения работы больших языковых моделей. Она интегрирует рефлексивное рассуждение и эффективное исследование, что приводит к лучшей эффективности использования токенов. BARL инструктирует языковую модель переключать стратегии на основе наблюдаемых результатов. Эмпирические результаты показывают, что BARL превосходит стандартные марковские подходы обучения с подкреплением при тестировании.",
  "emoji": "🧠",
  "title": "BARL: Умнее исследуем, эффективнее рассуждаем"
}
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL."

[28.05.2025 06:18] Response: ```python
['RL', 'RLHF', 'TRAINING']
```
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL."

[28.05.2025 06:18] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces BARL, a Bayes-Adaptive Reinforcement Learning framework designed to improve the performance of Large Language Models (LLMs) by incorporating reflective reasoning and efficient exploration strategies. Traditional Markovian RL methods limit exploration to the training phase and rely solely on current state information, which may hinder the emergence of reflective reasoning during training. BARL addresses this limitation by optimizing expected returns using a Bayesian approach, allowing the model to adaptively explore and exploit based on updated beliefs about the environment. Empirical results show that BARL significantly enhances token efficiency and effectiveness in reasoning tasks compared to standard Markovian RL methods.","title":"Enhancing LLMs with Reflective Exploration through BARL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces BARL, a Bayes-Adaptive Reinforcement Learning framework designed to improve the performance of Large Language Models (LLMs) by incorporating reflective reasoning and efficient exploration strategies. Traditional Markovian RL methods limit exploration to the training phase and rely solely on current state information, which may hinder the emergence of reflective reasoning during training. BARL addresses this limitation by optimizing expected returns using a Bayesian approach, allowing the model to adaptively explore and exploit based on updated beliefs about the environment. Empirical results show that BARL significantly enhances token efficiency and effectiveness in reasoning tasks compared to standard Markovian RL methods.', title='Enhancing LLMs with Reflective Exploration through BARL'))
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BARL是一种贝叶斯自适应强化学习框架，通过整合反思推理和高效探索，提升了大型语言模型（LLM）的性能。传统的马尔可夫强化学习限制了探索过程，仅依赖当前状态的历史上下文来学习最优策略，导致反思推理的出现和其在测试时的好处不明确。BARL框架通过优化马尔可夫决策过程的后验分布，鼓励模型在奖励最大化和信息收集之间进行平衡。实验结果表明，BARL在合成和数学推理任务中优于传统的马尔可夫强化学习方法，展现出更高的令牌效率和更有效的探索能力。","title":"BARL：提升LLM性能的贝叶斯自适应强化学习框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BARL是一种贝叶斯自适应强化学习框架，通过整合反思推理和高效探索，提升了大型语言模型（LLM）的性能。传统的马尔可夫强化学习限制了探索过程，仅依赖当前状态的历史上下文来学习最优策略，导致反思推理的出现和其在测试时的好处不明确。BARL框架通过优化马尔可夫决策过程的后验分布，鼓励模型在奖励最大化和信息收集之间进行平衡。实验结果表明，BARL在合成和数学推理任务中优于传统的马尔可夫强化学习方法，展现出更高的令牌效率和更有效的探索能力。', title='BARL：提升LLM性能的贝叶斯自适应强化学习框架'))
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "🧠", "ru": {"title": "VisTA: Автономное улучшение визуального мышления с помощью обучения с подкреплением", "desc": "VisTA - это новая система обучения с подкреплением для улучшения визуального мышления. 
[28.05.2025 06:18] Querying the API.
[28.05.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.
[28.05.2025 06:18] Response: {
  "desc": "DFIR-Metric - это комплексный инструмент для оценки больших языковых моделей (LLM) в области цифровой криминалистики. Он включает тесты на знания, реалистичные криминалистические задачи и практические случаи анализа. Benchmark оценивает 14 LLM по точности и согласованности результатов. Авторы также вводят новую метрику - Task Understanding Score (TUS), для оценки моделей в сценариях с почти нулевой точностью.",
  "emoji": "🔍",
  "title": "Комплексная оценка языковых моделей для цифровой криминалистики"
}
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric."

[28.05.2025 06:18] Response: ```python
['BENCHMARK', 'DATASET']
```
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric."

[28.05.2025 06:18] Response: ```python
['HALLUCINATIONS', 'REASONING', 'OPTIMIZATION', 'SCIENCE']
```
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces DFIR-Metric, a benchmark designed to evaluate Large Language Models (LLMs) in the field of Digital Forensics and Incident Response (DFIR). It consists of three main components: a Knowledge Assessment with expert-reviewed questions, Realistic Forensic Challenges that test reasoning and evidence correlation, and Practical Analysis using real forensic cases. The study also presents a new metric called the Task Understanding Score (TUS) to assess model performance in low-accuracy situations. By evaluating 14 LLMs, this benchmark aims to provide a reliable framework for improving AI applications in digital forensics.","title":"Evaluating AI in Digital Forensics: The DFIR-Metric Benchmark"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces DFIR-Metric, a benchmark designed to evaluate Large Language Models (LLMs) in the field of Digital Forensics and Incident Response (DFIR). It consists of three main components: a Knowledge Assessment with expert-reviewed questions, Realistic Forensic Challenges that test reasoning and evidence correlation, and Practical Analysis using real forensic cases. The study also presents a new metric called the Task Understanding Score (TUS) to assess model performance in low-accuracy situations. By evaluating 14 LLMs, this benchmark aims to provide a reliable framework for improving AI applications in digital forensics.', title='Evaluating AI in Digital Forensics: The DFIR-Metric Benchmark'))
[28.05.2025 06:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DFIR-Metric 是一个评估大型语言模型在数字取证领域表现的基准工具。它包含三个主要部分：知识评估、现实取证挑战和实际分析案例，旨在全面测试模型的能力。通过对 14 个大型语言模型的评估，研究者分析了它们在准确性和一致性方面的表现。新引入的任务理解分数（TUS）可以更有效地评估模型在接近零准确率的场景中的表现。","title":"DFIR-Metric：数字取证中的语言模型评估新标准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DFIR-Metric 是一个评估大型语言模型在数字取证领域表现的基准工具。它包含三个主要部分：知识评估、现实取证挑战和实际分析案例，旨在全面测试模型的能力。通过对 14 个大型语言模型的评估，研究者分析了它们在准确性和一致性方面的表现。新引入的任务理解分数（TUS）可以更有效地评估模型在接近零准确率的场景中的表现。', title='DFIR-Metric：数字取证中的语言模型评估新标准'))
[28.05.2025 06:18] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "⚡", "ru": {"title": "Короче мысль - быстрее вывод: оптимизация рассуждений в LLM", "desc": "Статья исследует эффективность коротких цепочек рассуждений в крупных языковых моделях (LLM) для задач рассуждения. Авторы предлагают метод 
[28.05.2025 06:18] Querying the API.
[28.05.2025 06:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.
[28.05.2025 06:18] Response: {
  "desc": "Статья представляет Code Graph Models (CGM) - новый подход к генерации кода на уровне репозитория. CGM интегрируют структуры графов кода в механизм внимания языковых моделей, улучшая понимание функций и файлов в кодовой базе. Эта техника позволяет открытым языковым моделям эффективно решать задачи на уровне репозитория без использования агентных подходов. В сочетании с безагентным фреймворком graph RAG, метод достигает высоких результатов на бенчмарке SWE-bench Lite, превосходя предыдущие открытые модели.",
  "emoji": "🧠",
  "title": "Графовые модели кода: новый уровень генерации в открытых ИИ-системах"
}
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%."

[28.05.2025 06:18] Response: ```python
['DATASET', 'MULTIMODAL', 'ARCHITECTURE', 'BENCHMARK']
```
[28.05.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%."

[28.05.2025 06:19] Response: ```python
['OPEN_SOURCE', 'GAMES']
```
[28.05.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach to enhance repository-level code generation tasks using open-source Code Graph Models (CGMs). By integrating code graph structures into the attention mechanisms of Large Language Models (LLMs), the authors demonstrate that these models can effectively understand the relationships and dependencies within codebases. This method eliminates the need for agent-based solutions, which often compromise data privacy and customization. The results show a significant improvement in performance, achieving a top ranking among open-source models on the SWE-bench Lite benchmark.","title":"Empowering Code Generation with Open-Source Graph Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach to enhance repository-level code generation tasks using open-source Code Graph Models (CGMs). By integrating code graph structures into the attention mechanisms of Large Language Models (LLMs), the authors demonstrate that these models can effectively understand the relationships and dependencies within codebases. This method eliminates the need for agent-based solutions, which often compromise data privacy and customization. The results show a significant improvement in performance, achieving a top ranking among open-source models on the SWE-bench Lite benchmark.', title='Empowering Code Generation with Open-Source Graph Models'))
[28.05.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种开源代码图模型（Code Graph Models, CGMs），旨在提升代码生成任务的性能。通过将代码图结构整合到大型语言模型（LLMs）的注意力机制中，CGMs能够更好地理解代码库中的函数和文件。与依赖代理的传统方法不同，我们的方法不需要代理，确保了数据隐私和模型定制的灵活性。实验结果表明，使用开源Qwen2.5-72B模型，我们的方法在SWE-bench Lite基准测试中达到了43.00%的解决率，表现优于其他开源模型。","title":"开源代码图模型提升代码生成性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种开源代码图模型（Code Graph Models, CGMs），旨在提升代码生成任务的性能。通过将代码图结构整合到大型语言模型（LLMs）的注意力机制中，CGMs能够更好地理解代码库中的函数和文件。与依赖代理的传统方法不同，我们的方法不需要代理，确保了数据隐私和模型定制的灵活性。实验结果表明，使用开源Qwen2.5-72B模型，我们的方法在SWE-bench Lite基准测试中达到了43.00%的解决率，表现优于其他开源模型。', title='开源代码图模型提升代码生成性能'))
[28.05.2025 06:19] Querying the API.
[28.05.2025 06:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.
[28.05.2025 06:19] Response: {
  "desc": "Share-GRPO - это новый подход в области обучения с подкреплением, который улучшает мультимодальные большие языковые модели (MLLM). Метод расширяет пространство вопросов, позволяя исследовать разнообразные траектории рассуждений. Share-GRPO использует иерархическое вычисление преимуществ для более точной оценки относительных выгод. Эксперименты на шести широко используемых тестах показали превосходную производительность этого метода.",
  "emoji": "🧠",
  "title": "Усиление рассуждений MLLM через разделяемое обучение с подкреплением"
}
[28.05.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL."

[28.05.2025 06:19] Response: ```python
['RL', 'MULTIMODAL', 'BENCHMARK', 'TRAINING']
```
[28.05.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL."

[28.05.2025 06:19] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[28.05.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Share-GRPO, a new reinforcement learning method designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). It addresses challenges like sparse rewards and advantage vanishing by expanding the question space and sharing diverse reasoning paths. The approach encourages MLLMs to explore various reasoning trajectories and share insights across different question variants. By hierarchically computing advantages, Share-GRPO enhances the stability of policy training and demonstrates superior performance on multiple reasoning benchmarks.","title":"Enhancing MLLMs with Share-GRPO: Expanding Questions and Sharing Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Share-GRPO, a new reinforcement learning method designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). It addresses challenges like sparse rewards and advantage vanishing by expanding the question space and sharing diverse reasoning paths. The approach encourages MLLMs to explore various reasoning trajectories and share insights across different question variants. By hierarchically computing advantages, Share-GRPO enhances the stability of policy training and demonstrates superior performance on multiple reasoning benchmarks.', title='Enhancing MLLMs with Share-GRPO: Expanding Questions and Sharing Reasoning'))
[28.05.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新颖的强化学习方法Share-GRPO，旨在增强多模态大型语言模型（MLLM）的推理能力。通过扩展问题空间和共享多样的推理轨迹，Share-GRPO有效地解决了强化学习中的稀疏奖励和优势消失问题。该方法首先通过数据转换技术扩展给定问题的空间，然后鼓励MLLM在扩展的问题空间中探索多样的推理轨迹，并在强化学习过程中共享这些轨迹。此外，Share-GRPO在优势计算中共享奖励信息，从而提高了相对优势的估计准确性，增强了策略训练的稳定性。","title":"Share-GRPO：提升多模态语言模型推理能力的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新颖的强化学习方法Share-GRPO，旨在增强多模态大型语言模型（MLLM）的推理能力。通过扩展问题空间和共享多样的推理轨迹，Share-GRPO有效地解决了强化学习中的稀疏奖励和优势消失问题。该方法首先通过数据转换技术扩展给定问题的空间，然后鼓励MLLM在扩展的问题空间中探索多样的推理轨迹，并在强化学习过程中共享这些轨迹。此外，Share-GRPO在优势计算中共享奖励信息，从而提高了相对优势的估计准确性，增强了策略训练的稳定性。', title='Share-GRPO：提升多模态语言模型推理能力的新方法'))
[28.05.2025 06:19] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "🎯", "ru": {"title": "Усовершенствованная атака на мультимодальные языковые модели через оптимальное выравнивание признаков", "desc": "Статья представляет новый метод атаки на мультимодальные языковые модели, называемый FOA-Attack. О
[28.05.2025 06:19] Querying the API.
[28.05.2025 06:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.
[28.05.2025 06:19] Response: {
  "desc": "В статье предлагается новый подход к генерации движений на основе текста, использующий абсолютные координаты суставов в глобальном пространстве. Этот метод обеспечивает более высокую точность движений, улучшенное соответствие тексту и хорошую масштабируемость даже с простой архитектурой Transformer. Подход также поддерживает задачи управления движением на основе текста и редактирования без дополнительной доработки. Авторы демонстрируют возможность прямой генерации вершин меша SMPL-H в движении из текста.",
  "emoji": "🤖",
  "title": "Глобальные координаты для улучшения генерации движений из текста"
}
[28.05.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications."

[28.05.2025 06:19] Response: ```python
['CV', 'MULTIMODAL', 'TRAINING']
```
[28.05.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications."

[28.05.2025 06:19] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[28.05.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new approach to text-to-motion generation by using absolute joint coordinates in global space instead of the traditional local-relative motion representation. This change enhances motion fidelity, improves text alignment, and allows for better scalability, even when using a simple Transformer model. The authors demonstrate that their method supports various downstream tasks without needing complex reengineering or additional guidance. Overall, this work lays a solid foundation for future advancements in motion generation and related applications.","title":"Revolutionizing Text-to-Motion with Global Coordinates"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new approach to text-to-motion generation by using absolute joint coordinates in global space instead of the traditional local-relative motion representation. This change enhances motion fidelity, improves text alignment, and allows for better scalability, even when using a simple Transformer model. The authors demonstrate that their method supports various downstream tasks without needing complex reengineering or additional guidance. Overall, this work lays a solid foundation for future advancements in motion generation and related applications.', title='Revolutionizing Text-to-Motion with Global Coordinates'))
[28.05.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的文本到运动生成方法，使用全局空间中的绝对关节坐标来提高运动的真实感、文本对齐和可扩展性。传统的运动表示方法依赖于相对运动，虽然简化了训练过程，但对扩散模型的应用造成了限制。通过系统分析，我们证明了这种新的表示方法在运动真实感和文本对齐方面显著提升，且能够支持下游任务。最终，我们展示了该方法在从文本直接生成运动的SMPL-H网格顶点方面的良好泛化能力，为未来的研究和运动相关应用奠定了基础。","title":"全局绝对坐标提升文本到运动生成的效果"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的文本到运动生成方法，使用全局空间中的绝对关节坐标来提高运动的真实感、文本对齐和可扩展性。传统的运动表示方法依赖于相对运动，虽然简化了训练过程，但对扩散模型的应用造成了限制。通过系统分析，我们证明了这种新的表示方法在运动真实感和文本对齐方面显著提升，且能够支持下游任务。最终，我们展示了该方法在从文本直接生成运动的SMPL-H网格顶点方面的良好泛化能力，为未来的研究和运动相关应用奠定了基础。', title='全局绝对坐标提升文本到运动生成的效果'))
[28.05.2025 06:19] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#science", "#dataset", "#open_source", "#data", "#multimodal"], "emoji": "🧪", "ru": {"title": "CLEANMOL: Улучшение понимания молекул языковыми моделями", "desc": "Статья представляет CLEANMOL - новый фреймворк для улучшения понимания молекулярных структур бо
[28.05.2025 06:19] Loading Chinese text from previous data.
[28.05.2025 06:19] Renaming data file.
[28.05.2025 06:19] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 06:19] Saving new data file.
[28.05.2025 06:19] Generating page.
[28.05.2025 06:19] Renaming previous page.
[28.05.2025 06:19] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 06:19] [Experimental] Generating Chinese page for reading.
[28.05.2025 06:19] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'}, {'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'}, {'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '快速', 'pinyin': 'kuài sù', 'trans': 'rapid'}, {'word': '发展', 'pinyin': 'fā zhǎn', 'trans': 'development'}, {'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'main'}, {'word': '通过', 'pinyin': 'tōng guò', 'trans': 'through'}, {'word': '增加', 'pinyin': 'zēng jiā', 'trans': 'increase'}, {'word': '参数', 'pinyin': 'cān shǔ', 'trans': 'parameter'}, {'word': '数量', 'pinyin': 'shù liàng', 'trans': 'quantity'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '硬件', 'pinyin': 'yìng jiàn', 'trans': 'hardware'}, {'word': '限制', 'pinyin': 'xiàn zhì', 'trans': 'limit'}, {'word': '自注意力', 'pinyin': 'zì zhù yì lì', 'trans': 'self-attention'}, {'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'}, {'word': '瓶颈', 'pinyin': 'píng lóng', 'trans': 'bottleneck'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '重点', 'pinyin': 'zhòng diǎn', 'trans': 'focus'}, {'word': '模型压缩', 'pinyin': 'mó xíng yā suō', 'trans': 'model compression'}, {'word': '转向', 'pinyin': 'zhuǎn xiàng', 'trans': 'turn to'}, {'word': '数据压缩', 'pinyin': 'shù jù yā suō', 'trans': 'data compression'}, {'word': '令牌', 'pinyin': 'lìng pái', 'trans': 'token'}, {'word': '压缩', 'pinyin': 'yā suō', 'trans': 'compression'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'}, {'word': '作者', 'pinyin': 'zuò zhě', 'trans': 'author'}, {'word': '分析', 'pinyin': 'fēn xī', 'trans': 'analyze'}, {'word': '长上下文', 'pinyin': 'cháng shàng xià wén', 'trans': 'long context'}, {'word': '数学框架', 'pinyin': 'shù xué kuàng jià', 'trans': 'mathematical framework'}, {'word': '探讨', 'pinyin': 'tàn tǎo', 'trans': 'explore'}, {'word': '优势', 'pinyin': 'yōu shì', 'trans': 'advantage'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}]
[28.05.2025 06:19] Renaming previous Chinese page.
[28.05.2025 06:19] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 06:19] Writing Chinese reading task.
[28.05.2025 06:19] Writing result.
[28.05.2025 06:19] Renaming log file.
[28.05.2025 06:19] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
