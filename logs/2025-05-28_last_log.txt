[28.05.2025 16:14] Read previous papers.
[28.05.2025 16:14] Generating top page (month).
[28.05.2025 16:14] Writing top page (month).
[28.05.2025 17:11] Read previous papers.
[28.05.2025 17:11] Get feed.
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19897
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19641
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21189
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21334
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17952
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14064
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21505
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16901
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21493
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20287
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20561
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20426
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21178
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19433
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20321
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17005
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11277
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19973
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18657
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18134
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17908
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16673
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21499
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20286
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19650
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19377
[28.05.2025 17:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.19094
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17190
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 17:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.21501
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21471
[28.05.2025 17:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.21097
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20793
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20162
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20052
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20036
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19954
[28.05.2025 17:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.19235
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17855
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17639
[28.05.2025 17:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15561
[28.05.2025 17:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 17:11] No deleted papers detected.
[28.05.2025 17:11] Downloading and parsing papers (pdf, html). Total: 67.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19897.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19897.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19897.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19641.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19641.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19641.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21189.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21189.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21189.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21496.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21496.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21334.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21334.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21334.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.17952.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.17952.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.17952.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.14064.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.14064.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.14064.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21505.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21505.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21505.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21500.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21500.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.16901.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.16901.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.16901.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21493.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21493.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21493.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20287.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20287.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20287.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20561.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20561.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20561.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20426.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20426.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20426.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21178.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21178.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21178.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19433.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19433.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19433.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20321.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20321.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20321.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.17005.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.17005.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.17005.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.11277.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.11277.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.11277.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19973.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19973.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19973.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.18657.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.18657.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.18657.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.18134.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.18134.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.18134.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.17908.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.17908.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.17908.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.16673.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.16673.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.16673.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21499.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21499.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21499.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20286.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20286.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20286.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19650.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19650.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19650.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19377.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19377.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19377.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19094.
[28.05.2025 17:11] Downloading paper 2505.19094 from http://arxiv.org/pdf/2505.19094v1...
[28.05.2025 17:11] Extracting affiliations from text.
[28.05.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 2 ] . [ 1 4 9 0 9 1 . 5 0 5 2 : r SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards Chuming Shen1, Wei Wei1, Xiaoye Qu1, Yu Cheng2 1 School of Computer Science & Technology, Huazhong University of Science and Technology 2 The Chinese University of Hong Kong {scm, weiw, xiaoye}@hust.edu.cn chengyu@cse.cuhk.edu.hk "
[28.05.2025 17:11] Response: ```python
[
    "School of Computer Science & Technology, Huazhong University of Science and Technology",
    "The Chinese University of Hong Kong"
]
```
[28.05.2025 17:11] Deleting PDF ./assets/pdf/2505.19094.pdf.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.17190.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.17190.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.17190.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.16340.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.16340.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21501.
[28.05.2025 17:11] Downloading paper 2505.21501 from http://arxiv.org/pdf/2505.21501v1...
[28.05.2025 17:11] Extracting affiliations from text.
[28.05.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 1 0 5 1 2 . 5 0 5 2 : r Vision Transformers with Self-Distilled Registers Yinjie Chen*1 Zipeng Yan*2 Chong Zhou3 Bo Dai2 Andrew F. Luo2 1 Zhejiang University 2 University of Hong Kong 3 Nanyang Technological University * Equal contribution Co-First authors chen.yinjie@zju.edu.cn, kelvinyzp@gmail.com Corresponding author: aluo@hku.hk "
[28.05.2025 17:11] Response: ```python
["Zhejiang University", "University of Hong Kong", "Nanyang Technological University"]
```
[28.05.2025 17:11] Deleting PDF ./assets/pdf/2505.21501.pdf.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21471.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.21471.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.21471.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.21097.
[28.05.2025 17:11] Downloading paper 2505.21097 from http://arxiv.org/pdf/2505.21097v1...
[28.05.2025 17:11] Extracting affiliations from text.
[28.05.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 7 9 0 1 2 . 5 0 5 2 : r Thinker: Learning to Think Fast and Slow Stephen Chung DualityRL Wenyu Du DualityRL Jie Fu Shanghai AI Lab "
[28.05.2025 17:11] Response: ```python
["DualityRL", "Shanghai AI Lab"]
```
[28.05.2025 17:11] Deleting PDF ./assets/pdf/2505.21097.pdf.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20793.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20793.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20793.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20162.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20162.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20162.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20052.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20052.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20052.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.20036.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.20036.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.20036.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19954.
[28.05.2025 17:11] Extra JSON file exists (./assets/json/2505.19954.json), skip PDF parsing.
[28.05.2025 17:11] Paper image links file exists (./assets/img_data/2505.19954.json), skip HTML parsing.
[28.05.2025 17:11] Success.
[28.05.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2505.19235.
[28.05.2025 17:11] Downloading paper 2505.19235 from http://arxiv.org/pdf/2505.19235v1...
[28.05.2025 17:15] Extracting affiliations from text.
[28.05.2025 17:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoreMatching: Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models Qinsi Wang 1 Hancheng Ye 1 Ming-Yu Chung 1 Yudong Liu 1 Yueqian Lin 1 Martin Kuo 1 Mingyuan Ma 1 Jianyi Zhang 1 Yiran Chen "
[28.05.2025 17:15] Response: []
[28.05.2025 17:15] Extracting affiliations from text.
[28.05.2025 17:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoreMatching: Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models Qinsi Wang 1 Hancheng Ye 1 Ming-Yu Chung 1 Yudong Liu 1 Yueqian Lin 1 Martin Kuo 1 Mingyuan Ma 1 Jianyi Zhang 1 Yiran Chen1. Introduction 5 2 0 2 5 2 ] . [ 1 5 3 2 9 1 . 5 0 5 2 : r Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, fundamental yet underexplored question remains: Do they truly operate in isolation, or is there deeper underlying interplay that has In this paper, we conyet to be uncovered? duct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-theart baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5 FLOPs reduction and 10 overall speedup. Code is released at https://github.com/wangqinsi1/2025ICML-CoreMatching/tree/main. 1Department of Electrical and Computer Engineering, Duke University, North Carolina, USA. Correspondence to: Qinsi Wang <qinsi.wang@duke.edu>, Jianyi Zhang <jianyi.zhang@duke.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Large language models (LLMs) have achieved outstanding performance in various applications and have exerted significant influence on our daily life (Brown, 2020; Chowdhery et al., 2023; Touvron et al., 2023a; Kuo et al., 2025; Qinsi et al.). As LLMs become increasingly aware of the physical world, researchers have discovered their potential for understanding visual information (Lin et al., 2023; Liu et al., 2024a; 2025b). As result, series of vision-language models (VLMs) such as LLaVA (Liu et al., 2024b), Blip(Li et al., 2022), and Llama(Touvron et al., 2023b) have been introduced, demonstrating impressive performance on tasks like image-based question answering. However, because of the requirement of long image-token inputs, VLMs typically demand more time and memory for inference than LLMs, limiting their practical adoption in real-world scenarios. Token Sparsity, which exploits the high degree of redundancy among image tokens, is promising solution to this challenge(Ye et al., 2024; Huang et al., 2024). Researchers have found that VLMs can retain strong performance using as little as 10% of the total tokens. This has led to extensive interest in determining which tokens are essential. For example, PruMerge (Shang et al., 2024) uses the average attention scores between image tokens and text tokens to measure token importance and retains 20% of the tokens with only minor performance loss; FastV (Chen et al., 2025) uses the total attention scores received by other token and finds that more than half of the tokens can be discarded from the second layer. Nevertheless, it is worth noting that most existing methods rely on attention scores as guide for selecting important tokens, but their validity and accuracy have not been thoroughly examined. Another effective approach to accelerating inference is model-internal sparsity, particularly adaptive Neuron Sparsity. It leverages the highly sparse activations in feedforward network (FFN) layers to skip the computation of inactive neurons, thereby reducing the computational in LLMs. For instance, methods such as Dejavu (Liu et al., 2023) and PowerInfer (Song et al., 2023) employ MLPbased predictors to identify which neurons are activated for given input, achieving up to 93% prediction accuracy. FurCoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Figure 1. Schematic diagram of CoreMatching. In the Pre-filling stage, CoreMatching calculates Core Neurons in the FFN block based on the activation. Core Neurons are the most frequently activated group of neurons. Afterwards, CoreMatching matches the neurons activated by different tokens with the core neurons, and selects group of tokens with the largest intersection as the Core Tokens. Only the Core Tokens are passed to the subsequent layers. During the decoding stage, the model only uses Core Neurons for calculations, and there are only core tokens in the kv cache. CoreMatching achieves comprehensive acceleration for inference of VLMs. ther, CoreInfer (Wang et al., 2024) defines core neurons as the subset of neurons most frequently activated by an input sentence, demonstrating that only these core neurons are needed to maintain performance. While neuron sparsity has shown great promise in LLMs, it remains underexplored and underutilized in VLMs. Although both token sparsity and neuron sparsity can individually accelerate the model, each has limitations in practical applications. Token sparsity primarily speeds up the pre-filling stage and can only provide limited acceleration during decoding by reducing key-value operations. In contrast, neuron sparsity primarily accelerates the decoding stage but cannot achieve high speedup ratio in the prefilling stage due to the large number of tokens and the resulting low level of sparsity. Hence, combining these two forms of sparsity is promising approach to achieving comprehensive acceleration. However, an interesting question that has been overlooked in previous work is: What is the relationship between these two sparse spaces? To answer this question, we first experimentally verify the existence of neuron sparsity in VLMs and the effectiveness of core neurons, which are the most important neurons determined by the activation distribution of all input tokens. Furthermore, we investigate the alignment between core neurons and tokens. By analyzing how core neurons influence t"
[28.05.2025 17:15] Mistral response. {"id": "6299d26ec1364045a55d5359b1849fd3", "object": "chat.completion", "created": 1748452524, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Electrical and Computer Engineering, Duke University, North Carolina, USA\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1678, "total_tokens": 1703, "completion_tokens": 25}}
[28.05.2025 17:15] Response: ```python
["Department of Electrical and Computer Engineering, Duke University, North Carolina, USA"]
```
[28.05.2025 17:15] Deleting PDF ./assets/pdf/2505.19235.pdf.
[28.05.2025 17:15] Success.
[28.05.2025 17:15] Downloading and parsing paper https://huggingface.co/papers/2505.17855.
[28.05.2025 17:15] Extra JSON file exists (./assets/json/2505.17855.json), skip PDF parsing.
[28.05.2025 17:15] Paper image links file exists (./assets/img_data/2505.17855.json), skip HTML parsing.
[28.05.2025 17:15] Success.
[28.05.2025 17:15] Downloading and parsing paper https://huggingface.co/papers/2505.17639.
[28.05.2025 17:15] Extra JSON file exists (./assets/json/2505.17639.json), skip PDF parsing.
[28.05.2025 17:15] Paper image links file exists (./assets/img_data/2505.17639.json), skip HTML parsing.
[28.05.2025 17:15] Success.
[28.05.2025 17:15] Downloading and parsing paper https://huggingface.co/papers/2505.15561.
[28.05.2025 17:15] Extra JSON file exists (./assets/json/2505.15561.json), skip PDF parsing.
[28.05.2025 17:15] Paper image links file exists (./assets/img_data/2505.15561.json), skip HTML parsing.
[28.05.2025 17:15] Success.
[28.05.2025 17:15] Enriching papers with extra data.
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 0. ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  					AI-generated summary 				 Large Language Models (LLMs) have extended their impact beyond Natural...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 1. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 2. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 3. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 4. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 5. SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have de...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 6. LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up t...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 7. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 8. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 9. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 10. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 11. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 12. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 13. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 14. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 15. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 16. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 17. HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at vi...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 18. Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled fro...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 19. NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.  					AI-generated summary 				 In many real-world applications, deployed models encounter inputs that differ from th...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 20. The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 21. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 22. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 23. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 24. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 25. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 26. Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown pro...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 27. The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verifica...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 28. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 29. MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  					AI-generated summary 				 Animating images with interactive motion control has garnered popularity for image-to-video (I2V) gen...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 30. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 31. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 32. BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) ha...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 33. Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understandin...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 34. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 35. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 36. As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 37. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 38. Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ig...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 39. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 40. BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.  					AI-generated summary 				 Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However,...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 41. R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinati...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 42. AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  					AI-generated summary 				 Large language models have ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 43. DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 44. MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.  					AI-generated summary 				 Recent advances in Multimodal Large Language Models (MLLMs) hav...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 45. VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  					AI-generated summary 				 Vision-language models (VLMs) have achieved strong results on coding and ma...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 46. ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  					AI-generated summary 				 With the rapid advan...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 47. Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 48. AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 49. Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.  					AI-generated summary 				 Recent advances in large language models (LLMs) have enabled agents to autono...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 50. UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent c...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 51. Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-a...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 52. SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.  					AI-generated summary 				 DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 53. Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 54. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 55. Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.  					AI-generated summary 				 Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 56. The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.  					AI-generated summary 				 With the rapid advancement of post-training techniques for reasoning and informa...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 57. A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.  					AI-generated summary 				 Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applyi...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 58. RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  					AI-generated summary 				 Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 59. Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  					AI-generated summary 				 As large language models grow in capability and a...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 60. A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.  					AI-generated summary 				 Protein language models (PLMs) have emerged as powerful tools to detect comple...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 61. The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatena...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 62. A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.  					AI-generated summary 				 The differential diagnosis of neurodegenerative dementias is a challeng...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 63. A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.  					AI-generated summary 				 Vision-Language Models (VLMs) excel across diverse tasks but suffe...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 64. CLUE generates natural language explanations for a language model's uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.  					AI-generated summary 				 Understanding sources of a model's ...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 65. Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud se...
[28.05.2025 17:15] ********************************************************************************
[28.05.2025 17:15] Abstract 66. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capabilit...
[28.05.2025 17:15] Read previous papers.
[28.05.2025 17:15] Generating reviews via LLM API.
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#science", "#multimodal", "#agents", "#benchmark"], "emoji": "", "ru": {"title": "ScienceBoard:     -   ", "desc": "ScienceBoard             
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "", "ru": {"title": "     ", "desc": "MME-Reasoning -            (MLLM)   .
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "", "ru": {"title": "   :    ", "desc": "            , 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "", "ru": {"title": "     ", "desc": "OmniConsistency -           
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "", "ru": {"title": "OpenS2V-Nexus:        ", "desc": "  OpenS2V-Nexus -        
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#training", "#rl", "#dataset", "#reasoning", "#open_source"], "emoji": "", "ru": {"title": "SynLogic:      ", "desc": "SynLogic -     ,       (LLM)   
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#data", "#long_context", "#architecture", "#multimodal"], "emoji": "", "ru": {"title": "      LLM", "desc": " ,     (LLM)       ,    
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "", "ru": {"title": "  -  :    LLM", "desc": "          (LLM)   .    
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "", "ru": {"title": "VerIPO:   -LLM   ", "desc": "   VerIPO    -LLM  .   Verif
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "", "ru": {"title": "MMMR:      ", "desc": "    MMMR         (MLLM). MMMR   
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#synthetic", "#open_source", "#dataset", "#agents", "#training", "#data"], "emoji": "", "ru": {"title": "UI-Genie:  -   ", "desc": "UI-Genie -      
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "SVG2 -            .  
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "", "ru": {"title": "     OCR  ", "desc": "    (MLLM)        
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "", "ru": {"title": "GraLoRA:        ", "desc": "          Granular L
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "", "ru": {"title": "   :     ", "desc": "Video-Holmes -               .  
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "", "ru": {"title": "rStar-Coder:        ", "desc": "  rStar-Coder -        (
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "", "ru": {"title": "MetaMind:      ", "desc": "MetaMind -   ,       
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#inference", "#video", "#optimization"], "emoji": "", "ru": {"title": "HoliTom:      -LLM", "desc": "HoliTom -         -LLM .     LLM   
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#healthcare", "#reasoning", "#benchmark", "#rl", "#dataset", "#interpretability"], "emoji": "", "ru": {"title": "    :    ", "desc": "  AlphaMed -      (LLM), 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#healthcare", "#cv", "#reasoning", "#benchmark", "#interpretability"], "emoji": "", "ru": {"title": "NOVA:        ", "desc": "NOVA -             .
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#alignment"], "emoji": "", "ru": {"title": "     LLM", "desc": "         -  -    
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "", "ru": {"title": "ImgEdit:       ", "desc": "  ImgEdit -      , 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "", "ru": {"title": "ACTIVE-O3:  MLLM      ", "desc": "  ACTIVE-O3 -       
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#3d", "#games"], "emoji": "", "ru": {"title": "       ", "desc": "    ViewSpatial-Bench        
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "", "ru": {"title": "  :       ", "desc": "   ,      
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "", "ru": {"title": "       ", "desc": "      Steering Target Atoms (STA)     
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#games", "#architecture", "#open_source"], "emoji": "", "ru": {"title": "  :      -", "desc": "  Code Graph Models (CGM) -        
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#optimization", "#math", "#rl", "#benchmark", "#training", "#reasoning"], "emoji": "", "ru": {"title": "VeriFree:  LLM     ", "desc": "        (LLM)   VeriFree. 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "", "ru": {"title": "      ", "desc": "DetailFlow -     ,       
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#benchmark", "#video", "#games", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "MotionPro -       ,        .  
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "", "ru": {"title": "SeePhys:     LLM  ", "desc": "SeePhys -           (LLM)  
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "", "ru": {"title": "         ", "desc": "        ,  FOA-Attack. 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "BARL:  ,  ", "desc": "BARL -              .  
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#3d", "#cv", "#interpretability", "#multimodal", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "MMPerspective:       ", "desc": "  MMPerspective -       
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "", "ru": {"title": "VisTA:         ", "desc": "VisTA -          . 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "            
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#rl", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": " -  :      ", "desc": "            
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "", "ru": {"title": "       ", "desc": "       -    Diffusion Transfor
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#agents", "#optimization", "#inference", "#benchmark"], "emoji": "", "ru": {"title": "ACBench:         LLM", "desc": "    ACBench         
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#audio"], "emoji": "", "ru": {"title": "SoloSpeech:      ", "desc": "SoloSpeech -           .    ,  , , 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#open_source", "#science", "#dataset", "#multimodal", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": "    -SQL   ", "desc": "BiomedSQL -            
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#hallucinations", "#optimization", "#rl", "#reasoning", "#rag"], "emoji": "", "ru": {"title": "  LLM:     ", "desc": "R1-Searcher++ -   ,      (LLM)    
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#optimization", "#rag", "#rl", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "AutoRefine:       ", "desc": "AutoRefine -         ,     
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#science", "#hallucinations", "#benchmark", "#dataset", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "      ", "desc": "DFIR-Metric -         (LLM)  
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#interpretability", "#training", "#agi", "#multimodal", "#architecture"], "emoji": "", "ru": {"title": "     -", "desc": "          (MLLM), 
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#games", "#cv", "#multimodal", "#benchmark", "#video"], "emoji": "", "ru": {"title": "     ", "desc": "VideoGameBench -                
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#games", "#training", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "ComfyMind:           ", "desc": "ComfyMind -    
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#rl", "#training", "#multimodal", "#benchmark", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "  MLLM     ", "desc": "Share-GRPO -        ,   
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#cv", "#agents", "#security"], "emoji": "", "ru": {"title": "  :    -  ", "desc": "AdInject -      -,     ,   -   
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#open_source", "#agi", "#agents", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": " -    -", "desc": " -    ,          
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#training", "#multimodal"], "emoji": "", "ru": {"title": "UNITE:       ", "desc": "  UNITE -      .  
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#optimization", "#cv"], "emoji": "", "ru": {"title": "       ", "desc": "          ,    
[28.05.2025 17:15] Querying the API.
[28.05.2025 17:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.  					AI-generated summary 				 DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.
[28.05.2025 17:15] Response: {
  "desc": "SATORI -        -  (VQA),    .   VQA    :   ,     . SATORI         ,       .       15.7%           VQA.",
  "emoji": "",
  "title": "SATORI:   VQA     "
}
[28.05.2025 17:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.  					AI-generated summary 				 DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1."

[28.05.2025 17:15] Response: ```python
['RL', 'MULTIMODAL', 'DATASET', 'BENCHMARK']
```
[28.05.2025 17:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.  					AI-generated summary 				 DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1."

[28.05.2025 17:15] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[28.05.2025 17:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces SATORI, a method that improves Visual Question Answering (VQA) by breaking it down into three clear stages: global image captioning, region localization, and answer prediction. This structured approach allows for explicit rewards at each stage, which helps the model focus on important areas of the image, thus enhancing accuracy. By addressing the issues of visual focus and policy-gradient variance, SATORI achieves significant performance gains, with improvements of up to 15.7% in accuracy over previous methods. Additionally, the authors provide a new dataset, VQA-Verify, to support the training of their model with annotated captions and bounding boxes.","title":"SATORI: Enhancing VQA with Structured Stages and Focused Rewards"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces SATORI, a method that improves Visual Question Answering (VQA) by breaking it down into three clear stages: global image captioning, region localization, and answer prediction. This structured approach allows for explicit rewards at each stage, which helps the model focus on important areas of the image, thus enhancing accuracy. By addressing the issues of visual focus and policy-gradient variance, SATORI achieves significant performance gains, with improvements of up to 15.7% in accuracy over previous methods. Additionally, the authors provide a new dataset, VQA-Verify, to support the training of their model with annotated captions and bounding boxes.', title='SATORI: Enhancing VQA with Structured Stages and Focused Rewards'))
[28.05.2025 17:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SATORIVQAVQA-Verify12kVQASATORI15.7%","title":"SATORI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SATORIVQAVQA-Verify12kVQASATORI15.7%', title='SATORI'))
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#security", "#math", "#reasoning"], "emoji": "", "ru": {"title": " :  -    ", "desc": "        '
[28.05.2025 17:15] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#science", "#dataset", "#open_source", "#data", "#multimodal"], "emoji": "", "ru": {"title": "CLEANMOL:     ", "desc": "  CLEANMOL -        
[28.05.2025 17:15] Querying the API.
[28.05.2025 17:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.  					AI-generated summary 				 Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly "absorb" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.
[28.05.2025 17:15] Response: {
  "desc": "   Post Hoc Registers (PH-Reg)     Vision Transformers (ViT).        ViT    ,        . PH-Reg         ViT,   ,       .                ,          .",
  "emoji": "",
  "title": "   Vision Transformers  "
}
[28.05.2025 17:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.  					AI-generated summary 				 Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly "absorb" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing."

[28.05.2025 17:15] Response: ```python
['CV', 'ARCHITECTURE', 'TRAINING']
```
[28.05.2025 17:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.  					AI-generated summary 				 Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly "absorb" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing."

[28.05.2025 17:15] Response: ```python
["OPTIMIZATION"]
```
[28.05.2025 17:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Post Hoc Registers (PH-Reg), a self-distillation technique designed to enhance Vision Transformers (ViTs) by integrating register tokens. These register tokens help to mitigate the issue of artifact tokens that can negatively impact the model\'s performance in tasks requiring precise localization. The method allows for the incorporation of these tokens into pre-trained ViTs without the need for extensive retraining or additional labeled data. By leveraging a frozen teacher network and optimizing a small subset of the student network\'s weights, PH-Reg effectively improves segmentation and depth prediction capabilities.","title":"Enhancing Vision Transformers with Post Hoc Registers"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Post Hoc Registers (PH-Reg), a self-distillation technique designed to enhance Vision Transformers (ViTs) by integrating register tokens. These register tokens help to mitigate the issue of artifact tokens that can negatively impact the model's performance in tasks requiring precise localization. The method allows for the incorporation of these tokens into pre-trained ViTs without the need for extensive retraining or additional labeled data. By leveraging a frozen teacher network and optimizing a small subset of the student network's weights, PH-Reg effectively improves segmentation and depth prediction capabilities.", title='Enhancing Vision Transformers with Post Hoc Registers'))
[28.05.2025 17:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Post Hoc RegistersPH-RegViTViTPH-RegViT","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Post Hoc RegistersPH-RegViTViTPH-RegViT', title=''))
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#long_context", "#inference", "#agents", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": "ExtAgents:   LLM    ", "desc": "    ExtAgents,      
[28.05.2025 17:16] Querying the API.
[28.05.2025 17:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.  					AI-generated summary 				 Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.
[28.05.2025 17:16] Response: {
  "desc": "    -        (LLM)    .            :  , ,    .      24.9%  27.9%   Qwen2.5-1.5B   45.9%  49.8%  DeepSeek-R1-Qwen-1.5B.  ,    deliberative  -  ,     ,     .",
  "emoji": "",
  "title": "  LLM     "
}
[28.05.2025 17:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.  					AI-generated summary 				 Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training."

[28.05.2025 17:16] Response: ```python
['RL', 'MATH', 'TRAINING', 'INFERENCE']
```
[28.05.2025 17:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.  					AI-generated summary 				 Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training."

[28.05.2025 17:16] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[28.05.2025 17:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel four-stage modification to question-answering tasks for Large Language Models (LLMs), inspired by Dual Process Theory. The stages include Fast Thinking, Verification, Slow Thinking, and Summarization, which help separate intuitive responses from more deliberate reasoning. By implementing this structured approach, the authors demonstrate significant improvements in accuracy for math and coding tasks, with notable efficiency in token usage. The results indicate that enhancing both intuitive and deliberative reasoning can lead to better performance in LLMs.","title":"Enhancing LLM Accuracy through Structured Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel four-stage modification to question-answering tasks for Large Language Models (LLMs), inspired by Dual Process Theory. The stages include Fast Thinking, Verification, Slow Thinking, and Summarization, which help separate intuitive responses from more deliberate reasoning. By implementing this structured approach, the authors demonstrate significant improvements in accuracy for math and coding tasks, with notable efficiency in token usage. The results indicate that enhancing both intuitive and deliberative reasoning can lead to better performance in LLMs.', title='Enhancing LLM Accuracy through Structured Reasoning'))
[28.05.2025 17:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LLMs","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LLMs', title=''))
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#cv", "#rag", "#transfer_learning", "#rl", "#games"], "emoji": "", "ru": {"title": "RLRF:   SVG     ", "desc": " RLRF        SVG       (VLM). 
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#security", "#alignment"], "emoji": "", "ru": {"title": "   -    ", "desc": " ,        (LLM)  ,     
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#healthcare", "#dataset", "#science"], "emoji": "", "ru": {"title": "      ", "desc": "         (PLM)  
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#leakage", "#architecture", "#dataset", "#science"], "emoji": "", "ru": {"title": "   -       ", "desc": "   
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#3d", "#interpretability", "#cv", "#healthcare", "#multimodal", "#rl", "#reasoning"], "emoji": "", "ru": {"title": " :   ", "desc": "           
[28.05.2025 17:16] Querying the API.
[28.05.2025 17:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.  					AI-generated summary 				 Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.
[28.05.2025 17:16] Response: {
  "desc": "      CoreMatching            .        ,    .               .  NVIDIA Titan Xp  5-  FLOPS  10-  .",
  "emoji": "",
  "title": "CoreMatching:        "
}
[28.05.2025 17:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.  					AI-generated summary 				 Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main."

[28.05.2025 17:16] Response: ```python
["INFERENCE", "CV", "ARCHITECTURE"]
```
[28.05.2025 17:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.  					AI-generated summary 				 Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main."

[28.05.2025 17:16] Response: ```python
["OPTIMIZATION"]
```
[28.05.2025 17:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new framework called CoreMatching that improves the efficiency of vision-language models (VLMs) by combining two types of sparsity: token sparsity and neuron sparsity. Token sparsity helps reduce the number of tokens used, while neuron sparsity minimizes the computations needed for high-dimensional data. The authors explore the interaction between key neurons and tokens, revealing that they influence each other, which leads to better performance. By leveraging this synergy, CoreMatching significantly enhances inference efficiency, achieving remarkable speedups and reductions in computational load across various tasks and devices.","title":"Unlocking Efficiency: Synergizing Token and Neuron Sparsity in VLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new framework called CoreMatching that improves the efficiency of vision-language models (VLMs) by combining two types of sparsity: token sparsity and neuron sparsity. Token sparsity helps reduce the number of tokens used, while neuron sparsity minimizes the computations needed for high-dimensional data. The authors explore the interaction between key neurons and tokens, revealing that they influence each other, which leads to better performance. By leveraging this synergy, CoreMatching significantly enhances inference efficiency, achieving remarkable speedups and reductions in computational load across various tasks and devices.', title='Unlocking Efficiency: Synergizing Token and Neuron Sparsity in VLMs'))
[28.05.2025 17:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CoreMatching","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CoreMatching', title=''))
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#training", "#reasoning", "#data"], "emoji": "", "ru": {"title": "CLUE:     ", "desc": "CLUE -           .  
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture", "#open_source", "#training"], "emoji": "", "ru": {"title": "        ", "desc": "  PreMoe -     
[28.05.2025 17:16] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#benchmark", "#hallucinations"], "emoji": "", "ru": {"title": "   RAG:   ,  ", "desc": "            Retrieval Augmented Generati
[28.05.2025 17:16] Loading Chinese text from previous data.
[28.05.2025 17:16] Renaming data file.
[28.05.2025 17:16] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 17:16] Saving new data file.
[28.05.2025 17:16] Generating page.
[28.05.2025 17:16] Renaming previous page.
[28.05.2025 17:16] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 17:16] [Experimental] Generating Chinese page for reading.
[28.05.2025 17:16] Chinese vocab [{'word': 'OmniConsistency', 'pinyin': '', 'trans': 'OmniConsistency'}, {'word': '', 'pinyin': 'ku sn', 'trans': 'diffusion'}, {'word': '', 'pinyin': 'bin y q', 'trans': 'transformer'}, {'word': '', 'pinyin': 'zng qing', 'trans': 'enhance'}, {'word': '', 'pinyin': 'fng g', 'trans': 'style'}, {'word': '', 'pinyin': 'y zh xng', 'trans': 'consistency'}, {'word': '', 'pinyin': 'fn hu', 'trans': 'generalization'}, {'word': '', 'pinyin': 'nng l', 'trans': 'ability'}, {'word': '', 'pinyin': 'tu hu', 'trans': 'degeneration'}, {'word': '', 'pinyin': 'shng xi wn', 'trans': 'context'}, {'word': '', 'pinyin': 'kung ji', 'trans': 'framework'}, {'word': '', 'pinyin': 'ling ji dun', 'trans': 'two-stage'}, {'word': '', 'pinyin': 'jin jn', 'trans': 'progressive'}, {'word': '', 'pinyin': 'c l', 'trans': 'strategy'}, {'word': '', 'pinyin': 'ch b', 'trans': 'interpolation'}, {'word': '', 'pinyin': 'sh j', 'trans': 'design'}, {'word': '', 'pinyin': 'bio xin', 'trans': 'performance'}, {'word': '', 'pinyin': 'ji jn', 'trans': 'approach'}, {'word': '', 'pinyin': 'shng y', 'trans': 'commercial'}, {'word': '', 'pinyin': 'dng jin', 'trans': 'top-notch'}, {'word': '', 'pinyin': 'm xng', 'trans': 'model'}, {'word': 'GPT-4o', 'pinyin': '', 'trans': 'GPT-4o'}]
[28.05.2025 17:16] Renaming previous Chinese page.
[28.05.2025 17:16] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 17:16] Writing Chinese reading task.
[28.05.2025 17:16] Writing result.
[28.05.2025 17:16] Renaming log file.
[28.05.2025 17:16] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
