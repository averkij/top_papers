[28.05.2025 07:13] Read previous papers.
[28.05.2025 07:13] Generating top page (month).
[28.05.2025 07:13] Writing top page (month).
[28.05.2025 08:16] Read previous papers.
[28.05.2025 08:16] Get feed.
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21505
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21334
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16901
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21178
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20561
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.19641
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19433
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16673
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11277
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.21189
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19973
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17908
[28.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.17005
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19377
[28.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.15561
[28.05.2025 08:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 08:16] No deleted papers detected.
[28.05.2025 08:16] Downloading and parsing papers (pdf, html). Total: 42.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21496.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21496.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21505.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21505.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21505.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21334.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21334.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21334.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.16901.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.16901.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.16901.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21500.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21500.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21178.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21178.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21178.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.20561.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.20561.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.20561.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.19641.
[28.05.2025 08:16] Downloading paper 2505.19641 from http://arxiv.org/pdf/2505.19641v2...
[28.05.2025 08:16] Extracting affiliations from text.
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond Junteng Liu1 Yuanxiang Fan2 Zhuo Jiang2 Han Ding2 Yongyi Hu2 Chi Zhang2 Yiqi Shi2 Yunan Huang2 Mozhi Zhang2 Shitong Weng2 Aili Chen2 Shiqi Chen3 Pengyu Zhao Junjie Yan2 Junxian He1 5 2 0 2 7 2 ] . [ 2 1 4 6 9 1 . 5 0 5 2 : r 1The Hong Kong University of Science and Technology 2MiniMax 3The City University of Hong Kong "
[28.05.2025 08:16] Response: ```python
["The Hong Kong University of Science and Technology", "MiniMax", "The City University of Hong Kong"]
```
[28.05.2025 08:16] Deleting PDF ./assets/pdf/2505.19641.pdf.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.19433.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.19433.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.19433.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.16673.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.16673.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.16673.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.11277.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.11277.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.11277.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.21189.
[28.05.2025 08:16] Downloading paper 2505.21189 from http://arxiv.org/pdf/2505.21189v1...
[28.05.2025 08:16] Extracting affiliations from text.
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Exploring the Latent Capacity of LLMs for One-Step Text Generation * Gleb Mezentsev AIRI Skoltech mezentsev@airi.net Ivan Oseledets AIRI Skoltech oseledets@airi.net 5 2 0 2 7 2 ] . [ 1 9 8 1 1 2 . 5 0 5 2 : r a "
[28.05.2025 08:16] Response: ```python
["AIRI Skoltech"]
```
[28.05.2025 08:16] Deleting PDF ./assets/pdf/2505.21189.pdf.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.19973.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.19973.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.19973.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.17908.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.17908.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.17908.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.17005.
[28.05.2025 08:16] Downloading paper 2505.17005 from http://arxiv.org/pdf/2505.17005v1...
[28.05.2025 08:16] Extracting affiliations from text.
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 5 0 0 7 1 . 5 0 5 2 : r R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning Huatong Song1*, Jinhao Jiang1*, Wenqing Tian3, Zhipeng Chen1, Yuhuan Wu1, Jiahao Zhao1, Yingqian Min1, Wayne Xin Zhao1, Lei Fang2, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China. 2DataCanvas Alaya NeW. 3Beijing Institute of Technology. {songhuatong123, jiangjinhao}@ruc.edu.cn, batmanfly@gmail.com "
[28.05.2025 08:16] Response: ```python
[
    "Gaoling School of Artificial Intelligence, Renmin University of China",
    "DataCanvas Alaya NeW",
    "Beijing Institute of Technology"
]
```
[28.05.2025 08:16] Deleting PDF ./assets/pdf/2505.17005.pdf.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.19377.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.19377.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.19377.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 08:16] Extra JSON file exists (./assets/json/2505.16340.json), skip PDF parsing.
[28.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.16340.json), skip HTML parsing.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.15561.
[28.05.2025 08:16] Downloading paper 2505.15561 from http://arxiv.org/pdf/2505.15561v1...
[28.05.2025 08:16] Extracting affiliations from text.
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Do RAG Systems Suffer From Positional Bias? Florin Cuconasu1,2*, Simone Filice2 Guy Horowitz2, Yoelle Maarek2, Fabrizio Silvestri1 1Sapienza University of Rome, 2Technology Innovation Institute 5 2 0 2 1 2 ] . [ 1 1 6 5 5 1 . 5 0 5 2 : r a "
[28.05.2025 08:16] Response: ```python
["Sapienza University of Rome", "Technology Innovation Institute"]
```
[28.05.2025 08:16] Deleting PDF ./assets/pdf/2505.15561.pdf.
[28.05.2025 08:16] Success.
[28.05.2025 08:16] Enriching papers with extra data.
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 0. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 1. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 2. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 3. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 4. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 5. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 6. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 7. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 8. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 9. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 10. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 11. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 12. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 13. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 14. The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual ...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 15. HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at vi...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 16. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 17. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 18. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 19. Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown pro...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 20. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 21. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 22. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 23. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 24. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 25. As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 26. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 27. BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) ha...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 28. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 29. SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have de...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 30. Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ig...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 31. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 32. Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 33. AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  					AI-generated summary 				 Large language models have ...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 34. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 35. LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up t...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 36. DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital ...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 37. ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  					AI-generated summary 				 With the rapid advan...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 38. R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinati...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 39. Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-a...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 40. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 08:16] ********************************************************************************
[28.05.2025 08:16] Abstract 41. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capabilit...
[28.05.2025 08:16] Read previous papers.
[28.05.2025 08:16] Generating reviews via LLM API.
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ç–∏–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "OmniConsistency - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "üñºÔ∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—É—á–Ω—ã—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤: –æ—Ç —Å—Ç–∞—Ç—å–∏ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –∏ –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤, —Å–æ–ø–æ—Å
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –≤ –ª–æ–≥–∏–∫–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "MME-Reasoning - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "üé•", "ru": {"title": "VerIPO: –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–∏–¥–µ–æ-LLM —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ VerIPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ-LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Verif
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#synthetic", "#open_source", "#dataset", "#agents", "#training", "#data"], "emoji": "üßû", "ru": {"title": "UI-Genie: —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "UI-Genie - —ç—Ç–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "üéûÔ∏è", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "SVG2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "üïµÔ∏è", "ru": {"title": "–®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å –¥–ª—è –ò–ò: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "Video-Holmes - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. –û–Ω –∏—Å
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "üß©", "ru": {"title": "GraLoRA: –ì—Ä–∞–Ω—É–ª—è—Ä–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Granular L
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "üé•", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–µ OCR –Ω–∞ –≤–∏–¥–µ–æ", "desc": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "MMMR: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMMR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). MMMR –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "üß†", "ru": {"title": "rStar-Coder: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –æ –∫–æ–¥–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ rStar-Coder - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "üé¨", "ru": {"title": "OpenS2V-Nexus: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenS2V-Nexus - –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "‚ö°", "ru": {"title": "–ö–æ—Ä–æ—á–µ –º—ã—Å–ª—å - –±—ã—Å—Ç—Ä–µ–µ –≤—ã–≤–æ–¥: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ 
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "üß†", "ru": {"title": "MetaMind: –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "MetaMind - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#alignment"], "emoji": "üåê", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–π—Ä–æ–Ω–∞—Ö LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏ —è–∑—ã–∫–æ–≤–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö 
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#inference", "#video", "#optimization"], "emoji": "üé¨", "ru": {"title": "HoliTom: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-LLM", "desc": "HoliTom - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–∏–¥–µ–æ-LLM –º–æ–¥–µ–ª—è—Ö. –û–Ω —Å–æ—á–µ—Ç–∞–µ—Ç –≤–Ω–µ—à–Ω—é—é –æ–±—Ä–µ–∑–∫—É LLM —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—É—é –≤—Ä–µ–º
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "üëÅÔ∏è", "ru": {"title": "ACTIVE-O3: –ù–∞–¥–µ–ª–µ–Ω–∏–µ MLLM –∞–∫—Ç–∏–≤–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ACTIVE-O3 - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞–¥–µ–ª–µ–Ω–∏—è 
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "ImgEdit: –ø—Ä–æ—Ä—ã–≤ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ImgEdit - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "üé¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ –∫–∞–¥—Ä–µ", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏, –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≥–µ–Ω
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#games", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–ì—Ä–∞—Ñ–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–¥–∞: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Code Graph Models (CGM) - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Steering Target Atoms (STA) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#3d", "#games"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ViewSpatial-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É", "desc": "DetailFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º 
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "üî¨", "ru": {"title": "SeePhys: –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM –≤ —Ñ–∏–∑–∏–∫–µ", "desc": "SeePhys - —ç—Ç–æ –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "üéØ", "ru": {"title": "–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –∞—Ç–∞–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π FOA-Attack. –û
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#rl", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ö—Ä–∞—Ç–∫–æ—Å—Ç—å - —Å–µ—Å—Ç—Ä–∞ —Ç–∞–ª–∞–Ω—Ç–∞: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "üéûÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transfor
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "BARL: –£–º–Ω–µ–µ –∏—Å—Å–ª–µ–¥—É–µ–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–∞—Å—Å—É–∂–¥–∞–µ–º", "desc": "BARL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "üß†", "ru": {"title": "VisTA: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "VisTA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. 
[28.05.2025 08:16] Querying the API.
[28.05.2025 08:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.
[28.05.2025 08:16] Response: {
  "desc": "SynLogic - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–ë–Ø–ú) –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –û–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ª–µ–≥–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –∏–¥–µ–∞–ª—å–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö SynLogic, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö SynLogic —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é —É–ª—É—á—à–∞–µ—Ç –æ–±—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.",

  "emoji": "üß†",

  "title": "SynLogic: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é"
}
[28.05.2025 08:16] Renaming some terms.
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."

[28.05.2025 08:16] Response: ```python
['DATASET', 'RL', 'TRAINING']
```
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."

[28.05.2025 08:16] Response: ```python
["REASONING", "OPEN_SOURCE"]
```
[28.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SynLogic is a framework designed to improve the logical reasoning skills of Large Language Models (LLMs) using Reinforcement Learning (RL). It generates a diverse set of logical reasoning tasks, allowing for controlled data synthesis that can be adjusted in difficulty and quantity. The framework not only enhances the reasoning capabilities of LLMs but also improves their generalization across different domains by mixing logical reasoning data with mathematical and coding tasks. The results show that models trained with SynLogic achieve state-of-the-art performance, demonstrating its potential as a valuable resource for advancing AI reasoning.","title":"Enhancing AI Reasoning with SynLogic Framework"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SynLogic is a framework designed to improve the logical reasoning skills of Large Language Models (LLMs) using Reinforcement Learning (RL). It generates a diverse set of logical reasoning tasks, allowing for controlled data synthesis that can be adjusted in difficulty and quantity. The framework not only enhances the reasoning capabilities of LLMs but also improves their generalization across different domains by mixing logical reasoning data with mathematical and coding tasks. The results show that models trained with SynLogic achieve state-of-the-art performance, demonstrating its potential as a valuable resource for advancing AI reasoning.', title='Enhancing AI Reasoning with SynLogic Framework'))
[28.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SynLogicÊòØ‰∏Ä‰∏™Êï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÈÄªËæëÊé®ÁêÜÊï∞ÊçÆÔºåÊ∂µÁõñ35‰∏™‰∏çÂêåÁöÑÊé®ÁêÜ‰ªªÂä°ÔºåÂπ∂ÂÖÅËÆ∏Ê†πÊçÆÈöæÂ∫¶ÂíåÊï∞ÈáèËøõË°åÊéßÂà∂ÂêàÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®SynLogicÊï∞ÊçÆËøõË°åRLËÆ≠ÁªÉÔºåËÉΩÂ§üÊòæËëóÊèêÈ´òÊé®ÁêÜÊÄßËÉΩÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂äÁé∞ÊúâÁöÑÂºÄÊ∫êÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáÂ∞ÜSynLogicÊï∞ÊçÆ‰∏éÊï∞Â≠¶ÂíåÁºñÁ†Å‰ªªÂä°Ê∑∑ÂêàËÆ≠ÁªÉÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜËøô‰∫õÈ¢ÜÂüüÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊé®ÁêÜÊ≥õÂåñËÉΩÂäõ„ÄÇ","title":"SynLogicÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SynLogicÊòØ‰∏Ä‰∏™Êï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈÄªËæëÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÈÄªËæëÊé®ÁêÜÊï∞ÊçÆÔºåÊ∂µÁõñ35‰∏™‰∏çÂêåÁöÑÊé®ÁêÜ‰ªªÂä°ÔºåÂπ∂ÂÖÅËÆ∏Ê†πÊçÆÈöæÂ∫¶ÂíåÊï∞ÈáèËøõË°åÊéßÂà∂ÂêàÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®SynLogicÊï∞ÊçÆËøõË°åRLËÆ≠ÁªÉÔºåËÉΩÂ§üÊòæËëóÊèêÈ´òÊé®ÁêÜÊÄßËÉΩÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂äÁé∞ÊúâÁöÑÂºÄÊ∫êÊï∞ÊçÆÈõÜ„ÄÇÈÄöËøáÂ∞ÜSynLogicÊï∞ÊçÆ‰∏éÊï∞Â≠¶ÂíåÁºñÁ†Å‰ªªÂä°Ê∑∑ÂêàËÆ≠ÁªÉÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜËøô‰∫õÈ¢ÜÂüüÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊé®ÁêÜÊ≥õÂåñËÉΩÂäõ„ÄÇ', title='SynLogicÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ'))
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#agents", "#optimization", "#inference", "#benchmark"], "emoji": "üî¨", "ru": {"title": "ACBench: –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Å–∂–∞—Ç—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ACBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è —Å–∂–∞—Ç–∏—è –Ω–∞ –∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "SoloSpeech: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "SoloSpeech - —ç—Ç–æ –Ω–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –∏–∑ —Å–º–µ—Å–∏ –≥–æ–ª–æ—Å–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–∂–∞—Ç–∏–µ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#rl", "#training", "#multimodal", "#benchmark", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π MLLM —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª—è–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "Share-GRPO - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#rag", "#rl", "#reasoning", "#benchmark"], "emoji": "üîç", "ru": {"title": "AutoRefine: –£–º–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "AutoRefine - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "üéûÔ∏è", "ru": {"title": "–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤ –º–µ–∂–¥—É –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º –∏ –∫–æ
[28.05.2025 08:16] Querying the API.
[28.05.2025 08:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.
[28.05.2025 08:16] Response: {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–ë–Ø–ú) —Å–ø–æ—Å–æ–±–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –¥–≤–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ë–Ø–ú –∫ –º–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç —Å–≤–æ–π—Å—Ç–≤–∞ —ç—Ç–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä—É—é –æ–Ω–∏ –∫–æ–¥–∏—Ä—É—é—Ç. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª—è–µ—Ç, —á—Ç–æ —ç—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–∏—Ä—É—é—Ç —Å–≤—è–∑–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.",
  "emoji": "üöÄ",
  "title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –ë–Ø–ú"
}
[28.05.2025 08:16] Renaming some terms.
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space."

[28.05.2025 08:16] Response: ```python
["DATA", "MULTIMODAL", "ARCHITECTURE"]
```
[28.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space."

[28.05.2025 08:16] Response: ```python
["LONG_CONTEXT"]
```
[28.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the ability of large language models (LLMs) to generate long text segments in a single forward pass using learned embeddings. It demonstrates that LLMs can produce hundreds of accurate tokens without relying on autoregressive methods, which typically require iterative decoding. The study reveals that by using just two learned embeddings, LLMs can reconstruct extensive text, showcasing a previously underexplored capability. Additionally, the authors analyze the properties of these embeddings, suggesting that they encode meaningful information and could lead to the development of a dedicated encoder for improved text generation.","title":"Unlocking Multi-Token Generation in LLMs Without Autoregression"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the ability of large language models (LLMs) to generate long text segments in a single forward pass using learned embeddings. It demonstrates that LLMs can produce hundreds of accurate tokens without relying on autoregressive methods, which typically require iterative decoding. The study reveals that by using just two learned embeddings, LLMs can reconstruct extensive text, showcasing a previously underexplored capability. Additionally, the authors analyze the properties of these embeddings, suggesting that they encode meaningful information and could lead to the development of a dedicated encoder for improved text generation.', title='Unlocking Multi-Token Generation in LLMs Without Autoregression'))
[28.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ê≤°ÊúâËá™ÂõûÂΩíÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÈïøÊñáÊú¨ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÜªÁªìÁöÑLLMsÂèØ‰ª•‰ªÖÈÄöËøá‰∏§‰∏™Â≠¶‰π†Âà∞ÁöÑÂµåÂÖ•ÔºåÂú®‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠‰∏≠ÁîüÊàêÊï∞Áôæ‰∏™ÂáÜÁ°ÆÁöÑÊ†áËÆ∞„ÄÇËøôÊè≠Á§∫‰∫ÜLLMsÂú®Â§öÊ†áËÆ∞ÁîüÊàêÊñπÈù¢ÁöÑÊΩúÂäõÔºåË∂ÖÂá∫‰∫Ü‰º†ÁªüÁöÑËø≠‰ª£Ëß£Á†ÅÊñπÊ≥ï„ÄÇÊàë‰ª¨ËøòÂàÜÊûê‰∫ÜËøô‰∫õÂµåÂÖ•ÁöÑË°å‰∏∫ÔºåÂπ∂Êèê‰æõ‰∫ÜÂÆÉ‰ª¨ÊâÄÁºñÁ†Å‰ø°ÊÅØÁöÑËßÅËß£„ÄÇ","title":"Êé¢Á¥¢LLMsÁöÑÂ§öÊ†áËÆ∞ÁîüÊàêËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ê≤°ÊúâËá™ÂõûÂΩíÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÈïøÊñáÊú¨ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂÜªÁªìÁöÑLLMsÂèØ‰ª•‰ªÖÈÄöËøá‰∏§‰∏™Â≠¶‰π†Âà∞ÁöÑÂµåÂÖ•ÔºåÂú®‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠‰∏≠ÁîüÊàêÊï∞Áôæ‰∏™ÂáÜÁ°ÆÁöÑÊ†áËÆ∞„ÄÇËøôÊè≠Á§∫‰∫ÜLLMsÂú®Â§öÊ†áËÆ∞ÁîüÊàêÊñπÈù¢ÁöÑÊΩúÂäõÔºåË∂ÖÂá∫‰∫Ü‰º†ÁªüÁöÑËø≠‰ª£Ëß£Á†ÅÊñπÊ≥ï„ÄÇÊàë‰ª¨ËøòÂàÜÊûê‰∫ÜËøô‰∫õÂµåÂÖ•ÁöÑË°å‰∏∫ÔºåÂπ∂Êèê‰æõ‰∫ÜÂÆÉ‰ª¨ÊâÄÁºñÁ†Å‰ø°ÊÅØÁöÑËßÅËß£„ÄÇ', title='Êé¢Á¥¢LLMsÁöÑÂ§öÊ†áËÆ∞ÁîüÊàêËÉΩÂäõ'))
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#science", "#hallucinations", "#benchmark", "#dataset", "#optimization", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–π –∫—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç–∏–∫–∏", "desc": "DFIR-Metric - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞
[28.05.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#games", "#training", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "ComfyMind: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "ComfyMind - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª
[28.05.2025 08:16] Querying the API.
[28.05.2025 08:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus.
[28.05.2025 08:17] Response: {
  "desc": "R1-Searcher++ - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–ë–Ø–ú) –ø—É—Ç–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏ –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –∑–∞—Ç–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π. R1-Searcher++ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ—á–µ—Ç–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –≤–Ω–µ—à–Ω–∏–º –ø–æ–∏—Å–∫–æ–º, –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —É–ª—É—á—à–∞—è —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.",
  "emoji": "üß†",
  "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ë–Ø–ú: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è"
}
[28.05.2025 08:17] Renaming some terms.
[28.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus."

[28.05.2025 08:17] Response: ```python
['RAG', 'RL']
```
[28.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus."

[28.05.2025 08:17] Response: ```python
['REASONING', 'HALLUCINATIONS', 'OPTIMIZATION']
```
[28.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"R1-Searcher++ is a new framework that enhances Large Language Models (LLMs) by combining internal knowledge with external information through a two-stage training process. The first stage, called SFT Cold-start, helps the model learn basic formats, while the second stage uses Reinforcement Learning (RL) to improve knowledge acquisition dynamically. This RL phase encourages the model to explore and rewards it for effectively using its internal knowledge, while also allowing it to memorize and integrate new information from external sources. As a result, R1-Searcher++ significantly boosts the model\'s reasoning efficiency and performance compared to existing methods.","title":"Enhancing LLMs with Adaptive Knowledge Integration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="R1-Searcher++ is a new framework that enhances Large Language Models (LLMs) by combining internal knowledge with external information through a two-stage training process. The first stage, called SFT Cold-start, helps the model learn basic formats, while the second stage uses Reinforcement Learning (RL) to improve knowledge acquisition dynamically. This RL phase encourages the model to explore and rewards it for effectively using its internal knowledge, while also allowing it to memorize and integrate new information from external sources. As a result, R1-Searcher++ significantly boosts the model's reasoning efficiency and performance compared to existing methods.", title='Enhancing LLMs with Adaptive Knowledge Integration'))
[28.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"R1-Searcher++ÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÔºåÂ¢ûÂº∫‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËÉΩÂäõÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Êï¥ÂêàÂÜÖÈÉ®ÂíåÂ§ñÈÉ®Áü•ËØÜ„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàËøõË°åÂÜ∑ÂêØÂä®ÁöÑÂàùÂßãÊ†ºÂºèÂ≠¶‰π†ÔºåÁÑ∂ÂêéÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËøõË°åÂä®ÊÄÅÁü•ËØÜËé∑Âèñ„ÄÇÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÈááÁî®ÁªìÊûúÁõëÁù£ÔºåÈºìÂä±Ê®°ÂûãÊé¢Á¥¢ÔºåÂêåÊó∂ÂºïÂÖ•Â•ñÂä±Êú∫Âà∂‰ª•‰øÉËøõÂÜÖÈÉ®Áü•ËØÜÁöÑÂà©Áî®ÔºåÂπ∂ÁªìÂêàËÆ∞ÂøÜÊú∫Âà∂‰∏çÊñ≠Âê∏Êî∂Ê£ÄÁ¥¢Âà∞ÁöÑ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåR1-Searcher++Âú®Ê£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÊïàÁéáÂíåÊÄßËÉΩ‰∏ä‰ºò‰∫é‰πãÂâçÁöÑRAGÂíåÊé®ÁêÜÊñπÊ≥ï„ÄÇ","title":"R1-Searcher++ÔºöÊô∫ËÉΩÊï¥ÂêàÂÜÖÂ§ñÈÉ®Áü•ËØÜÁöÑÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='R1-Searcher++ÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÔºåÂ¢ûÂº∫‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËÉΩÂäõÔºåËÉΩÂ§üËá™ÈÄÇÂ∫îÂú∞Êï¥ÂêàÂÜÖÈÉ®ÂíåÂ§ñÈÉ®Áü•ËØÜ„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàËøõË°åÂÜ∑ÂêØÂä®ÁöÑÂàùÂßãÊ†ºÂºèÂ≠¶‰π†ÔºåÁÑ∂ÂêéÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËøõË°åÂä®ÊÄÅÁü•ËØÜËé∑Âèñ„ÄÇÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÈááÁî®ÁªìÊûúÁõëÁù£ÔºåÈºìÂä±Ê®°ÂûãÊé¢Á¥¢ÔºåÂêåÊó∂ÂºïÂÖ•Â•ñÂä±Êú∫Âà∂‰ª•‰øÉËøõÂÜÖÈÉ®Áü•ËØÜÁöÑÂà©Áî®ÔºåÂπ∂ÁªìÂêàËÆ∞ÂøÜÊú∫Âà∂‰∏çÊñ≠Âê∏Êî∂Ê£ÄÁ¥¢Âà∞ÁöÑ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåR1-Searcher++Âú®Ê£ÄÁ¥¢Â¢ûÂº∫Êé®ÁêÜÊïàÁéáÂíåÊÄßËÉΩ‰∏ä‰ºò‰∫é‰πãÂâçÁöÑRAGÂíåÊé®ÁêÜÊñπÊ≥ï„ÄÇ', title='R1-Searcher++ÔºöÊô∫ËÉΩÊï¥ÂêàÂÜÖÂ§ñÈÉ®Áü•ËØÜÁöÑÊ°ÜÊû∂'))
[28.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#optimization", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å—É—Å—Ç–∞
[28.05.2025 08:17] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#science", "#dataset", "#open_source", "#data", "#multimodal"], "emoji": "üß™", "ru": {"title": "CLEANMOL: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLEANMOL - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –±–æ
[28.05.2025 08:17] Querying the API.
[28.05.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.
[28.05.2025 08:17] Response: {
  "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ Retrieval Augmented Generation. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–∞—Å—Ç–æ –ø–æ–º–µ—â–∞—é—Ç –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–µ –æ—Ç—Ä—ã–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –±–æ–ª–µ–µ 60% –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–∞–∫ –º–∏–Ω–∏–º—É–º –æ–¥–∏–Ω —Å–∏–ª—å–Ω–æ –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–π –æ—Ç—Ä—ã–≤–æ–∫ —Å—Ä–µ–¥–∏ —Ç–æ–ø-10 –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Å—Å–∞–∂–µ–π. –í –∏—Ç–æ–≥–µ, —Å–ª–æ–∂–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è –æ—Ç—Ä—ã–≤–∫–æ–≤ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –ø–µ—Ä–µ–¥ —Å–ª—É—á–∞–π–Ω—ã–º –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ–º.",
  "emoji": "üîç",
  "title": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ RAG: –Ω–µ —Ç–∞–∫ —Å—Ç—Ä–∞—à–Ω–æ, –∫–∞–∫ –∫–∞–∂–µ—Ç—Å—è"
}
[28.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling."

[28.05.2025 08:17] Response: ```python
['RAG', 'BENCHMARK']
```
[28.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling."

[28.05.2025 08:17] Response: ```python
["INTERPRETABILITY", "HALLUCINATIONS"]
```
[28.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how Retrieval Augmented Generation (RAG) can improve the accuracy of large language models (LLMs) by incorporating relevant passages from an external corpus. It specifically examines the effect of positional bias, which is how LLMs prioritize information based on its location in the input prompt. The study finds that while retrieval systems aim to provide relevant information, they often inadvertently highlight distracting passages, affecting the LLM\'s performance. Ultimately, the research shows that attempts to optimize passage arrangement based on positional bias do not yield better results than random ordering, indicating a need for improved retrieval strategies.","title":"Navigating Positional Bias in Retrieval Augmented Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores how Retrieval Augmented Generation (RAG) can improve the accuracy of large language models (LLMs) by incorporating relevant passages from an external corpus. It specifically examines the effect of positional bias, which is how LLMs prioritize information based on its location in the input prompt. The study finds that while retrieval systems aim to provide relevant information, they often inadvertently highlight distracting passages, affecting the LLM's performance. Ultimately, the research shows that attempts to optimize passage arrangement based on positional bias do not yield better results than random ordering, indicating a need for improved retrieval strategies.", title='Navigating Positional Bias in Retrieval Augmented Generation'))
[28.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÂ¶Ç‰ΩïÈÄöËøáÂ∞ÜÂ§ñÈÉ®ËØ≠ÊñôÂ∫ì‰∏≠ÁöÑÊÆµËêΩÊ∑ªÂä†Âà∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊèêÁ§∫‰∏≠Êù•ÊèêÈ´òÂÖ∂ÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Êé¢ËÆ®‰∫Ü‰ΩçÁΩÆÂÅèÂ∑ÆÔºåÂç≥LLMÊ†πÊçÆ‰ø°ÊÅØÂú®ÊèêÁ§∫‰∏≠ÁöÑ‰ΩçÁΩÆ‰∏çÂêåËÄåÂä†ÊùÉÁöÑÂÄæÂêëÔºåÂ¶Ç‰ΩïÂΩ±ÂìçÂÖ∂Âà©Áî®Áõ∏ÂÖ≥ÊÆµËêΩÁöÑËÉΩÂäõ‰ª•ÂèäÂØπÂπ≤Êâ∞ÊÆµËêΩÁöÑÊïèÊÑüÊÄß„ÄÇÈÄöËøáÂú®‰∏â‰∏™Âü∫ÂáÜ‰∏äËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞Â∞ΩÁÆ°Ê£ÄÁ¥¢ÁÆ°ÈÅìÊó®Âú®Ëé∑ÂèñÁõ∏ÂÖ≥ÊÆµËêΩÔºå‰ΩÜÂç¥Á≥ªÁªüÊÄßÂú∞Â∞ÜÈ´òÂ∫¶Âπ≤Êâ∞ÁöÑÊÆµËêΩÊéíÂú®ÂâçÂàóÔºåË∂ÖËøá60%ÁöÑÊü•ËØ¢Âú®Ââç10‰∏™Ê£ÄÁ¥¢ÊÆµËêΩ‰∏≠ÂåÖÂê´Ëá≥Â∞ë‰∏Ä‰∏™È´òÂ∫¶Âπ≤Êâ∞ÁöÑÊÆµËêΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËØïÂõæÊ†πÊçÆLLM‰ΩçÁΩÆÂÅèÂ•ΩÈáçÊñ∞ÊéíÂàóÊÆµËêΩÁöÑÂ§çÊùÇÁ≠ñÁï•Âπ∂Ê≤°ÊúâÊØîÈöèÊú∫Êâì‰π±Ë°®Áé∞Êõ¥Â•Ω„ÄÇ","title":"‰ΩçÁΩÆÂÅèÂ∑ÆÂΩ±ÂìçÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁöÑÊïàÊûú"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÂ¶Ç‰ΩïÈÄöËøáÂ∞ÜÂ§ñÈÉ®ËØ≠ÊñôÂ∫ì‰∏≠ÁöÑÊÆµËêΩÊ∑ªÂä†Âà∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊèêÁ§∫‰∏≠Êù•ÊèêÈ´òÂÖ∂ÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Êé¢ËÆ®‰∫Ü‰ΩçÁΩÆÂÅèÂ∑ÆÔºåÂç≥LLMÊ†πÊçÆ‰ø°ÊÅØÂú®ÊèêÁ§∫‰∏≠ÁöÑ‰ΩçÁΩÆ‰∏çÂêåËÄåÂä†ÊùÉÁöÑÂÄæÂêëÔºåÂ¶Ç‰ΩïÂΩ±ÂìçÂÖ∂Âà©Áî®Áõ∏ÂÖ≥ÊÆµËêΩÁöÑËÉΩÂäõ‰ª•ÂèäÂØπÂπ≤Êâ∞ÊÆµËêΩÁöÑÊïèÊÑüÊÄß„ÄÇÈÄöËøáÂú®‰∏â‰∏™Âü∫ÂáÜ‰∏äËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞Â∞ΩÁÆ°Ê£ÄÁ¥¢ÁÆ°ÈÅìÊó®Âú®Ëé∑ÂèñÁõ∏ÂÖ≥ÊÆµËêΩÔºå‰ΩÜÂç¥Á≥ªÁªüÊÄßÂú∞Â∞ÜÈ´òÂ∫¶Âπ≤Êâ∞ÁöÑÊÆµËêΩÊéíÂú®ÂâçÂàóÔºåË∂ÖËøá60%ÁöÑÊü•ËØ¢Âú®Ââç10‰∏™Ê£ÄÁ¥¢ÊÆµËêΩ‰∏≠ÂåÖÂê´Ëá≥Â∞ë‰∏Ä‰∏™È´òÂ∫¶Âπ≤Êâ∞ÁöÑÊÆµËêΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËØïÂõæÊ†πÊçÆLLM‰ΩçÁΩÆÂÅèÂ•ΩÈáçÊñ∞ÊéíÂàóÊÆµËêΩÁöÑÂ§çÊùÇÁ≠ñÁï•Âπ∂Ê≤°ÊúâÊØîÈöèÊú∫Êâì‰π±Ë°®Áé∞Êõ¥Â•Ω„ÄÇ', title='‰ΩçÁΩÆÂÅèÂ∑ÆÂΩ±ÂìçÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁöÑÊïàÊûú'))
[28.05.2025 08:17] Loading Chinese text from previous data.
[28.05.2025 08:17] Renaming data file.
[28.05.2025 08:17] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 08:17] Saving new data file.
[28.05.2025 08:17] Generating page.
[28.05.2025 08:17] Renaming previous page.
[28.05.2025 08:17] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 08:17] [Experimental] Generating Chinese page for reading.
[28.05.2025 08:17] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√† x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«î y√°n m√≥ x√≠ng', 'trans': 'language model'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Âø´ÈÄü', 'pinyin': 'ku√†i s√π', 'trans': 'rapid'}, {'word': 'ÂèëÂ±ï', 'pinyin': 'fƒÅ zh«én', 'trans': 'development'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«î y√†o', 'trans': 'main'}, {'word': 'ÈÄöËøá', 'pinyin': 't≈çng gu√≤', 'trans': 'through'}, {'word': 'Â¢ûÂä†', 'pinyin': 'zƒìng jiƒÅ', 'trans': 'increase'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅn sh«î', 'trans': 'parameter'}, {'word': 'Êï∞Èáè', 'pinyin': 'sh√π li√†ng', 'trans': 'quantity'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'Á°¨‰ª∂', 'pinyin': 'y√¨ng ji√†n', 'trans': 'hardware'}, {'word': 'ÈôêÂà∂', 'pinyin': 'xi√†n zh√¨', 'trans': 'limit'}, {'word': 'Ëá™Ê≥®ÊÑèÂäõ', 'pinyin': 'z√¨ zh√π y√¨ l√¨', 'trans': 'self-attention'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ng bƒõn', 'trans': 'cost'}, {'word': 'Áì∂È¢à', 'pinyin': 'p√≠ng l√≥ng', 'trans': 'bottleneck'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÈáçÁÇπ', 'pinyin': 'zh√≤ng di«én', 'trans': 'focus'}, {'word': 'Ê®°ÂûãÂéãÁº©', 'pinyin': 'm√≥ x√≠ng yƒÅ su≈ç', 'trans': 'model compression'}, {'word': 'ËΩ¨Âêë', 'pinyin': 'zhu«én xi√†ng', 'trans': 'turn to'}, {'word': 'Êï∞ÊçÆÂéãÁº©', 'pinyin': 'sh√π j√π yƒÅ su≈ç', 'trans': 'data compression'}, {'word': '‰ª§Áâå', 'pinyin': 'l√¨ng p√°i', 'trans': 'token'}, {'word': 'ÂéãÁº©', 'pinyin': 'yƒÅ su≈ç', 'trans': 'compression'}, {'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«én sh«éo', 'trans': 'reduce'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'training'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'inference'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ ch√©ng', 'trans': 'process'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†o l«ú', 'trans': 'efficiency'}, {'word': '‰ΩúËÄÖ', 'pinyin': 'zu√≤ zhƒõ', 'trans': 'author'}, {'word': 'ÂàÜÊûê', 'pinyin': 'fƒìn xƒ´', 'trans': 'analyze'}, {'word': 'Èïø‰∏ä‰∏ãÊñá', 'pinyin': 'ch√°ng sh√†ng xi√† w√©n', 'trans': 'long context'}, {'word': 'Êï∞Â≠¶Ê°ÜÊû∂', 'pinyin': 'sh√π xu√© ku√†ng ji√†', 'trans': 'mathematical framework'}, {'word': 'Êé¢ËÆ®', 'pinyin': 't√†n t«éo', 'trans': 'explore'}, {'word': '‰ºòÂäø', 'pinyin': 'y≈çu sh√¨', 'trans': 'advantage'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}]
[28.05.2025 08:17] Renaming previous Chinese page.
[28.05.2025 08:17] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 08:17] Writing Chinese reading task.
[28.05.2025 08:17] Writing result.
[28.05.2025 08:17] Renaming log file.
[28.05.2025 08:17] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
