[28.05.2025 15:12] Read previous papers.
[28.05.2025 15:12] Generating top page (month).
[28.05.2025 15:12] Writing top page (month).
[28.05.2025 16:12] Read previous papers.
[28.05.2025 16:12] Get feed.
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19897
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19641
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21189
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21334
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17952
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14064
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21505
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21493
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16901
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20561
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20426
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21178
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19433
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20321
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17005
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11277
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19973
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18657
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18134
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17908
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16673
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21499
[28.05.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2505.20287
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20286
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19650
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19377
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17190
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21471
[28.05.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2505.20793
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20162
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20052
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20036
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19954
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17855
[28.05.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2505.17639
[28.05.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15561
[28.05.2025 16:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 16:13] No deleted papers detected.
[28.05.2025 16:13] Downloading and parsing papers (pdf, html). Total: 63.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19897.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19897.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19897.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19641.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19641.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19641.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21189.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21189.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21189.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21496.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21496.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21334.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21334.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21334.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.17952.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.17952.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.17952.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.14064.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.14064.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.14064.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21505.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21505.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21505.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21500.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21500.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21493.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21493.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21493.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.16901.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.16901.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.16901.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20561.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20561.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20561.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20426.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20426.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20426.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21178.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21178.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21178.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19433.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19433.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19433.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20321.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20321.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20321.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.17005.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.17005.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.17005.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.11277.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.11277.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.11277.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19973.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19973.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19973.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.18657.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.18657.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.18657.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.18134.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.18134.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.18134.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.17908.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.17908.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.17908.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.16673.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.16673.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.16673.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21499.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21499.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21499.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20287.
[28.05.2025 16:13] Downloading paper 2505.20287 from http://arxiv.org/pdf/2505.20287v1...
[28.05.2025 16:13] Extracting affiliations from text.
[28.05.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MotionPro: Precise Motion Controller for Image-to-Video Generation* Zhongwei Zhang1, Fuchen Long2, Zhaofan Qiu2, Yingwei Pan2, Wu Liu1, Ting Yao2, and Tao Mei2 1University of Science and Technology of China 2HiDream.ai Inc. zhwzhang@mail.ustc.edu.cn, {longfuchen, qiuzhaofan, pandy}@hidream.ai liuwu@live.cn, {tiyao, tmei}@hidream.ai 5 2 0 M 6 2 ] . [ 1 7 8 2 0 2 . 5 0 5 2 : r a "
[28.05.2025 16:13] Response: ```python
["University of Science and Technology of China", "HiDream.ai Inc."]
```
[28.05.2025 16:13] Deleting PDF ./assets/pdf/2505.20287.pdf.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20286.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20286.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20286.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19650.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19650.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19650.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19377.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19377.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19377.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.17190.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.17190.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.17190.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.16340.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.16340.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.21471.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.21471.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.21471.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20793.
[28.05.2025 16:13] Downloading paper 2505.20793 from http://arxiv.org/pdf/2505.20793v1...
[28.05.2025 16:13] Extracting affiliations from text.
[28.05.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 3 9 7 0 2 . 5 0 5 2 : r Rendering-Aware Reinforcement Learning for Vector Graphics Generation , Haotian Zhang5,, Abhay Puri1, Aarash Feizi1,2,11, Juan A. Rodriguez1,2,3, Rishav Pramanik7, Pascal Wichmann6, Arnab Mondal2,8, Mohammad Reza Samsami2,9 Rabiul Awal1,2, Perouz Taslakian1, Spandana Gella1, Sai Rajeswar1,2 David Vazquez1, Christopher Pal1,2,4,10, Marco Pedersoli1,2,3 1ServiceNow Research 2Mila 3ÉTS Montréal 4Polytechnique Montréal 5Columbia University 6Independent Scholar 7Stony Brook University 8Apple 9Google Research 10Canada CIFAR AI Chair 11McGill University "
[28.05.2025 16:13] Response: ```python
[
    "ServiceNow Research",
    "Mila",
    "ÉTS Montréal",
    "Polytechnique Montréal",
    "Columbia University",
    "Independent Scholar",
    "Stony Brook University",
    "Apple",
    "Google Research",
    "Canada CIFAR AI Chair",
    "McGill University"
]
```
[28.05.2025 16:13] Deleting PDF ./assets/pdf/2505.20793.pdf.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20162.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20162.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20162.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20052.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20052.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20052.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.20036.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.20036.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.20036.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.19954.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.19954.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.19954.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.17855.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.17855.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.17855.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.17639.
[28.05.2025 16:13] Downloading paper 2505.17639 from http://arxiv.org/pdf/2505.17639v1...
[28.05.2025 16:13] Extracting affiliations from text.
[28.05.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 9 3 6 7 1 . 5 0 5 2 : r PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval Zehua Pei1, Ying Zhang2, Hui-Ling Zhen2, Xianzhi Yu2, Wulong Liu2, Sinno Jialin Pan1, Mingxuan Yuan2, Bei Yu1 1The Chinese University of Hong Kong 2Noahs Ark Lab, Huawei "
[28.05.2025 16:13] Response: ```python
["The Chinese University of Hong Kong", "Noahs Ark Lab, Huawei"]
```
[28.05.2025 16:13] Deleting PDF ./assets/pdf/2505.17639.pdf.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2505.15561.
[28.05.2025 16:13] Extra JSON file exists (./assets/json/2505.15561.json), skip PDF parsing.
[28.05.2025 16:13] Paper image links file exists (./assets/img_data/2505.15561.json), skip HTML parsing.
[28.05.2025 16:13] Success.
[28.05.2025 16:13] Enriching papers with extra data.
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 0. ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  					AI-generated summary 				 Large Language Models (LLMs) have extended their impact beyond Natural...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 1. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 2. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 3. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 4. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 5. SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have de...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 6. LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up t...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 7. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 8. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 9. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 10. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 11. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 12. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 13. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 14. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 15. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 16. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 17. HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at vi...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 18. Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled fro...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 19. NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.  					AI-generated summary 				 In many real-world applications, deployed models encounter inputs that differ from th...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 20. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 21. The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 22. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 23. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 24. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 25. The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verifica...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 26. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 27. Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown pro...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 28. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 29. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 30. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 31. BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) ha...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 32. Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understandin...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 33. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 34. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 35. As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 36. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 37. Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ig...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 38. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 39. BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.  					AI-generated summary 				 Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However,...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 40. R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinati...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 41. AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  					AI-generated summary 				 Large language models have ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 42. DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 43. MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.  					AI-generated summary 				 Recent advances in Multimodal Large Language Models (MLLMs) hav...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 44. VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  					AI-generated summary 				 Vision-language models (VLMs) have achieved strong results on coding and ma...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 45. ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  					AI-generated summary 				 With the rapid advan...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 46. Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 47. AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 48. MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  					AI-generated summary 				 Animating images with interactive motion control has garnered popularity for image-to-video (I2V) gen...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 49. Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.  					AI-generated summary 				 Recent advances in large language models (LLMs) have enabled agents to autono...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 50. UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent c...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 51. Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-a...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 52. Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 53. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 54. The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.  					AI-generated summary 				 With the rapid advancement of post-training techniques for reasoning and informa...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 55. RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  					AI-generated summary 				 Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 56. Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  					AI-generated summary 				 As large language models grow in capability and a...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 57. A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.  					AI-generated summary 				 Protein language models (PLMs) have emerged as powerful tools to detect comple...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 58. The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatena...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 59. A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.  					AI-generated summary 				 The differential diagnosis of neurodegenerative dementias is a challeng...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 60. CLUE generates natural language explanations for a language model's uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.  					AI-generated summary 				 Understanding sources of a model's ...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 61. Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud se...
[28.05.2025 16:13] ********************************************************************************
[28.05.2025 16:13] Abstract 62. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capabilit...
[28.05.2025 16:13] Read previous papers.
[28.05.2025 16:13] Generating reviews via LLM API.
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#science", "#multimodal", "#agents", "#benchmark"], "emoji": "🧪", "ru": {"title": "ScienceBoard: реалистичная среда для оценки ИИ-агентов в научных задачах", "desc": "ScienceBoard представляет собой реалистичную среду для научных рабочих процессов и бенчмарк для оценки производитель
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "🧠", "ru": {"title": "Раскрывая пробелы в логике искусственного интеллекта", "desc": "MME-Reasoning - это новый комплексный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) к логическому рассуждению.
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "🖼️", "ru": {"title": "Автоматическая генерация научных постеров: от статьи к визуализации", "desc": "Эта статья представляет первый эталонный тест и набор метрик для генерации академических постеров, сопос
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "🎨", "ru": {"title": "Универсальная согласованность стиля в генерации изображений", "desc": "OmniConsistency - это универсальный плагин для улучшения согласованности стилизации в задачах преобразования изобра
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "🎬", "ru": {"title": "OpenS2V-Nexus: Революция в генерации видео на основе заданного содержания", "desc": "Статья представляет OpenS2V-Nexus - инфраструктуру для генерации видео на основе заданного содержания
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#training", "#rl", "#dataset", "#reasoning", "#open_source"], "emoji": "🧠", "ru": {"title": "SynLogic: прорыв в обучении ИИ логическому мышлению", "desc": "SynLogic - это фреймворк для синтеза данных, который улучшает способности больших языковых моделей (LLM) к логическому рассужде
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#data", "#long_context", "#architecture", "#multimodal"], "emoji": "🚀", "ru": {"title": "Мгновенная генерация длинных текстов с помощью LLM", "desc": "Исследование показывает, что большие языковые модели (LLM) способны генерировать длинные тексты в один проход, используя только два 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "🎥", "ru": {"title": "VerIPO: Улучшение рассуждений видео-LLM с помощью верификатора", "desc": "Статья представляет метод VerIPO для улучшения способностей видео-LLM к рассуждениям. Метод использует Verif
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "⚡", "ru": {"title": "Короче мысль - быстрее вывод: оптимизация рассуждений в LLM", "desc": "Статья исследует эффективность коротких цепочек рассуждений в крупных языковых моделях (LLM) для задач рассуждения. Авторы предлагают метод 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "🧠", "ru": {"title": "MMMR: Новый стандарт оценки мультимодального мышления ИИ", "desc": "Статья представляет новый бенчмарк MMMR для оценки мультимодального рассуждения в крупных языковых моделях (MLLM). MMMR включает набор данных
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#synthetic", "#open_source", "#dataset", "#agents", "#training", "#data"], "emoji": "🧞", "ru": {"title": "UI-Genie: самообучающийся ИИ-помощник для графических интерфейсов", "desc": "UI-Genie - это самосовершенствующаяся система для агентов графическог
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "🎞️", "ru": {"title": "Семантическая оптимизация для быстрой и качественной генерации видео", "desc": "SVG2 - это фреймворк для улучшения эффективности и качества генерации видео без дополнительного обучения. Он использу
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "🎥", "ru": {"title": "Ограничения мультимодальных моделей в задаче OCR на видео", "desc": "Мультимодальные большие языковые модели (MLLM) показывают невысокую точность в задаче оптического распознавания символов
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "🧩", "ru": {"title": "GraLoRA: Гранулярная низкоранговая адаптация для эффективной настройки генеративных моделей", "desc": "Статья представляет новый метод адаптации моделей машинного обучения под названием Granular L
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "🕵️", "ru": {"title": "Шерлок Холмс для ИИ: новый вызов в понимании видео", "desc": "Video-Holmes - это новый бенчмарк для оценки способностей мультимодальных языковых моделей к сложным рассуждениям на основе видео. Он ис
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "🧠", "ru": {"title": "rStar-Coder: прорыв в обучении языковых моделей рассуждениям о коде", "desc": "Исследователи представили rStar-Coder - крупномасштабный датасет для улучшения способностей языковых моделей (
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "🧠", "ru": {"title": "MetaMind: Искусственный интеллект с человеческим социальным пониманием", "desc": "MetaMind - это многоагентная система, улучшающая способность больших языковых моделей выполнять задачи
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#inference", "#video", "#optimization"], "emoji": "🎬", "ru": {"title": "HoliTom: Революционное сжатие токенов для эффективных видео-LLM", "desc": "HoliTom - это новый фреймворк для эффективного сжатия токенов в видео-LLM моделях. Он сочетает внешнюю обрезку LLM через глобальную врем
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#healthcare", "#reasoning", "#benchmark", "#rl", "#dataset", "#interpretability"], "emoji": "🧠", "ru": {"title": "Революция в обучении медицинских ИИ: рассуждение без явных инструкций", "desc": "Статья представляет AlphaMed - первую медицинскую модель большого языка (LLM), демонстри
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#healthcare", "#cv", "#reasoning", "#benchmark", "#interpretability"], "emoji": "🧠", "ru": {"title": "NOVA: Экстремальный тест на обобщение ИИ в медицинской визуализации", "desc": "NOVA - это новый бенчмарк для оценки мультимодальных моделей на редких патологиях МРТ головного мозга.
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "🖼️", "ru": {"title": "ImgEdit: прорыв в редактировании изображений с помощью ИИ", "desc": "Исследователи представили ImgEdit - крупномасштабный набор данных для редактирования изображений, содержащи
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#alignment"], "emoji": "🌐", "ru": {"title": "Раскрытие тайн многоязычности в нейронах LLM", "desc": "Исследование предлагает алгоритм более точной идентификации нейронов для обнаружения языково-специфичных и языково-агностических нейронов в больших 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "👁️", "ru": {"title": "ACTIVE-O3: Наделение MLLM активным восприятием для эффективного принятия решений", "desc": "Статья представляет ACTIVE-O3 - фреймворк обучения с подкреплением для наделения 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "🎬", "ru": {"title": "Управляемая генерация видео: новый уровень контроля над объектами в кадре", "desc": "Статья посвящена улучшению контролируемости, временной согласованности и детализации в ген
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#3d", "#games"], "emoji": "🧠", "ru": {"title": "Новый рубеж в пространственном интеллекте моделей компьютерного зрения", "desc": "Статья представляет новый бенчмарк ViewSpatial-Bench для оценки способностей моделей компьютерного зрения к пространст
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#math", "#rl", "#benchmark", "#training", "#reasoning"], "emoji": "🧠", "ru": {"title": "VeriFree: обучение LLM без верификатора для универсальных рассуждений", "desc": "Статья представляет новый метод обучения больших языковых моделей (LLM) под названием VeriFree. Э
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "🎯", "ru": {"title": "Точное управление языковыми моделями через атомарные компоненты знаний", "desc": "Статья представляет новый метод под названием Steering Target Atoms (STA) для точного контроля над генерацией
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#games", "#architecture", "#open_source"], "emoji": "🧠", "ru": {"title": "Графовые модели кода: новый уровень генерации в открытых ИИ-системах", "desc": "Статья представляет Code Graph Models (CGM) - новый подход к генерации кода на уровне ре
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "🖼️", "ru": {"title": "Эффективная генерация изображений от общего к частному", "desc": "DetailFlow - это новый метод генерации изображений, использующий авторегрессионный подход с последовательным уточнением 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "🔬", "ru": {"title": "SeePhys: выявление ограничений визуального мышления LLM в физике", "desc": "SeePhys - это новый мультимодальный бенчмарк для оценки способностей больших языковых моделей (LLM) в обл
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "🎯", "ru": {"title": "Усовершенствованная атака на мультимодальные языковые модели через оптимальное выравнивание признаков", "desc": "Статья представляет новый метод атаки на мультимодальные языковые модели, называемый FOA-Attack. О
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#reasoning"], "emoji": "🧠", "ru": {"title": "BARL: Умнее исследуем, эффективнее рассуждаем", "desc": "BARL - это новая система Байесовского адаптивного обучения с подкреплением для улучшения работы больших языковых моделей. Она интегриру
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#3d", "#cv", "#interpretability", "#multimodal", "#reasoning", "#benchmark"], "emoji": "🔬", "ru": {"title": "MMPerspective: новый рубеж в оценке пространственного понимания ИИ", "desc": "Статья представляет MMPerspective - первый бенчмарк для оценки понимания перспективы мультимодал
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "🧠", "ru": {"title": "VisTA: Автономное улучшение визуального мышления с помощью обучения с подкреплением", "desc": "VisTA - это новая система обучения с подкреплением для улучшения визуального мышления. 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "🎞️", "ru": {"title": "Симметричное внедрение граничных кадров для улучшенного синтеза видео", "desc": "Статья представляет новый подход к синтезу промежуточных видеокадров между заданными начальным и ко
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#rl", "#reasoning", "#benchmark"], "emoji": "🧠", "ru": {"title": "Краткость - сестра таланта: новый метод для улучшения рассуждений ИИ", "desc": "Исследователи предложили новый метод обучения языковых моделей для улучшения их способности к расс
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "🎞️", "ru": {"title": "Ускорение генерации длинных видео с помощью распределенного вывода", "desc": "Статья представляет новую стратегию распределенного вывода для видео-диффузионных моделей на основе Diffusion Transfor
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#agents", "#optimization", "#inference", "#benchmark"], "emoji": "🔬", "ru": {"title": "ACBench: первый комплексный бенчмарк для оценки агентных способностей сжатых LLM", "desc": "Статья представляет новый бенчмарк ACBench для оценки влияния сжатия на агентные способности больших язы
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#audio"], "emoji": "🎙️", "ru": {"title": "SoloSpeech: генеративное извлечение целевой речи нового поколения", "desc": "SoloSpeech - это новый генеративный подход к извлечению целевой речи из смеси голосов. Он использует каскадный пайплайн, включающий сжатие, извлечение, реконструкци
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#open_source", "#science", "#dataset", "#multimodal", "#benchmark", "#reasoning"], "emoji": "🧬", "ru": {"title": "Оценка научного мышления в тексто-SQL задачах для биомедицины", "desc": "BiomedSQL - это новый эталонный тест для оценки научного мышления в задачах преобразования текст
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#hallucinations", "#optimization", "#rl", "#reasoning", "#rag"], "emoji": "🧠", "ru": {"title": "Адаптивное обучение LLM: объединяем внутренние и внешние знания", "desc": "R1-Searcher++ - это новая система, улучшающая работу больших языковых моделей (LLM) путем адаптивной интеграции 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#rag", "#rl", "#reasoning", "#benchmark"], "emoji": "🔍", "ru": {"title": "AutoRefine: Умное уточнение знаний для улучшения рассуждений ИИ", "desc": "AutoRefine - это система обучения с подкреплением для больших языковых моделей, которая улучшает рассуждения на основ
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#science", "#hallucinations", "#benchmark", "#dataset", "#optimization", "#reasoning"], "emoji": "🔍", "ru": {"title": "Комплексная оценка языковых моделей для цифровой криминалистики", "desc": "DFIR-Metric - это комплексный инструмент для оценки больших языковых моделей (LLM) в обла
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#interpretability", "#training", "#agi", "#multimodal", "#architecture"], "emoji": "🔬", "ru": {"title": "Преодоление языкового доминирования в мультимодальных ИИ-системах", "desc": "Статья исследует проблему модальной предвзятости в мультимодальных больших языковых моделях (MLLM), к
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#games", "#cv", "#multimodal", "#benchmark", "#video"], "emoji": "🎮", "ru": {"title": "Видеоигры как вызов для искусственного интеллекта", "desc": "VideoGameBench - это новый бенчмарк для оценки способностей моделей машинного зрения и обработки естественного языка в контексте взаимо
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#games", "#training", "#reasoning", "#benchmark"], "emoji": "🧠", "ru": {"title": "ComfyMind: Улучшение генеративных рабочих процессов с помощью семантического интерфейса и адаптивного планирования", "desc": "ComfyMind - это система искусственного интел
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#rl", "#training", "#multimodal", "#benchmark", "#optimization", "#reasoning"], "emoji": "🧠", "ru": {"title": "Усиление рассуждений MLLM через разделяемое обучение с подкреплением", "desc": "Share-GRPO - это новый подход в области обучения с подкреплением, который улучшает мультимод
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#cv", "#agents", "#security"], "emoji": "🕵️", "ru": {"title": "Реклама как оружие: новая угроза для ИИ-агентов в сети", "desc": "AdInject - это новый метод атаки на веб-агентов, основанных на мультимодальных языковых моделях, с использованием интернет-рекламы для внедрения вредоносн
[28.05.2025 16:13] Querying the API.
[28.05.2025 16:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  					AI-generated summary 				 Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/.
[28.05.2025 16:13] Response: {
  "desc": "MotionPro - это новый метод генерации видео из изображений, использующий траектории движения по регионам и маски движения. Он позволяет более точно контролировать движение и разделять движение объектов и камеры. MotionPro оценивает карты потока для каждого обучающего видео и использует траектории по регионам вместо больших гауссовых ядер. Метод также включает маску движения для захвата общей динамики движущихся областей.",
  "emoji": "🎬",
  "title": "Точный контроль движения при генерации видео из изображений"
}
[28.05.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  					AI-generated summary 				 Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/."

[28.05.2025 16:13] Response: ```python
["VIDEO", "BENCHMARK"]
```
[28.05.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  					AI-generated summary 				 Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/."

[28.05.2025 16:13] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[28.05.2025 16:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MotionPro is a novel approach for image-to-video generation that enhances motion control by using region-wise trajectories and motion masks. This method allows for precise regulation of motion synthesis, effectively distinguishing between object and camera movements. Instead of relying on large Gaussian kernels, MotionPro utilizes localized trajectories to achieve fine-grained control over motion dynamics. Additionally, it incorporates a motion mask to improve video denoising and has been evaluated using a newly created benchmark dataset, MC-Bench, demonstrating its effectiveness in motion control tasks.","title":"Precision in Motion Control for Image-to-Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MotionPro is a novel approach for image-to-video generation that enhances motion control by using region-wise trajectories and motion masks. This method allows for precise regulation of motion synthesis, effectively distinguishing between object and camera movements. Instead of relying on large Gaussian kernels, MotionPro utilizes localized trajectories to achieve fine-grained control over motion dynamics. Additionally, it incorporates a motion mask to improve video denoising and has been evaluated using a newly created benchmark dataset, MC-Bench, demonstrating its effectiveness in motion control tasks.', title='Precision in Motion Control for Image-to-Video Generation'))
[28.05.2025 16:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MotionPro 是一种精确的运动控制器，利用区域轨迹和运动掩码来改善图像到视频的生成。它通过估计每个训练视频的流动图，并从中采样区域轨迹，来实现细粒度的运动合成。与传统方法不同，MotionPro 直接使用局部区域内的轨迹，从而实现更精确的运动控制。该方法还通过特征调制增强视频去噪，构建了一个基准数据集 MC-Bench，用于评估细粒度和对象级的运动控制效果。","title":"精确运动控制，提升图像到视频生成"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MotionPro 是一种精确的运动控制器，利用区域轨迹和运动掩码来改善图像到视频的生成。它通过估计每个训练视频的流动图，并从中采样区域轨迹，来实现细粒度的运动合成。与传统方法不同，MotionPro 直接使用局部区域内的轨迹，从而实现更精确的运动控制。该方法还通过特征调制增强视频去噪，构建了一个基准数据集 MC-Bench，用于评估细粒度和对象级的运动控制效果。', title='精确运动控制，提升图像到视频生成'))
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#open_source", "#agi", "#agents", "#benchmark", "#reasoning"], "emoji": "🤖", "ru": {"title": "Простота - ключ к универсальному ИИ-агенту", "desc": "Алита - это агент широкого профиля, достигающий высокой производительности на различных тестах благодаря минимальному предопределению и
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#training", "#multimodal"], "emoji": "🔍", "ru": {"title": "UNITE: Преодоление разрыва между модальностями в мультимодальном поиске", "desc": "Статья представляет UNITE - универсальную систему для мультимодального информационного поиска. Авторы реша
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#optimization", "#cv"], "emoji": "🤖", "ru": {"title": "Глобальные координаты для улучшения генерации движений из текста", "desc": "В статье предлагается новый подход к генерации движений на основе текста, использующий абсолютные координаты суста
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#security", "#math", "#reasoning"], "emoji": "🌴", "ru": {"title": "Тропическое внимание: острое масштабно-инвариантное рассуждение для нейроалгоритмических задач", "desc": "В статье представлен новый метод внимания под названием 'Тропи
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#science", "#dataset", "#open_source", "#data", "#multimodal"], "emoji": "🧪", "ru": {"title": "CLEANMOL: Улучшение понимания молекул языковыми моделями", "desc": "Статья представляет CLEANMOL - новый фреймворк для улучшения понимания молекулярных структур бо
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#long_context", "#inference", "#agents", "#benchmark", "#reasoning"], "emoji": "🤖", "ru": {"title": "ExtAgents: Расширение возможностей LLM без увеличения контекстного окна", "desc": "Статья представляет новую систему ExtAgents, которая улучшает масштабируемость интеграции знаний во
[28.05.2025 16:13] Querying the API.
[28.05.2025 16:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  					AI-generated summary 				 Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.
[28.05.2025 16:13] Response: {
  "desc": "Метод RLRF использует обучение с подкреплением для улучшения генерации SVG в моделях компьютерного зрения и языка (VLM). Он сравнивает отрендеренные SVG с исходными изображениями для вычисления награды. Это позволяет модели создавать более точные, эффективные и семантически согласованные SVG. RLRF значительно превосходит обычное обучение с учителем, решая распространенные проблемы и обеспечивая высококачественную генерацию SVG.",
  "emoji": "🎨",
  "title": "RLRF: Улучшение генерации SVG через обратную связь от рендеринга"
}
[28.05.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  					AI-generated summary 				 Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization."

[28.05.2025 16:13] Response: ```python
['RL', 'RAG', 'CV']
```
[28.05.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  					AI-generated summary 				 Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization."

[28.05.2025 16:13] Response: ```python
["GAMES", "TRANSFER_LEARNING"]
```
[28.05.2025 16:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents RLRF, a novel reinforcement learning method that improves the generation of Scalable Vector Graphics (SVG) using rendering feedback. By comparing rendered SVG outputs to original images, RLRF provides evaluative feedback that enhances the model\'s ability to produce accurate and efficient SVGs. This approach leverages the strengths of vision-language models (VLMs) to capture both global semantics and detailed visual patterns, addressing limitations of previous methods that lacked direct rendering feedback. Ultimately, RLRF demonstrates superior performance over traditional supervised fine-tuning, leading to high-quality SVG generation with better structural understanding and generalization capabilities.","title":"Enhancing SVG Generation with Reinforcement Learning from Rendering Feedback"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper presents RLRF, a novel reinforcement learning method that improves the generation of Scalable Vector Graphics (SVG) using rendering feedback. By comparing rendered SVG outputs to original images, RLRF provides evaluative feedback that enhances the model's ability to produce accurate and efficient SVGs. This approach leverages the strengths of vision-language models (VLMs) to capture both global semantics and detailed visual patterns, addressing limitations of previous methods that lacked direct rendering feedback. Ultimately, RLRF demonstrates superior performance over traditional supervised fine-tuning, leading to high-quality SVG generation with better structural understanding and generalization capabilities.", title='Enhancing SVG Generation with Reinforcement Learning from Rendering Feedback'))
[28.05.2025 16:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RLRF是一种利用渲染反馈的强化学习方法，旨在提高视觉语言模型（VLMs）生成可缩放矢量图形（SVG）的能力。通过将生成的SVG与原始图像进行比较，RLRF能够提供视觉保真度反馈，从而指导模型生成更准确和高效的SVG。该方法解决了现有VLM在训练过程中未观察渲染图像的问题，显著提升了SVG生成的质量和结构理解能力。RLRF在性能上超越了传统的监督微调方法，展现出更强的泛化能力。","title":"利用渲染反馈提升SVG生成的强化学习方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RLRF是一种利用渲染反馈的强化学习方法，旨在提高视觉语言模型（VLMs）生成可缩放矢量图形（SVG）的能力。通过将生成的SVG与原始图像进行比较，RLRF能够提供视觉保真度反馈，从而指导模型生成更准确和高效的SVG。该方法解决了现有VLM在训练过程中未观察渲染图像的问题，显著提升了SVG生成的质量和结构理解能力。RLRF在性能上超越了传统的监督微调方法，展现出更强的泛化能力。', title='利用渲染反馈提升SVG生成的强化学习方法'))
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#security", "#alignment"], "emoji": "🛡️", "ru": {"title": "Превосходство в возможностях - ключ к безопасности ИИ", "desc": "Исследование показывает, что эффективность атак на большие языковые модели (LLM) резко снижается, когда возможности целевой модели прево
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#healthcare", "#dataset", "#science"], "emoji": "🧬", "ru": {"title": "Многозадачное обучение раскрывает потенциал белковых языковых моделей", "desc": "Исследователи разработали новую стратегию предварительного обучения белковых языковых моделей (PLM) с 
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#leakage", "#architecture", "#dataset", "#science"], "emoji": "🧬", "ru": {"title": "Улучшение предсказания аффинности белок-белковых взаимодействий с помощью продвинутых архитектур языковых моделей", "desc": "Исследование представляет курированный набор
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#3d", "#interpretability", "#cv", "#healthcare", "#multimodal", "#rl", "#reasoning"], "emoji": "🧠", "ru": {"title": "Прозрачная нейродиагностика: ИИ с обоснованием", "desc": "Статья представляет фреймворк для улучшения диагностической ясности моделей глубокого обучения при нейродеге
[28.05.2025 16:13] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#training", "#reasoning", "#data"], "emoji": "🔍", "ru": {"title": "CLUE: Прозрачные объяснения неопределенности языковых моделей", "desc": "CLUE - это новый метод генерации объяснений неопределенности языковых моделей на естественном языке. Он выя
[28.05.2025 16:13] Querying the API.
[28.05.2025 16:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\% expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning (87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and 81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe.
[28.05.2025 16:14] Response: {
  "desc": "Исследование представляет PreMoe - новую систему для эффективного развертывания крупных моделей смеси экспертов (MoE) в условиях ограниченной памяти. PreMoe включает вероятностную обрезку экспертов (PEP) и адаптивное извлечение экспертов под задачу (TAER). PEP использует новую метрику TCESS для определения важности экспертов, а TAER предварительно вычисляет и хранит компактные паттерны экспертов для различных задач. Эксперименты показывают, что PreMoe позволяет значительно сократить объем памяти при сохранении высокой точности на различных задачах.",
  "emoji": "🧠",
  "title": "Эффективное развертывание гигантских языковых моделей в условиях ограниченной памяти"
}
[28.05.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\% expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning (87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and 81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe."

[28.05.2025 16:14] Response: ```python
['ARCHITECTURE', 'INFERENCE', 'TRAINING']
```
[28.05.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\% expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning (87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and 81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe."

[28.05.2025 16:14] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[28.05.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents PreMoe, a framework designed to efficiently deploy large Mixture-of-Experts (MoE) models in environments with limited memory. It introduces two key components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER), which work together to optimize expert selection based on task-specific needs. PEP uses a new metric to determine the importance of experts for particular tasks, allowing for a significant reduction in the number of experts needed. TAER enhances inference speed by pre-computing expert patterns, ensuring that only the most relevant experts are loaded for each user query, thus minimizing memory usage while maintaining high accuracy.","title":"Efficiently Scaling MoE Models with PreMoe"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents PreMoe, a framework designed to efficiently deploy large Mixture-of-Experts (MoE) models in environments with limited memory. It introduces two key components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER), which work together to optimize expert selection based on task-specific needs. PEP uses a new metric to determine the importance of experts for particular tasks, allowing for a significant reduction in the number of experts needed. TAER enhances inference speed by pre-computing expert patterns, ensuring that only the most relevant experts are loaded for each user query, thus minimizing memory usage while maintaining high accuracy.', title='Efficiently Scaling MoE Models with PreMoe'))
[28.05.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"混合专家（MoE）架构可以在不显著增加计算成本的情况下，扩展大型语言模型（LLM）的参数数量。然而，大型MoE模型的内存需求使其在各种计算环境中的部署受到限制。本文首先展示了MoE层中专家激活模式的任务特定专业化。基于此，我们提出了PreMoe框架，通过概率专家修剪（PEP）和任务自适应专家检索（TAER）来实现大规模MoE模型在内存受限环境中的高效部署。","title":"高效部署大规模MoE模型的创新方案"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='混合专家（MoE）架构可以在不显著增加计算成本的情况下，扩展大型语言模型（LLM）的参数数量。然而，大型MoE模型的内存需求使其在各种计算环境中的部署受到限制。本文首先展示了MoE层中专家激活模式的任务特定专业化。基于此，我们提出了PreMoe框架，通过概率专家修剪（PEP）和任务自适应专家检索（TAER）来实现大规模MoE模型在内存受限环境中的高效部署。', title='高效部署大规模MoE模型的创新方案'))
[28.05.2025 16:14] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#benchmark", "#hallucinations"], "emoji": "🔍", "ru": {"title": "Позиционное смещение в RAG: не так страшно, как кажется", "desc": "Данная статья исследует влияние позиционного смещения на точность языковых моделей в контексте Retrieval Augmented Generati
[28.05.2025 16:14] Loading Chinese text from previous data.
[28.05.2025 16:14] Renaming data file.
[28.05.2025 16:14] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 16:14] Saving new data file.
[28.05.2025 16:14] Generating page.
[28.05.2025 16:14] Renaming previous page.
[28.05.2025 16:14] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 16:14] [Experimental] Generating Chinese page for reading.
[28.05.2025 16:14] Chinese vocab [{'word': 'OmniConsistency', 'pinyin': '', 'trans': 'OmniConsistency'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '变压器', 'pinyin': 'biàn yā qì', 'trans': 'transformer'}, {'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'}, {'word': '风格', 'pinyin': 'fēng gé', 'trans': 'style'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '泛化', 'pinyin': 'fàn huà', 'trans': 'generalization'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '退化', 'pinyin': 'tuì huà', 'trans': 'degeneration'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '两阶段', 'pinyin': 'liǎng jiē duàn', 'trans': 'two-stage'}, {'word': '渐进', 'pinyin': 'jiàn jìn', 'trans': 'progressive'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '插播', 'pinyin': 'chā bō', 'trans': 'interpolation'}, {'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '接近', 'pinyin': 'jiē jìn', 'trans': 'approach'}, {'word': '商业', 'pinyin': 'shāng yè', 'trans': 'commercial'}, {'word': '顶尖', 'pinyin': 'dǐng jiān', 'trans': 'top-notch'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': 'GPT-4o', 'pinyin': '', 'trans': 'GPT-4o'}]
[28.05.2025 16:14] Renaming previous Chinese page.
[28.05.2025 16:14] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 16:14] Writing Chinese reading task.
[28.05.2025 16:14] Writing result.
[28.05.2025 16:14] Renaming log file.
[28.05.2025 16:14] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
