[28.05.2025 08:17] Read previous papers.
[28.05.2025 08:17] Generating top page (month).
[28.05.2025 08:17] Writing top page (month).
[28.05.2025 09:13] Read previous papers.
[28.05.2025 09:13] Get feed.
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21189
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21505
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21334
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16901
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20561
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19641
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21178
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19433
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16673
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11277
[28.05.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2505.21499
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19973
[28.05.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2505.19650
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17908
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17005
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19377
[28.05.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2505.17190
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15561
[28.05.2025 09:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 09:13] No deleted papers detected.
[28.05.2025 09:13] Downloading and parsing papers (pdf, html). Total: 45.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21496.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21496.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21189.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21189.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21189.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21505.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21505.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21505.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21334.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21334.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21334.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.16901.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.16901.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.16901.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21500.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21500.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.20561.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.20561.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.20561.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19641.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.19641.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.19641.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21178.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21178.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21178.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19433.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.19433.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.19433.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.16673.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.16673.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.16673.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.11277.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.11277.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.11277.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21499.
[28.05.2025 09:13] Downloading paper 2505.21499 from http://arxiv.org/pdf/2505.21499v1...
[28.05.2025 09:13] Extracting affiliations from text.
[28.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 9 9 4 1 2 . 5 0 5 2 : r AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery Haowei Wang1,2,3, Junjie Wang1,2,3 , Xiaojun Jia4, Rupeng Zhang1,2,3, Mingyang Li1,2,3, Zhe Liu1,2,3, Yang Liu4, Qing Wang1,2,3 1State Key Laboratory of Intelligent Game, Beijing, China 2Institute of Software, Chinese Academy of Sciences, Beijing, China 3University of Chinese Academy of Sciences, Beijing, China 4Nanyang Technological University, Singapore {wanghaowei2023}@iscas.ac.cn {junjie, wq}@iscas.ac.cn "
[28.05.2025 09:13] Response: ```python
[
    "State Key Laboratory of Intelligent Game, Beijing, China",
    "Institute of Software, Chinese Academy of Sciences, Beijing, China",
    "University of Chinese Academy of Sciences, Beijing, China",
    "Nanyang Technological University, Singapore"
]
```
[28.05.2025 09:13] Deleting PDF ./assets/pdf/2505.21499.pdf.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19973.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.19973.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.19973.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19650.
[28.05.2025 09:13] Downloading paper 2505.19650 from http://arxiv.org/pdf/2505.19650v2...
[28.05.2025 09:13] Extracting affiliations from text.
[28.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 2 0 5 6 9 1 . 5 0 5 2 : r Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval Fanheng Kong1,2*, Jingyuan Zhang2*, Yahui Liu2, Hongzhi Zhang2, Shi Feng1, Xiaocui Yang1, Daling Wang1, Yu Tian2, Victoria W., Fuzheng Zhang2, Guorui Zhou2 1Northeastern University 2Kuaishou Technology kongfanheng426@gmail.com, fengshi@cse.neu.edu.cn "
[28.05.2025 09:13] Response: ```python
["Northeastern University", "Kuaishou Technology"]
```
[28.05.2025 09:13] Deleting PDF ./assets/pdf/2505.19650.pdf.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.17908.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.17908.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.17908.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.17005.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.17005.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.17005.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.19377.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.19377.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.19377.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.17190.
[28.05.2025 09:13] Downloading paper 2505.17190 from http://arxiv.org/pdf/2505.17190v1...
[28.05.2025 09:13] Extracting affiliations from text.
[28.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 0 9 1 7 1 . 5 0 5 2 : r Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms Baran Hashemi Origins Data Science Lab Technical University of Munich Munich, Germany baran.hashemi@tum.de Kurt Pasque Naval Postgraduate School Monterey, California kurt.pasque@nps.edu Chris Teska Naval Postgraduate School Monterey, California christopher.teska@nps.edu Ruriko Yoshida Naval Postgraduate School Monterey, California ryoshida@nps.edu "
[28.05.2025 09:13] Response: ```python
["Origins Data Science Lab, Technical University of Munich, Munich, Germany", "Naval Postgraduate School, Monterey, California", "Naval Postgraduate School, Monterey, California", "Naval Postgraduate School, Monterey, California"]
```
[28.05.2025 09:13] Deleting PDF ./assets/pdf/2505.17190.pdf.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.16340.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.16340.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2505.15561.
[28.05.2025 09:13] Extra JSON file exists (./assets/json/2505.15561.json), skip PDF parsing.
[28.05.2025 09:13] Paper image links file exists (./assets/img_data/2505.15561.json), skip HTML parsing.
[28.05.2025 09:13] Success.
[28.05.2025 09:13] Enriching papers with extra data.
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 0. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 1. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 2. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 3. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 4. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 5. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 6. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 7. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 8. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 9. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 10. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 11. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 12. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 13. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 14. LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up t...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 15. The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 16. HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at vi...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 17. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 18. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 19. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 20. Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown pro...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 21. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 22. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 23. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 24. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 25. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 26. BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) ha...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 27. SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have de...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 28. As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 29. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 30. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 31. Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ig...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 32. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 33. Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 34. AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  					AI-generated summary 				 Large language models have ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 35. AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 36. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 37. DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital ...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 38. UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent c...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 39. ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  					AI-generated summary 				 With the rapid advan...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 40. R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinati...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 41. Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-a...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 42. Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 43. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 09:13] ********************************************************************************
[28.05.2025 09:13] Abstract 44. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capabilit...
[28.05.2025 09:13] Read previous papers.
[28.05.2025 09:13] Generating reviews via LLM API.
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ç–∏–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "OmniConsistency - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "üñºÔ∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—É—á–Ω—ã—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤: –æ—Ç —Å—Ç–∞—Ç—å–∏ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –∏ –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤, —Å–æ–ø–æ—Å
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –≤ –ª–æ–≥–∏–∫–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "MME-Reasoning - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "üé•", "ru": {"title": "VerIPO: –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–∏–¥–µ–æ-LLM —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ VerIPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ-LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Verif
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#synthetic", "#open_source", "#dataset", "#agents", "#training", "#data"], "emoji": "üßû", "ru": {"title": "UI-Genie: —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "UI-Genie - —ç—Ç–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "üéûÔ∏è", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "SVG2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "üïµÔ∏è", "ru": {"title": "–®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å –¥–ª—è –ò–ò: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "Video-Holmes - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. –û–Ω –∏—Å
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "üß©", "ru": {"title": "GraLoRA: –ì—Ä–∞–Ω—É–ª—è—Ä–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Granular L
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "üé•", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–µ OCR –Ω–∞ –≤–∏–¥–µ–æ", "desc": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "MMMR: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMMR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). MMMR –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "‚ö°", "ru": {"title": "–ö–æ—Ä–æ—á–µ –º—ã—Å–ª—å - –±—ã—Å—Ç—Ä–µ–µ –≤—ã–≤–æ–¥: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ 
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "üß†", "ru": {"title": "rStar-Coder: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –æ –∫–æ–¥–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ rStar-Coder - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "üé¨", "ru": {"title": "OpenS2V-Nexus: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenS2V-Nexus - –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "üß†", "ru": {"title": "MetaMind: –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "MetaMind - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#data", "#long_context", "#architecture", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) —Å–ø–æ—Å–æ–±–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –¥–≤–∞ 
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#alignment"], "emoji": "üåê", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –Ω–µ–π—Ä–æ–Ω–∞—Ö LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏ —è–∑—ã–∫–æ–≤–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö 
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#inference", "#video", "#optimization"], "emoji": "üé¨", "ru": {"title": "HoliTom: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-LLM", "desc": "HoliTom - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–∏–¥–µ–æ-LLM –º–æ–¥–µ–ª—è—Ö. –û–Ω —Å–æ—á–µ—Ç–∞–µ—Ç –≤–Ω–µ—à–Ω—é—é –æ–±—Ä–µ–∑–∫—É LLM —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—É—é –≤—Ä–µ–º
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "ImgEdit: –ø—Ä–æ—Ä—ã–≤ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ImgEdit - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "üëÅÔ∏è", "ru": {"title": "ACTIVE-O3: –ù–∞–¥–µ–ª–µ–Ω–∏–µ MLLM –∞–∫—Ç–∏–≤–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ACTIVE-O3 - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞–¥–µ–ª–µ–Ω–∏—è 
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "üé¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ –∫–∞–¥—Ä–µ", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏, –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≥–µ–Ω
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#games", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–ì—Ä–∞—Ñ–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–¥–∞: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Code Graph Models (CGM) - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Steering Target Atoms (STA) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#3d", "#games"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ViewSpatial-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É", "desc": "DetailFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º 
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "üî¨", "ru": {"title": "SeePhys: –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM –≤ —Ñ–∏–∑–∏–∫–µ", "desc": "SeePhys - —ç—Ç–æ –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "üéØ", "ru": {"title": "–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –∞—Ç–∞–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π FOA-Attack. –û
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "BARL: –£–º–Ω–µ–µ –∏—Å—Å–ª–µ–¥—É–µ–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–∞—Å—Å—É–∂–¥–∞–µ–º", "desc": "BARL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#rl", "#dataset", "#reasoning", "#open_source"], "emoji": "üß†", "ru": {"title": "SynLogic: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é", "desc": "SynLogic - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#rl", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ö—Ä–∞—Ç–∫–æ—Å—Ç—å - —Å–µ—Å—Ç—Ä–∞ —Ç–∞–ª–∞–Ω—Ç–∞: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "üéûÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transfor
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "üß†", "ru": {"title": "VisTA: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "VisTA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. 
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#agents", "#optimization", "#inference", "#benchmark"], "emoji": "üî¨", "ru": {"title": "ACBench: –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Å–∂–∞—Ç—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ACBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è —Å–∂–∞—Ç–∏—è –Ω–∞ –∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "SoloSpeech: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "SoloSpeech - —ç—Ç–æ –Ω–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –∏–∑ —Å–º–µ—Å–∏ –≥–æ–ª–æ—Å–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–∂–∞—Ç–∏–µ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#rl", "#training", "#multimodal", "#benchmark", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π MLLM —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª—è–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "Share-GRPO - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥
[28.05.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#rag", "#rl", "#reasoning", "#benchmark"], "emoji": "üîç", "ru": {"title": "AutoRefine: –£–º–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "AutoRefine - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤
[28.05.2025 09:13] Querying the API.
[28.05.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.
[28.05.2025 09:13] Response: {
  "desc": "AdInject - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ–∫–ª–∞–º—ã –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —É—Å–ª–æ–≤–∏—è—Ö —á–µ—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞, –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–∞ –∏–ª–∏ –∑–Ω–∞–Ω–∏—è –æ –Ω–∞–º–µ—Ä–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. AdInject –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—é—â—É—é –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö 100% —É—Å–ø–µ—Ö–∞, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–µ—Ä—å–µ–∑–Ω—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤.",
  "emoji": "üïµÔ∏è",
  "title": "–†–µ–∫–ª–∞–º–∞ –∫–∞–∫ –æ—Ä—É–∂–∏–µ: –Ω–æ–≤–∞—è —É–≥—Ä–æ–∑–∞ –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–µ—Ç–∏"
}
[28.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject."

[28.05.2025 09:13] Response: ```python
["AGENTS", "CV"]
```
[28.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject."

[28.05.2025 09:13] Response: ```python
["SECURITY"]
```
[28.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AdInject is a new method that shows how internet advertising can be used to attack vision-language model-based web agents. This method highlights the security weaknesses of these agents when they operate in uncontrolled online environments. Unlike previous attacks that required detailed knowledge of the agent\'s workings, AdInject works in a black-box setting, making it more applicable to real-world scenarios. The research demonstrates that by cleverly designing malicious ads, attackers can significantly mislead web agents, achieving high success rates in their attacks.","title":"AdInject: Harnessing Ads for Real-World Attacks on Web Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="AdInject is a new method that shows how internet advertising can be used to attack vision-language model-based web agents. This method highlights the security weaknesses of these agents when they operate in uncontrolled online environments. Unlike previous attacks that required detailed knowledge of the agent's workings, AdInject works in a black-box setting, making it more applicable to real-world scenarios. The research demonstrates that by cleverly designing malicious ads, attackers can significantly mislead web agents, achieving high success rates in their attacks.", title='AdInject: Harnessing Ads for Real-World Attacks on Web Agents'))
[28.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AdInjectÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÈªëÁÆ±ÊîªÂáªÊñπÊ≥ïÔºåÂà©Áî®‰∫íËÅîÁΩëÂπøÂëäÂêëÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁΩëÁªú‰ª£ÁêÜÊ≥®ÂÖ•ÊÅ∂ÊÑèÂÜÖÂÆπÔºåÊòæÁ§∫Âá∫ÁΩëÁªú‰ª£ÁêÜÂÆâÂÖ®ÊÄßÁöÑÈáçË¶ÅÊºèÊ¥û„ÄÇËØ•ÊñπÊ≥ïÂú®Êõ¥Áé∞ÂÆûÁöÑÂ®ÅËÉÅÊ®°Âûã‰∏ãËøêË°åÔºå‰∏çÈúÄË¶ÅÂØπÁî®Êà∑ÊÑèÂõæÁöÑÂÖ∑‰Ωì‰∫ÜËß£Ôºå‰∏îÂÅáËÆæ‰ª£ÁêÜ‰∏∫ÈªëÁÆ±„ÄÇAdInjectÈÄöËøáËÆæËÆ°ËØØÂØºÊÄßÁöÑÂπøÂëäÂÜÖÂÆπÔºåËØ±‰Ωø‰ª£ÁêÜÁÇπÂáªÔºåÂπ∂‰ΩøÁî®Âü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπøÂëäÂÜÖÂÆπ‰ºòÂåñÊäÄÊúØÔºå‰ΩøÂπøÂëäÂÜÖÂÆπ‰∏éÁõÆÊ†áÁΩëÁ´ôÁöÑ‰∏ä‰∏ãÊñáÊõ¥Áõ∏ÂÖ≥Ôºå‰ªéËÄåÊèêÈ´òÊîªÂáªÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAdInjectÂú®Â§ßÂ§öÊï∞Âú∫ÊôØ‰∏≠ÁöÑÊîªÂáªÊàêÂäüÁéáË∂ÖËøá60%ÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÊé•Ëøë100%„ÄÇ","title":"Âà©Áî®ÂπøÂëäÊ≥®ÂÖ•ÊîªÂáªÁΩëÁªú‰ª£ÁêÜÁöÑÂÆâÂÖ®ÊºèÊ¥û"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AdInjectÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÈªëÁÆ±ÊîªÂáªÊñπÊ≥ïÔºåÂà©Áî®‰∫íËÅîÁΩëÂπøÂëäÂêëÂü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÁΩëÁªú‰ª£ÁêÜÊ≥®ÂÖ•ÊÅ∂ÊÑèÂÜÖÂÆπÔºåÊòæÁ§∫Âá∫ÁΩëÁªú‰ª£ÁêÜÂÆâÂÖ®ÊÄßÁöÑÈáçË¶ÅÊºèÊ¥û„ÄÇËØ•ÊñπÊ≥ïÂú®Êõ¥Áé∞ÂÆûÁöÑÂ®ÅËÉÅÊ®°Âûã‰∏ãËøêË°åÔºå‰∏çÈúÄË¶ÅÂØπÁî®Êà∑ÊÑèÂõæÁöÑÂÖ∑‰Ωì‰∫ÜËß£Ôºå‰∏îÂÅáËÆæ‰ª£ÁêÜ‰∏∫ÈªëÁÆ±„ÄÇAdInjectÈÄöËøáËÆæËÆ°ËØØÂØºÊÄßÁöÑÂπøÂëäÂÜÖÂÆπÔºåËØ±‰Ωø‰ª£ÁêÜÁÇπÂáªÔºåÂπ∂‰ΩøÁî®Âü∫‰∫éËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπøÂëäÂÜÖÂÆπ‰ºòÂåñÊäÄÊúØÔºå‰ΩøÂπøÂëäÂÜÖÂÆπ‰∏éÁõÆÊ†áÁΩëÁ´ôÁöÑ‰∏ä‰∏ãÊñáÊõ¥Áõ∏ÂÖ≥Ôºå‰ªéËÄåÊèêÈ´òÊîªÂáªÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAdInjectÂú®Â§ßÂ§öÊï∞Âú∫ÊôØ‰∏≠ÁöÑÊîªÂáªÊàêÂäüÁéáË∂ÖËøá60%ÔºåÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÊé•Ëøë100%„ÄÇ', title='Âà©Áî®ÂπøÂëäÊ≥®ÂÖ•ÊîªÂáªÁΩëÁªú‰ª£ÁêÜÁöÑÂÆâÂÖ®ÊºèÊ¥û'))
[28.05.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "üéûÔ∏è", "ru": {"title": "–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤ –º–µ–∂–¥—É –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º –∏ –∫–æ
[28.05.2025 09:14] Using data from previous issue: {"categories": ["#science", "#hallucinations", "#benchmark", "#dataset", "#optimization", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–π –∫—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç–∏–∫–∏", "desc": "DFIR-Metric - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞
[28.05.2025 09:14] Querying the API.
[28.05.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE.
[28.05.2025 09:14] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UNITE - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–∞–ª—å–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ Modal-Aware Masked Contrastive Learning (MAMCL) –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–º—è–≥—á–∏—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —ç–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. UNITE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.",
  "emoji": "üîç",
  "title": "UNITE: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ"
}
[28.05.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE."

[28.05.2025 09:14] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'MULTIMODAL', 'TRAINING']
```
[28.05.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE."

[28.05.2025 09:14] Response: ```python
[]
```
[28.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UNITE is a framework designed to improve multimodal information retrieval (MIR) by focusing on data curation and modality-aware training. It addresses the challenges of aligning different types of data, which often have varying characteristics. The framework introduces Modal-Aware Masked Contrastive Learning (MAMCL) to enhance the learning process by reducing conflicts between different modalities. By conducting extensive experiments, UNITE demonstrates significant improvements in retrieval performance across various benchmarks, setting a new standard in the field.","title":"UNITE: Bridging Modalities for Superior Information Retrieval"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UNITE is a framework designed to improve multimodal information retrieval (MIR) by focusing on data curation and modality-aware training. It addresses the challenges of aligning different types of data, which often have varying characteristics. The framework introduces Modal-Aware Masked Contrastive Learning (MAMCL) to enhance the learning process by reducing conflicts between different modalities. By conducting extensive experiments, UNITE demonstrates significant improvements in retrieval performance across various benchmarks, setting a new standard in the field.', title='UNITE: Bridging Modalities for Superior Information Retrieval'))
[28.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UNITEÊòØ‰∏Ä‰∏™ÈíàÂØπÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢ÔºàMIRÔºâÊåëÊàòÁöÑÈÄöÁî®Ê°ÜÊû∂Ôºå‰∏ªË¶ÅÈÄöËøáÊï∞ÊçÆÊï¥ÁêÜÂíåÊ®°ÊÄÅÊÑüÁü•ËÆ≠ÁªÉÊù•Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇËØ•Á†îÁ©∂È¶ñÊ¨°Á≥ªÁªüÂàÜÊûê‰∫ÜÊ®°ÊÄÅÁâπÂÆöÊï∞ÊçÆÂ±ûÊÄßÂ¶Ç‰ΩïÂΩ±Âìç‰∏ãÊ∏∏‰ªªÂä°ÁöÑË°®Áé∞ÔºåÂπ∂ÊèêÂá∫‰∫ÜÊ®°ÊÄÅÊÑüÁü•Êé©ËîΩÂØπÊØîÂ≠¶‰π†ÔºàMAMCLÔºâÔºå‰ª•ÂáèËΩª‰∏çÂêåÊ®°ÊÄÅÂÆû‰æã‰πãÈó¥ÁöÑÁ´û‰∫âÂÖ≥Á≥ª„ÄÇÈÄöËøáÂú®Â§ö‰∏™Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåUNITEÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇËØ•Â∑•‰Ωú‰∏∫Êú™Êù•Â§öÊ®°ÊÄÅÁ≥ªÁªüÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂü∫Á°ÄËìùÂõæ„ÄÇ","title":"UNITEÔºöÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢ÁöÑÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UNITEÊòØ‰∏Ä‰∏™ÈíàÂØπÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢ÔºàMIRÔºâÊåëÊàòÁöÑÈÄöÁî®Ê°ÜÊû∂Ôºå‰∏ªË¶ÅÈÄöËøáÊï∞ÊçÆÊï¥ÁêÜÂíåÊ®°ÊÄÅÊÑüÁü•ËÆ≠ÁªÉÊù•Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇËØ•Á†îÁ©∂È¶ñÊ¨°Á≥ªÁªüÂàÜÊûê‰∫ÜÊ®°ÊÄÅÁâπÂÆöÊï∞ÊçÆÂ±ûÊÄßÂ¶Ç‰ΩïÂΩ±Âìç‰∏ãÊ∏∏‰ªªÂä°ÁöÑË°®Áé∞ÔºåÂπ∂ÊèêÂá∫‰∫ÜÊ®°ÊÄÅÊÑüÁü•Êé©ËîΩÂØπÊØîÂ≠¶‰π†ÔºàMAMCLÔºâÔºå‰ª•ÂáèËΩª‰∏çÂêåÊ®°ÊÄÅÂÆû‰æã‰πãÈó¥ÁöÑÁ´û‰∫âÂÖ≥Á≥ª„ÄÇÈÄöËøáÂú®Â§ö‰∏™Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äËøõË°åÂπøÊ≥õÂÆûÈ™åÔºåUNITEÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇËØ•Â∑•‰Ωú‰∏∫Êú™Êù•Â§öÊ®°ÊÄÅÁ≥ªÁªüÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂü∫Á°ÄËìùÂõæ„ÄÇ', title='UNITEÔºöÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊ£ÄÁ¥¢ÁöÑÊñ∞Á™ÅÁ†¥'))
[28.05.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#games", "#training", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "ComfyMind: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "ComfyMind - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª
[28.05.2025 09:14] Using data from previous issue: {"categories": ["#hallucinations", "#optimization", "#rl", "#reasoning", "#rag"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è", "desc": "R1-Searcher++ - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—É—Ç–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ 
[28.05.2025 09:14] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#optimization", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å—É—Å—Ç–∞
[28.05.2025 09:14] Querying the API.
[28.05.2025 09:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax.
[28.05.2025 09:14] Response: {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–¢—Ä–æ–ø–∏—á–µ—Å–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ', –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ç—Ä–æ–ø–∏—á–µ—Å–∫–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏. –ê–≤—Ç–æ—Ä—ã –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –º–æ–∂–µ—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–æ–ø–∏—á–µ—Å–∫–∏–µ —Å—Ö–µ–º—ã –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å —Ç—Ä–æ–ø–∏—á–µ—Å–∫–∏–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å softmax –ø–æ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –∞—Ç–∞–∫–∞–º.",

  "emoji": "üå¥",

  "title": "–¢—Ä–æ–ø–∏—á–µ—Å–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –æ—Å—Ç—Ä–æ–µ –º–∞—Å—à—Ç–∞–±–Ω–æ-–∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –Ω–µ–π—Ä–æ–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á"
}
[28.05.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax."

[28.05.2025 09:14] Response: ```python
['MATH', 'BENCHMARK', 'ARCHITECTURE']
```
[28.05.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax."

[28.05.2025 09:14] Response: ```python
["REASONING", "OPTIMIZATION", "SECURITY"]
```
[28.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Tropical attention, a new attention mechanism designed for combinatorial optimization problems that traditionally use dynamic programming. Unlike existing models that use softmax-normalized attention, which can blur important structures in data, Tropical attention operates within the max-plus semiring, preserving the sharp characteristics of value functions. The authors demonstrate that Tropical attention can effectively approximate tropical circuits used in dynamic programming algorithms, leading to improved performance on out-of-distribution tasks. Additionally, they highlight the robustness of Tropical transformers against adversarial attacks, making them a strong alternative to softmax-based models in algorithmic reasoning.","title":"Tropical Attention: Enhancing Algorithmic Reasoning with Sharp Structures"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Tropical attention, a new attention mechanism designed for combinatorial optimization problems that traditionally use dynamic programming. Unlike existing models that use softmax-normalized attention, which can blur important structures in data, Tropical attention operates within the max-plus semiring, preserving the sharp characteristics of value functions. The authors demonstrate that Tropical attention can effectively approximate tropical circuits used in dynamic programming algorithms, leading to improved performance on out-of-distribution tasks. Additionally, they highlight the robustness of Tropical transformers against adversarial attacks, making them a strong alternative to softmax-based models in algorithmic reasoning.', title='Tropical Attention: Enhancing Algorithmic Reasoning with Sharp Structures'))
[28.05.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫ÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÔºåÊó®Âú®Ëß£ÂÜ≥ÁªÑÂêà‰ºòÂåñÈóÆÈ¢ò‰∏≠ÁöÑÂä®ÊÄÅËßÑÂàíÁÆóÊ≥ï„ÄÇ‰º†ÁªüÁöÑÁ•ûÁªèÁÆóÊ≥ïÊé®ÁêÜÊ®°Âûã‰ΩøÁî®softmaxÂΩí‰∏ÄÂåñÁöÑÁÇπÁßØÊ≥®ÊÑèÂäõÔºåËøô‰ºöÊ®°Á≥äÊéâÈáçË¶ÅÁöÑÂá†‰ΩïÁªìÊûÑ„ÄÇÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÂú®ÁÉ≠Â∏¶Âá†‰ΩïÁöÑÊúÄÂ§ßÂä†Ê≥ïÂçäÁéØ‰∏≠ÂéüÁîüÊìç‰ΩúÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Ëøë‰ººÂä®ÊÄÅËßÑÂàíÁ±ªÂûãÁöÑÁîµË∑Ø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁÉ≠Â∏¶ÂèòÊç¢Âô®Âú®ÁÆóÊ≥ïÊé®ÁêÜ‰ªªÂä°‰∏≠Âú®ÈïøÂ∫¶Ê≥õÂåñÂíå‰ª∑ÂÄºÊ≥õÂåñÊñπÈù¢ÁöÑË°®Áé∞‰ºò‰∫ésoftmaxÂü∫Á∫øÔºåÂπ∂‰∏îÂú®ÂØπÊäóÊîªÂáª‰∏ã‰øùÊåÅÁ®≥ÂÆö„ÄÇ","title":"ÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÔºöÊèêÂçáÁÆóÊ≥ïÊé®ÁêÜÁöÑÁ®≥ÂÆöÊÄß‰∏éÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫ÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÔºåÊó®Âú®Ëß£ÂÜ≥ÁªÑÂêà‰ºòÂåñÈóÆÈ¢ò‰∏≠ÁöÑÂä®ÊÄÅËßÑÂàíÁÆóÊ≥ï„ÄÇ‰º†ÁªüÁöÑÁ•ûÁªèÁÆóÊ≥ïÊé®ÁêÜÊ®°Âûã‰ΩøÁî®softmaxÂΩí‰∏ÄÂåñÁöÑÁÇπÁßØÊ≥®ÊÑèÂäõÔºåËøô‰ºöÊ®°Á≥äÊéâÈáçË¶ÅÁöÑÂá†‰ΩïÁªìÊûÑ„ÄÇÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÂú®ÁÉ≠Â∏¶Âá†‰ΩïÁöÑÊúÄÂ§ßÂä†Ê≥ïÂçäÁéØ‰∏≠ÂéüÁîüÊìç‰ΩúÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞Ëøë‰ººÂä®ÊÄÅËßÑÂàíÁ±ªÂûãÁöÑÁîµË∑Ø„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁÉ≠Â∏¶ÂèòÊç¢Âô®Âú®ÁÆóÊ≥ïÊé®ÁêÜ‰ªªÂä°‰∏≠Âú®ÈïøÂ∫¶Ê≥õÂåñÂíå‰ª∑ÂÄºÊ≥õÂåñÊñπÈù¢ÁöÑË°®Áé∞‰ºò‰∫ésoftmaxÂü∫Á∫øÔºåÂπ∂‰∏îÂú®ÂØπÊäóÊîªÂáª‰∏ã‰øùÊåÅÁ®≥ÂÆö„ÄÇ', title='ÁÉ≠Â∏¶Ê≥®ÊÑèÂäõÔºöÊèêÂçáÁÆóÊ≥ïÊé®ÁêÜÁöÑÁ®≥ÂÆöÊÄß‰∏éÊÄßËÉΩ'))
[28.05.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#science", "#dataset", "#open_source", "#data", "#multimodal"], "emoji": "üß™", "ru": {"title": "CLEANMOL: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLEANMOL - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –±–æ
[28.05.2025 09:14] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#benchmark", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ RAG: –Ω–µ —Ç–∞–∫ —Å—Ç—Ä–∞—à–Ω–æ, –∫–∞–∫ –∫–∞–∂–µ—Ç—Å—è", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ Retrieval Augmented Generati
[28.05.2025 09:14] Trying to get texts in Chinese.
[28.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose OmniConsistency, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.
[28.05.2025 09:14] Mistral response. {"id": "be8f117d59c946638d476bcce91171c1", "object": "chat.completion", "created": 1748423676, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ebOmniConsistency\u7684\u65b9\u6cd5\u3002\u5b83\u7528\u5927\u89c4\u6a21\u6269\u6563\u53d8\u538b\u5668\u6765\u589e\u5f3a\u56fe\u50cf\u5230\u56fe\u50cf\u6d41\u6c34\u7ebf\u4e2d\u7684\u98ce\u683c\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4e0d\u4f1a\u5bfc\u81f4\u98ce\u683c\u9000\u5316\u3002\u5b83\u901a\u8fc7\u4e09\u4e2a\u8d21\u732e\u6765\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff1a\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5b66\u4e60\u6846\u67b6\u3001\u4e24\u9636\u6bb5\u6e10\u8fdb\u5b66\u4e60\u7b56\u7565\u548c\u63d2\u64ad\u8bbe\u8ba1\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5b83\u7684\u8868\u73b0\u63a5\u8fd1\u5546\u4e1a\u9876\u5c16\u6a21\u578bGPT-4o\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 322, "total_tokens": 463, "completion_tokens": 141}}
[28.05.2025 09:14] Response: ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂè´OmniConsistencyÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁî®Â§ßËßÑÊ®°Êâ©Êï£ÂèòÂéãÂô®Êù•Â¢ûÂº∫ÂõæÂÉèÂà∞ÂõæÂÉèÊµÅÊ∞¥Á∫ø‰∏≠ÁöÑÈ£éÊ†º‰∏ÄËá¥ÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåËÄå‰∏ç‰ºöÂØºËá¥È£éÊ†ºÈÄÄÂåñ„ÄÇÂÆÉÈÄöËøá‰∏â‰∏™Ë¥°ÁåÆÊù•ÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºö‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄßÂ≠¶‰π†Ê°ÜÊû∂„ÄÅ‰∏§Èò∂ÊÆµÊ∏êËøõÂ≠¶‰π†Á≠ñÁï•ÂíåÊèíÊí≠ËÆæËÆ°„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåÂÆÉÁöÑË°®Áé∞Êé•ËøëÂïÜ‰∏öÈ°∂Â∞ñÊ®°ÂûãGPT-4o„ÄÇ
[28.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂè´OmniConsistencyÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁî®Â§ßËßÑÊ®°Êâ©Êï£ÂèòÂéãÂô®Êù•Â¢ûÂº∫ÂõæÂÉèÂà∞ÂõæÂÉèÊµÅÊ∞¥Á∫ø‰∏≠ÁöÑÈ£éÊ†º‰∏ÄËá¥ÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåËÄå‰∏ç‰ºöÂØºËá¥È£éÊ†ºÈÄÄÂåñ„ÄÇÂÆÉÈÄöËøá‰∏â‰∏™Ë¥°ÁåÆÊù•ÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºö‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄßÂ≠¶‰π†Ê°ÜÊû∂„ÄÅ‰∏§Èò∂ÊÆµÊ∏êËøõÂ≠¶‰π†Á≠ñÁï•ÂíåÊèíÊí≠ËÆæËÆ°„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåÂÆÉÁöÑË°®Áé∞Êé•ËøëÂïÜ‰∏öÈ°∂Â∞ñÊ®°ÂûãGPT-4o„ÄÇ
[28.05.2025 09:14] Mistral response. {"id": "0cfef09f32e8475abe158988f8a533e4", "object": "chat.completion", "created": 1748423685, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ebOmniConsistency\u7684\u65b9\u6cd5\u3002\u5b83\u7528\u5927\u89c4\u6a21\u6269\u6563\u53d8\u538b\u5668\u6765\u589e\u5f3a\u56fe\u50cf\u5230\u56fe\u50cf\u6d41\u6c34\u7ebf\u4e2d\u7684\u98ce\u683c\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u4e0d\u4f1a\u5bfc\u81f4\u98ce\u683c\u9000\u5316\u3002\u5b83\u901a\u8fc7\u4e09\u4e2a\u8d21\u732e\u6765\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\uff1a\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5b66\u4e60\u6846\u67b6\u3001\u4e24\u9636\u6bb5\u6e10\u8fdb\u5b66\u4e60\u7b56\u7565\u548c\u63d2\u64ad\u8bbe\u8ba1\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5b83\u7684\u8868\u73b0\u63a5\u8fd1\u5546\u4e1a\u9876\u5c16\u6a21\u578bGPT-4o\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng ji\u00e8 sh\u00e0o le y\u012b zh\u01d2ng ji\u00e0o OmniConsistency de f\u0101ng f\u01ce. t\u0101 y\u00f2ng d\u00e0 gu\u012b m\u00f3 ku\u00f2 s\u00e0n bi\u00e0n sh\u016b zh\u011b l\u00e1i z\u0113ng qi\u00e1ng t\u00fa xi\u00e0ng d\u00e0o t\u00fa xi\u00e0ng li\u00fa shu\u01d0 xi\u00e0n zh\u014dng de f\u0113ng g\u00e9 y\u012b zh\u00ec x\u00ecng h\u00e9 f\u00e0n hu\u00e0 n\u00e9ng l\u00ec, \u00e9r b\u00f9 hu\u00ec d\u01ceo zh\u00ec f\u0113ng g\u00e9 tu\u00ec hu\u00ec. t\u0101 t\u014dng gu\u00f2 s\u0101n g\u00e8 g\u00f2ng xi\u00e0n l\u00e1i sh\u00ed xi\u00e0n zh\u00e8 g\u00e8 mu biao: sh\u00e0ng xi\u00e0 t\u00fa y\u012b zh\u00ec x\u00ecng xu\u00e9 x\u00ed ku\u0101ng ji\u00e0, li\u01ceng ji\u0113 du\u00e0n ji\u00e0n j\u00ecn xu\u00e9 x\u00ed c\u00e8 l\u00fc\u00e8 h\u00e9 ch\u0101 b\u014d sh\u00e8 j\u00ec. sh\u00ed y\u00e0n xi\u01cen sh\u00ec, t\u0101 de bi\u01ceo xi\u00e0n ji\u0113 j\u00ecn sh\u0101ng y\u00e8 d\u01d0ng ji\u0101n m\u00f3 x\u00edng GPT-4o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 155, "total_tokens": 597, "completion_tokens": 442}}
[28.05.2025 09:14] Response: ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂè´OmniConsistencyÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁî®Â§ßËßÑÊ®°Êâ©Êï£ÂèòÂéãÂô®Êù•Â¢ûÂº∫ÂõæÂÉèÂà∞ÂõæÂÉèÊµÅÊ∞¥Á∫ø‰∏≠ÁöÑÈ£éÊ†º‰∏ÄËá¥ÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåËÄå‰∏ç‰ºöÂØºËá¥È£éÊ†ºÈÄÄÂåñ„ÄÇÂÆÉÈÄöËøá‰∏â‰∏™Ë¥°ÁåÆÊù•ÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºö‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄßÂ≠¶‰π†Ê°ÜÊû∂„ÄÅ‰∏§Èò∂ÊÆµÊ∏êËøõÂ≠¶‰π†Á≠ñÁï•ÂíåÊèíÊí≠ËÆæËÆ°„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåÂÆÉÁöÑË°®Áé∞Êé•ËøëÂïÜ‰∏öÈ°∂Â∞ñÊ®°ÂûãGPT-4o„ÄÇ

zh√® piƒÅn w√©n zhƒÅng ji√® sh√†o le yƒ´ zh«íng ji√†o OmniConsistency de fƒÅng f«é. tƒÅ y√≤ng d√† guƒ´ m√≥ ku√≤ s√†n bi√†n sh≈´ zhƒõ l√°i zƒìng qi√°ng t√∫ xi√†ng d√†o t√∫ xi√†ng li√∫ shu«ê xi√†n zh≈çng de fƒìng g√© yƒ´ zh√¨ x√¨ng h√© f√†n hu√† n√©ng l√¨, √©r b√π hu√¨ d«éo zh√¨ fƒìng g√© tu√¨ hu√¨. tƒÅ t≈çng gu√≤ sƒÅn g√® g√≤ng xi√†n l√°i sh√≠ xi√†n zh√® g√® mu biao: sh√†ng xi√† t√∫ yƒ´ zh√¨ x√¨ng xu√© x√≠ kuƒÅng ji√†, li«éng jiƒì du√†n ji√†n j√¨n xu√© x√≠ c√® l√º√® h√© chƒÅ b≈ç sh√® j√¨. sh√≠ y√†n xi«én sh√¨, tƒÅ de bi«éo xi√†n jiƒì j√¨n shƒÅng y√® d«êng jiƒÅn m√≥ x√≠ng GPT-4o.
[28.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂè´OmniConsistencyÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁî®Â§ßËßÑÊ®°Êâ©Êï£ÂèòÂéãÂô®Êù•Â¢ûÂº∫ÂõæÂÉèÂà∞ÂõæÂÉèÊµÅÊ∞¥Á∫ø‰∏≠ÁöÑÈ£éÊ†º‰∏ÄËá¥ÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåËÄå‰∏ç‰ºöÂØºËá¥È£éÊ†ºÈÄÄÂåñ„ÄÇÂÆÉÈÄöËøá‰∏â‰∏™Ë¥°ÁåÆÊù•ÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºö‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄßÂ≠¶‰π†Ê°ÜÊû∂„ÄÅ‰∏§Èò∂ÊÆµÊ∏êËøõÂ≠¶‰π†Á≠ñÁï•ÂíåÊèíÊí≠ËÆæËÆ°„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåÂÆÉÁöÑË°®Áé∞Êé•ËøëÂïÜ‰∏öÈ°∂Â∞ñÊ®°ÂûãGPT-4o„ÄÇ
[28.05.2025 09:14] Mistral response. {"id": "c7d493ab290c47eb81aa64bba09c779c", "object": "chat.completion", "created": 1748423693, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"OmniConsistency\", \"pinyin\": \"\", \"trans\": \"OmniConsistency\"},\n    {\"word\": \"\u6269\u6563\", \"pinyin\": \"ku\u00f2 s\u00e0n\", \"trans\": \"diffusion\"},\n    {\"word\": \"\u53d8\u538b\u5668\", \"pinyin\": \"bi\u00e0n y\u0101 q\u00ec\", \"trans\": \"transformer\"},\n    {\"word\": \"\u589e\u5f3a\", \"pinyin\": \"z\u0113ng qi\u00e1ng\", \"trans\": \"enhance\"},\n    {\"word\": \"\u98ce\u683c\", \"pinyin\": \"f\u0113ng g\u00e9\", \"trans\": \"style\"},\n    {\"word\": \"\u4e00\u81f4\u6027\", \"pinyin\": \"y\u012b zh\u00ec x\u00ecng\", \"trans\": \"consistency\"},\n    {\"word\": \"\u6cdb\u5316\", \"pinyin\": \"f\u00e0n hu\u00e0\", \"trans\": \"generalization\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ng l\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u9000\u5316\", \"pinyin\": \"tu\u00ec hu\u00e0\", \"trans\": \"degeneration\"},\n    {\"word\": \"\u4e0a\u4e0b\u6587\", \"pinyin\": \"sh\u00e0ng xi\u00e0 w\u00e9n\", \"trans\": \"context\"},\n    {\"word\": \"\u6846\u67b6\", \"pinyin\": \"ku\u00e0ng ji\u00e0\", \"trans\": \"framework\"},\n    {\"word\": \"\u4e24\u9636\u6bb5\", \"pinyin\": \"li\u01ceng ji\u0113 du\u00e0n\", \"trans\": \"two-stage\"},\n    {\"word\": \"\u6e10\u8fdb\", \"pinyin\": \"ji\u00e0n j\u00ecn\", \"trans\": \"progressive\"},\n    {\"word\": \"\u7b56\u7565\", \"pinyin\": \"c\u00e8 l\u00fc\u00e8\", \"trans\": \"strategy\"},\n    {\"word\": \"\u63d2\u64ad\", \"pinyin\": \"ch\u0101 b\u014d\", \"trans\": \"interpolation\"},\n    {\"word\": \"\u8bbe\u8ba1\", \"pinyin\": \"sh\u00e8 j\u00ec\", \"trans\": \"design\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u63a5\u8fd1\", \"pinyin\": \"ji\u0113 j\u00ecn\", \"trans\": \"approach\"},\n    {\"word\": \"\u5546\u4e1a\", \"pinyin\": \"sh\u0101ng y\u00e8\", \"trans\": \"commercial\"},\n    {\"word\": \"\u9876\u5c16\", \"pinyin\": \"d\u01d0ng ji\u0101n\", \"trans\": \"top-notch\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"GPT-4o\", \"pinyin\": \"\", \"trans\": \"GPT-4o\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 187, "total_tokens": 830, "completion_tokens": 643}}
[28.05.2025 09:14] Response: [
    {"word": "OmniConsistency", "pinyin": "", "trans": "OmniConsistency"},
    {"word": "Êâ©Êï£", "pinyin": "ku√≤ s√†n", "trans": "diffusion"},
    {"word": "ÂèòÂéãÂô®", "pinyin": "bi√†n yƒÅ q√¨", "trans": "transformer"},
    {"word": "Â¢ûÂº∫", "pinyin": "zƒìng qi√°ng", "trans": "enhance"},
    {"word": "È£éÊ†º", "pinyin": "fƒìng g√©", "trans": "style"},
    {"word": "‰∏ÄËá¥ÊÄß", "pinyin": "yƒ´ zh√¨ x√¨ng", "trans": "consistency"},
    {"word": "Ê≥õÂåñ", "pinyin": "f√†n hu√†", "trans": "generalization"},
    {"word": "ËÉΩÂäõ", "pinyin": "n√©ng l√¨", "trans": "ability"},
    {"word": "ÈÄÄÂåñ", "pinyin": "tu√¨ hu√†", "trans": "degeneration"},
    {"word": "‰∏ä‰∏ãÊñá", "pinyin": "sh√†ng xi√† w√©n", "trans": "context"},
    {"word": "Ê°ÜÊû∂", "pinyin": "ku√†ng ji√†", "trans": "framework"},
    {"word": "‰∏§Èò∂ÊÆµ", "pinyin": "li«éng jiƒì du√†n", "trans": "two-stage"},
    {"word": "Ê∏êËøõ", "pinyin": "ji√†n j√¨n", "trans": "progressive"},
    {"word": "Á≠ñÁï•", "pinyin": "c√® l√º√®", "trans": "strategy"},
    {"word": "ÊèíÊí≠", "pinyin": "chƒÅ b≈ç", "trans": "interpolation"},
    {"word": "ËÆæËÆ°", "pinyin": "sh√® j√¨", "trans": "design"},
    {"word": "Ë°®Áé∞", "pinyin": "bi«éo xi√†n", "trans": "performance"},
    {"word": "Êé•Ëøë", "pinyin": "jiƒì j√¨n", "trans": "approach"},
    {"word": "ÂïÜ‰∏ö", "pinyin": "shƒÅng y√®", "trans": "commercial"},
    {"word": "È°∂Â∞ñ", "pinyin": "d«êng jiƒÅn", "trans": "top-notch"},
    {"word": "Ê®°Âûã", "pinyin": "m√≥ x√≠ng", "trans": "model"},
    {"word": "GPT-4o", "pinyin": "", "trans": "GPT-4o"}
]
[28.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂè´OmniConsistencyÁöÑÊñπÊ≥ï„ÄÇÂÆÉÁî®Â§ßËßÑÊ®°Êâ©Êï£ÂèòÂéãÂô®Êù•Â¢ûÂº∫ÂõæÂÉèÂà∞ÂõæÂÉèÊµÅÊ∞¥Á∫ø‰∏≠ÁöÑÈ£éÊ†º‰∏ÄËá¥ÊÄßÂíåÊ≥õÂåñËÉΩÂäõÔºåËÄå‰∏ç‰ºöÂØºËá¥È£éÊ†ºÈÄÄÂåñ„ÄÇÂÆÉÈÄöËøá‰∏â‰∏™Ë¥°ÁåÆÊù•ÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºö‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄßÂ≠¶‰π†Ê°ÜÊû∂„ÄÅ‰∏§Èò∂ÊÆµÊ∏êËøõÂ≠¶‰π†Á≠ñÁï•ÂíåÊèíÊí≠ËÆæËÆ°„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåÂÆÉÁöÑË°®Áé∞Êé•ËøëÂïÜ‰∏öÈ°∂Â∞ñÊ®°ÂûãGPT-4o„ÄÇ
[28.05.2025 09:15] Mistral response. {"id": "0092d8e7292e4f57bda5934f5e3eff62", "object": "chat.completion", "created": 1748423699, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a method called OmniConsistency. It uses large-scale diffusion transformers to enhance style consistency and generalization ability in image-to-image pipelines without causing style degradation. It achieves this goal through three contributions: a context consistency learning framework, a two-stage progressive learning strategy, and an interpolation design. Experiments show that its performance approaches that of the top commercial model, GPT-4o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 153, "total_tokens": 247, "completion_tokens": 94}}
[28.05.2025 09:15] Response: This article introduces a method called OmniConsistency. It uses large-scale diffusion transformers to enhance style consistency and generalization ability in image-to-image pipelines without causing style degradation. It achieves this goal through three contributions: a context consistency learning framework, a two-stage progressive learning strategy, and an interpolation design. Experiments show that its performance approaches that of the top commercial model, GPT-4o.
[28.05.2025 09:15] Renaming data file.
[28.05.2025 09:15] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 09:15] Saving new data file.
[28.05.2025 09:15] Generating page.
[28.05.2025 09:15] Renaming previous page.
[28.05.2025 09:15] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 09:15] [Experimental] Generating Chinese page for reading.
[28.05.2025 09:15] Chinese vocab [{'word': 'OmniConsistency', 'pinyin': '', 'trans': 'OmniConsistency'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'ÂèòÂéãÂô®', 'pinyin': 'bi√†n yƒÅ q√¨', 'trans': 'transformer'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìng qi√°ng', 'trans': 'enhance'}, {'word': 'È£éÊ†º', 'pinyin': 'fƒìng g√©', 'trans': 'style'}, {'word': '‰∏ÄËá¥ÊÄß', 'pinyin': 'yƒ´ zh√¨ x√¨ng', 'trans': 'consistency'}, {'word': 'Ê≥õÂåñ', 'pinyin': 'f√†n hu√†', 'trans': 'generalization'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ÈÄÄÂåñ', 'pinyin': 'tu√¨ hu√†', 'trans': 'degeneration'}, {'word': '‰∏ä‰∏ãÊñá', 'pinyin': 'sh√†ng xi√† w√©n', 'trans': 'context'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': '‰∏§Èò∂ÊÆµ', 'pinyin': 'li«éng jiƒì du√†n', 'trans': 'two-stage'}, {'word': 'Ê∏êËøõ', 'pinyin': 'ji√†n j√¨n', 'trans': 'progressive'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'ÊèíÊí≠', 'pinyin': 'chƒÅ b≈ç', 'trans': 'interpolation'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√® j√¨', 'trans': 'design'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Êé•Ëøë', 'pinyin': 'jiƒì j√¨n', 'trans': 'approach'}, {'word': 'ÂïÜ‰∏ö', 'pinyin': 'shƒÅng y√®', 'trans': 'commercial'}, {'word': 'È°∂Â∞ñ', 'pinyin': 'd«êng jiƒÅn', 'trans': 'top-notch'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'GPT-4o', 'pinyin': '', 'trans': 'GPT-4o'}]
[28.05.2025 09:15] Renaming previous Chinese page.
[28.05.2025 09:15] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 09:15] Writing Chinese reading task.
[28.05.2025 09:15] Writing result.
[28.05.2025 09:15] Renaming log file.
[28.05.2025 09:15] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
