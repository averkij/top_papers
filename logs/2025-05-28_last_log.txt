[28.05.2025 14:13] Read previous papers.
[28.05.2025 14:13] Generating top page (month).
[28.05.2025 14:13] Writing top page (month).
[28.05.2025 15:11] Read previous papers.
[28.05.2025 15:11] Get feed.
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19897
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19641
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21189
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21334
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17952
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21505
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14064
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16901
[28.05.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.21493
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20561
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20426
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21178
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19433
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20321
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17005
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19973
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18657
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18134
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17908
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16673
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11277
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21499
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20286
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19650
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19377
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17190
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21471
[28.05.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.20162
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20052
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20036
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19954
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17855
[28.05.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15561
[28.05.2025 15:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 15:11] No deleted papers detected.
[28.05.2025 15:11] Downloading and parsing papers (pdf, html). Total: 60.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19897.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19897.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19897.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19641.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19641.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19641.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21189.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21189.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21189.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21496.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21496.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21334.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21334.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21334.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.17952.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.17952.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.17952.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21505.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21505.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21505.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.14064.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.14064.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.14064.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21500.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21500.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.16901.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.16901.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.16901.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21493.
[28.05.2025 15:11] Downloading paper 2505.21493 from http://arxiv.org/pdf/2505.21493v1...
[28.05.2025 15:11] Extracting affiliations from text.
[28.05.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 3 9 4 1 2 . 5 0 5 2 : r a Xiangxin Zhou23, Zichen Liu14, Anya Sims15, Haonan Wang14, Tianyu Pang1, Chongxuan Li6, Liang Wang23, Min Lin1, Chao Du1 1Sea AI Lab, Singapore; 2University of Chinese Academy of Sciences; 3Institute of Automation, Chinese Academy of Sciences; 4National University of Singapore; 5University of Oxford; 6Renmin University of China zhouxiangxin1998@gmail.com; {liuzc, tianyupang, linmin, duchao}@sea.com "
[28.05.2025 15:11] Response: ```python
[
    "Sea AI Lab, Singapore",
    "University of Chinese Academy of Sciences",
    "Institute of Automation, Chinese Academy of Sciences",
    "National University of Singapore",
    "University of Oxford",
    "Renmin University of China"
]
```
[28.05.2025 15:11] Deleting PDF ./assets/pdf/2505.21493.pdf.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20561.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20561.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20561.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20426.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20426.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20426.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21178.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21178.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21178.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19433.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19433.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19433.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20321.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20321.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20321.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.17005.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.17005.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.17005.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19973.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19973.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19973.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.18657.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.18657.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.18657.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.18134.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.18134.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.18134.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.17908.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.17908.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.17908.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.16673.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.16673.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.16673.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.11277.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.11277.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.11277.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21499.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21499.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21499.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20286.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20286.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20286.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19650.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19650.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19650.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19377.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19377.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19377.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.17190.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.17190.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.17190.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.16340.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.16340.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.21471.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.21471.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.21471.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20162.
[28.05.2025 15:11] Downloading paper 2505.20162 from http://arxiv.org/pdf/2505.20162v1...
[28.05.2025 15:11] Extracting affiliations from text.
[28.05.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 2 6 1 0 2 . 5 0 5 2 : r Capability-Based Scaling Laws for LLM Red-Teaming Alexander Panfilov1,2,3 Paul Kassianik4 Maksym Andriushchenko5 Jonas Geiping1,2,3 1ELLIS Institute T端bingen 3T端bingen AI Center 2Max Planck Institute for Intelligent Systems 4Foundation AI Cisco Systems Inc. 5EPFL "
[28.05.2025 15:11] Response: ```python
[
    "ELLIS Institute T端bingen",
    "T端bingen AI Center",
    "Max Planck Institute for Intelligent Systems",
    "Foundation AI",
    "Cisco Systems Inc.",
    "EPFL"
]
```
[28.05.2025 15:11] Deleting PDF ./assets/pdf/2505.20162.pdf.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20052.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20052.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20052.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.20036.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.20036.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.20036.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.19954.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.19954.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.19954.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.17855.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.17855.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.17855.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2505.15561.
[28.05.2025 15:11] Extra JSON file exists (./assets/json/2505.15561.json), skip PDF parsing.
[28.05.2025 15:11] Paper image links file exists (./assets/img_data/2505.15561.json), skip HTML parsing.
[28.05.2025 15:11] Success.
[28.05.2025 15:11] Enriching papers with extra data.
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 0. ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  					AI-generated summary 				 Large Language Models (LLMs) have extended their impact beyond Natural...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 1. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 2. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 3. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 4. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 5. SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have de...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 6. LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up t...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 7. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 8. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 9. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 10. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 11. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 12. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 13. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 14. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 15. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 16. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 17. HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at vi...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 18. Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled fro...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 19. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 20. The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 21. NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.  					AI-generated summary 				 In many real-world applications, deployed models encounter inputs that differ from th...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 22. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 23. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 24. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 25. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 26. Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown pro...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 27. The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verifica...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 28. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 29. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 30. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 31. BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) ha...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 32. Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understandin...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 33. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 34. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 35. As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 36. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 37. Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ig...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 38. BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.  					AI-generated summary 				 Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However,...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 39. R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinati...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 40. DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 41. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 42. MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.  					AI-generated summary 				 Recent advances in Multimodal Large Language Models (MLLMs) hav...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 43. VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  					AI-generated summary 				 Vision-language models (VLMs) have achieved strong results on coding and ma...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 44. ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  					AI-generated summary 				 With the rapid advan...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 45. Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 46. AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  					AI-generated summary 				 Large language models have ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 47. AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 48. Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.  					AI-generated summary 				 Recent advances in large language models (LLMs) have enabled agents to autono...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 49. UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent c...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 50. Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-a...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 51. Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 52. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 53. The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.  					AI-generated summary 				 With the rapid advancement of post-training techniques for reasoning and informa...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 54. Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  					AI-generated summary 				 As large language models grow in capability and a...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 55. A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.  					AI-generated summary 				 Protein language models (PLMs) have emerged as powerful tools to detect comple...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 56. The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatena...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 57. A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.  					AI-generated summary 				 The differential diagnosis of neurodegenerative dementias is a challeng...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 58. CLUE generates natural language explanations for a language model's uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.  					AI-generated summary 				 Understanding sources of a model's ...
[28.05.2025 15:11] ********************************************************************************
[28.05.2025 15:11] Abstract 59. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capabilit...
[28.05.2025 15:11] Read previous papers.
[28.05.2025 15:11] Generating reviews via LLM API.
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#science", "#multimodal", "#agents", "#benchmark"], "emoji": "И", "ru": {"title": "ScienceBoard: 亠舒仍亳亳仆舒 亠亟舒 亟仍 仂亠仆从亳 -舒亞亠仆仂于 于 仆舒仆 亰舒亟舒舒", "desc": "ScienceBoard 仗亠亟舒于仍磳 仂弍仂亶 亠舒仍亳亳仆 亠亟 亟仍 仆舒仆 舒弍仂亳 仗仂亠仂于 亳 弍亠仆仄舒从 亟仍 仂亠仆从亳 仗仂亳亰于仂亟亳亠仍
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "", "ru": {"title": "舒从于舒 仗仂弍亠仍 于 仍仂亞亳从亠 亳从于亠仆仆仂亞仂 亳仆亠仍仍亠从舒", "desc": "MME-Reasoning - 仂 仆仂于亶 从仂仄仗仍亠从仆亶 弍亠仆仄舒从 亟仍 仂亠仆从亳 仗仂仂弍仆仂亠亶 仄仍亳仄仂亟舒仍仆 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶 (MLLM) 从 仍仂亞亳亠从仂仄 舒亢亟亠仆亳.
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "ｨ", "ru": {"title": "丕仆亳于亠舒仍仆舒 仂亞仍舒仂于舒仆仆仂 亳仍 于 亞亠仆亠舒亳亳 亳亰仂弍舒亢亠仆亳亶", "desc": "OmniConsistency - 仂 仆亳于亠舒仍仆亶 仗仍舒亞亳仆 亟仍 仍亠仆亳 仂亞仍舒仂于舒仆仆仂亳 亳仍亳亰舒亳亳 于 亰舒亟舒舒 仗亠仂弍舒亰仂于舒仆亳 亳亰仂弍舒
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "種", "ru": {"title": "于仂仄舒亳亠从舒 亞亠仆亠舒亳 仆舒仆 仗仂亠仂于: 仂 舒亳 从 于亳亰舒仍亳亰舒亳亳", "desc": "亅舒 舒 仗亠亟舒于仍磳 仗亠于亶 舒仍仂仆仆亶 亠 亳 仆舒弍仂 仄亠亳从 亟仍 亞亠仆亠舒亳亳 舒从舒亟亠仄亳亠从亳 仗仂亠仂于, 仂仗仂
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "ｬ", "ru": {"title": "OpenS2V-Nexus: 亠于仂仍ﾑ亳 于 亞亠仆亠舒亳亳 于亳亟亠仂 仆舒 仂仆仂于亠 亰舒亟舒仆仆仂亞仂 仂亟亠亢舒仆亳", "desc": "弌舒 仗亠亟舒于仍磳 OpenS2V-Nexus - 亳仆舒从 亟仍 亞亠仆亠舒亳亳 于亳亟亠仂 仆舒 仂仆仂于亠 亰舒亟舒仆仆仂亞仂 仂亟亠亢舒仆亳
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#training", "#rl", "#dataset", "#reasoning", "#open_source"], "emoji": "", "ru": {"title": "SynLogic: 仗仂于 于 仂弍亠仆亳亳  仍仂亞亳亠从仂仄 仄仍亠仆亳", "desc": "SynLogic - 仂 亠亶仄于仂从 亟仍 亳仆亠亰舒 亟舒仆仆, 从仂仂亶 仍舒亠 仗仂仂弍仆仂亳 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶 (LLM) 从 仍仂亞亳亠从仂仄 舒亢亟亠
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#data", "#long_context", "#architecture", "#multimodal"], "emoji": "", "ru": {"title": "亞仆仂于亠仆仆舒 亞亠仆亠舒亳 亟仍亳仆仆 亠从仂于  仗仂仄仂 LLM", "desc": "仍亠亟仂于舒仆亳亠 仗仂从舒亰于舒亠, 仂 弍仂仍亳亠 磶从仂于亠 仄仂亟亠仍亳 (LLM) 仗仂仂弍仆 亞亠仆亠亳仂于舒 亟仍亳仆仆亠 亠从 于 仂亟亳仆 仗仂仂亟, 亳仗仂仍亰 仂仍从仂 亟于舒 
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "･", "ru": {"title": "VerIPO: 丕仍亠仆亳亠 舒亢亟亠仆亳亶 于亳亟亠仂-LLM  仗仂仄仂 于亠亳亳从舒仂舒", "desc": "弌舒 仗亠亟舒于仍磳 仄亠仂亟 VerIPO 亟仍 仍亠仆亳 仗仂仂弍仆仂亠亶 于亳亟亠仂-LLM 从 舒亢亟亠仆亳礆. 亠仂亟 亳仗仂仍亰亠 Verif
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "", "ru": {"title": "仂仂亠 仄仍 - 弍亠亠 于于仂亟: 仂仗亳仄亳亰舒亳 舒亢亟亠仆亳亶 于 LLM", "desc": "弌舒 亳仍亠亟亠 亠从亳于仆仂 从仂仂从亳 亠仗仂亠从 舒亢亟亠仆亳亶 于 从仗仆 磶从仂于 仄仂亟亠仍 (LLM) 亟仍 亰舒亟舒 舒亢亟亠仆亳. 于仂 仗亠亟仍舒亞舒ﾑ 仄亠仂亟 
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "", "ru": {"title": "MMMR: 仂于亶 舒仆亟舒 仂亠仆从亳 仄仍亳仄仂亟舒仍仆仂亞仂 仄仍亠仆亳 ", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 弍亠仆仄舒从 MMMR 亟仍 仂亠仆从亳 仄仍亳仄仂亟舒仍仆仂亞仂 舒亢亟亠仆亳 于 从仗仆 磶从仂于 仄仂亟亠仍 (MLLM). MMMR 于从仍ﾑ舒亠 仆舒弍仂 亟舒仆仆
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#synthetic", "#open_source", "#dataset", "#agents", "#training", "#data"], "emoji": "", "ru": {"title": "UI-Genie: 舒仄仂仂弍舒ﾑ亳亶 -仗仂仄仂仆亳从 亟仍 亞舒亳亠从亳 亳仆亠亠亶仂于", "desc": "UI-Genie - 仂 舒仄仂仂于亠亠仆于ﾑ舒 亳亠仄舒 亟仍 舒亞亠仆仂于 亞舒亳亠从仂亞
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "鏝", "ru": {"title": "弌亠仄舒仆亳亠从舒 仂仗亳仄亳亰舒亳 亟仍 弍仂亶 亳 从舒亠于亠仆仆仂亶 亞亠仆亠舒亳亳 于亳亟亠仂", "desc": "SVG2 - 仂 亠亶仄于仂从 亟仍 仍亠仆亳 亠从亳于仆仂亳 亳 从舒亠于舒 亞亠仆亠舒亳亳 于亳亟亠仂 弍亠亰 亟仂仗仂仍仆亳亠仍仆仂亞仂 仂弍亠仆亳. 仆 亳仗仂仍亰
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "･", "ru": {"title": "亞舒仆亳亠仆亳 仄仍亳仄仂亟舒仍仆 仄仂亟亠仍亠亶 于 亰舒亟舒亠 OCR 仆舒 于亳亟亠仂", "desc": "仍亳仄仂亟舒仍仆亠 弍仂仍亳亠 磶从仂于亠 仄仂亟亠仍亳 (MLLM) 仗仂从舒亰于舒ﾑ 仆亠于仂从 仂仆仂 于 亰舒亟舒亠 仂仗亳亠从仂亞仂 舒仗仂亰仆舒于舒仆亳 亳仄于仂仍仂于
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "З", "ru": {"title": "GraLoRA: 舒仆仍仆舒 仆亳亰从仂舒仆亞仂于舒 舒亟舒仗舒亳 亟仍 亠从亳于仆仂亶 仆舒仂亶从亳 亞亠仆亠舒亳于仆 仄仂亟亠仍亠亶", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 仄亠仂亟 舒亟舒仗舒亳亳 仄仂亟亠仍亠亶 仄舒亳仆仆仂亞仂 仂弍亠仆亳 仗仂亟 仆舒亰于舒仆亳亠仄 Granular L
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "居", "ru": {"title": "丿亠仍仂从 丱仂仍仄 亟仍 : 仆仂于亶 于亰仂于 于 仗仂仆亳仄舒仆亳亳 于亳亟亠仂", "desc": "Video-Holmes - 仂 仆仂于亶 弍亠仆仄舒从 亟仍 仂亠仆从亳 仗仂仂弍仆仂亠亶 仄仍亳仄仂亟舒仍仆 磶从仂于 仄仂亟亠仍亠亶 从 仍仂亢仆仄 舒亢亟亠仆亳礆 仆舒 仂仆仂于亠 于亳亟亠仂. 仆 亳
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "", "ru": {"title": "rStar-Coder: 仗仂于 于 仂弍亠仆亳亳 磶从仂于 仄仂亟亠仍亠亶 舒亢亟亠仆亳礆 仂 从仂亟亠", "desc": "仍亠亟仂于舒亠仍亳 仗亠亟舒于亳仍亳 rStar-Coder - 从仗仆仂仄舒舒弍仆亶 亟舒舒亠 亟仍 仍亠仆亳 仗仂仂弍仆仂亠亶 磶从仂于 仄仂亟亠仍亠亶 (
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "", "ru": {"title": "MetaMind: 从于亠仆仆亶 亳仆亠仍仍亠从  亠仍仂于亠亠从亳仄 仂亳舒仍仆仄 仗仂仆亳仄舒仆亳亠仄", "desc": "MetaMind - 仂 仄仆仂亞仂舒亞亠仆仆舒 亳亠仄舒, 仍舒ﾑ舒 仗仂仂弍仆仂 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶 于仗仂仍仆 亰舒亟舒亳
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#inference", "#video", "#optimization"], "emoji": "ｬ", "ru": {"title": "HoliTom: 亠于仂仍ﾑ亳仂仆仆仂亠 亢舒亳亠 仂从亠仆仂于 亟仍 亠从亳于仆 于亳亟亠仂-LLM", "desc": "HoliTom - 仂 仆仂于亶 亠亶仄于仂从 亟仍 亠从亳于仆仂亞仂 亢舒亳 仂从亠仆仂于 于 于亳亟亠仂-LLM 仄仂亟亠仍. 仆 仂亠舒亠 于仆亠仆ﾑ 仂弍亠亰从 LLM 亠亠亰 亞仍仂弍舒仍仆 于亠仄
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#healthcare", "#reasoning", "#benchmark", "#rl", "#dataset", "#interpretability"], "emoji": "", "ru": {"title": "亠于仂仍ﾑ亳 于 仂弍亠仆亳亳 仄亠亟亳亳仆从亳 : 舒亢亟亠仆亳亠 弍亠亰 磦仆 亳仆从亳亶", "desc": "弌舒 仗亠亟舒于仍磳 AlphaMed - 仗亠于 仄亠亟亳亳仆从 仄仂亟亠仍 弍仂仍仂亞仂 磶从舒 (LLM), 亟亠仄仂仆亳
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "種", "ru": {"title": "ImgEdit: 仗仂于 于 亠亟舒从亳仂于舒仆亳亳 亳亰仂弍舒亢亠仆亳亶  仗仂仄仂 ", "desc": "仍亠亟仂于舒亠仍亳 仗亠亟舒于亳仍亳 ImgEdit - 从仗仆仂仄舒舒弍仆亶 仆舒弍仂 亟舒仆仆 亟仍 亠亟舒从亳仂于舒仆亳 亳亰仂弍舒亢亠仆亳亶, 仂亟亠亢舒亳
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#alignment"], "emoji": "", "ru": {"title": "舒从亳亠 舒亶仆 仄仆仂亞仂磶仆仂亳 于 仆亠亶仂仆舒 LLM", "desc": "仍亠亟仂于舒仆亳亠 仗亠亟仍舒亞舒亠 舒仍亞仂亳仄 弍仂仍亠亠 仂仆仂亶 亳亟亠仆亳亳从舒亳亳 仆亠亶仂仆仂于 亟仍 仂弍仆舒亢亠仆亳 磶从仂于仂-仗亠亳亳仆 亳 磶从仂于仂-舒亞仆仂亳亠从亳 仆亠亶仂仆仂于 于 弍仂仍亳 
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#healthcare", "#cv", "#reasoning", "#benchmark", "#interpretability"], "emoji": "", "ru": {"title": "NOVA: 亅从亠仄舒仍仆亶 亠 仆舒 仂弍仂弍亠仆亳亠  于 仄亠亟亳亳仆从仂亶 于亳亰舒仍亳亰舒亳亳", "desc": "NOVA - 仂 仆仂于亶 弍亠仆仄舒从 亟仍 仂亠仆从亳 仄仍亳仄仂亟舒仍仆 仄仂亟亠仍亠亶 仆舒 亠亟从亳 仗舒仂仍仂亞亳 丐 亞仂仍仂于仆仂亞仂 仄仂亰亞舒.
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "鏝", "ru": {"title": "ACTIVE-O3: 舒亟亠仍亠仆亳亠 MLLM 舒从亳于仆仄 于仂仗亳亳亠仄 亟仍 亠从亳于仆仂亞仂 仗亳仆亳 亠亠仆亳亶", "desc": "弌舒 仗亠亟舒于仍磳 ACTIVE-O3 - 亠亶仄于仂从 仂弍亠仆亳  仗仂亟从亠仗仍亠仆亳亠仄 亟仍 仆舒亟亠仍亠仆亳 
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "ｬ", "ru": {"title": "丕仗舒于仍磳仄舒 亞亠仆亠舒亳 于亳亟亠仂: 仆仂于亶 仂于亠仆 从仂仆仂仍 仆舒亟 仂弍亠从舒仄亳 于 从舒亟亠", "desc": "弌舒 仗仂于亠仆舒 仍亠仆亳 从仂仆仂仍亳亠仄仂亳, 于亠仄亠仆仆仂亶 仂亞仍舒仂于舒仆仆仂亳 亳 亟亠舒仍亳亰舒亳亳 于 亞亠仆
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#3d", "#games"], "emoji": "", "ru": {"title": "仂于亶 弍亠亢 于 仗仂舒仆于亠仆仆仂仄 亳仆亠仍仍亠从亠 仄仂亟亠仍亠亶 从仂仄仗ﾑ亠仆仂亞仂 亰亠仆亳", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 弍亠仆仄舒从 ViewSpatial-Bench 亟仍 仂亠仆从亳 仗仂仂弍仆仂亠亶 仄仂亟亠仍亠亶 从仂仄仗ﾑ亠仆仂亞仂 亰亠仆亳 从 仗仂舒仆
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "ｯ", "ru": {"title": "丐仂仆仂亠 仗舒于仍亠仆亳亠 磶从仂于仄亳 仄仂亟亠仍礆亳 亠亠亰 舒仂仄舒仆亠 从仂仄仗仂仆亠仆 亰仆舒仆亳亶", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 仄亠仂亟 仗仂亟 仆舒亰于舒仆亳亠仄 Steering Target Atoms (STA) 亟仍 仂仆仂亞仂 从仂仆仂仍 仆舒亟 亞亠仆亠舒亳亠亶
[28.05.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#games", "#architecture", "#open_source"], "emoji": "", "ru": {"title": "舒仂于亠 仄仂亟亠仍亳 从仂亟舒: 仆仂于亶 仂于亠仆 亞亠仆亠舒亳亳 于 仂从 -亳亠仄舒", "desc": "弌舒 仗亠亟舒于仍磳 Code Graph Models (CGM) - 仆仂于亶 仗仂亟仂亟 从 亞亠仆亠舒亳亳 从仂亟舒 仆舒 仂于仆亠 亠
[28.05.2025 15:11] Querying the API.
[28.05.2025 15:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree.
[28.05.2025 15:11] Response: {
  "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 仄亠仂亟 仂弍亠仆亳 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶 (LLM) 仗仂亟 仆舒亰于舒仆亳亠仄 VeriFree. 亅仂 仄亠仂亟 亳仗仂仍亰亠 仂弍亠仆亳亠  仗仂亟从亠仗仍亠仆亳亠仄 亟仍 仄舒从亳仄亳亰舒亳亳 于亠仂仆仂亳 亞亠仆亠舒亳亳 舒仍仂仆仆仂亞仂 仂于亠舒, 亳亰弍亠亞舒 仆亠仂弍仂亟亳仄仂亳 于 仂亟亠仍仆仂亶 仄仂亟亠仍亳-于亠亳亳从舒仂亠. VeriFree 仗仂从舒亰于舒亠 亠亰仍舒, 仂仗仂舒于亳仄亠 亳仍亳 仗亠于仂仂亟亳亠 仄亠仂亟  于亠亳亳从舒仂仂仄 仆舒 舒亰仍亳仆 亠舒, 于从仍ﾑ舒 MMLU-Pro 亳 GPQA. 亠仂亟 仗仂亰于仂仍磳 舒亳亳 仂弍亠仆亳亠 于 亳仍亠 DeepSeek-R1-Zero 仆舒 仂弍亳亠 仂弍仍舒亳 舒亢亟亠仆亳亶, 舒从亳亠 从舒从 亳仄亳, 仄亠亟亳亳仆舒 亳 从仂仆仂仄亳从舒.",

  "emoji": "",

  "title": "VeriFree: 仂弍亠仆亳亠 LLM 弍亠亰 于亠亳亳从舒仂舒 亟仍 仆亳于亠舒仍仆 舒亢亟亠仆亳亶"
}
[28.05.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree."

[28.05.2025 15:11] Response: ```python
["RL", "TRAINING", "BENCHMARK", "MATH"]
```
[28.05.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree."

[28.05.2025 15:11] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[28.05.2025 15:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new method called VeriFree for training large language models (LLMs) without the need for a separate verifier. Traditional reinforcement learning (RL) approaches rely on rule-based verification, which limits their application in complex real-world domains. VeriFree simplifies the process by directly maximizing the likelihood of generating correct answers, eliminating the need for an additional verifier model. The results show that VeriFree not only reduces computational demands but also performs as well or better than existing verifier-based methods across various benchmarks.","title":"VeriFree: Reinventing Reinforcement Learning for Language Models Without Verifiers"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new method called VeriFree for training large language models (LLMs) without the need for a separate verifier. Traditional reinforcement learning (RL) approaches rely on rule-based verification, which limits their application in complex real-world domains. VeriFree simplifies the process by directly maximizing the likelihood of generating correct answers, eliminating the need for an additional verifier model. The results show that VeriFree not only reduces computational demands but also performs as well or better than existing verifier-based methods across various benchmarks.', title='VeriFree: Reinventing Reinforcement Learning for Language Models Without Verifiers'))
[28.05.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"菴鐚篏睡DeepSeek-R1-Zero蕋主綣阪絖箙鐚RL鐚莅膸紊у莚荐罔≦鐚LLMs鐚篁ｇ医ｨ拷√緇篋乗菴絮区鐚菴腱号篁篋篁ヨ茵坂茹膈罅薨莚篁糸￥羈倶絮医絖紫綏ョ羈緇筝膸闈膈ｰ絎筝蘂茹ｅ恰号篏睡蘂紊LLM篏筝堺─薨莚鐚篏菴篌綣ュ劫失薨莚LLM箴莎絅演絎∽糸紫蕋篁ュ莅膸雁惨ら莚罔≦絎茣筝坂茹ｅ活篋蘂綛九DeepSeek-R1-Zero蕋主莅膸絮遺ｨ蘂鐚篁坂筝腱薨莚号鐚VeriFree鐚鐚莚ユ号膸菴膈罅薨莚鐚贋･篏睡RL紊у膈罅网","title":"薨莚綣阪絖箙井号"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='菴鐚篏睡DeepSeek-R1-Zero蕋主綣阪絖箙鐚RL鐚莅膸紊у莚荐罔≦鐚LLMs鐚篁ｇ医ｨ拷√緇篋乗菴絮区鐚菴腱号篁篋篁ヨ茵坂茹膈罅薨莚篁糸￥羈倶絮医絖紫綏ョ羈緇筝膸闈膈ｰ絎筝蘂茹ｅ恰号篏睡蘂紊LLM篏筝堺─薨莚鐚篏菴篌綣ュ劫失薨莚LLM箴莎絅演絎∽糸紫蕋篁ュ莅膸雁惨ら莚罔≦絎茣筝坂茹ｅ活篋蘂綛九DeepSeek-R1-Zero蕋主莅膸絮遺ｨ蘂鐚篁坂筝腱薨莚号鐚VeriFree鐚鐚莚ユ号膸菴膈罅薨莚鐚贋･篏睡RL紊у膈罅网', title='薨莚綣阪絖箙井号'))
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "種", "ru": {"title": "亅亠从亳于仆舒 亞亠仆亠舒亳 亳亰仂弍舒亢亠仆亳亶 仂 仂弍亠亞仂 从 舒仆仂仄", "desc": "DetailFlow - 仂 仆仂于亶 仄亠仂亟 亞亠仆亠舒亳亳 亳亰仂弍舒亢亠仆亳亶, 亳仗仂仍亰ﾑ亳亶 舒于仂亠亞亠亳仂仆仆亶 仗仂亟仂亟  仗仂仍亠亟仂于舒亠仍仆仄 仂仆亠仆亳亠仄 
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "", "ru": {"title": "SeePhys: 于磦仍亠仆亳亠 仂亞舒仆亳亠仆亳亶 于亳亰舒仍仆仂亞仂 仄仍亠仆亳 LLM 于 亳亰亳从亠", "desc": "SeePhys - 仂 仆仂于亶 仄仍亳仄仂亟舒仍仆亶 弍亠仆仄舒从 亟仍 仂亠仆从亳 仗仂仂弍仆仂亠亶 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶 (LLM) 于 仂弍仍
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "ｯ", "ru": {"title": "丕仂于亠亠仆于仂于舒仆仆舒 舒舒从舒 仆舒 仄仍亳仄仂亟舒仍仆亠 磶从仂于亠 仄仂亟亠仍亳 亠亠亰 仂仗亳仄舒仍仆仂亠 于舒于仆亳于舒仆亳亠 仗亳亰仆舒从仂于", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 仄亠仂亟 舒舒从亳 仆舒 仄仍亳仄仂亟舒仍仆亠 磶从仂于亠 仄仂亟亠仍亳, 仆舒亰于舒亠仄亶 FOA-Attack. 
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "BARL: 丕仄仆亠亠 亳仍亠亟亠仄, 亠从亳于仆亠亠 舒亢亟舒亠仄", "desc": "BARL - 仂 仆仂于舒 亳亠仄舒 舒亶亠仂于从仂亞仂 舒亟舒仗亳于仆仂亞仂 仂弍亠仆亳  仗仂亟从亠仗仍亠仆亳亠仄 亟仍 仍亠仆亳 舒弍仂 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶. 仆舒 亳仆亠亞亳
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#3d", "#cv", "#interpretability", "#multimodal", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "MMPerspective: 仆仂于亶 弍亠亢 于 仂亠仆从亠 仗仂舒仆于亠仆仆仂亞仂 仗仂仆亳仄舒仆亳 ", "desc": "弌舒 仗亠亟舒于仍磳 MMPerspective - 仗亠于亶 弍亠仆仄舒从 亟仍 仂亠仆从亳 仗仂仆亳仄舒仆亳 仗亠仗亠从亳于 仄仍亳仄仂亟舒仍
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "", "ru": {"title": "VisTA: 于仂仆仂仄仆仂亠 仍亠仆亳亠 于亳亰舒仍仆仂亞仂 仄仍亠仆亳  仗仂仄仂 仂弍亠仆亳  仗仂亟从亠仗仍亠仆亳亠仄", "desc": "VisTA - 仂 仆仂于舒 亳亠仄舒 仂弍亠仆亳  仗仂亟从亠仗仍亠仆亳亠仄 亟仍 仍亠仆亳 于亳亰舒仍仆仂亞仂 仄仍亠仆亳. 
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "鏝", "ru": {"title": "弌亳仄仄亠亳仆仂亠 于仆亠亟亠仆亳亠 亞舒仆亳仆 从舒亟仂于 亟仍 仍亠仆仆仂亞仂 亳仆亠亰舒 于亳亟亠仂", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 仗仂亟仂亟 从 亳仆亠亰 仗仂仄亠亢仂仆 于亳亟亠仂从舒亟仂于 仄亠亢亟 亰舒亟舒仆仆仄亳 仆舒舒仍仆仄 亳 从仂
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#rl", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "舒从仂 - 亠舒 舒仍舒仆舒: 仆仂于亶 仄亠仂亟 亟仍 仍亠仆亳 舒亢亟亠仆亳亶 ", "desc": "仍亠亟仂于舒亠仍亳 仗亠亟仍仂亢亳仍亳 仆仂于亶 仄亠仂亟 仂弍亠仆亳 磶从仂于 仄仂亟亠仍亠亶 亟仍 仍亠仆亳 亳 仗仂仂弍仆仂亳 从 舒
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "鏝", "ru": {"title": "丕从仂亠仆亳亠 亞亠仆亠舒亳亳 亟仍亳仆仆 于亳亟亠仂  仗仂仄仂 舒仗亠亟亠仍亠仆仆仂亞仂 于于仂亟舒", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于 舒亠亞亳 舒仗亠亟亠仍亠仆仆仂亞仂 于于仂亟舒 亟仍 于亳亟亠仂-亟亳亰亳仂仆仆 仄仂亟亠仍亠亶 仆舒 仂仆仂于亠 Diffusion Transfor
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#agents", "#optimization", "#inference", "#benchmark"], "emoji": "", "ru": {"title": "ACBench: 仗亠于亶 从仂仄仗仍亠从仆亶 弍亠仆仄舒从 亟仍 仂亠仆从亳 舒亞亠仆仆 仗仂仂弍仆仂亠亶 亢舒 LLM", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于亶 弍亠仆仄舒从 ACBench 亟仍 仂亠仆从亳 于仍亳礌亳 亢舒亳 仆舒 舒亞亠仆仆亠 仗仂仂弍仆仂亳 弍仂仍亳 磶
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#open_source", "#science", "#dataset", "#multimodal", "#benchmark", "#reasoning"], "emoji": "К", "ru": {"title": "亠仆从舒 仆舒仆仂亞仂 仄仍亠仆亳 于 亠从仂-SQL 亰舒亟舒舒 亟仍 弍亳仂仄亠亟亳亳仆", "desc": "BiomedSQL - 仂 仆仂于亶 舒仍仂仆仆亶 亠 亟仍 仂亠仆从亳 仆舒仆仂亞仂 仄仍亠仆亳 于 亰舒亟舒舒 仗亠仂弍舒亰仂于舒仆亳 亠从
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#hallucinations", "#optimization", "#rl", "#reasoning", "#rag"], "emoji": "", "ru": {"title": "亟舒仗亳于仆仂亠 仂弍亠仆亳亠 LLM: 仂弍亠亟亳仆磳仄 于仆亠仆仆亳亠 亳 于仆亠仆亳亠 亰仆舒仆亳", "desc": "R1-Searcher++ - 仂 仆仂于舒 亳亠仄舒, 仍舒ﾑ舒 舒弍仂 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶 (LLM) 仗亠仄 舒亟舒仗亳于仆仂亶 亳仆亠亞舒亳亳 
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#science", "#hallucinations", "#benchmark", "#dataset", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "仂仄仗仍亠从仆舒 仂亠仆从舒 磶从仂于 仄仂亟亠仍亠亶 亟仍 亳仂于仂亶 从亳仄亳仆舒仍亳亳从亳", "desc": "DFIR-Metric - 仂 从仂仄仗仍亠从仆亶 亳仆仄亠仆 亟仍 仂亠仆从亳 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶 (LLM) 于 仂弍仍舒
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#audio"], "emoji": "鏝", "ru": {"title": "SoloSpeech: 亞亠仆亠舒亳于仆仂亠 亳亰于仍亠亠仆亳亠 亠仍亠于仂亶 亠亳 仆仂于仂亞仂 仗仂从仂仍亠仆亳", "desc": "SoloSpeech - 仂 仆仂于亶 亞亠仆亠舒亳于仆亶 仗仂亟仂亟 从 亳亰于仍亠亠仆亳 亠仍亠于仂亶 亠亳 亳亰 仄亠亳 亞仂仍仂仂于. 仆 亳仗仂仍亰亠 从舒从舒亟仆亶 仗舒亶仗仍舒亶仆, 于从仍ﾑ舒ﾑ亳亶 亢舒亳亠, 亳亰于仍亠亠仆亳亠, 亠从仂仆从亳
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#interpretability", "#training", "#agi", "#multimodal", "#architecture"], "emoji": "", "ru": {"title": "亠仂亟仂仍亠仆亳亠 磶从仂于仂亞仂 亟仂仄亳仆亳仂于舒仆亳 于 仄仍亳仄仂亟舒仍仆 -亳亠仄舒", "desc": "弌舒 亳仍亠亟亠 仗仂弍仍亠仄 仄仂亟舒仍仆仂亶 仗亠亟于亰仂亳 于 仄仍亳仄仂亟舒仍仆 弍仂仍亳 磶从仂于 仄仂亟亠仍 (MLLM), 从
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#games", "#cv", "#multimodal", "#benchmark", "#video"], "emoji": "ｮ", "ru": {"title": "亳亟亠仂亳亞 从舒从 于亰仂于 亟仍 亳从于亠仆仆仂亞仂 亳仆亠仍仍亠从舒", "desc": "VideoGameBench - 仂 仆仂于亶 弍亠仆仄舒从 亟仍 仂亠仆从亳 仗仂仂弍仆仂亠亶 仄仂亟亠仍亠亶 仄舒亳仆仆仂亞仂 亰亠仆亳 亳 仂弍舒弍仂从亳 亠亠于亠仆仆仂亞仂 磶从舒 于 从仂仆亠从亠 于亰舒亳仄仂
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#games", "#training", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "ComfyMind: 丕仍亠仆亳亠 亞亠仆亠舒亳于仆 舒弍仂亳 仗仂亠仂于  仗仂仄仂 亠仄舒仆亳亠从仂亞仂 亳仆亠亠亶舒 亳 舒亟舒仗亳于仆仂亞仂 仗仍舒仆亳仂于舒仆亳", "desc": "ComfyMind - 仂 亳亠仄舒 亳从于亠仆仆仂亞仂 亳仆亠仍
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#rl", "#training", "#multimodal", "#benchmark", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "丕亳仍亠仆亳亠 舒亢亟亠仆亳亶 MLLM 亠亠亰 舒亰亟亠仍磳仄仂亠 仂弍亠仆亳亠  仗仂亟从亠仗仍亠仆亳亠仄", "desc": "Share-GRPO - 仂 仆仂于亶 仗仂亟仂亟 于 仂弍仍舒亳 仂弍亠仆亳  仗仂亟从亠仗仍亠仆亳亠仄, 从仂仂亶 仍舒亠 仄仍亳仄仂亟
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#rag", "#rl", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "AutoRefine: 丕仄仆仂亠 仂仆亠仆亳亠 亰仆舒仆亳亶 亟仍 仍亠仆亳 舒亢亟亠仆亳亶 ", "desc": "AutoRefine - 仂 亳亠仄舒 仂弍亠仆亳  仗仂亟从亠仗仍亠仆亳亠仄 亟仍 弍仂仍亳 磶从仂于 仄仂亟亠仍亠亶, 从仂仂舒 仍舒亠 舒亢亟亠仆亳 仆舒 仂仆仂于
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#cv", "#agents", "#security"], "emoji": "居", "ru": {"title": "亠从仍舒仄舒 从舒从 仂亢亳亠: 仆仂于舒 亞仂亰舒 亟仍 -舒亞亠仆仂于 于 亠亳", "desc": "AdInject - 仂 仆仂于亶 仄亠仂亟 舒舒从亳 仆舒 于亠弍-舒亞亠仆仂于, 仂仆仂于舒仆仆 仆舒 仄仍亳仄仂亟舒仍仆 磶从仂于 仄仂亟亠仍,  亳仗仂仍亰仂于舒仆亳亠仄 亳仆亠仆亠-亠从仍舒仄 亟仍 于仆亠亟亠仆亳 于亠亟仂仆仂仆
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#open_source", "#agi", "#agents", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": "仂仂舒 - 从仍ﾑ 从 仆亳于亠舒仍仆仂仄 -舒亞亠仆", "desc": "仍亳舒 - 仂 舒亞亠仆 亳仂从仂亞仂 仗仂亳仍, 亟仂亳亞舒ﾑ亳亶 于仂从仂亶 仗仂亳亰于仂亟亳亠仍仆仂亳 仆舒 舒亰仍亳仆 亠舒 弍仍舒亞仂亟舒 仄亳仆亳仄舒仍仆仂仄 仗亠亟仂仗亠亟亠仍亠仆亳 亳
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#training", "#multimodal"], "emoji": "", "ru": {"title": "UNITE: 亠仂亟仂仍亠仆亳亠 舒亰于舒 仄亠亢亟 仄仂亟舒仍仆仂礆亳 于 仄仍亳仄仂亟舒仍仆仂仄 仗仂亳从亠", "desc": "弌舒 仗亠亟舒于仍磳 UNITE - 仆亳于亠舒仍仆 亳亠仄 亟仍 仄仍亳仄仂亟舒仍仆仂亞仂 亳仆仂仄舒亳仂仆仆仂亞仂 仗仂亳从舒. 于仂 亠舒
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#optimization", "#cv"], "emoji": "", "ru": {"title": "仍仂弍舒仍仆亠 从仂仂亟亳仆舒 亟仍 仍亠仆亳 亞亠仆亠舒亳亳 亟于亳亢亠仆亳亶 亳亰 亠从舒", "desc": " 舒亠 仗亠亟仍舒亞舒亠 仆仂于亶 仗仂亟仂亟 从 亞亠仆亠舒亳亳 亟于亳亢亠仆亳亶 仆舒 仂仆仂于亠 亠从舒, 亳仗仂仍亰ﾑ亳亶 舒弍仂仍ﾑ仆亠 从仂仂亟亳仆舒 舒
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#security", "#math", "#reasoning"], "emoji": "", "ru": {"title": "丐仂仗亳亠从仂亠 于仆亳仄舒仆亳亠: 仂仂亠 仄舒舒弍仆仂-亳仆于舒亳舒仆仆仂亠 舒亢亟亠仆亳亠 亟仍 仆亠亶仂舒仍亞仂亳仄亳亠从亳 亰舒亟舒", "desc": " 舒亠 仗亠亟舒于仍亠仆 仆仂于亶 仄亠仂亟 于仆亳仄舒仆亳 仗仂亟 仆舒亰于舒仆亳亠仄 '丐仂仗亳
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#science", "#dataset", "#open_source", "#data", "#multimodal"], "emoji": "И", "ru": {"title": "CLEANMOL: 丕仍亠仆亳亠 仗仂仆亳仄舒仆亳 仄仂仍亠从仍 磶从仂于仄亳 仄仂亟亠仍礆亳", "desc": "弌舒 仗亠亟舒于仍磳 CLEANMOL - 仆仂于亶 亠亶仄于仂从 亟仍 仍亠仆亳 仗仂仆亳仄舒仆亳 仄仂仍亠从仍仆 从 弍仂
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#long_context", "#inference", "#agents", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": "ExtAgents: 舒亳亠仆亳亠 于仂亰仄仂亢仆仂亠亶 LLM 弍亠亰 于亠仍亳亠仆亳 从仂仆亠从仆仂亞仂 仂从仆舒", "desc": "弌舒 仗亠亟舒于仍磳 仆仂于 亳亠仄 ExtAgents, 从仂仂舒 仍舒亠 仄舒舒弍亳亠仄仂 亳仆亠亞舒亳亳 亰仆舒仆亳亶 于仂
[28.05.2025 15:12] Querying the API.
[28.05.2025 15:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  					AI-generated summary 				 As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.
[28.05.2025 15:12] Response: {
  "desc": "仍亠亟仂于舒仆亳亠 仗仂从舒亰于舒亠, 仂 亠从亳于仆仂 舒舒从 仆舒 弍仂仍亳亠 磶从仂于亠 仄仂亟亠仍亳 (LLM) 亠亰从仂 仆亳亢舒亠, 从仂亞亟舒 于仂亰仄仂亢仆仂亳 亠仍亠于仂亶 仄仂亟亠仍亳 仗亠于仂仂亟 于仂亰仄仂亢仆仂亳 舒舒从ﾑ亠亞仂. 于仂 仗仂于亠仍亳 从仗亠亳仄亠仆  弍仂仍亠亠 亠仄 500 仗舒舒仄亳 舒舒从ﾑ亳亶-亠仍, 亳仗仂仍亰 舒舒从亳 亳仗舒 jailbreak, 亳仄亳亳ﾑ亳亠 亟亠亶于亳 亠仍仂于亠从舒. 磦仍亠仆 亳 仂仆仂于仆亠 亠仆亟亠仆亳亳: 弍仂仍亠亠 仗仂仂弍仆亠 仄仂亟亠仍亳 磦仍ﾑ 仍亳仄亳 舒舒从ﾑ亳仄亳, 仗亠 舒舒从亳 亠亰从仂 仗舒亟舒亠 仗亳 仗亠于仂仂亟于亠 亠仍亳, 亳 仗亠 舒舒从亳 从仂亠仍亳亠  于仂从仂亶 仗仂亳亰于仂亟亳亠仍仆仂 仆舒 仂亳舒仍仆 舒亰亟亠仍舒 弍亠仆仄舒从舒 MMLU-Pro. 亠亰仍舒 从舒亰于舒ﾑ 仆舒 仆亠仂弍仂亟亳仄仂 仆仂于 舒亠亞亳亶 仂亠仆从亳 亳 仆亳亢亠仆亳 亳从仂于 亟仍 弍亟亳 仄仂亟亠仍亠亶 亳从于亠仆仆仂亞仂 亳仆亠仍仍亠从舒.",
  "emoji": "￥",
  "title": "亠于仂仂亟于仂 于 于仂亰仄仂亢仆仂 - 从仍ﾑ 从 弍亠亰仂仗舒仆仂亳 "
}
[28.05.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  					AI-generated summary 				 As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers."

[28.05.2025 15:12] Response: ```python
["BENCHMARK", "RLHF"]
```
[28.05.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  					AI-generated summary 				 As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers."

[28.05.2025 15:12] Response: ```python
['SECURITY', 'ALIGNMENT']
```
[28.05.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the effectiveness of red-teaming, which is a method used to identify vulnerabilities in large language models (LLMs). It finds that as LLMs become more capable, traditional red-teaming strategies may fail, especially when the target model is stronger than the attacker. The study evaluates over 500 pairs of attackers and targets, revealing that attack success rates drop significantly when the target model\'s capabilities exceed those of the attacker. The authors propose a scaling law to predict attack success based on the capability gap, emphasizing the need for new strategies to ensure the safe deployment of advanced AI models.","title":"Bridging the Capability Gap: Rethinking Red-Teaming for Advanced AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores the effectiveness of red-teaming, which is a method used to identify vulnerabilities in large language models (LLMs). It finds that as LLMs become more capable, traditional red-teaming strategies may fail, especially when the target model is stronger than the attacker. The study evaluates over 500 pairs of attackers and targets, revealing that attack success rates drop significantly when the target model's capabilities exceed those of the attacker. The authors propose a scaling law to predict attack success based on the capability gap, emphasizing the need for new strategies to ensure the safe deployment of advanced AI models.", title='Bridging the Capability Gap: Rethinking Red-Teaming for Advanced AI'))
[28.05.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"菴膀莅堺｢莅篋紊у莚荐罔≦膾∫羌莚筝綺鐚綣肴篋糸肢筝罔≦遵綏莊荀с腥九ｰ鐚綵罔≦遵莇菴糸肢駈糸紙篌乗筝菴500紊筝糸肢筝絲刻莅堺坂筝腱亥羌莚篌亥ワ篁ュ絲号ョ蕋膸茵鐚遵翫失罔≦糸私賢茵ｰ翫ソ鐚阪遵糸肢鐚絋篋榊瓜鐚遵√号ユ─九緇","title":"遵綏莊喝糸紙"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='菴膀莅堺｢莅篋紊у莚荐罔≦膾∫羌莚筝綺鐚綣肴篋糸肢筝罔≦遵綏莊荀с腥九ｰ鐚綵罔≦遵莇菴糸肢駈糸紙篌乗筝菴500紊筝糸肢筝絲刻莅堺坂筝腱亥羌莚篌亥ワ篁ュ絲号ョ蕋膸茵鐚遵翫失罔≦糸私賢茵ｰ翫ソ鐚阪遵糸肢鐚絋篋榊瓜鐚遵√号ユ─九緇', title='遵綏莊喝糸紙'))
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#healthcare", "#dataset", "#science"], "emoji": "К", "ru": {"title": "仆仂亞仂亰舒亟舒仆仂亠 仂弍亠仆亳亠 舒从于舒亠 仗仂亠仆亳舒仍 弍亠仍从仂于 磶从仂于 仄仂亟亠仍亠亶", "desc": "仍亠亟仂于舒亠仍亳 舒亰舒弍仂舒仍亳 仆仂于 舒亠亞亳 仗亠亟于舒亳亠仍仆仂亞仂 仂弍亠仆亳 弍亠仍从仂于 磶从仂于 仄仂亟亠仍亠亶 (PLM)  
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#leakage", "#architecture", "#dataset", "#science"], "emoji": "К", "ru": {"title": "丕仍亠仆亳亠 仗亠亟从舒亰舒仆亳 舒亳仆仆仂亳 弍亠仍仂从-弍亠仍从仂于 于亰舒亳仄仂亟亠亶于亳亶  仗仂仄仂 仗仂亟于亳仆 舒亳亠从 磶从仂于 仄仂亟亠仍亠亶", "desc": "仍亠亟仂于舒仆亳亠 仗亠亟舒于仍磳 从亳仂于舒仆仆亶 仆舒弍仂
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#3d", "#interpretability", "#cv", "#healthcare", "#multimodal", "#rl", "#reasoning"], "emoji": "", "ru": {"title": "仂亰舒仆舒 仆亠亶仂亟亳舒亞仆仂亳从舒:   仂弍仂仆仂于舒仆亳亠仄", "desc": "弌舒 仗亠亟舒于仍磳 亠亶仄于仂从 亟仍 仍亠仆亳 亟亳舒亞仆仂亳亠从仂亶 仆仂亳 仄仂亟亠仍亠亶 亞仍弍仂从仂亞仂 仂弍亠仆亳 仗亳 仆亠亶仂亟亠亞亠
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#training", "#reasoning", "#data"], "emoji": "", "ru": {"title": "CLUE: 仂亰舒仆亠 仂弍仆亠仆亳 仆亠仂仗亠亟亠仍亠仆仆仂亳 磶从仂于 仄仂亟亠仍亠亶", "desc": "CLUE - 仂 仆仂于亶 仄亠仂亟 亞亠仆亠舒亳亳 仂弍仆亠仆亳亶 仆亠仂仗亠亟亠仍亠仆仆仂亳 磶从仂于 仄仂亟亠仍亠亶 仆舒 亠亠于亠仆仆仂仄 磶从亠. 仆 于
[28.05.2025 15:12] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#benchmark", "#hallucinations"], "emoji": "", "ru": {"title": "仂亰亳亳仂仆仆仂亠 仄亠亠仆亳亠 于 RAG: 仆亠 舒从 舒仆仂, 从舒从 从舒亢亠", "desc": "舒仆仆舒 舒 亳仍亠亟亠 于仍亳礌亳亠 仗仂亰亳亳仂仆仆仂亞仂 仄亠亠仆亳 仆舒 仂仆仂 磶从仂于 仄仂亟亠仍亠亶 于 从仂仆亠从亠 Retrieval Augmented Generati
[28.05.2025 15:12] Loading Chinese text from previous data.
[28.05.2025 15:12] Renaming data file.
[28.05.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 15:12] Saving new data file.
[28.05.2025 15:12] Generating page.
[28.05.2025 15:12] Renaming previous page.
[28.05.2025 15:12] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 15:12] [Experimental] Generating Chinese page for reading.
[28.05.2025 15:12] Chinese vocab [{'word': 'OmniConsistency', 'pinyin': '', 'trans': 'OmniConsistency'}, {'word': '', 'pinyin': 'ku嘆 sn', 'trans': 'diffusion'}, {'word': '', 'pinyin': 'bin y q狸', 'trans': 'transformer'}, {'word': '紜綣', 'pinyin': 'zng qi叩ng', 'trans': 'enhance'}, {'word': '蕋', 'pinyin': 'fng g辿', 'trans': 'style'}, {'word': '筝贋', 'pinyin': 'y朝 zh狸 x狸ng', 'trans': 'consistency'}, {'word': '羈', 'pinyin': 'fn hu', 'trans': 'generalization'}, {'word': '遵', 'pinyin': 'n辿ng l狸', 'trans': 'ability'}, {'word': '', 'pinyin': 'tu狸 hu', 'trans': 'degeneration'}, {'word': '筝筝', 'pinyin': 'shng xi w辿n', 'trans': 'context'}, {'word': '罅', 'pinyin': 'kung ji', 'trans': 'framework'}, {'word': '筝ら倶', 'pinyin': 'ling ji dun', 'trans': 'two-stage'}, {'word': '羝菴', 'pinyin': 'jin j狸n', 'trans': 'progressive'}, {'word': '膈', 'pinyin': 'c竪 l端竪', 'trans': 'strategy'}, {'word': '', 'pinyin': 'ch b', 'trans': 'interpolation'}, {'word': '莅乗', 'pinyin': 'sh竪 j狸', 'trans': 'design'}, {'word': '茵ｰ', 'pinyin': 'bio xin', 'trans': 'performance'}, {'word': '･菴', 'pinyin': 'ji j狸n', 'trans': 'approach'}, {'word': '筝', 'pinyin': 'shng y竪', 'trans': 'commercial'}, {'word': '蕁九', 'pinyin': 'dng jin', 'trans': 'top-notch'}, {'word': '罔≦', 'pinyin': 'm坦 x鱈ng', 'trans': 'model'}, {'word': 'GPT-4o', 'pinyin': '', 'trans': 'GPT-4o'}]
[28.05.2025 15:12] Renaming previous Chinese page.
[28.05.2025 15:12] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 15:12] Writing Chinese reading task.
[28.05.2025 15:12] Writing result.
[28.05.2025 15:12] Renaming log file.
[28.05.2025 15:12] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
