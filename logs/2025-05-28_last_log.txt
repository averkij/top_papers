[28.05.2025 13:28] Read previous papers.
[28.05.2025 13:28] Generating top page (month).
[28.05.2025 13:28] Writing top page (month).
[28.05.2025 14:12] Read previous papers.
[28.05.2025 14:12] Get feed.
[28.05.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.19897
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19641
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21189
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21334
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17952
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21505
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14064
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16901
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20561
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21178
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19433
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17005
[28.05.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.20426
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19973
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18657
[28.05.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.18134
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17908
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16673
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11277
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21499
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20286
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19650
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19377
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20321
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21471
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20052
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20036
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19954
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17855
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17190
[28.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15561
[28.05.2025 14:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 14:12] No deleted papers detected.
[28.05.2025 14:12] Downloading and parsing papers (pdf, html). Total: 58.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.19897.
[28.05.2025 14:12] Downloading paper 2505.19897 from http://arxiv.org/pdf/2505.19897v1...
[28.05.2025 14:12] Extracting affiliations from text.
[28.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 7 9 8 9 1 . 5 0 5 2 : r SCIENCEBOARD: Evaluating Multimodal Qiushi Sun Zhoumianze Liu Chang Ma Zichen Ding Fangzhi Xu Zhangyue Yin Haiteng Zhao Zhenyu Wu Kanzhi Cheng Zhaoyang Liu Qintong Li Jianing Wang Xiangru Tang Tianbao Xie Xiachong Feng Xiang Li Ben Kao Wenhai Wang Biqing Qi Lingpeng Kong Zhiyong Wu The University of Hong Kong Shanghai AI Laboratory Fudan University Peking University Nanjing University East China Normal University Yale University {qiushisun,changma}@connect.hku.hk, {kao,lpk}@cs.hku.hk {liuzhoumianze,wangwenhai,qibiqing,wuzhiyong}@pjlab.org.cn "
[28.05.2025 14:12] Response: ```python
[
    "The University of Hong Kong",
    "Shanghai AI Laboratory",
    "Fudan University",
    "Peking University",
    "Nanjing University",
    "East China Normal University",
    "Yale University"
]
```
[28.05.2025 14:12] Deleting PDF ./assets/pdf/2505.19897.pdf.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.19641.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.19641.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.19641.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21189.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21189.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21189.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21496.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21496.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21334.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21334.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21334.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.17952.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.17952.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.17952.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21505.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21505.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21505.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.14064.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.14064.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.14064.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21500.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21500.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.16901.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.16901.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.16901.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.20561.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.20561.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.20561.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21178.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21178.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21178.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.19433.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.19433.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.19433.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.17005.
[28.05.2025 14:12] Extra JSON file exists (./assets/json/2505.17005.json), skip PDF parsing.
[28.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.17005.json), skip HTML parsing.
[28.05.2025 14:12] Success.
[28.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.20426.
[28.05.2025 14:12] Downloading paper 2505.20426 from http://arxiv.org/pdf/2505.20426v1...
[28.05.2025 14:13] Extracting affiliations from text.
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 6 2 4 0 2 . 5 0 5 2 : r MMPerspective: Do MLLMs Understand Perspective? Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness Yunlong Tang1,, Pinxin Liu1,, Mingqian Feng1,, Zhangyun Tan1,, Rui Mao1,, Chao Huang1, Jing Bi1, Yunzhong Xiao2, Susan Liang1, Hang Hua1, Ali Vosoughi1, Luchuan Song1, Zeliang Zhang1, Chenliang Xu1 1University of Rochester, 2Carnegie Mellon University {yunlong.tang, mingqian.feng, jing.bi, chenliang.xu}@rochester.edu, {pliu23, rmao6, lsong11, zzh136}@ur.rochester.edu, ztan12@u.rochester.edu, {chuang65, sliang22, hhua2}@cs.rochester.edu, avosoughi@ece.rochester.edu, yunzhonx@andrew.cmu.edu Figure 1: MMPerspective benchmark overview. We introduce 10 tasks spanning 3 complementary dimensions of perspective understanding: Perspective Perception, Reasoning, and Robustness. "
[28.05.2025 14:13] Response: ```python
["University of Rochester", "Carnegie Mellon University"]
```
[28.05.2025 14:13] Deleting PDF ./assets/pdf/2505.20426.pdf.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.19973.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.19973.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.19973.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.18657.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.18657.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.18657.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.18134.
[28.05.2025 14:13] Downloading paper 2505.18134 from http://arxiv.org/pdf/2505.18134v1...
[28.05.2025 14:13] Extracting affiliations from text.
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 4 3 1 8 1 . 5 0 5 2 : r VideoGameBench: Can Vision-Language Models complete popular video games? Alex L. Zhang Thomas L. Griffiths Karthik R. Narasimhan Ofir Press "
[28.05.2025 14:13] Response: []
[28.05.2025 14:13] Extracting affiliations from text.
[28.05.2025 14:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 4 3 1 8 1 . 5 0 5 2 : r VideoGameBench: Can Vision-Language Models complete popular video games? Alex L. Zhang Thomas L. Griffiths Karthik R. Narasimhan Ofir PressVision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humanssuch as perception, spatial navigation, and memory managementremains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and high-level description of objectives and controls, significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, setting where the game pauses while waiting for the LMs next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.Language Models (LMs) and vision-language models (VLMs) perform complex tasks remarkably well, even those that are challenging to humans such as advanced mathematics (Azerbayev et al., 2024; Lin et al., 2025) and coding (Li et al., 2022; Luo et al., 2025; OpenAI et al., 2025). However, that does not necessarily mean that they demonstrate human-level performance on all tasks. Humans have perceptual, spatial, and memory management abilities that provide strong inductive biases for learning new tasks (Lake et al., 2017; Dubey et al., 2018). To evaluate whether current AI systems are approaching those abilities, we propose new challenge: completing video games from the 1990s (also known as the 32-bit era). We introduce VideoGameBench, benchmark which challenges VLMs to complete, in real-time, suite of 10 different popular video games from both hand-held consoles (Game Boy and Game Boy Color) and PC (Microsoft DOS). Solving video games relies on fundamental multi-modal reasoning abilities (Shao et al., 2019)e.g. spatial awareness, memory retention, efficient exploration strategies, and real-time reaction to dynamic events. Video games are carefully crafted to be learnable and playable by humans, catering to human inductive biases (Allen et al., 2024). As result, they provide an ideal setting for exploring how well agents reproduce those inductive biases (Dubey et al., 2018). Correspondence to altzhang@mit.edu. Code and data at vgbench.com. Figure 1: VideoGameBench provides an environment for vision-language models (VLMs) to interact with video game emulators for example, the emulator above is playing The Legend of Zelda: Links Awakening. Given information about the game controls and emulator and access to the games raw frames, models provide actions in natural language. VideoGameBench has three important novel features: 1. It challenges VLMs with significantly more complex and realistic environments than those found in earlier benchmarks, such as grid-world or text-only games (Paglieri et al., 2025; Nasir et al., 2024), and is one of the first benchmarks to use video games from the 1990s. 2. It evaluates how single agent performs across different games, including three secret games specifically designed to test generalization to unseen or out-of-distribution scenarios. Unlike previous works (Mnih et al., 2013; Berner et al., 2019; Vinyals et al., 2019; Rubinstein et al., 2025), it challenges agents with environments that they may not have been trained on. 3. It only provides agents with raw game visual inputs, and does not allow game-specific hints, visual overlays or tools (Kempka et al., 2016; Hershey, 2025). Recently, Gemini Plays Pokemon (Joel Z, 2025) showed that frontier VLM (Google DeepMind, 2025), with tailored tools for pathfinding, game-specific hints, and memory, could complete Pokemon Blue. Although VideoGameBench includes similar games, we focus on evaluating VLMs without human or tool-assisted intervention. We evaluate multiple frontier VLMs on VideoGameBench using our VG-Agent scaffolding and find that all models struggle to progress on any game the best performing model, Gemini 2.5 Pro (Google DeepMind, 2025), is able to achieve score of 0.48% on VideoGameBench, which represents the average amount of each game that the agent completes. Our VG-Agent uses ReAct (Yao et al., 2023) with the ability to store information in context (Shinn et al., 2023), and has basic information on the controls and objectives for each game. We also introduce set of simpler practice games to evaluate skills such as mouse movement and navigation and find that most models perform poorly. Finally, to enable more granular progress tracking, we release VideoGameBench Litea smaller benchmark where the emulator pauses during agent inference, eliminating latency issues from slow model responses. Developing scalable benchmarks based on video games requires new tools for measuring how well agents perform in games. As one of our contributions, we developed novel method for detecting an agents game progress: we scrape YouTube walkthroughs of the games in our benchmark, and use the timestamps in their description to make dataset of (image-frame, milestone) pairs. We then use perceptual hashing (Buchner, 2013; Marr and Hildreth, 1980) on the frames sent to the agent to detect how much of the game it completed. This approach significantly increases the ease of incorporating new tasks into benchmarks for VLMs, opening the door to creating other challenging benchmarks that assess how well agents align with human inductive biases.VideoGameBench is benchmark composed of diverse suite of 23 curated video games split across dev and test set, with an environment to evaluate and communicate with VLM-based agents. The task is to "
[28.05.2025 14:13] Mistral response. {"id": "60c8cd04b14d4062aaf46bdd9f1af05f", "object": "chat.completion", "created": 1748441592, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1695, "total_tokens": 1697, "completion_tokens": 2}}
[28.05.2025 14:13] Response: []
[28.05.2025 14:13] Deleting PDF ./assets/pdf/2505.18134.pdf.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.17908.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.17908.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.17908.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.16673.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.16673.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.16673.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.11277.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.11277.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.11277.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.21499.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.21499.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.21499.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.20286.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.20286.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.20286.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.19650.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.19650.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.19650.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.19377.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.19377.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.19377.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.20321.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.20321.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.20321.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.16340.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.16340.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.21471.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.21471.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.21471.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.20052.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.20052.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.20052.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.20036.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.20036.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.20036.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.19954.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.19954.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.19954.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.17855.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.17855.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.17855.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.17190.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.17190.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.17190.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2505.15561.
[28.05.2025 14:13] Extra JSON file exists (./assets/json/2505.15561.json), skip PDF parsing.
[28.05.2025 14:13] Paper image links file exists (./assets/img_data/2505.15561.json), skip HTML parsing.
[28.05.2025 14:13] Success.
[28.05.2025 14:13] Enriching papers with extra data.
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 0. ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  					AI-generated summary 				 Large Language Models (LLMs) have extended their impact beyond Natural...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 1. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 2. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 3. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 4. SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  					AI-generated summary 				 Recent advances such as OpenAI-o1 and DeepSeek R1 have de...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 5. LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  					AI-generated summary 				 A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up t...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 6. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 7. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 8. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 9. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 10. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 11. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 12. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 13. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 14. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 15. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 16. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 17. HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  					AI-generated summary 				 Video large language models (video LLMs) excel at vi...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 18. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 19. Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled fro...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 20. The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 21. NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.  					AI-generated summary 				 In many real-world applications, deployed models encounter inputs that differ from th...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 22. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 23. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 24. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 25. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 26. Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have shown pro...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 27. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 28. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 29. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 30. BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  					AI-generated summary 				 Large Language Models (LLMs) trained via Reinforcement Learning (RL) ha...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 31. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 32. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 33. As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 34. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 35. Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ig...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 36. R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  					AI-generated summary 				 Large Language Models (LLMs) are powerful but prone to hallucinati...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 37. Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understandin...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 38. DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  					AI-generated summary 				 Digital ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 39. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 40. MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.  					AI-generated summary 				 Recent advances in Multimodal Large Language Models (MLLMs) hav...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 41. VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  					AI-generated summary 				 Vision-language models (VLMs) have achieved strong results on coding and ma...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 42. ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  					AI-generated summary 				 With the rapid advan...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 43. Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  					AI-generated summary 				 In this work, we aim to incentivize the reasoning ability of...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 44. AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  					AI-generated summary 				 Large language models have ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 45. AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  					AI-generated summary 				 Vision-Language Model (VLM) based Web Agents ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 46. Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.  					AI-generated summary 				 Recent advances in large language models (LLMs) have enabled agents to autono...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 47. UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  					AI-generated summary 				 Multimodal information retrieval (MIR) faces inherent c...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 48. Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  					AI-generated summary 				 State-of-the-art text-to-motion generation models rely on the kinematic-a...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 49. BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.  					AI-generated summary 				 Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However,...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 50. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 51. The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.  					AI-generated summary 				 With the rapid advancement of post-training techniques for reasoning and informa...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 52. A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.  					AI-generated summary 				 Protein language models (PLMs) have emerged as powerful tools to detect comple...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 53. The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatena...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 54. A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.  					AI-generated summary 				 The differential diagnosis of neurodegenerative dementias is a challeng...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 55. CLUE generates natural language explanations for a language model's uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.  					AI-generated summary 				 Understanding sources of a model's ...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 56. Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning...
[28.05.2025 14:13] ********************************************************************************
[28.05.2025 14:13] Abstract 57. Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capabilit...
[28.05.2025 14:13] Read previous papers.
[28.05.2025 14:13] Generating reviews via LLM API.
[28.05.2025 14:13] Querying the API.
[28.05.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  					AI-generated summary 				 Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.
[28.05.2025 14:13] Response: {
  "desc": "ScienceBoard                    (LLM).                ,   ,   .   169      .    ,     15%   ,       .",
  "emoji": "",
  "title": "ScienceBoard:     -   "
}
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  					AI-generated summary 				 Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/."

[28.05.2025 14:13] Response: ```python
['BENCHMARK', 'AGENTS', 'MULTIMODAL']
```
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  					AI-generated summary 				 Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/."

[28.05.2025 14:13] Response: ```python
['SCIENCE']
```
[28.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces ScienceBoard, a platform designed to evaluate the performance of Large Language Model (LLM)-based agents in scientific workflows. It features a realistic environment with dynamic tasks across various scientific domains, allowing agents to interact with professional software. Despite the advancements in LLMs, evaluations reveal that these agents struggle with complex tasks, achieving only a 15% success rate. The study highlights the need for improved design principles to enhance the capabilities of these agents in aiding scientific discovery.","title":"Unlocking the Potential of LLMs in Scientific Workflows"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces ScienceBoard, a platform designed to evaluate the performance of Large Language Model (LLM)-based agents in scientific workflows. It features a realistic environment with dynamic tasks across various scientific domains, allowing agents to interact with professional software. Despite the advancements in LLMs, evaluations reveal that these agents struggle with complex tasks, achieving only a 15% success rate. The study highlights the need for improved design principles to enhance the capabilities of these agents in aiding scientific discovery.', title='Unlocking the Potential of LLMs in Scientific Workflows'))
[28.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ScienceBoardLLM15%","title":"ScienceBoard"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ScienceBoardLLM15%', title='ScienceBoard'))
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "", "ru": {"title": "     ", "desc": "MME-Reasoning -            (MLLM)   .
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "", "ru": {"title": "     ", "desc": "OmniConsistency -           
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "", "ru": {"title": "   :    ", "desc": "            , 
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#rl", "#dataset", "#reasoning", "#open_source"], "emoji": "", "ru": {"title": "SynLogic:      ", "desc": "SynLogic -     ,       (LLM)   
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#data", "#long_context", "#architecture", "#multimodal"], "emoji": "", "ru": {"title": "      LLM", "desc": " ,     (LLM)       ,    
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "", "ru": {"title": "VerIPO:   -LLM   ", "desc": "   VerIPO    -LLM  .   Verif
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "", "ru": {"title": "  -  :    LLM", "desc": "          (LLM)   .    
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "", "ru": {"title": "OpenS2V-Nexus:        ", "desc": "  OpenS2V-Nexus -        
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "", "ru": {"title": "MMMR:      ", "desc": "    MMMR         (MLLM). MMMR   
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#synthetic", "#open_source", "#dataset", "#agents", "#training", "#data"], "emoji": "", "ru": {"title": "UI-Genie:  -   ", "desc": "UI-Genie -      
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "", "ru": {"title": "       ", "desc": "SVG2 -            .  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "", "ru": {"title": "     OCR  ", "desc": "    (MLLM)        
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "", "ru": {"title": "GraLoRA:        ", "desc": "          Granular L
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "", "ru": {"title": "   :     ", "desc": "Video-Holmes -               .  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "", "ru": {"title": "rStar-Coder:        ", "desc": "  rStar-Coder -        (
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "", "ru": {"title": "MetaMind:      ", "desc": "MetaMind -   ,       
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#inference", "#video", "#optimization"], "emoji": "", "ru": {"title": "HoliTom:      -LLM", "desc": "HoliTom -         -LLM .     LLM   
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "", "ru": {"title": "ImgEdit:       ", "desc": "  ImgEdit -      , 
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#healthcare", "#reasoning", "#benchmark", "#rl", "#dataset", "#interpretability"], "emoji": "", "ru": {"title": "    :    ", "desc": "  AlphaMed -      (LLM), 
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#alignment"], "emoji": "", "ru": {"title": "     LLM", "desc": "         -  -    
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#healthcare", "#cv", "#reasoning", "#benchmark", "#interpretability"], "emoji": "", "ru": {"title": "NOVA:        ", "desc": "NOVA -             .
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "", "ru": {"title": "ACTIVE-O3:  MLLM      ", "desc": "  ACTIVE-O3 -       
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "", "ru": {"title": "  :       ", "desc": "   ,      
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#3d", "#games"], "emoji": "", "ru": {"title": "       ", "desc": "    ViewSpatial-Bench        
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "", "ru": {"title": "       ", "desc": "      Steering Target Atoms (STA)     
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#games", "#architecture", "#open_source"], "emoji": "", "ru": {"title": "  :      -", "desc": "  Code Graph Models (CGM) -        
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "", "ru": {"title": "      ", "desc": "DetailFlow -     ,       
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "", "ru": {"title": "SeePhys:     LLM  ", "desc": "SeePhys -           (LLM)  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "", "ru": {"title": "         ", "desc": "        ,  FOA-Attack. 
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "BARL:  ,  ", "desc": "BARL -              .  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "", "ru": {"title": "VisTA:         ", "desc": "VisTA -          . 
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "            
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#rl", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": " -  :      ", "desc": "            
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "", "ru": {"title": "       ", "desc": "       -    Diffusion Transfor
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#agents", "#optimization", "#inference", "#benchmark"], "emoji": "", "ru": {"title": "ACBench:         LLM", "desc": "    ACBench         
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#hallucinations", "#optimization", "#rl", "#reasoning", "#rag"], "emoji": "", "ru": {"title": "  LLM:     ", "desc": "R1-Searcher++ -   ,      (LLM)    
[28.05.2025 14:13] Querying the API.
[28.05.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/
[28.05.2025 14:13] Response: {
  "desc": "  MMPerspective -           (MLLM).   10    :  ,   ,   2,711    .  43  MLLM          .       ,       .",
  "emoji": "",
  "title": "MMPerspective:       "
}
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/"

[28.05.2025 14:13] Response: ```python
['BENCHMARK', 'MULTIMODAL', 'CV', '3D']
```
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/"

[28.05.2025 14:13] Response: ```python
["REASONING", "INTERPRETABILITY"]
```
[28.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces MMPerspective, a benchmark designed to evaluate how well multimodal large language models (MLLMs) understand perspective geometry. It includes 10 tasks that assess three key areas: how models perceive perspective, their reasoning abilities, and their robustness to changes. The benchmark features over 2,700 images and more than 5,000 question-answer pairs that test various skills like recognizing vanishing points and understanding 3D spatial relationships. The study finds that while MLLMs perform well on basic tasks, they struggle with complex reasoning and maintaining spatial accuracy when faced with transformations, revealing important insights into their limitations and potential improvements.","title":"Evaluating Perspective Understanding in Multimodal Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces MMPerspective, a benchmark designed to evaluate how well multimodal large language models (MLLMs) understand perspective geometry. It includes 10 tasks that assess three key areas: how models perceive perspective, their reasoning abilities, and their robustness to changes. The benchmark features over 2,700 images and more than 5,000 question-answer pairs that test various skills like recognizing vanishing points and understanding 3D spatial relationships. The study finds that while MLLMs perform well on basic tasks, they struggle with complex reasoning and maintaining spatial accuracy when faced with transformations, revealing important insights into their limitations and potential improvements.', title='Evaluating Perspective Understanding in Multimodal Models'))
[28.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MLLMsMMPerspective10MLLMs27115083-3D43MLLMs","title":"MMPerspective"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MLLMsMMPerspective10MLLMs27115083-3D43MLLMs', title='MMPerspective'))
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#science", "#hallucinations", "#benchmark", "#dataset", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "      ", "desc": "DFIR-Metric -         (LLM)  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#audio"], "emoji": "", "ru": {"title": "SoloSpeech:      ", "desc": "SoloSpeech -           .    ,  , , 
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#interpretability", "#training", "#agi", "#multimodal", "#architecture"], "emoji": "", "ru": {"title": "     -", "desc": "          (MLLM), 
[28.05.2025 14:13] Querying the API.
[28.05.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  					AI-generated summary 				 Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.
[28.05.2025 14:13] Response: {
  "desc": "VideoGameBench -                     .   10   1990-       ,           .  ,          .       ,         .",
  "emoji": "",
  "title": "     "
}
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  					AI-generated summary 				 Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions."

[28.05.2025 14:13] Response: ```python
['BENCHMARK', 'CV', 'MULTIMODAL', 'VIDEO']
```
[28.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  					AI-generated summary 				 Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions."

[28.05.2025 14:13] Response: ```python
['GAMES']
```
[28.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VideoGameBench is a new benchmark designed to assess the capabilities of vision-language models (VLMs) in real-time video game interactions using only visual inputs and high-level objectives. It highlights the gap in VLMs\' performance on tasks that require human-like skills such as perception and spatial navigation, which are essential for mastering video games. The benchmark includes 10 classic video games from the 1990s, challenging models to complete them without additional game-specific information. Results show that even advanced models struggle significantly, indicating the need for further research to enhance VLMs\' abilities in these areas.","title":"Evaluating Human-Like Skills in AI through Video Games"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="VideoGameBench is a new benchmark designed to assess the capabilities of vision-language models (VLMs) in real-time video game interactions using only visual inputs and high-level objectives. It highlights the gap in VLMs' performance on tasks that require human-like skills such as perception and spatial navigation, which are essential for mastering video games. The benchmark includes 10 classic video games from the 1990s, challenging models to complete them without additional game-specific information. Results show that even advanced models struggle significantly, indicating the need for further research to enhance VLMs' abilities in these areas.", title='Evaluating Human-Like Skills in AI through Video Games'))
[28.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VideoGameBench -VLM VLM  VLM ","title":"-"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VideoGameBench -VLM VLM  VLM ', title='-'))
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#games", "#training", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "ComfyMind:           ", "desc": "ComfyMind -    
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#rl", "#training", "#multimodal", "#benchmark", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "  MLLM     ", "desc": "Share-GRPO -        ,   
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#rag", "#rl", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "AutoRefine:       ", "desc": "AutoRefine -         ,     
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#cv", "#agents", "#security"], "emoji": "", "ru": {"title": "  :    -  ", "desc": "AdInject -      -,     ,   -   
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#open_source", "#agi", "#agents", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": " -    -", "desc": " -    ,          
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#training", "#multimodal"], "emoji": "", "ru": {"title": "UNITE:       ", "desc": "  UNITE -      .  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#optimization", "#cv"], "emoji": "", "ru": {"title": "       ", "desc": "          ,    
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#open_source", "#science", "#dataset", "#multimodal", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": "    -SQL   ", "desc": "BiomedSQL -            
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#science", "#dataset", "#open_source", "#data", "#multimodal"], "emoji": "", "ru": {"title": "CLEANMOL:     ", "desc": "  CLEANMOL -        
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#long_context", "#inference", "#agents", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": "ExtAgents:   LLM    ", "desc": "    ExtAgents,      
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#healthcare", "#dataset", "#science"], "emoji": "", "ru": {"title": "      ", "desc": "         (PLM)  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#leakage", "#architecture", "#dataset", "#science"], "emoji": "", "ru": {"title": "   -       ", "desc": "   
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#3d", "#interpretability", "#cv", "#healthcare", "#multimodal", "#rl", "#reasoning"], "emoji": "", "ru": {"title": " :   ", "desc": "           
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#training", "#reasoning", "#data"], "emoji": "", "ru": {"title": "CLUE:     ", "desc": "CLUE -           .  
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#security", "#math", "#reasoning"], "emoji": "", "ru": {"title": " :  -    ", "desc": "        '
[28.05.2025 14:13] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#benchmark", "#hallucinations"], "emoji": "", "ru": {"title": "   RAG:   ,  ", "desc": "            Retrieval Augmented Generati
[28.05.2025 14:13] Loading Chinese text from previous data.
[28.05.2025 14:13] Renaming data file.
[28.05.2025 14:13] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 14:13] Saving new data file.
[28.05.2025 14:13] Generating page.
[28.05.2025 14:13] Renaming previous page.
[28.05.2025 14:13] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 14:13] [Experimental] Generating Chinese page for reading.
[28.05.2025 14:13] Chinese vocab [{'word': 'OmniConsistency', 'pinyin': '', 'trans': 'OmniConsistency'}, {'word': '', 'pinyin': 'ku sn', 'trans': 'diffusion'}, {'word': '', 'pinyin': 'bin y q', 'trans': 'transformer'}, {'word': '', 'pinyin': 'zng qing', 'trans': 'enhance'}, {'word': '', 'pinyin': 'fng g', 'trans': 'style'}, {'word': '', 'pinyin': 'y zh xng', 'trans': 'consistency'}, {'word': '', 'pinyin': 'fn hu', 'trans': 'generalization'}, {'word': '', 'pinyin': 'nng l', 'trans': 'ability'}, {'word': '', 'pinyin': 'tu hu', 'trans': 'degeneration'}, {'word': '', 'pinyin': 'shng xi wn', 'trans': 'context'}, {'word': '', 'pinyin': 'kung ji', 'trans': 'framework'}, {'word': '', 'pinyin': 'ling ji dun', 'trans': 'two-stage'}, {'word': '', 'pinyin': 'jin jn', 'trans': 'progressive'}, {'word': '', 'pinyin': 'c l', 'trans': 'strategy'}, {'word': '', 'pinyin': 'ch b', 'trans': 'interpolation'}, {'word': '', 'pinyin': 'sh j', 'trans': 'design'}, {'word': '', 'pinyin': 'bio xin', 'trans': 'performance'}, {'word': '', 'pinyin': 'ji jn', 'trans': 'approach'}, {'word': '', 'pinyin': 'shng y', 'trans': 'commercial'}, {'word': '', 'pinyin': 'dng jin', 'trans': 'top-notch'}, {'word': '', 'pinyin': 'm xng', 'trans': 'model'}, {'word': 'GPT-4o', 'pinyin': '', 'trans': 'GPT-4o'}]
[28.05.2025 14:13] Renaming previous Chinese page.
[28.05.2025 14:13] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 14:13] Writing Chinese reading task.
[28.05.2025 14:13] Writing result.
[28.05.2025 14:13] Renaming log file.
[28.05.2025 14:13] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
