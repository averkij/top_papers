[28.05.2025 04:17] Read previous papers.
[28.05.2025 04:17] Generating top page (month).
[28.05.2025 04:17] Writing top page (month).
[28.05.2025 05:12] Read previous papers.
[28.05.2025 05:12] Get feed.
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18445
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21497
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21327
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19000
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21333
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20355
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21374
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18943
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18875
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21297
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20292
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16459
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20275
[28.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.21496
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21491
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21457
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19099
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20322
[28.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.21500
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21473
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21070
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19314
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21205
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20289
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17813
[28.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21494
[28.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.16340
[28.05.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.05.2025 05:12] No deleted papers detected.
[28.05.2025 05:12] Downloading and parsing papers (pdf, html). Total: 27.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.18445.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.18445.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.18445.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21497.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21497.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21497.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21327.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21327.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21327.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.19000.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.19000.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.19000.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21333.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21333.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21333.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.20355.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.20355.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.20355.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21374.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21374.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21374.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.18943.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.18943.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.18943.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.18875.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.18875.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.18875.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21297.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21297.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21297.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.20292.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.20292.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.20292.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.16459.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.16459.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.16459.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.20275.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.20275.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.20275.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21496.
[28.05.2025 05:12] Downloading paper 2505.21496 from http://arxiv.org/pdf/2505.21496v1...
[28.05.2025 05:12] Extracting affiliations from text.
[28.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 6 9 4 1 2 . 5 0 5 2 : r UI-Genie: Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents Han Xiao1,2, Guozhi Wang2, Yuxiang Chai1,2, Zimu Lu1, Weifeng Lin1,2, Hao He1, Lue Fan1, Liuyang Bian2, Rui Hu2, Liang Liu2, Shuai Ren2(cid:66), Yafei Wen2, Xiaoxin Chen2, Aojun Zhou1 (cid:66), Hongsheng Li1,3 (cid:66) 1CUHK MMLab 2vivo AI Lab 3CPII under InnoHK {1155229123@link,hsli@ee,aojunzhou@link}.cuhk.edu.hk shuai.ren@vivo.com Project lead (cid:66)Corresponding author Interns at vivo "
[28.05.2025 05:12] Response: ```python
["CUHK MMLab", "vivo AI Lab", "CPII under InnoHK"]
```
[28.05.2025 05:12] Deleting PDF ./assets/pdf/2505.21496.pdf.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21491.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21491.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21491.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21457.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21457.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21457.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.19099.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.19099.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.19099.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.20322.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.20322.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.20322.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21500.
[28.05.2025 05:12] Downloading paper 2505.21500 from http://arxiv.org/pdf/2505.21500v1...
[28.05.2025 05:12] Extracting affiliations from text.
[28.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 0 0 5 1 2 . 5 0 5 2 : r ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models Dingming Li1,2,* Hongxing Li1,* Zixuan Wang1 Yuchen Yan1 Hang Zhang1 Siqi Chen1 Guiyang Hou1 Wenqi Zhang1 Yongliang Shen1, Weiming Lu1 Yueting Zhuang1 Shengpei Jiang3 1 Zhejiang University 2 University of Electronic Science and Technology of China 3 The Chinese University of Hong Kong lidingm@std.uestc.edu.cn, shenyl@zju.edu.cn Project: https://zju-real.github.io/ViewSpatial-Page "
[28.05.2025 05:12] Response: ```python
["Zhejiang University", "University of Electronic Science and Technology of China", "The Chinese University of Hong Kong"]
```
[28.05.2025 05:12] Deleting PDF ./assets/pdf/2505.21500.pdf.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21473.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21473.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21473.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21070.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21070.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21070.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.19314.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.19314.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.19314.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21205.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21205.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21205.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.20289.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.20289.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.20289.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.17813.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.17813.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.17813.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.21494.
[28.05.2025 05:12] Extra JSON file exists (./assets/json/2505.21494.json), skip PDF parsing.
[28.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.21494.json), skip HTML parsing.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.16340.
[28.05.2025 05:12] Downloading paper 2505.16340 from http://arxiv.org/pdf/2505.16340v1...
[28.05.2025 05:12] Extracting affiliations from text.
[28.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Yunhui Jang KAIST yunhuijang@kaist.ac.kr Jaehyung Kim Yonsei University jaehyungk@yonsei.ac.kr Sungsoo Ahn KAIST sungsoo.ahn@kaist.ac.kr 5 2 0 2 2 2 ] . [ 1 0 4 3 6 1 . 5 0 5 2 : r a "
[28.05.2025 05:12] Response: ```python
["KAIST", "Yonsei University"]
```
[28.05.2025 05:12] Deleting PDF ./assets/pdf/2505.16340.pdf.
[28.05.2025 05:12] Success.
[28.05.2025 05:12] Enriching papers with extra data.
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 0. OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  					AI-generated summary 				 Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) m...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 1. Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which p...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 2. MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  					AI-generated summary 				 Logical reasoning is a fundamental aspect of human ...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 3. A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  					AI-generated summary 				 Applying Reinforcement Learning (RL) to Video Lar...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 4. MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved considerable accur...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 5. Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 6. Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  					AI-generated summary 				 Recent advances in CoT reasoning and RL post-training have been reported ...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 7. MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  					AI-generated summary 				 Human social inte...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 8. SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  					AI-generated summary 				 Diffusion Transformers (DiTs) are essential for video generati...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 9. A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  					AI-generated summary 				 Advancing code reasoning in large language models (LLMs) is fundamentally limited by the ...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 10. Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, a...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 11. The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  					AI-generated summary 				 Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, visio...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 12. Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 13. In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectivel...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 14. Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can contro...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 15. Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimod...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 16. SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  					AI-generated summary 				 We present SeePhys, a large-scale multimodal benchmark for LL...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 17. Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This int...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 18. Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at e...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 19. This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process t...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 20. Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 21. Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 22. Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitti...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 23. VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  					AI-generated summary 				 We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dyn...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 24. Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge t...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 25. Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information enc...
[28.05.2025 05:12] ********************************************************************************
[28.05.2025 05:12] Abstract 26. Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, cur...
[28.05.2025 05:12] Read previous papers.
[28.05.2025 05:12] Generating reviews via LLM API.
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#diffusion", "#cv", "#training"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ç–∏–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "OmniConsistency - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#agents", "#science"], "emoji": "üñºÔ∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—É—á–Ω—ã—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤: –æ—Ç —Å—Ç–∞—Ç—å–∏ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –∏ –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤, —Å–æ–ø–æ—Å
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –≤ –ª–æ–≥–∏–∫–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "MME-Reasoning - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#video", "#optimization", "#rl", "#rlhf"], "emoji": "üé•", "ru": {"title": "VerIPO: –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–∏–¥–µ–æ-LLM —Å –ø–æ–º–æ—â—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ VerIPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ-LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Verif
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#reasoning", "#video"], "emoji": "üé•", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–µ OCR –Ω–∞ –≤–∏–¥–µ–æ", "desc": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#optimization"], "emoji": "üß©", "ru": {"title": "GraLoRA: –ì—Ä–∞–Ω—É–ª—è—Ä–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Granular L
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#video"], "emoji": "üïµÔ∏è", "ru": {"title": "–®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å –¥–ª—è –ò–ò: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "Video-Holmes - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. –û–Ω –∏—Å
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agents", "#reasoning", "#alignment"], "emoji": "üß†", "ru": {"title": "MetaMind: –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "MetaMind - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#diffusion", "#training", "#video", "#optimization"], "emoji": "üéûÔ∏è", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "SVG2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#reasoning", "#data"], "emoji": "üß†", "ru": {"title": "rStar-Coder: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –æ –∫–æ–¥–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ rStar-Coder - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#synthetic", "#video"], "emoji": "üé¨", "ru": {"title": "OpenS2V-Nexus: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenS2V-Nexus - –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark"], "emoji": "üß†", "ru": {"title": "MMMR: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMMR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). MMMR –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
[28.05.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#optimization", "#cv", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "ImgEdit: –ø—Ä–æ—Ä—ã–≤ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ImgEdit - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏
[28.05.2025 05:12] Querying the API.
[28.05.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.
[28.05.2025 05:12] Response: {
  "desc": "UI-Genie - —ç—Ç–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—ã –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è UI-Genie-RM —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç, –∏ –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö UI-Genie-RM-517k –∏ UI-Genie-Agent-16k –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ UI-Genie –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.",
  "emoji": "üßû",
  "title": "UI-Genie: —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤"
}
[28.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie."

[28.05.2025 05:12] Response: ```python
['DATASET', 'DATA', 'AGENTS', 'TRAINING', 'BENCHMARK']
```
[28.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie."

[28.05.2025 05:12] Response: ```python
['OPTIMIZATION', 'SYNTHETIC', 'OPEN_SOURCE']
```
[28.05.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents UI-Genie, a framework designed to improve GUI agents by tackling the challenges of verifying outcomes and scaling high-quality training data. It introduces a reward model, UI-Genie-RM, which uses an image-text interleaved architecture to effectively process historical data and combine different levels of rewards. The framework also includes innovative data generation strategies to create training data without manual effort, such as rule-based verification and hard negative mining. Experimental results indicate that UI-Genie outperforms existing methods in GUI agent tasks, showcasing the effectiveness of its self-improvement approach.","title":"UI-Genie: Revolutionizing GUI Agents with Self-Improvement and Reward Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents UI-Genie, a framework designed to improve GUI agents by tackling the challenges of verifying outcomes and scaling high-quality training data. It introduces a reward model, UI-Genie-RM, which uses an image-text interleaved architecture to effectively process historical data and combine different levels of rewards. The framework also includes innovative data generation strategies to create training data without manual effort, such as rule-based verification and hard negative mining. Experimental results indicate that UI-Genie outperforms existing methods in GUI agent tasks, showcasing the effectiveness of its self-improvement approach.', title='UI-Genie: Revolutionizing GUI Agents with Self-Improvement and Reward Models'))
[28.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜUI-GenieÔºåËøôÊòØ‰∏Ä‰∏™Ëá™ÊàëÊîπËøõÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥GUI‰ª£ÁêÜ‰∏≠ÁöÑ‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöËΩ®ËøπÁªìÊûúÁöÑÈ™åËØÅÂõ∞ÈöæÂíåÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÂ•ñÂä±Ê®°ÂûãÂíåËá™ÊàëÊîπËøõÁÆ°ÈÅìÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÂ•ñÂä±Ê®°ÂûãUI-Genie-RMÈááÁî®ÂõæÂÉè-ÊñáÊú¨‰∫§ÈîôÊû∂ÊûÑÔºåÊúâÊïàÂ§ÑÁêÜÂéÜÂè≤‰∏ä‰∏ãÊñáÔºåÂπ∂Áªü‰∏Ä‰∫ÜÂä®‰ΩúÁ∫ßÂíå‰ªªÂä°Á∫ßÂ•ñÂä±„ÄÇËá™ÊàëÊîπËøõÁÆ°ÈÅìÈÄöËøáÂ•ñÂä±ÂºïÂØºÊé¢Á¥¢ÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÁªìÊûúÈ™åËØÅÔºåÈÄêÊ≠•Êâ©Â±ïÂèØËß£ÂÜ≥ÁöÑÂ§çÊùÇGUI‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçá‰ª£ÁêÜÂíåÂ•ñÂä±Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ","title":"UI-GenieÔºöËá™ÊàëÊîπËøõÁöÑGUI‰ª£ÁêÜÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜUI-GenieÔºåËøôÊòØ‰∏Ä‰∏™Ëá™ÊàëÊîπËøõÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥GUI‰ª£ÁêÜ‰∏≠ÁöÑ‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöËΩ®ËøπÁªìÊûúÁöÑÈ™åËØÅÂõ∞ÈöæÂíåÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÂ•ñÂä±Ê®°ÂûãÂíåËá™ÊàëÊîπËøõÁÆ°ÈÅìÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇÂ•ñÂä±Ê®°ÂûãUI-Genie-RMÈááÁî®ÂõæÂÉè-ÊñáÊú¨‰∫§ÈîôÊû∂ÊûÑÔºåÊúâÊïàÂ§ÑÁêÜÂéÜÂè≤‰∏ä‰∏ãÊñáÔºåÂπ∂Áªü‰∏Ä‰∫ÜÂä®‰ΩúÁ∫ßÂíå‰ªªÂä°Á∫ßÂ•ñÂä±„ÄÇËá™ÊàëÊîπËøõÁÆ°ÈÅìÈÄöËøáÂ•ñÂä±ÂºïÂØºÊé¢Á¥¢ÂíåÂä®ÊÄÅÁéØÂ¢É‰∏≠ÁöÑÁªìÊûúÈ™åËØÅÔºåÈÄêÊ≠•Êâ©Â±ïÂèØËß£ÂÜ≥ÁöÑÂ§çÊùÇGUI‰ªªÂä°Ôºå‰ªéËÄåÊèêÂçá‰ª£ÁêÜÂíåÂ•ñÂä±Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ', title='UI-GenieÔºöËá™ÊàëÊîπËøõÁöÑGUI‰ª£ÁêÜÊ°ÜÊû∂'))
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#games", "#diffusion", "#architecture", "#video"], "emoji": "üé¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ –∫–∞–¥—Ä–µ", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏, –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≥–µ–Ω
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rl", "#agents", "#reasoning"], "emoji": "üëÅÔ∏è", "ru": {"title": "ACTIVE-O3: –ù–∞–¥–µ–ª–µ–Ω–∏–µ MLLM –∞–∫—Ç–∏–≤–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ACTIVE-O3 - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞–¥–µ–ª–µ–Ω–∏—è 
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#benchmark", "#cv", "#multimodal"], "emoji": "üî¨", "ru": {"title": "SeePhys: –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM –≤ —Ñ–∏–∑–∏–∫–µ", "desc": "SeePhys - —ç—Ç–æ –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#security", "#training", "#alignment"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Steering Target Atoms (STA) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
[28.05.2025 05:13] Querying the API.
[28.05.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.
[28.05.2025 05:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ViewSpatial-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ vision-language –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –∞–ª–ª–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –ø—è—Ç—å —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π 3D-–∫–æ–Ω–≤–µ–π–µ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–æ–∑–≤–æ–ª–∏–ª–∞ —É–ª—É—á—à–∏—Ç—å –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ 46.24%.",
  "emoji": "üß†",
  "title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"
}
[28.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities."

[28.05.2025 05:13] Response: ```python
['BENCHMARK', 'CV', '3D']
```
[28.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities."

[28.05.2025 05:13] Response: ```python
["REASONING", "GAMES"]
```
[28.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the limitations of vision-language models (VLMs) in understanding spatial relationships from different viewpoints. It highlights that while VLMs perform well in egocentric spatial reasoning, they struggle with allocentric perspectives, which are essential for tasks requiring understanding from another entity\'s viewpoint. The authors introduce ViewSpatial-Bench, a new benchmark for evaluating multi-viewpoint spatial localization, along with a 3D annotation pipeline for accurate directional labeling. By fine-tuning VLMs on this dataset, they demonstrate a significant performance improvement, emphasizing the importance of 3D spatial modeling in enhancing VLMs\' spatial reasoning capabilities.","title":"Enhancing VLMs with Multi-Viewpoint Spatial Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper addresses the limitations of vision-language models (VLMs) in understanding spatial relationships from different viewpoints. It highlights that while VLMs perform well in egocentric spatial reasoning, they struggle with allocentric perspectives, which are essential for tasks requiring understanding from another entity's viewpoint. The authors introduce ViewSpatial-Bench, a new benchmark for evaluating multi-viewpoint spatial localization, along with a 3D annotation pipeline for accurate directional labeling. By fine-tuning VLMs on this dataset, they demonstrate a significant performance improvement, emphasizing the importance of 3D spatial modeling in enhancing VLMs' spatial reasoning capabilities.", title='Enhancing VLMs with Multi-Viewpoint Spatial Reasoning'))
[28.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÁêÜËß£ÂíåÊé®ÁêÜËßÜËßâÂÜÖÂÆπÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶ÅË∑®ËßÜËßíÁêÜËß£ÂíåÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠‰ªçÈù¢‰∏¥ÈáçÂ§ßÊåëÊàò„ÄÇÊàë‰ª¨ÂèëÁé∞ÂΩìÂâçÁöÑVLMs‰∏ªË¶ÅÂú®Ëá™Êàë‰∏≠ÂøÉÁöÑÁ©∫Èó¥Êé®ÁêÜ‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ÈúÄË¶ÅÈááÁî®ÂÖ∂‰ªñÂÆû‰ΩìÁöÑÁ©∫Èó¥ÂèÇËÄÉÊ°ÜÊû∂Êó∂ÔºåÊó†Ê≥ïÂæàÂ•ΩÂú∞Êé®ÂπøÂà∞‰ªñÂøÉ‰∏≠ÂøÉËßÜËßí„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜViewSpatial-BenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®‰∏∫Â§öËßÜËßíÁ©∫Èó¥ÂÆö‰ΩçËØÜÂà´ËØÑ‰º∞ËÆæËÆ°ÁöÑÁªºÂêàÂü∫ÂáÜÔºåÊîØÊåÅËá™Âä®ÂåñÁöÑ3DÊ†áÊ≥®ÊµÅÁ®ãÁîüÊàêÁ≤æÁ°ÆÁöÑÊñπÂêëÊ†áÁ≠æ„ÄÇÈÄöËøáÂú®Êàë‰ª¨ÁöÑÂ§öËßÜËßíÁ©∫Èó¥Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉVLMsÔºåÊàë‰ª¨Âú®ÂêÑÈ°π‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü46.24%ÁöÑÊï¥‰ΩìÊÄßËÉΩÊèêÂçáÔºåËØÅÊòé‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ","title":"ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÁêÜËß£ÂíåÊé®ÁêÜËßÜËßâÂÜÖÂÆπÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈúÄË¶ÅË∑®ËßÜËßíÁêÜËß£ÂíåÁ©∫Èó¥Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠‰ªçÈù¢‰∏¥ÈáçÂ§ßÊåëÊàò„ÄÇÊàë‰ª¨ÂèëÁé∞ÂΩìÂâçÁöÑVLMs‰∏ªË¶ÅÂú®Ëá™Êàë‰∏≠ÂøÉÁöÑÁ©∫Èó¥Êé®ÁêÜ‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ÈúÄË¶ÅÈááÁî®ÂÖ∂‰ªñÂÆû‰ΩìÁöÑÁ©∫Èó¥ÂèÇËÄÉÊ°ÜÊû∂Êó∂ÔºåÊó†Ê≥ïÂæàÂ•ΩÂú∞Êé®ÂπøÂà∞‰ªñÂøÉ‰∏≠ÂøÉËßÜËßí„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜViewSpatial-BenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®‰∏∫Â§öËßÜËßíÁ©∫Èó¥ÂÆö‰ΩçËØÜÂà´ËØÑ‰º∞ËÆæËÆ°ÁöÑÁªºÂêàÂü∫ÂáÜÔºåÊîØÊåÅËá™Âä®ÂåñÁöÑ3DÊ†áÊ≥®ÊµÅÁ®ãÁîüÊàêÁ≤æÁ°ÆÁöÑÊñπÂêëÊ†áÁ≠æ„ÄÇÈÄöËøáÂú®Êàë‰ª¨ÁöÑÂ§öËßÜËßíÁ©∫Èó¥Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉVLMsÔºåÊàë‰ª¨Âú®ÂêÑÈ°π‰ªªÂä°‰∏äÂÆûÁé∞‰∫Ü46.24%ÁöÑÊï¥‰ΩìÊÄßËÉΩÊèêÂçáÔºåËØÅÊòé‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ', title='ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõ'))
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#training", "#architecture", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É", "desc": "DetailFlow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º 
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#video", "#inference"], "emoji": "üéûÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transfor
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "SoloSpeech: –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "SoloSpeech - —ç—Ç–æ –Ω–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Ü–µ–ª–µ–≤–æ–π —Ä–µ—á–∏ –∏–∑ —Å–º–µ—Å–∏ –≥–æ–ª–æ—Å–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–∂–∞—Ç–∏–µ, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#video", "#training"], "emoji": "üéûÔ∏è", "ru": {"title": "–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤–∏–¥–µ–æ–∫–∞–¥—Ä–æ–≤ –º–µ–∂–¥—É –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º –∏ –∫–æ
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#optimization", "#cv", "#rl"], "emoji": "üß†", "ru": {"title": "VisTA: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "VisTA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. 
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training"], "emoji": "‚ö°", "ru": {"title": "–ö–æ—Ä–æ—á–µ –º—ã—Å–ª—å - –±—ã—Å—Ç—Ä–µ–µ –≤—ã–≤–æ–¥: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ 
[28.05.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#security", "#training"], "emoji": "üéØ", "ru": {"title": "–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –∞—Ç–∞–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π FOA-Attack. –û
[28.05.2025 05:13] Querying the API.
[28.05.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark.
[28.05.2025 05:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLEANMOL - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). CLEANMOL —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–∑–±–æ—Ä SMILES-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–æ–ª–µ–∫—É–ª –≤ –≤–∏–¥–µ –Ω–∞–±–æ—Ä–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–ª–µ–∫—É–ª. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–æ–±—É—á–∏–ª–∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ LLM –Ω–∞ —ç—Ç–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CLEANMOL —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–ª–µ–∫—É–ª –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö –∏–ª–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ Mol-Instructions.",
  "emoji": "üß™",
  "title": "CLEANMOL: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–ª–µ–∫—É–ª —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"
}
[28.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark."

[28.05.2025 05:13] Response: ```python
["DATASET", "DATA", "BENCHMARK", "MULTIMODAL"]
```
[28.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark."

[28.05.2025 05:13] Response: ```python
['GRAPHS', 'SCIENCE', 'OPEN_SOURCE']
```
[28.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents CLEANMOL, a new framework aimed at improving how large language models (LLMs) understand molecular structures represented in SMILES format. The authors identify that existing LLMs struggle with basic molecular tasks, such as counting rings in molecules. CLEANMOL addresses this by breaking down SMILES parsing into clear, structured tasks that enhance graph-level comprehension of molecular properties. The framework includes a pretraining dataset with varying difficulty levels, leading to improved performance on molecular understanding benchmarks.","title":"CLEANMOL: Enhancing LLMs for Molecular Understanding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents CLEANMOL, a new framework aimed at improving how large language models (LLMs) understand molecular structures represented in SMILES format. The authors identify that existing LLMs struggle with basic molecular tasks, such as counting rings in molecules. CLEANMOL addresses this by breaking down SMILES parsing into clear, structured tasks that enhance graph-level comprehension of molecular properties. The framework includes a pretraining dataset with varying difficulty levels, leading to improved performance on molecular understanding benchmarks.', title='CLEANMOL: Enhancing LLMs for Molecular Understanding'))
[28.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂàÜÂ≠êÁßëÂ≠¶ÁöÑÁßëÂ≠¶ÂèëÁé∞‰∏≠Ë¢´Ë∂äÊù•Ë∂äÂ§öÂú∞ËÆ§ÂèØ‰∏∫Âº∫Â§ßÁöÑÂ∑•ÂÖ∑„ÄÇ‰∏∫‰∫Ü‰ΩøËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂáÜÁ°ÆÁêÜËß£ÂàÜÂ≠êÁªìÊûÑÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜCLEANMOLÊ°ÜÊû∂ÔºåÂ∞ÜSMILESËß£ÊûêËΩ¨Âåñ‰∏∫‰∏ÄÁ≥ªÂàóÊ∏ÖÊô∞‰∏îÁ°ÆÂÆöÁöÑ‰ªªÂä°Ôºå‰ª•‰øÉËøõÂõæÁ∫ßÂàÜÂ≠êÁêÜËß£„ÄÇËøô‰∫õ‰ªªÂä°ÂåÖÊã¨Â≠êÂõæÂåπÈÖçÂíåÂÖ®Â±ÄÂõæÂåπÈÖçÔºåÊèê‰æõ‰∏éÂàÜÂ≠êÁªìÊûÑÁâπÊÄßÁõ∏‰∏ÄËá¥ÁöÑÁªìÊûÑÂåñÁõëÁù£„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåCLEANMOL‰∏ç‰ªÖÂ¢ûÂº∫‰∫ÜÁªìÊûÑÁêÜËß£ËÉΩÂäõÔºåËøòÂú®Mol-InstructionsÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ","title":"CLEANMOLÔºöÊèêÂçáÂàÜÂ≠êÁªìÊûÑÁêÜËß£ÁöÑÂàõÊñ∞Ê°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂàÜÂ≠êÁßëÂ≠¶ÁöÑÁßëÂ≠¶ÂèëÁé∞‰∏≠Ë¢´Ë∂äÊù•Ë∂äÂ§öÂú∞ËÆ§ÂèØ‰∏∫Âº∫Â§ßÁöÑÂ∑•ÂÖ∑„ÄÇ‰∏∫‰∫Ü‰ΩøËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂáÜÁ°ÆÁêÜËß£ÂàÜÂ≠êÁªìÊûÑÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜCLEANMOLÊ°ÜÊû∂ÔºåÂ∞ÜSMILESËß£ÊûêËΩ¨Âåñ‰∏∫‰∏ÄÁ≥ªÂàóÊ∏ÖÊô∞‰∏îÁ°ÆÂÆöÁöÑ‰ªªÂä°Ôºå‰ª•‰øÉËøõÂõæÁ∫ßÂàÜÂ≠êÁêÜËß£„ÄÇËøô‰∫õ‰ªªÂä°ÂåÖÊã¨Â≠êÂõæÂåπÈÖçÂíåÂÖ®Â±ÄÂõæÂåπÈÖçÔºåÊèê‰æõ‰∏éÂàÜÂ≠êÁªìÊûÑÁâπÊÄßÁõ∏‰∏ÄËá¥ÁöÑÁªìÊûÑÂåñÁõëÁù£„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåCLEANMOL‰∏ç‰ªÖÂ¢ûÂº∫‰∫ÜÁªìÊûÑÁêÜËß£ËÉΩÂäõÔºåËøòÂú®Mol-InstructionsÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ', title='CLEANMOLÔºöÊèêÂçáÂàÜÂ≠êÁªìÊûÑÁêÜËß£ÁöÑÂàõÊñ∞Ê°ÜÊû∂'))
[28.05.2025 05:13] Loading Chinese text from previous data.
[28.05.2025 05:13] Renaming data file.
[28.05.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-05-28.json
[28.05.2025 05:13] Saving new data file.
[28.05.2025 05:13] Generating page.
[28.05.2025 05:13] Renaming previous page.
[28.05.2025 05:13] Renaming previous data. index.html to ./d/2025-05-28.html
[28.05.2025 05:13] [Experimental] Generating Chinese page for reading.
[28.05.2025 05:13] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√† x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«î y√°n m√≥ x√≠ng', 'trans': 'language model'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Âø´ÈÄü', 'pinyin': 'ku√†i s√π', 'trans': 'rapid'}, {'word': 'ÂèëÂ±ï', 'pinyin': 'fƒÅ zh«én', 'trans': 'development'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«î y√†o', 'trans': 'main'}, {'word': 'ÈÄöËøá', 'pinyin': 't≈çng gu√≤', 'trans': 'through'}, {'word': 'Â¢ûÂä†', 'pinyin': 'zƒìng jiƒÅ', 'trans': 'increase'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅn sh«î', 'trans': 'parameter'}, {'word': 'Êï∞Èáè', 'pinyin': 'sh√π li√†ng', 'trans': 'quantity'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'Á°¨‰ª∂', 'pinyin': 'y√¨ng ji√†n', 'trans': 'hardware'}, {'word': 'ÈôêÂà∂', 'pinyin': 'xi√†n zh√¨', 'trans': 'limit'}, {'word': 'Ëá™Ê≥®ÊÑèÂäõ', 'pinyin': 'z√¨ zh√π y√¨ l√¨', 'trans': 'self-attention'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ng bƒõn', 'trans': 'cost'}, {'word': 'Áì∂È¢à', 'pinyin': 'p√≠ng l√≥ng', 'trans': 'bottleneck'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÈáçÁÇπ', 'pinyin': 'zh√≤ng di«én', 'trans': 'focus'}, {'word': 'Ê®°ÂûãÂéãÁº©', 'pinyin': 'm√≥ x√≠ng yƒÅ su≈ç', 'trans': 'model compression'}, {'word': 'ËΩ¨Âêë', 'pinyin': 'zhu«én xi√†ng', 'trans': 'turn to'}, {'word': 'Êï∞ÊçÆÂéãÁº©', 'pinyin': 'sh√π j√π yƒÅ su≈ç', 'trans': 'data compression'}, {'word': '‰ª§Áâå', 'pinyin': 'l√¨ng p√°i', 'trans': 'token'}, {'word': 'ÂéãÁº©', 'pinyin': 'yƒÅ su≈ç', 'trans': 'compression'}, {'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«én sh«éo', 'trans': 'reduce'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'training'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'inference'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ ch√©ng', 'trans': 'process'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†o l«ú', 'trans': 'efficiency'}, {'word': '‰ΩúËÄÖ', 'pinyin': 'zu√≤ zhƒõ', 'trans': 'author'}, {'word': 'ÂàÜÊûê', 'pinyin': 'fƒìn xƒ´', 'trans': 'analyze'}, {'word': 'Èïø‰∏ä‰∏ãÊñá', 'pinyin': 'ch√°ng sh√†ng xi√† w√©n', 'trans': 'long context'}, {'word': 'Êï∞Â≠¶Ê°ÜÊû∂', 'pinyin': 'sh√π xu√© ku√†ng ji√†', 'trans': 'mathematical framework'}, {'word': 'Êé¢ËÆ®', 'pinyin': 't√†n t«éo', 'trans': 'explore'}, {'word': '‰ºòÂäø', 'pinyin': 'y≈çu sh√¨', 'trans': 'advantage'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}]
[28.05.2025 05:13] Renaming previous Chinese page.
[28.05.2025 05:13] Renaming previous data. zh.html to ./d/2025-05-27_zh_reading_task.html
[28.05.2025 05:13] Writing Chinese reading task.
[28.05.2025 05:13] Writing result.
[28.05.2025 05:13] Renaming log file.
[28.05.2025 05:13] Renaming previous data. log.txt to ./logs/2025-05-28_last_log.txt
