[14.11.2024 06:14] Read previous papers.
[14.11.2024 06:14] Generating top page (month).
[14.11.2024 06:14] Writing top page (month).
[14.11.2024 07:10] Read previous papers.
[14.11.2024 07:10] Get feed.
[14.11.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.08380
[14.11.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.07618
[14.11.2024 07:10] Extract page data from URL. URL: https://huggingface.co/papers/2411.08147
[14.11.2024 07:10] ********************************************************************************
[14.11.2024 07:10] Abstract 0. Video generation has emerged as a promising tool for world simulation, leveraging visual data to replicate real-world environments. Within this context, egocentric video generation, which centers on the human perspective, holds significant potential for enhancing applications in virtual reality, aug...
[14.11.2024 07:10] ********************************************************************************
[14.11.2024 07:10] Abstract 1. The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencie...
[14.11.2024 07:10] ********************************************************************************
[14.11.2024 07:10] Abstract 2. Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus ...
[14.11.2024 07:10] Read previous papers.
[14.11.2024 07:10] Generating reviews via LLM API.
[14.11.2024 07:10] Using data from previous issue: {"categories": ["#data", "#synthetic", "#games", "#video", "#dataset"], "emoji": "üëÄ", "ru": {"title": "EgoVid-5M: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EgoVid-5M - –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 5 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∫–ª–∏–ø–æ–≤ 
[14.11.2024 07:10] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ - Feature-level cons
[14.11.2024 07:10] Querying the API.
[14.11.2024 07:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose \ours, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of \ours, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, \ours achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.
[14.11.2024 07:10] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ú–µ—Ç–æ–¥, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π \ours, –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –ü–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Ö –æ—Ü–µ–Ω–∫—É —Å –ø–æ–º–æ—â—å—é –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –∏ –ø–æ—Å–ª–µ–¥—É—é—â—É—é –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.",

  "emoji": "üß†",

  "title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
}
[14.11.2024 07:10] Error. Failed to parse JSON from LLM. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ú–µ—Ç–æ–¥, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π \ours, –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –ü–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Ö –æ—Ü–µ–Ω–∫—É —Å –ø–æ–º–æ—â—å—é –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –∏ –ø–æ—Å–ª–µ–¥—É—é—â—É—é –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.",

  "emoji": "üß†",

  "title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"
}
[14.11.2024 07:10] Fallback to OpenAI.
[14.11.2024 07:10] Response: ParsedChatCompletionMessage[ArticleFull](content='{"desc":"–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLMs) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ \\\\\\\\ours, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—Ç—å—Å—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Ö –æ—Ü–µ–Ω–∫–µ —Å –ø–æ–º–æ—â—å—é –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ \\\\\\\\ours –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Llama-3.1-8B-Instruct.","emoji":"üß†","title":"–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ LLMs –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=ArticleFull(desc='–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLMs) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ \\\\ours, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—Ç—å—Å—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –æ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Ö –æ—Ü–µ–Ω–∫–µ —Å –ø–æ–º–æ—â—å—é –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ \\\\ours –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Llama-3.1-8B-Instruct.', emoji='üß†', title='–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ LLMs –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤'))
[14.11.2024 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose \ours, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of \ours, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, \ours achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs."

[14.11.2024 07:10] Response: ```python
['TRAINING', 'RLHF']
```
[14.11.2024 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose \ours, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of \ours, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, \ours achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs."

[14.11.2024 07:10] Response: ```python
["LONG_CONTEXT", "SYNTHETIC"]
```
[14.11.2024 07:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges that large language models (LLMs) face in reasoning over long contexts. The authors propose a new method called \\textit{ours}, which allows LLMs to enhance their long-context reasoning capabilities without relying on human-annotated data. Instead, the approach involves generating multiple outputs for each question, scoring them using Minimum Bayes Risk, and then fine-tuning the model based on these scores. Experimental results show that \\textit{ours} significantly improves performance on Llama-3.1-8B-Instruct, outperforming previous methods that depend on expert-generated data.","title":"Empowering LLMs to Self-Improve Long-Context Reasoning"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper addresses the challenges that large language models (LLMs) face in reasoning over long contexts. The authors propose a new method called \textit{ours}, which allows LLMs to enhance their long-context reasoning capabilities without relying on human-annotated data. Instead, the approach involves generating multiple outputs for each question, scoring them using Minimum Bayes Risk, and then fine-tuning the model based on these scores. Experimental results show that \textit{ours} significantly improves performance on Llama-3.1-8B-Instruct, outperforming previous methods that depend on expert-generated data.', title='Empowering LLMs to Self-Improve Long-Context Reasoning'))
[14.11.2024 07:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÈïøÊñáÊú¨ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÈïøÊñáÊú¨Êé®ÁêÜ‰∏ä‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫é‰∫∫Â∑•‰∏ìÂÆ∂ÊàñÂÖàËøõÊ®°ÂûãÔºàÂ¶ÇGPT-4ÔºâÊèê‰æõÁöÑÂêàÊàêÊï∞ÊçÆËøõË°åÂæÆË∞ÉÔºåËøôÈôêÂà∂‰∫ÜËøõ‰∏ÄÊ≠•ÁöÑÂèëÂ±ï„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫\\\\oursÁöÑÊñπÊ≥ïÔºåÊó®Âú®ËÆ©LLMsÂú®ÈïøÊñáÊú¨Êé®ÁêÜ‰∏≠Ëá™ÊàëÊîπËøõ„ÄÇÈÄöËøáÂØπÊØè‰∏™ÈóÆÈ¢òÈááÊ†∑Â§ö‰∏™ËæìÂá∫ÔºåÂπ∂‰ΩøÁî®ÊúÄÂ∞èË¥ùÂè∂ÊñØÈ£éÈô©ËøõË°åËØÑÂàÜÔºåÊàë‰ª¨ÂèØ‰ª•Âü∫‰∫éËøô‰∫õËæìÂá∫ËøõË°åÁõëÁù£ÂæÆË∞ÉÊàñÂÅèÂ•Ω‰ºòÂåñÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ","title":"ËÆ©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËá™ÊàëÊèêÂçáÈïøÊñáÊú¨Êé®ÁêÜËÉΩÂäõ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÈïøÊñáÊú¨ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÈïøÊñáÊú¨Êé®ÁêÜ‰∏ä‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫é‰∫∫Â∑•‰∏ìÂÆ∂ÊàñÂÖàËøõÊ®°ÂûãÔºàÂ¶ÇGPT-4ÔºâÊèê‰æõÁöÑÂêàÊàêÊï∞ÊçÆËøõË°åÂæÆË∞ÉÔºåËøôÈôêÂà∂‰∫ÜËøõ‰∏ÄÊ≠•ÁöÑÂèëÂ±ï„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫\\oursÁöÑÊñπÊ≥ïÔºåÊó®Âú®ËÆ©LLMsÂú®ÈïøÊñáÊú¨Êé®ÁêÜ‰∏≠Ëá™ÊàëÊîπËøõ„ÄÇÈÄöËøáÂØπÊØè‰∏™ÈóÆÈ¢òÈááÊ†∑Â§ö‰∏™ËæìÂá∫ÔºåÂπ∂‰ΩøÁî®ÊúÄÂ∞èË¥ùÂè∂ÊñØÈ£éÈô©ËøõË°åËØÑÂàÜÔºåÊàë‰ª¨ÂèØ‰ª•Âü∫‰∫éËøô‰∫õËæìÂá∫ËøõË°åÁõëÁù£ÂæÆË∞ÉÊàñÂÅèÂ•Ω‰ºòÂåñÔºå‰ªéËÄåÊòæËëóÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ', title='ËÆ©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËá™ÊàëÊèêÂçáÈïøÊñáÊú¨Êé®ÁêÜËÉΩÂäõ'))
[14.11.2024 07:10] Loading Chinese text from previous data.
[14.11.2024 07:10] Renaming data file.
[14.11.2024 07:10] Renaming previous data. hf_papers.json to ./d/2024-11-14.json
[14.11.2024 07:10] Saving new data file.
[14.11.2024 07:10] Generating page.
[14.11.2024 07:10] Renaming previous page.
[14.11.2024 07:10] Renaming previous data. index.html to ./d/2024-11-14.html
[14.11.2024 07:10] [Experimental] Generating Chinese page for reading.
[14.11.2024 07:10] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': '3DÈÉ®‰ª∂ÂàÜÂâ≤', 'pinyin': '3D b√π ji√†n fƒìn gƒì', 'trans': '3D part segmentation'}, {'word': 'ÈáçË¶ÅÊÄß', 'pinyin': 'zh√≤ng y√†o x√¨ng', 'trans': 'importance'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'application'}, {'word': 'Áé∞ÊúâÊñπÊ≥ï', 'pinyin': 'xi√†n y«íu fƒÅng f«é', 'trans': 'existing methods'}, {'word': 'ËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language model'}, {'word': 'Áü•ËØÜËí∏È¶è', 'pinyin': 'zhƒ´ shi zhƒìng li√∫', 'trans': 'knowledge distillation'}, {'word': 'Èõ∂Ê†∑Êú¨', 'pinyin': 'l√≠ng y√†ng bƒõn', 'trans': 'zero-shot'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´ l√†i', 'trans': 'rely on'}, {'word': 'ÊñáÊú¨ÊèêÁ§∫', 'pinyin': 'w√©n bƒõn t√≠ sh√¨', 'trans': 'textual prompts'}, {'word': 'ÈôêÂà∂', 'pinyin': 'xi√†n zh√¨', 'trans': 'limit'}, {'word': 'Êâ©Â±ïÊÄß', 'pinyin': 'ku√≤ zh«én x√¨ng', 'trans': 'scalability'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«î l«ê', 'trans': 'handle'}, {'word': 'ÈÉ®‰ª∂Ê≠ß‰πâ', 'pinyin': 'b√π ji√†n q√≠ y√¨', 'trans': 'part ambiguity'}, {'word': 'ÁÅµÊ¥ªÊÄß', 'pinyin': 'l√≠ng hu√≥ x√¨ng', 'trans': 'flexibility'}, {'word': 'SAMPart3D', 'pinyin': 'SAMPart3D', 'trans': 'SAMPart3D'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'È¢ÑÂÆö‰πâ', 'pinyin': 'y√π d√¨ng y√¨', 'trans': 'predefined'}, {'word': 'Ê†áÁ≠æÈõÜ', 'pinyin': 'biƒÅo qiƒÅn j√≠', 'trans': 'label set'}, {'word': 'ËßÜËßâÂü∫Á°ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© jƒ´ ch«î m√≥ x√≠ng', 'trans': 'vision foundation model'}, {'word': 'Â§öÂ∞∫Â∫¶', 'pinyin': 'du≈ç ch«ê d√π', 'trans': 'multi-scale'}, {'word': 'ÈÉ®‰ª∂ÊÑüÁü•', 'pinyin': 'b√π ji√†n g«én zhƒ´', 'trans': 'part-aware'}, {'word': '3DÁâπÂæÅ', 'pinyin': '3D t√® zhƒìng', 'trans': '3D features'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'Â§ßËßÑÊ®°', 'pinyin': 'd√† guƒ´ m√≥', 'trans': 'large-scale'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}]
[14.11.2024 07:10] Renaming previous Chinese page.
[14.11.2024 07:10] Renaming previous data. zh.html to ./d/2024-11-13_zh_reading_task.html
[14.11.2024 07:10] Writing Chinese reading task.
[14.11.2024 07:10] Writing result.
[14.11.2024 07:10] Renaming log file.
[14.11.2024 07:10] Renaming previous data. log.txt to ./logs/2024-11-14_last_log.txt
