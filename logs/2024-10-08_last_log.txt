[09.10.2024 18:15] Get feed.
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04717
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04199
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.01912
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05193
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.03290
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.02705
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04422
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.02743
[09.10.2024 18:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.05076
[09.10.2024 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.03399
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 0. Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for i...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 1. Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to gen...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 2. This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression ...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 3. With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason ...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 4. Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific vide...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 5. Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 6. Long-context language models (LCLM), characterized by their extensive context window, is becoming increasingly popular. Meanwhile, many long-context benchmarks present challenging tasks that even the most advanced LCLMs struggle to complete. However, the underlying sources of various challenging lon...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 7. Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to disce...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 8. Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularl...
[09.10.2024 18:15] ********************************************************************************
[09.10.2024 18:15] Abstract 9. Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standard...
[09.10.2024 18:15] Read previous papers.
[09.10.2024 18:15] Generating reviews via LLM API.
[09.10.2024 18:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ñ–∞–∫—Ç–æ—Ä—ã, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –æ–±–æ–±—â–∞—Ç—å –Ω–∞–≤—ã–∫–∏ –Ω–∞ –Ω–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ, —á—Ç–æ —Ç–∞–∫–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏
[09.10.2024 18:15] Using data from previous issue: {"desc": "LongGenBench - —ç—Ç–æ –Ω–æ–≤—ã–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∏—Ö—Å—è –Ω–∞ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, LongGenBench —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –≤—Å–µ 
[09.10.2024 18:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 2-Dimensional Autoregression (DnD) Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. DnD-Transformer —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏, –≤–≤–æ–¥—è –Ω–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ - –≥–ª—É–±–∏–Ω—É –º–æ–¥–µ–ª–∏. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω
[09.10.2024 18:15] Using data from previous issue: {"desc": "RevisEval - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥ –æ—Ç–≤–µ—Ç —ç—Ç–∞–ª–æ–Ω—ã. –ú–µ—Ç–æ–¥ –∑–∞–¥–µ–π—Å—Ç–≤—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —ç—Ç–∞–ª–æ–Ω–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RevisEval –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫
[09.10.2024 18:15] Using data from previous issue: {"desc": "Grounded-VideoLLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Video-LLM, —Å–ø–æ—Å–æ–±–Ω–∞—è –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Ç–æ–∫ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ—ç—Ç–∞–ø–Ω
[09.10.2024 18:15] Using data from previous issue: {"desc": "ControlAR - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. ControlAR –ø—Ä–∏–º–µ–Ω—è–µ—Ç —É—Å–ª–æ–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ
[09.10.2024 18:15] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (LCLM) –∏ –∏—Ö —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–∏ –∑–∞–¥–∞—á–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –≥–∏–ø–µ—Ä–º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–º–∏, —á—Ç–æ –ø—Ä–µ–≤—ã—à–∞
[09.10.2024 18:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ MA-RLHF –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞–∫—Ä–æ–¥–µ–π—Å—Ç–≤–∏—è - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏–ª–∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ - –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.
[09.10.2024 18:15] Querying the API.
[09.10.2024 18:15] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TidalDecode - –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ —Ç–æ—á–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–∏. TidalDecode –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∏ –≤–≤–æ–¥–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–µ–≤ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –∏ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ TidalDecode —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Å –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, —É–º–µ–Ω—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫—É –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è LLM –¥–æ 2,1 —Ä–∞–∑–∞.",
  "tags": ["#—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ_–≤–Ω–∏–º–∞–Ω–∏–µ", "#–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ_LLM", "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_–ø–∞–º—è—Ç–∏"],
  "emoji": "üåä",
  "title": "TidalDecode: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ LLM —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è"
}
[09.10.2024 18:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EBES - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ —Å–æ–±—ã—Ç–∏–π. EBES —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–∫–ª—é—á–∞–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ
[09.10.2024 18:15] Renaming data file.
[09.10.2024 18:15] Renaming previous data. hf_papers.json to 2024-10-08_hf_papers.json
[09.10.2024 18:15] Saving new data file.
[09.10.2024 18:15] Generating page.
[09.10.2024 18:15] Renaming previous page.
[09.10.2024 18:15] Renaming previous data. index.html to 2024-10-08_hf_papers.html
[09.10.2024 18:15] Writing result.
[09.10.2024 18:15] Renaming log file.
[09.10.2024 18:15] Renaming previous data. log.txt to 2024-10-08_last_log.txt
