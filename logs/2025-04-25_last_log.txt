[25.04.2025 19:09] Read previous papers.
[25.04.2025 19:09] Generating top page (month).
[25.04.2025 19:09] Writing top page (month).
[25.04.2025 20:12] Read previous papers.
[25.04.2025 20:12] Get feed.
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17761
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17502
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17192
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17432
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17207
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16511
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17040
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17789
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16828
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17069
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16921
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17414
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17343
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16064
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15921
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17601
[25.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17670
[25.04.2025 20:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.04.2025 20:12] No deleted papers detected.
[25.04.2025 20:12] Downloading and parsing papers (pdf, html). Total: 17.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17761.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17761.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17761.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17502.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17502.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17502.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17192.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17192.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17192.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17432.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17432.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17432.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17207.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17207.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17207.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16511.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16511.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16511.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17040.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17040.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17040.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17789.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17789.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17789.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16828.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16828.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16828.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17069.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17069.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17069.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16921.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16921.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16921.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17414.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17414.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17414.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17343.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17343.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17343.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16064.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16064.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16064.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.15921.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.15921.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.15921.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17601.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17601.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17601.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17670.
[25.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17670.json), skip PDF parsing.
[25.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17670.json), skip HTML parsing.
[25.04.2025 20:12] Success.
[25.04.2025 20:12] Enriching papers with extra data.
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 0. In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a ...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 1. Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability -- ranging from enhanced personalization in image generation to consisten...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 2. Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific d...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 3. The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-tex...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 4. We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for env...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 5. Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approach...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 6. We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embedd...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 7. Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training a...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 8. Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the sol...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 9. Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural o...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 10. Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fal...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 11. Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DV-TON, a novel diffusion-based framework for generating high-fidelity and t...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 12. The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Larg...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 13. Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 14. We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a system to summarise hour long videos with no-supervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are spar...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 15. Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining t...
[25.04.2025 20:12] ********************************************************************************
[25.04.2025 20:12] Abstract 16. With the advent of large-scale 3D datasets, feed-forward 3D generative models, such as the Large Reconstruction Model (LRM), have gained significant attention and achieved remarkable success. However, we observe that RGB images often lead to conflicting training objectives and lack the necessary cla...
[25.04.2025 20:12] Read previous papers.
[25.04.2025 20:12] Generating reviews via LLM API.
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#data", "#diffusion", "#training", "#multimodal", "#cv", "#dataset", "#open_source", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "Step1X-Edit: –û—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –ª—É—á—à–∏—Ö –∑–∞–∫—Ä—ã—Ç—ã—Ö —Ä–µ—à–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#dataset", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "RefVNLI: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#agents", "#architecture", "#benchmark", "#dataset", "#open_source", "#science"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç—å–∏ –∫ –∫–æ–¥—É: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "PaperCoder - —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∫–æ—Ç–æ
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "üîç", "ru": {"title": "UniME: –£–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É", "desc": "UniME - —ç—Ç–æ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–º –º—É–ª—å—Ç
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#cv", "#benchmark", "#agents"], "emoji": "üîÑ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò —á–µ—Ä–µ–∑ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#training", "#data", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QuaDMix - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). QuaDMi
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#cv", "#optimization", "#inference", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö VLM –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "DyMU - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ vision-language –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#diffusion", "#cv", "#benchmark", "#optimization", "#multimodal"], "emoji": "üîÄ", "ru": {"title": "Token-Shuffle: –ø—Ä–æ—Ä—ã–≤ –≤ –≤—ã—Å–æ–∫–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Token-Shuffle –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#data", "#reasoning", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "ThinkPRM: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ—à–µ–Ω–∏–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ThinkPRM - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (P
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training"], "emoji": "üß©", "ru": {"title": "–£–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –ø–∞—Ç—á–∞–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—á–∏ –≤ –ø—Ä
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#open_source", "#multilingual", "#dataset", "#low_resource", "#benchmark"], "emoji": "üåê", "ru": {"title": "IberBench: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ LLM –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ", "desc": "IberBench - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏ —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#video", "#3d", "#leakage", "#diffusion", "#dataset", "#benchmark"], "emoji": "üëö", "ru": {"title": "3D-–º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ –æ–¥–µ–∂–¥—ã –≤ –≤–∏–¥–µ–æ", "desc": "3DV-TON - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–µ –æ–¥–µ–∂–¥—ã –≤ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ –∏
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#video", "#long_context", "#dataset", "#games", "#benchmark"], "emoji": "üé•", "ru": {"title": "TimeChat-Online: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ", "desc": "TimeChat-Online - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å VideoLLM –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ö–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç - –º–æ–¥—É–ª—å
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#cv", "#inference", "#optimization", "#training", "#diffusion", "#architecture"], "emoji": "üñºÔ∏è", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#video", "#multimodal", "#dataset", "#optimization", "#transfer_learning"], "emoji": "üé•", "ru": {"title": "ViSMap: –£–º–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "ViSMap - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –Ω–µ—Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–Ω–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–µ—Ç–∞–ø—Ä–æ–º–ø—Ç–∏–Ω–≥. –û–Ω–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç —Ä–∞–∑—Ä—ã
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#data", "#interpretability", "#architecture"], "emoji": "üî¨", "ru": {"title": "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: –º–æ—â–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, —Å–æ—á–µ—Ç–∞—é—â–∏–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –ª–∏–Ω–µ–π–Ω—ã—Ö
[25.04.2025 20:12] Using data from previous issue: {"categories": ["#3d", "#optimization", "#dataset"], "emoji": "üßä", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DiMeR - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –≤—Ö–æ–¥–Ω—ã
[25.04.2025 20:12] Loading Chinese text from previous data.
[25.04.2025 20:12] Renaming data file.
[25.04.2025 20:12] Renaming previous data. hf_papers.json to ./d/2025-04-25.json
[25.04.2025 20:12] Saving new data file.
[25.04.2025 20:12] Generating page.
[25.04.2025 20:12] Renaming previous page.
[25.04.2025 20:12] Renaming previous data. index.html to ./d/2025-04-25.html
[25.04.2025 20:12] [Experimental] Generating Chinese page for reading.
[25.04.2025 20:12] Chinese vocab [{'word': 'ÂõæÂÉè', 'pinyin': 't√∫ xi√†ng', 'trans': 'image'}, {'word': 'ÁºñËæë', 'pinyin': 'biƒÅn j√≠', 'trans': 'edit'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'remarkable'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«én sh√¨', 'trans': 'demonstrate'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ng d√†', 'trans': 'powerful'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'capability'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open-source'}, {'word': 'ÁÆóÊ≥ï', 'pinyin': 'su√†n f«é', 'trans': 'algorithm'}, {'word': 'Èó≠Ê∫ê', 'pinyin': 'b√¨ yu√°n', 'trans': 'closed-source'}, {'word': 'Â∑ÆË∑ù', 'pinyin': 'chƒÅ j√π', 'trans': 'gap'}, {'word': 'ÂèëÂ∏É', 'pinyin': 'fƒÅ b√π', 'trans': 'release'}, {'word': 'Âêç‰∏∫', 'pinyin': 'm√≠ng w√©i', 'trans': 'named'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'Êèê‰æõ', 'pinyin': 't√≠ g≈çng', 'trans': 'provide'}, {'word': 'Áõ∏Â™≤Áæé', 'pinyin': 'xiƒÅng p√¨ mƒõi', 'trans': 'comparable'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}]
[25.04.2025 20:12] Renaming previous Chinese page.
[25.04.2025 20:12] Renaming previous data. zh.html to ./d/2025-04-24_zh_reading_task.html
[25.04.2025 20:12] Writing Chinese reading task.
[25.04.2025 20:12] Writing result.
[25.04.2025 20:12] Renaming log file.
[25.04.2025 20:12] Renaming previous data. log.txt to ./logs/2025-04-25_last_log.txt
