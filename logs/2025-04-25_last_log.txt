[25.04.2025 03:33] Read previous papers.
[25.04.2025 03:33] Generating top page (month).
[25.04.2025 03:33] Writing top page (month).
[25.04.2025 04:13] Read previous papers.
[25.04.2025 04:13] Get feed.
[25.04.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17761
[25.04.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17432
[25.04.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17207
[25.04.2025 04:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.04.2025 04:13] No deleted papers detected.
[25.04.2025 04:13] Downloading and parsing papers (pdf, html). Total: 3.
[25.04.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2504.17761.
[25.04.2025 04:13] Extra JSON file exists (./assets/json/2504.17761.json), skip PDF parsing.
[25.04.2025 04:13] Paper image links file exists (./assets/img_data/2504.17761.json), skip HTML parsing.
[25.04.2025 04:13] Success.
[25.04.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2504.17432.
[25.04.2025 04:13] Extra JSON file exists (./assets/json/2504.17432.json), skip PDF parsing.
[25.04.2025 04:13] Paper image links file exists (./assets/img_data/2504.17432.json), skip HTML parsing.
[25.04.2025 04:13] Success.
[25.04.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2504.17207.
[25.04.2025 04:13] Extra JSON file exists (./assets/json/2504.17207.json), skip PDF parsing.
[25.04.2025 04:13] Paper image links file exists (./assets/img_data/2504.17207.json), skip HTML parsing.
[25.04.2025 04:13] Success.
[25.04.2025 04:13] Enriching papers with extra data.
[25.04.2025 04:13] ********************************************************************************
[25.04.2025 04:13] Abstract 0. In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a ...
[25.04.2025 04:13] ********************************************************************************
[25.04.2025 04:13] Abstract 1. The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-tex...
[25.04.2025 04:13] ********************************************************************************
[25.04.2025 04:13] Abstract 2. We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for env...
[25.04.2025 04:13] Read previous papers.
[25.04.2025 04:13] Generating reviews via LLM API.
[25.04.2025 04:13] Using data from previous issue: {"categories": ["#data", "#diffusion", "#training", "#multimodal", "#cv", "#dataset", "#open_source", "#benchmark"], "emoji": "🖼️", "ru": {"title": "Step1X-Edit: Открытая модель редактирования изображений на уровне лучших закрытых решений", "desc": "Статья представляет новую модель редактирования из
[25.04.2025 04:13] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "🔍", "ru": {"title": "UniME: Улучшение мультимодальных представлений через дистилляцию знаний и инструктивную настройку", "desc": "UniME - это новый двухэтапный фреймворк для обучения дискриминативным мульт
[25.04.2025 04:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#cv", "#benchmark", "#agents"], "emoji": "🔄", "ru": {"title": "Улучшение пространственного мышления ИИ через абстрактное изменение перспективы", "desc": "Статья представляет фреймворк для перспективно-ориентированных рассуждений в визуально-языковых моде
[25.04.2025 04:13] Loading Chinese text from previous data.
[25.04.2025 04:13] Renaming data file.
[25.04.2025 04:13] Renaming previous data. hf_papers.json to ./d/2025-04-25.json
[25.04.2025 04:13] Saving new data file.
[25.04.2025 04:13] Generating page.
[25.04.2025 04:13] Renaming previous page.
[25.04.2025 04:13] Renaming previous data. index.html to ./d/2025-04-25.html
[25.04.2025 04:13] [Experimental] Generating Chinese page for reading.
[25.04.2025 04:13] Chinese vocab [{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'vision'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'}, {'word': '核心', 'pinyin': 'héxīn', 'trans': 'core'}, {'word': '组成部分', 'pinyin': 'zǔchéng bùfēn', 'trans': 'component'}, {'word': '先进', 'pinyin': 'xiānjìn', 'trans': 'advanced'}, {'word': '多模态', 'pinyin': 'duō móshì', 'trans': 'multimodal'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '依赖', 'pinyin': 'yīlài', 'trans': 'rely on'}, {'word': '描述', 'pinyin': 'miáoshù', 'trans': 'description'}, {'word': '允许', 'pinyin': 'yǔnxǔ', 'trans': 'allow'}, {'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'}, {'word': '捷径', 'pinyin': 'jiéjìng', 'trans': 'shortcut'}, {'word': '衡量', 'pinyin': 'héngliáng', 'trans': 'measure'}, {'word': '真正', 'pinyin': 'zhēnzhèng', 'trans': 'genuine'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'}, {'word': '验证', 'pinyin': 'yànzhèng', 'trans': 'verification'}, {'word': '问题', 'pinyin': 'wèntí', 'trans': 'question'}, {'word': '涵盖', 'pinyin': 'hángài', 'trans': 'cover'}, {'word': '类别', 'pinyin': 'lèibié', 'trans': 'category'}, {'word': '量化', 'pinyin': 'liànghuà', 'trans': 'quantification'}, {'word': '转换', 'pinyin': 'zhuǎnhuàn', 'trans': 'conversion'}, {'word': '空间', 'pinyin': 'kōngjiān', 'trans': 'space'}, {'word': '关系', 'pinyin': 'guānxì', 'trans': 'relationship'}, {'word': '属性', 'pinyin': 'shǔxìng', 'trans': 'attribute'}, {'word': '比较', 'pinyin': 'bǐjiào', 'trans': 'comparison'}, {'word': '角度', 'pinyin': 'jiǎodù', 'trans': 'angle'}, {'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'}, {'word': '领先', 'pinyin': 'lǐngxiān', 'trans': 'leading'}, {'word': '分析', 'pinyin': 'fēnxī', 'trans': 'analyze'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '识别', 'pinyin': 'shíbié', 'trans': 'identify'}, {'word': '模式', 'pinyin': 'móshì', 'trans': 'pattern'}, {'word': '准确率', 'pinyin': 'zhǔnquèlǜ', 'trans': 'accuracy'}, {'word': '随机', 'pinyin': 'suíjī', 'trans': 'random'}, {'word': '基线', 'pinyin': 'jīxiàn', 'trans': 'baseline'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '差距', 'pinyin': 'chājù', 'trans': 'gap'}, {'word': '补充', 'pinyin': 'bǔchōng', 'trans': 'supplement'}, {'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'}, {'word': '强化', 'pinyin': 'qiánghuà', 'trans': 'reinforce'}, {'word': '学习', 'pinyin': 'xuéxí', 'trans': 'learning'}, {'word': '支持', 'pinyin': 'zhīchí', 'trans': 'support'}, {'word': '进展', 'pinyin': 'jìnzhǎn', 'trans': 'progress'}]
[25.04.2025 04:13] Renaming previous Chinese page.
[25.04.2025 04:13] Renaming previous data. zh.html to ./d/2025-04-24_zh_reading_task.html
[25.04.2025 04:13] Writing Chinese reading task.
[25.04.2025 04:13] Writing result.
[25.04.2025 04:13] Renaming log file.
[25.04.2025 04:13] Renaming previous data. log.txt to ./logs/2025-04-25_last_log.txt
