[25.04.2025 21:10] Read previous papers.
[25.04.2025 21:10] Generating top page (month).
[25.04.2025 21:10] Writing top page (month).
[25.04.2025 22:10] Read previous papers.
[25.04.2025 22:10] Get feed.
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17761
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17502
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17192
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17432
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17207
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16511
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17789
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17040
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16828
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16921
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16064
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17414
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17343
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17069
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15921
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17601
[25.04.2025 22:10] Extract page data from URL. URL: https://huggingface.co/papers/2504.17788
[25.04.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17670
[25.04.2025 22:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.04.2025 22:10] No deleted papers detected.
[25.04.2025 22:10] Downloading and parsing papers (pdf, html). Total: 18.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17761.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17761.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17761.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17502.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17502.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17502.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17192.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17192.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17192.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17432.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17432.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17432.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17207.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17207.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17207.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.16511.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.16511.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.16511.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17789.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17789.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17789.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17040.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17040.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17040.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.16828.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.16828.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.16828.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.16921.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.16921.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.16921.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.16064.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.16064.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.16064.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17414.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17414.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17414.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17343.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17343.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17343.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17069.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17069.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17069.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.15921.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.15921.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.15921.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17601.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17601.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17601.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17788.
[25.04.2025 22:10] Downloading paper 2504.17788 from http://arxiv.org/pdf/2504.17788v1...
[25.04.2025 22:10] Extracting affiliations from text.
[25.04.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Chris Rockwell1,2 Joseph Tung3 Ming-Yu Liu1 David F. Fouhey3 University of Michigan2 NVIDIA1 Tsung-Yi Lin1 Chen-Hsuan Lin1 New York University 5 2 0 2 4 2 ] . [ 1 8 8 7 7 1 . 4 0 5 2 : r https://research.nvidia.com/labs/dir/dynpose-100k Figure 1. We introduce DynPose-100K, large-scale video dataset of dynamic content with camera annotations. DynPose-100K consists of 100,131 Internet videos that span diverse settings. We curate DynPose-100K such that videos contain dynamic content while ensuring the cameras are able to be estimated (including intrinsics and poses). Towards this end, we address two challenging problems: (a) identifying the videos suitable for camera estimation, and (b) improving the camera estimation algorithm for dynamic videos. "
[25.04.2025 22:10] Response: ```python
["University of Michigan", "NVIDIA", "New York University"]
```
[25.04.2025 22:10] Deleting PDF ./assets/pdf/2504.17788.pdf.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2504.17670.
[25.04.2025 22:10] Extra JSON file exists (./assets/json/2504.17670.json), skip PDF parsing.
[25.04.2025 22:10] Paper image links file exists (./assets/img_data/2504.17670.json), skip HTML parsing.
[25.04.2025 22:10] Success.
[25.04.2025 22:10] Enriching papers with extra data.
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 0. In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a ...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 1. Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability -- ranging from enhanced personalization in image generation to consisten...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 2. Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific d...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 3. The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-tex...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 4. We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for env...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 5. Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approach...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 6. Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training a...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 7. We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embedd...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 8. Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the sol...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 9. Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fal...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 10. Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 11. Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DV-TON, a novel diffusion-based framework for generating high-fidelity and t...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 12. The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Larg...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 13. Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural o...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 14. We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a system to summarise hour long videos with no-supervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are spar...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 15. Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining t...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 16. Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation. However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation. Furthermore, annotating dynamic Internet videos pr...
[25.04.2025 22:10] ********************************************************************************
[25.04.2025 22:10] Abstract 17. With the advent of large-scale 3D datasets, feed-forward 3D generative models, such as the Large Reconstruction Model (LRM), have gained significant attention and achieved remarkable success. However, we observe that RGB images often lead to conflicting training objectives and lack the necessary cla...
[25.04.2025 22:10] Read previous papers.
[25.04.2025 22:10] Generating reviews via LLM API.
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#data", "#diffusion", "#training", "#multimodal", "#cv", "#dataset", "#open_source", "#benchmark"], "emoji": "🖼️", "ru": {"title": "Step1X-Edit: Открытая модель редактирования изображений на уровне лучших закрытых решений", "desc": "Статья представляет новую модель редактирования из
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#dataset", "#multimodal"], "emoji": "🖼️", "ru": {"title": "RefVNLI: Новый стандарт оценки генерации изображений по тексту", "desc": "Статья представляет новый метод оценки генерации изображений по текстовому описанию с сохранением визуальной идентичности объекта
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#agents", "#architecture", "#benchmark", "#dataset", "#open_source", "#science"], "emoji": "🤖", "ru": {"title": "От статьи к коду: автоматизация реализации алгоритмов машинного обучения", "desc": "PaperCoder - это мультиагентная система на основе больших языковых моделей (LLM), кото
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "🔍", "ru": {"title": "UniME: Улучшение мультимодальных представлений через дистилляцию знаний и инструктивную настройку", "desc": "UniME - это новый двухэтапный фреймворк для обучения дискриминативным мульт
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#cv", "#benchmark", "#agents"], "emoji": "🔄", "ru": {"title": "Улучшение пространственного мышления ИИ через абстрактное изменение перспективы", "desc": "Статья представляет фреймворк для перспективно-ориентированных рассуждений в визуально-языковых моде
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#training", "#data", "#optimization"], "emoji": "🔄", "ru": {"title": "Баланс качества и разнообразия данных для улучшения языковых моделей", "desc": "Эта статья представляет QuaDMix - новый подход к оптимизации данных для обучения больших языковых моделей (LLM). QuaDMi
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#diffusion", "#cv", "#benchmark", "#optimization", "#multimodal"], "emoji": "🔀", "ru": {"title": "Token-Shuffle: прорыв в высокоэффективной генерации изображений высокого разрешения", "desc": "Статья представляет метод Token-Shuffle для улучшения авторе
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#cv", "#optimization", "#inference", "#multimodal"], "emoji": "🔬", "ru": {"title": "Динамическое сокращение токенов для эффективных VLM без переобучения", "desc": "DyMU - это эффективный фреймворк для снижения вычислительной нагрузки vision-language моделей без дополнительного обуче
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#data", "#reasoning", "#training", "#benchmark"], "emoji": "🧠", "ru": {"title": "ThinkPRM: Эффективная верификация решений с минимальным обучением", "desc": "Статья представляет ThinkPRM - новый подход к созданию процессных моделей вознаграждения (P
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#multilingual", "#dataset", "#low_resource", "#benchmark"], "emoji": "🌐", "ru": {"title": "IberBench: многоязычная оценка LLM за пределами английского", "desc": "IberBench - это комплексный и расширяемый бенчмарк для оценки производительности крупных языковых моделей
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#cv", "#inference", "#optimization", "#training", "#diffusion", "#architecture"], "emoji": "🖼️", "ru": {"title": "Объединение латентных и семантических представлений для улучшенной генерации изображений", "desc": "Статья представляет новый подход к генеративному моделированию изобра
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#video", "#3d", "#leakage", "#diffusion", "#dataset", "#benchmark"], "emoji": "👚", "ru": {"title": "3D-модели для реалистичной виртуальной примерки одежды в видео", "desc": "3DV-TON - это новый подход к виртуальной примерке одежды в видео, основанный на диффузионных моделях. Метод и
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#video", "#long_context", "#dataset", "#games", "#benchmark"], "emoji": "🎥", "ru": {"title": "TimeChat-Online: революция в понимании потокового видео", "desc": "TimeChat-Online - это новая модель VideoLLM для обработки потокового видео в реальном времени. Ключевой компонент - модуль
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training"], "emoji": "🧩", "ru": {"title": "Умная генерация изображений: правильный порядок имеет значение", "desc": "Статья представляет новый подход к авторегрессивной генерации изображений по патчам. Авторы предлагают обучать модель генерировать патчи в пр
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#video", "#multimodal", "#dataset", "#optimization", "#transfer_learning"], "emoji": "🎥", "ru": {"title": "ViSMap: Умное суммирование видео без учителя", "desc": "ViSMap - это система для несупервизорного суммаризации длинных видео, использующая метапромптинг. Она преодолевает разры
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#optimization", "#data", "#interpretability", "#architecture"], "emoji": "🔬", "ru": {"title": "Интерпретируемое нелинейное снижение размерности: мощность и прозрачность", "desc": "Эта статья представляет новый метод снижения размерности данных, сочетающий интерпретируемость линейных
[25.04.2025 22:10] Querying the API.
[25.04.2025 22:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation. However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation. Furthermore, annotating dynamic Internet videos present significant challenges even for state-of-theart methods. In this paper, we introduce DynPose-100K, a large-scale dataset of dynamic Internet videos annotated with camera poses. Our collection pipeline addresses filtering using a carefully combined set of task-specific and generalist models. For pose estimation, we combine the latest techniques of point tracking, dynamic masking, and structure-from-motion to achieve improvements over the state-of-the-art approaches. Our analysis and experiments demonstrate that DynPose-100K is both large-scale and diverse across several key attributes, opening up avenues for advancements in various downstream applications.
[25.04.2025 22:10] Response: {
  "desc": "DynPose-100K - это крупномасштабный набор данных динамических видео из интернета с аннотациями положения камеры. Авторы разработали конвейер сбора данных, использующий комбинацию специализированных и общих моделей для фильтрации видео. Для оценки положения камеры применяются современные методы отслеживания точек, динамического маскирования и восстановления структуры из движения. Эксперименты показывают, что DynPose-100K является масштабным и разнообразным набором данных, открывающим возможности для развития различных приложений.",
  "emoji": "🎥",
  "title": "DynPose-100K: Масштабный датасет для оценки положения камеры в динамических видео"
}
[25.04.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation. However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation. Furthermore, annotating dynamic Internet videos present significant challenges even for state-of-theart methods. In this paper, we introduce DynPose-100K, a large-scale dataset of dynamic Internet videos annotated with camera poses. Our collection pipeline addresses filtering using a carefully combined set of task-specific and generalist models. For pose estimation, we combine the latest techniques of point tracking, dynamic masking, and structure-from-motion to achieve improvements over the state-of-the-art approaches. Our analysis and experiments demonstrate that DynPose-100K is both large-scale and diverse across several key attributes, opening up avenues for advancements in various downstream applications."

[25.04.2025 22:10] Response: ```python
["DATASET", "DATA", "CV"]
```
[25.04.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation. However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation. Furthermore, annotating dynamic Internet videos present significant challenges even for state-of-theart methods. In this paper, we introduce DynPose-100K, a large-scale dataset of dynamic Internet videos annotated with camera poses. Our collection pipeline addresses filtering using a carefully combined set of task-specific and generalist models. For pose estimation, we combine the latest techniques of point tracking, dynamic masking, and structure-from-motion to achieve improvements over the state-of-the-art approaches. Our analysis and experiments demonstrate that DynPose-100K is both large-scale and diverse across several key attributes, opening up avenues for advancements in various downstream applications."

[25.04.2025 22:10] Response: []
[25.04.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents DynPose-100K, a new large-scale dataset designed for annotating camera poses in dynamic Internet videos. The authors highlight the challenges of using typical Internet videos for pose estimation and propose a novel collection pipeline that utilizes a mix of specialized and general models for effective filtering. They employ advanced techniques such as point tracking, dynamic masking, and structure-from-motion to enhance pose estimation accuracy. The dataset\'s diversity and scale are shown to provide significant opportunities for improving various applications in video generation and simulation.","title":"Unlocking Dynamic Video Analysis with DynPose-100K"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents DynPose-100K, a new large-scale dataset designed for annotating camera poses in dynamic Internet videos. The authors highlight the challenges of using typical Internet videos for pose estimation and propose a novel collection pipeline that utilizes a mix of specialized and general models for effective filtering. They employ advanced techniques such as point tracking, dynamic masking, and structure-from-motion to enhance pose estimation accuracy. The dataset's diversity and scale are shown to provide significant opportunities for improving various applications in video generation and simulation.", title='Unlocking Dynamic Video Analysis with DynPose-100K'))
[25.04.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文介绍了DynPose-100K，这是一个大规模的动态互联网视频数据集，专门标注了相机姿态。收集这样的数据集非常困难，因为大多数互联网视频不适合进行姿态估计。我们提出了一种数据收集流程，结合了特定任务和通用模型进行过滤，以确保数据的质量。通过结合最新的点跟踪、动态遮罩和运动重建技术，我们在姿态估计上取得了显著的进展，DynPose-100K为多个下游应用的研究提供了新的机会。","title":"动态视频姿态标注的新突破"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文介绍了DynPose-100K，这是一个大规模的动态互联网视频数据集，专门标注了相机姿态。收集这样的数据集非常困难，因为大多数互联网视频不适合进行姿态估计。我们提出了一种数据收集流程，结合了特定任务和通用模型进行过滤，以确保数据的质量。通过结合最新的点跟踪、动态遮罩和运动重建技术，我们在姿态估计上取得了显著的进展，DynPose-100K为多个下游应用的研究提供了新的机会。', title='动态视频姿态标注的新突破'))
[25.04.2025 22:10] Using data from previous issue: {"categories": ["#3d", "#optimization", "#dataset"], "emoji": "🧊", "ru": {"title": "Разделяй и властвуй: новый подход к 3D-реконструкции", "desc": "Статья представляет DiMeR - новую модель для реконструкции трехмерных объектов по нескольким изображениям. Основная идея заключается в разделении входны
[25.04.2025 22:10] Loading Chinese text from previous data.
[25.04.2025 22:10] Renaming data file.
[25.04.2025 22:10] Renaming previous data. hf_papers.json to ./d/2025-04-25.json
[25.04.2025 22:10] Saving new data file.
[25.04.2025 22:10] Generating page.
[25.04.2025 22:10] Renaming previous page.
[25.04.2025 22:10] Renaming previous data. index.html to ./d/2025-04-25.html
[25.04.2025 22:10] [Experimental] Generating Chinese page for reading.
[25.04.2025 22:10] Chinese vocab [{'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '编辑', 'pinyin': 'biān jí', 'trans': 'edit'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'remarkable'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}, {'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '强大', 'pinyin': 'qiáng dà', 'trans': 'powerful'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'capability'}, {'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'}, {'word': '算法', 'pinyin': 'suàn fǎ', 'trans': 'algorithm'}, {'word': '闭源', 'pinyin': 'bì yuán', 'trans': 'closed-source'}, {'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'}, {'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}, {'word': '名为', 'pinyin': 'míng wéi', 'trans': 'named'}, {'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}, {'word': '相媲美', 'pinyin': 'xiāng pì měi', 'trans': 'comparable'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}]
[25.04.2025 22:10] Renaming previous Chinese page.
[25.04.2025 22:10] Renaming previous data. zh.html to ./d/2025-04-24_zh_reading_task.html
[25.04.2025 22:10] Writing Chinese reading task.
[25.04.2025 22:10] Writing result.
[25.04.2025 22:10] Renaming log file.
[25.04.2025 22:10] Renaming previous data. log.txt to ./logs/2025-04-25_last_log.txt
