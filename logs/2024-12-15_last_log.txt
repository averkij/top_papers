[15.12.2024 06:31] Read previous papers.
[15.12.2024 06:31] Generating top page (month).
[15.12.2024 06:31] Writing top page (month).
[15.12.2024 12:42] Read previous papers.
[15.12.2024 12:42] Get feed.
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09596
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08905
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08737
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08635
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09618
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09605
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09619
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09501
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09569
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09593
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05994
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08687
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09585
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09405
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08972
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09622
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09573
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09013
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09370
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06745
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09349
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09586
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06329
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09460
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09025
[15.12.2024 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05552
[15.12.2024 12:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.12.2024 12:42] No deleted papers detected.
[15.12.2024 12:42] Downloading and parsing papers (pdf, html). Total: 26.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09596.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09596.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09596.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.08905.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.08905.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.08905.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.08737.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.08737.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.08737.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.08635.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.08635.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.08635.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09618.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09618.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09618.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09605.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09605.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09605.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09619.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09619.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09619.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09501.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09501.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09501.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09569.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09569.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09569.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09593.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09593.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09593.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.05994.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.05994.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.05994.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.08687.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.08687.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.08687.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09585.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09585.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09585.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09405.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09405.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09405.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.08972.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.08972.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.08972.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09622.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09622.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09622.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09573.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09573.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09573.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09013.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09013.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09013.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09370.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09370.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09370.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.06745.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.06745.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.06745.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09349.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09349.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09349.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09586.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09586.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09586.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.06329.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.06329.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.06329.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09460.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09460.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09460.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.09025.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.09025.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.09025.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Downloading and parsing paper https://huggingface.co/papers/2412.05552.
[15.12.2024 12:42] Extra JSON file exists (./assets/json/2412.05552.json), skip PDF parsing.
[15.12.2024 12:42] Paper image links file exists (./assets/img_data/2412.05552.json), skip HTML parsing.
[15.12.2024 12:42] Success.
[15.12.2024 12:42] Enriching papers with extra data.
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 0. Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuou...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 1. We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data ...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 2. Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robot...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 3. Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 4. Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among image...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 5. Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective t...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 6. Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and h...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 7. As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra,...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 8. Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this ...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 9. Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifica...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 10. The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accu...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 11. With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 12. The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 13. Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing high...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 14. This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses ...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 15. Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entangl...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 16. Existing sparse-view reconstruction models heavily rely on accurate known camera poses. However, deriving camera extrinsics and intrinsics from sparse-view images presents significant challenges. In this work, we present FreeSplatter, a highly scalable, feed-forward reconstruction framework capable ...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 17. This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of t...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 18. Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the ti...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 19. Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users ...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 20. Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignme...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 21. We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a person's gaze target requires reasoning both about the person's appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 22. Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than pr...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 23. The use of copyrighted materials in training generative language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of copyrighted materials on the performance of large language models (LLMs) for Norwegian. We f...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 24. Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even ...
[15.12.2024 12:42] ********************************************************************************
[15.12.2024 12:42] Abstract 25. The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the ...
[15.12.2024 12:42] Read previous papers.
[15.12.2024 12:42] Generating reviews via LLM API.
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#long_context", "#audio", "#cv", "#multimodal", "#reasoning"], "emoji": "🧠", "ru": {"title": "Непрерывное восприятие и рассуждение: новый шаг к человекоподобному ИИ", "desc": "Данная статья представляет новый подход к созданию систем искусственного интеллекта, способных к длительном
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#data", "#reasoning", "#synthetic", "#training", "#benchmark", "#architecture"], "emoji": "🧠", "ru": {"title": "Качество данных - ключ к превосходству языковой модели", "desc": "Статья представляет phi-4 - языковую модель с 14 миллиардами параметров, разработанную с акцентом на каче
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#low_resource", "#synthetic", "#multimodal", "#optimization", "#training", "#data"], "emoji": "📐", "ru": {"title": "Прорыв в геометрическом восприятии для мультимодальных ИИ-моделей", "desc": "Статья представляет новый бенчмарк Geoperception для оценки
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#audio", "#video", "#cv", "#multimodal", "#training"], "emoji": "🧠", "ru": {"title": "LatentLM: Объединяя дискретное и непрерывное в мультимодальных моделях", "desc": "Статья представляет Latent Language Modeling (LatentLM) - унифицированный подход к о
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal", "#diffusion", "#optimization", "#training"], "emoji": "🖼️", "ru": {"title": "EasyRef: умная адаптация диффузионных моделей к нескольким референсам", "desc": "Статья представляет EasyRef - новый метод адаптации для диффузионных моделей, позволя
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#data", "#dataset", "#training", "#optimization", "#agents", "#synthetic"], "emoji": "🤖", "ru": {"title": "AgentTrek: Революция в обучении GUI-агентов с помощью веб-руководств", "desc": "Статья представляет AgentTrek - масштабируемый конвейер для синтеза данных, который генерирует в
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#training", "#optimization", "#diffusion", "#benchmark"], "emoji": "📱", "ru": {"title": "Компактная и быстрая генерация изображений на мобильных устройствах", "desc": "Статья представляет новую модель генерации изображений по тексту, SnapGen, разраб
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#long_context", "#multimodal", "#benchmark"], "emoji": "🎭", "ru": {"title": "Lyra: Эффективная мультимодальная модель для комплексного понимания речи и изображений", "desc": "Lyra - это эффективная мультимодальная большая языковая модель (MLLM), которая у
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#benchmark", "#rlhf"], "emoji": "⚖️", "ru": {"title": "LLM как беспристрастные судьи ИИ-систем", "desc": "Это исследование посвящено оценке качества языковых моделей (LLM) в роли судей для сравнения и выбора между различными генеративными ИИ-систем
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#open_source", "#cv", "#3d", "#dataset"], "emoji": "💡", "ru": {"title": "Улучшение оценки 3D-геометрии с помощью синтетического освещения", "desc": "Статья представляет Neural LightRig - новый подход к оценке внутренних свойств объектов на изображении. Ме
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#optimization", "#math", "#architecture"], "emoji": "📊", "ru": {"title": "Адаптивные гауссовы функции для точного решения сложных дифференциальных уравнений", "desc": "В статье представлен новый подход к аппроксимации дифференциальных уравнений в частных производных (ДУЧП) с использ
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#games", "#training", "#reasoning", "#interpretability", "#dataset"], "emoji": "🖼️", "ru": {"title": "VisionArena: реальные диалоги для оценки зрительно-языковых моделей", "desc": "Статья представляет VisionArena - набор данных из 230 тыс
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#open_source", "#cv", "#multimodal"], "emoji": "👁️", "ru": {"title": "Оптимизация визуального понимания языковых моделей через призму зрения", "desc": "Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM)
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#architecture", "#inference", "#synthetic", "#dataset", "#optimization", "#multimodal"], "emoji": "🗜️", "ru": {"title": "WaLLoC: эффективное сжатие для машинного обучения на данных высокого разрешения", "desc": "Статья представляет WaLLoC - новую архитектуру нейронного кодека для сж
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#long_context", "#math", "#benchmark", "#reasoning"], "emoji": "🧠", "ru": {"title": "Большие языковые модели спотыкаются на сложных правилах реального мира", "desc": "RuleArena - это новый сложный бенчмарк для оценки способности больших языковых моделей следовать сложным правилам в 
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv"], "emoji": "🎨", "ru": {"title": "LoRACLR: Гармоничное слияние концепций для персонализированной генерации изображений", "desc": "LoRACLR - это новый подход к генерации изображений с множественными концепциями, объединяющий несколько моделей LoRA без 
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "🔍", "ru": {"title": "Революция в 3D-реконструкции: от неоткалиброванных изображений к точным моделям", "desc": "FreeSplatter - это инновационная модель реконструкции 3D-объектов, работающая с неоткалиброванными изображениями с разных ракурсов. Она и
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#architecture", "#diffusion"], "emoji": "🔍", "ru": {"title": "Улучшение сверхразрешения изображений с помощью инверсии диффузии", "desc": "Это исследование представляет новую технику сверхразрешения изображений, основанную на инверсии диффузии. Мет
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#multilingual", "#reasoning", "#dataset", "#architecture"], "emoji": "🔗", "ru": {"title": "От WSD к WSL: Новый подход к пониманию смысла слов в тексте", "desc": "Статья представляет новую задачу под названием Word Sense Linking (WSL), которая расширяет традиционную задачу Word Sense
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#survey", "#optimization", "#benchmark", "#dataset"], "emoji": "🧪", "ru": {"title": "Единый открытый бенчмарк для комплексной оценки фундаментальных моделей", "desc": "ONEBench - это новая парадигма тестирования, объединяющая отдельные наборы данных для оценки в единый расширяемый п
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video"], "emoji": "🎭", "ru": {"title": "DisPose: Улучшенный контроль анимации человека без дополнительных входных данных", "desc": "Статья представляет DisPose - новый метод для контролируемой анимации изображений человека. Авторы предлагают разделить скелетну
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#cv", "#reasoning"], "emoji": "👀", "ru": {"title": "Простой и эффективный метод оценки направления взгляда с помощью трансформеров", "desc": "Статья представляет Gaze-LLE - новый метод оценки направления взгляда человека на основе тран
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#optimization", "#training", "#open_source"], "emoji": "🌊", "ru": {"title": "TarFlow: Возрождение нормализующих потоков для высококачественной генерации изображений", "desc": "TarFlow - это новая архитектура нормализующих потоков, основанная на трансфо
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#dataset", "#benchmark", "#ethics"], "emoji": "📚", "ru": {"title": "Авторское право в эпоху искусственного интеллекта: оценка влияния на языковые модели", "desc": "Статья исследует влияние защищенных авторским правом материалов на эффективность боль
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#low_resource", "#machine_translation", "#dataset", "#open_source", "#training"], "emoji": "🌏", "ru": {"title": "Новый корпус для улучшения машинного перевода индийских языков", "desc": "Эта статья представляет новый многоязычный параллельный корпус дл
[15.12.2024 12:42] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#optimization"], "emoji": "🧭", "ru": {"title": "Универсальный агент для визуальной навигации с языковыми инструкциями", "desc": "Это исследование предлагает универсальную модель для различных задач визуальной навигации с языковыми инструкциями
[15.12.2024 12:42] Trying to get texts in Chinese.
[15.12.2024 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.
[15.12.2024 12:42] Mistral response. {"id": "dac40f01a3344468b4ffe3451bb989a5", "object": "chat.completion", "created": 1734266525, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u521b\u5efa\u80fd\u591f\u957f\u65f6\u95f4\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\u76ee\u6807\u3002\u8fd1\u671f\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f00\u653e\u4e16\u754c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u8fde\u7eed\u548c\u540c\u65f6\u8fdb\u884c\u7684\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u63a2\u7d22\u7684\u6311\u6218\u3002\u5f53\u524d\u7684MLLMs\u53d7\u9650\u4e8e\u5176\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u67b6\u6784\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8f93\u5165\u548c\u751f\u6210\u54cd\u5e94\u3002\u56e0\u6b64\uff0c\u8fd9\u4e2a\u9879\u76ee\u5f15\u5165\u4e86\u5206\u79bb\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u89c6\u9891\u548c\u97f3\u9891\u8f93\u5165\u3002\u63d0\u51fa\u7684\u6846\u67b6IXC2.5-OL\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u5b9e\u65f6\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u7684\u611f\u77e5\u6a21\u5757\uff0c\u6574\u5408\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7684\u591a\u6a21\u6001\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u4ee5\u53ca\u54cd\u5e94\u67e5\u8be2\u548c\u6267\u884c\u63a8\u7406\u4efb\u52a1\u7684\u63a8\u7406\u6a21\u5757\u3002\u8fd9\u4e2a\u9879\u76ee\u6a21\u62df\u4e86\u4eba\u7c7b\u8ba4\u77e5\uff0c\u4f7f\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u6301\u7eed\u548c\u9002\u5e94\u6027\u7684\u670d\u52a1\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 400, "total_tokens": 697, "completion_tokens": 297}}
[15.12.2024 12:42] Response: 这篇文章讨论了创建能够长时间与环境互动的人工智能系统的研究目标。近期，多模态大型语言模型（MLLMs）在开放世界理解方面取得了显著进展。然而，连续和同时进行的感知、记忆和推理仍然是一个未探索的挑战。当前的MLLMs受限于其序列到序列的架构，无法同时处理输入和生成响应。因此，这个项目引入了分离的感知、推理和记忆机制，使得模型能够实时处理视频和音频输入。提出的框架IXC2.5-OL包括三个关键模块：实时处理多模态信息的感知模块，整合短期和长期记忆的多模态长期记忆模块，以及响应查询和执行推理任务的推理模块。这个项目模拟了人类认知，使多模态大型语言模型能够提供持续和适应性的服务。
[15.12.2024 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章讨论了创建能够长时间与环境互动的人工智能系统的研究目标。近期，多模态大型语言模型（MLLMs）在开放世界理解方面取得了显著进展。然而，连续和同时进行的感知、记忆和推理仍然是一个未探索的挑战。当前的MLLMs受限于其序列到序列的架构，无法同时处理输入和生成响应。因此，这个项目引入了分离的感知、推理和记忆机制，使得模型能够实时处理视频和音频输入。提出的框架IXC2.5-OL包括三个关键模块：实时处理多模态信息的感知模块，整合短期和长期记忆的多模态长期记忆模块，以及响应查询和执行推理任务的推理模块。这个项目模拟了人类认知，使多模态大型语言模型能够提供持续和适应性的服务。
[15.12.2024 12:42] Mistral response. {"id": "dadc1df25a72431897b553599466b10d", "object": "chat.completion", "created": 1734266531, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Zh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9nle chu\u00e0ngji\u00e0n n\u00e9ngg\u00f2u ch\u00e1ng sh\u00edji\u0101n y\u01d4 hu\u00e1nj\u00ecng h\u00f9d\u00f2ng de r\u00e9ng\u014dng zh\u00ecn\u00e9ng x\u00ect\u01d2ng de y\u00e1nji\u016b m\u00f9bi\u0101o. J\u00ecnq\u012b, du\u014d m\u00f3shu\u01cei d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng (MLLMs) z\u00e0i k\u0101if\u00e0ng sh\u00ecji\u00e8 l\u01d0ji\u011b f\u0101ngmi\u00e0n qu\u00e8d\u00e9le xi\u01cenzh\u00f9 j\u00ecnb\u00f9. R\u00e1n'\u00e9r, li\u00e1nx\u00f9 h\u00e9 t\u00f3ngsh\u00ed j\u00ecnx\u00edng de g\u01cenju\u00e9, j\u00ecy\u00ec h\u00e9 tu\u00edli sh\u00ec y\u012bg\u00e8 w\u00e8i t\u00e0nsu\u01d2 de ti\u00e1ozh\u00e0n. D\u0101ngqi\u00e1n de MLLMs sh\u00f2uxi\u00e0n y\u00fa q\u00ed x\u00f9li\u00e8 d\u00e0o x\u00f9li\u00e8 de ji\u00e0g\u00f2u, w\u00faf\u01ce t\u00f3ngsh\u00ed ch\u01d4l\u01d0 sh\u016br\u00f9 h\u00e9 sh\u0113ngch\u00e9ng xi\u01cengy\u00ecng. Y\u012bnc\u01d0, zh\u00e8g\u00e8 xi\u00e0ngm\u00f9 y\u01d0nr\u00f9le f\u0113nli\u00e8 de g\u01cenju\u00e9, tu\u00edli h\u00e9 j\u00ecy\u00ec j\u012bzh\u00ec, sh\u01d0d\u00e9 m\u00f3x\u00edng n\u00e9ngg\u00f2u sh\u00edsh\u00ed ch\u01d4l\u01d0 sh\u00ecp\u01d0n h\u00e9 y\u012bnp\u01d0n sh\u016br\u00f9. T\u00edch\u016b de ku\u00e0ngji\u00e0 IXC2.5-OL b\u0101oku\u00f2 s\u0101n g\u00e8 gu\u01cenji\u00e0n m\u00f3d\u00f9: sh\u00edsh\u00ed ch\u01d4l\u01d0 du\u014d m\u00f3shu\u01cei x\u00ecnx\u012b de g\u01cenju\u00e9 m\u00f3d\u00f9, zh\u011bngh\u00e9 du\u01cenq\u012b h\u00e9 ch\u00e1ngq\u012b j\u00ecy\u00ec de du\u014d m\u00f3shu\u01cei ch\u00e1ngq\u012b j\u00ecy\u00ec m\u00f3d\u00f9, y\u01d0ji\u01ce xi\u01cengy\u00ecng ch\u00e1x\u00fan h\u00e9 zh\u00edx\u00edng tu\u00edli r\u00e8nw\u00f9 de tu\u00edli m\u00f3d\u00f9. Zh\u00e8g\u00e8 xi\u00e0ngm\u00f9 m\u00f3n\u01d0le r\u00e9nl\u00e8i r\u00e8ng\u014dng, sh\u01d0 du\u014d m\u00f3shu\u01cei d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng n\u00e9ngg\u00f2u t\u00edg\u014dng ch\u00edx\u00f9 h\u00e9 sh\u00ecy\u00ecngx\u00ecng de f\u00faw\u00f9."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 312, "total_tokens": 904, "completion_tokens": 592}}
[15.12.2024 12:42] Response: Zhè piān wénzhāng tǎolùnle chuàngjiàn nénggòu cháng shíjiān yǔ huánjìng hùdòng de réngōng zhìnéng xìtǒng de yánjiū mùbiāo. Jìnqī, duō móshuǎi dàxíng yǔyán móxíng (MLLMs) zài kāifàng shìjiè lǐjiě fāngmiàn quèdéle xiǎnzhù jìnbù. Rán'ér, liánxù hé tóngshí jìnxíng de gǎnjué, jìyì hé tuíli shì yīgè wèi tànsuǒ de tiáozhàn. Dāngqián de MLLMs shòuxiàn yú qí xùliè dào xùliè de jiàgòu, wúfǎ tóngshí chǔlǐ shūrù hé shēngchéng xiǎngyìng. Yīncǐ, zhègè xiàngmù yǐnrùle fēnliè de gǎnjué, tuíli hé jìyì jīzhì, shǐdé móxíng nénggòu shíshí chǔlǐ shìpǐn hé yīnpǐn shūrù. Tíchū de kuàngjià IXC2.5-OL bāokuò sān gè guǎnjiàn módù: shíshí chǔlǐ duō móshuǎi xìnxī de gǎnjué módù, zhěnghé duǎnqī hé chángqī jìyì de duō móshuǎi chángqī jìyì módù, yǐjiǎ xiǎngyìng cháxún hé zhíxíng tuíli rènwù de tuíli módù. Zhègè xiàngmù mónǐle rénlèi rèngōng, shǐ duō móshuǎi dàxíng yǔyán móxíng nénggòu tígōng chíxù hé shìyìngxìng de fúwù.
[15.12.2024 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章讨论了创建能够长时间与环境互动的人工智能系统的研究目标。近期，多模态大型语言模型（MLLMs）在开放世界理解方面取得了显著进展。然而，连续和同时进行的感知、记忆和推理仍然是一个未探索的挑战。当前的MLLMs受限于其序列到序列的架构，无法同时处理输入和生成响应。因此，这个项目引入了分离的感知、推理和记忆机制，使得模型能够实时处理视频和音频输入。提出的框架IXC2.5-OL包括三个关键模块：实时处理多模态信息的感知模块，整合短期和长期记忆的多模态长期记忆模块，以及响应查询和执行推理任务的推理模块。这个项目模拟了人类认知，使多模态大型语言模型能够提供持续和适应性的服务。
[15.12.2024 12:43] Mistral response. {"id": "21aa0a82e4bb4f5e9814f1afe8eca7c8", "object": "chat.completion", "created": 1734266545, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u521b\u5efa\", \"pinyin\": \"chu\u00e0ng ji\u00e0n\", \"trans\": \"create\"},\n    {\"word\": \"\u80fd\u591f\", \"pinyin\": \"n\u00e9ng g\u00f2u\", \"trans\": \"be able to\"},\n    {\"word\": \"\u957f\u65f6\u95f4\", \"pinyin\": \"ch\u00e1ng sh\u00ed ji\u0101n\", \"trans\": \"long period of time\"},\n    {\"word\": \"\u4e92\u52a8\", \"pinyin\": \"h\u00f9 d\u00f2ng\", \"trans\": \"interact\"},\n    {\"word\": \"\u4eba\u5de5\u667a\u80fd\", \"pinyin\": \"r\u00e9n g\u014dng zh\u00ec n\u00e9ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"\u7cfb\u7edf\", \"pinyin\": \"x\u00ec t\u01d2ng\", \"trans\": \"system\"},\n    {\"word\": \"\u7814\u7a76\", \"pinyin\": \"y\u00e1n ji\u016b\", \"trans\": \"research\"},\n    {\"word\": \"\u76ee\u6807\", \"pinyin\": \"m\u00f9 bi\u0101o\", \"trans\": \"goal\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 shu\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u5927\u578b\", \"pinyin\": \"d\u00e0 x\u00edng\", \"trans\": \"large-scale\"},\n    {\"word\": \"\u8bed\u8a00\", \"pinyin\": \"y\u01d4 y\u00e1n\", \"trans\": \"language\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u5f00\u653e\", \"pinyin\": \"k\u0101i f\u00e0ng\", \"trans\": \"open\"},\n    {\"word\": \"\u4e16\u754c\", \"pinyin\": \"sh\u00ec ji\u00e8\", \"trans\": \"world\"},\n    {\"word\": \"\u7406\u89e3\", \"pinyin\": \"l\u01d0 ji\u011b\", \"trans\": \"understand\"},\n    {\"word\": \"\u65b9\u9762\", \"pinyin\": \"f\u0101ng mi\u00e0n\", \"trans\": \"aspect\"},\n    {\"word\": \"\u53d6\u5f97\", \"pinyin\": \"q\u01d4 d\u00e9\", \"trans\": \"achieve\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u8fdb\u5c55\", \"pinyin\": \"j\u00ecn zh\u01cen\", \"trans\": \"progress\"},\n    {\"word\": \"\u7136\u800c\", \"pinyin\": \"r\u00e1n \u00e9r\", \"trans\": \"however\"},\n    {\"word\": \"\u8fde\u7eed\", \"pinyin\": \"li\u00e1n x\u00f9\", \"trans\": \"continuous\"},\n    {\"word\": \"\u540c\u65f6\", \"pinyin\": \"t\u00f3ng sh\u00ed\", \"trans\": \"simultaneous\"},\n    {\"word\": \"\u611f\u77e5\", \"pinyin\": \"g\u01cen zh\u012b\", \"trans\": \"perception\"},\n    {\"word\": \"\u8bb0\u5fc6\", \"pinyin\": \"j\u00ec y\u00ec\", \"trans\": \"memory\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012b l\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u6311\u6218\", \"pinyin\": \"ti\u01ceo zh\u00e0n\", \"trans\": \"challenge\"},\n    {\"word\": \"\u672a\", \"pinyin\": \"w\u00e8i\", \"trans\": \"not yet\"},\n    {\"word\": \"\u63a2\u7d22\", \"pinyin\": \"t\u00e0n su\u01d2\", \"trans\": \"explore\"},\n    {\"word\": \"\u53d7\u9650\u4e8e\", \"pinyin\": \"sh\u00f2u xi\u00e0n y\u00fa\", \"trans\": \"be limited to\"},\n    {\"word\": \"\u5176\", \"pinyin\": \"q\u00ed\", \"trans\": \"its\"},\n    {\"word\": \"\u5e8f\u5217\", \"pinyin\": \"x\u00f9 li\u00e8\", \"trans\": \"sequence\"},\n    {\"word\": \"\u67b6\u6784\", \"pinyin\": \"ji\u00e0 g\u00f2u\", \"trans\": \"architecture\"},\n    {\"word\": \"\u65e0\u6cd5\", \"pinyin\": \"w\u00fa f\u01ce\", \"trans\": \"unable to\"},\n    {\"word\": \"\u5904\u7406\", \"pinyin\": \"ch\u01d4 l\u01d0\", \"trans\": \"process\"},\n    {\"word\": \"\u8f93\u5165\", \"pinyin\": \"sh\u016b r\u00f9\", \"trans\": \"input\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ng ch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u54cd\u5e94\", \"pinyin\": \"xi\u01ceng y\u00ecng\", \"trans\": \"response\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u5206\u79bb\", \"pinyin\": \"f\u0113n l\u00ed\", \"trans\": \"separate\"},\n    {\"word\": \"\u673a\u5236\", \"pinyin\": \"j\u012b zh\u00ec\", \"trans\": \"mechanism\"},\n    {\"word\": \"\u5b9e\u65f6\", \"pinyin\": \"sh\u00ed sh\u00ed\", \"trans\": \"real-time\"},\n    {\"word\": \"\u89c6\u9891\", \"pinyin\": \"sh\u00ec p\u00edn\", \"trans\": \"video\"},\n    {\"word\": \"\u97f3\u9891\", \"pinyin\": \"y\u012bn p\u00edn\", \"trans\": \"audio\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u6846\u67b6\", \"pinyin\": \"ku\u00e0ng ji\u00e0\", \"trans\": \"framework\"},\n    {\"word\": \"IXC2.5-OL\", \"pinyin\": \"IXC2.5-OL\", \"trans\": \"IXC2.5-OL\"},\n    {\"word\": \"\u5305\u62ec\", \"pinyin\": \"b\u0101o ku\u00f2\", \"trans\": \"include\"},\n    {\"word\": \"\u5173\u952e\", \"pinyin\": \"gu\u01cen ji\u00e0n\", \"trans\": \"key\"},\n    {\"word\": \"\u6a21\u5757\", \"pinyin\": \"m\u00f3 ku\u00e0i\", \"trans\": \"module\"},\n    {\"word\": \"\u6574\u5408\", \"pinyin\": \"zh\u011bng h\u00e9\", \"trans\": \"integrate\"},\n    {\"word\": \"\u77ed\u671f\", \"pinyin\": \"du\u01cen q\u012b\", \"trans\": \"short-term\"},\n    {\"word\": \"\u957f\u671f\", \"pinyin\": \"ch\u00e1ng q\u012b\", \"trans\": \"long-term\"},\n    {\"word\": \"\u591a\u6a21\u6001\u957f\u671f\u8bb0\u5fc6\", \"pinyin\": \"du\u014d m\u00f3 shu\u00e0i ch\u00e1ng q\u012b j\u00ec y\u00ec\", \"trans\": \"multimodal long-term memory\"},\n    {\"word\": \"\u54cd\u5e94\u67e5\u8be2\", \"pinyin\": \"xi\u01ceng y\u00ecng ch\u00e1 x\u00fan\", \"trans\": \"respond to queries\"},\n    {\"word\": \"\u6267\u884c\", \"pinyin\": \"zh\u00ed x\u00edng\", \"trans\": \"execute\"},\n    {\"word\": \"\u4efb\u52a1\", \"pinyin\": \"r\u00e8n w\u00f9\", \"trans\": \"task\"},\n    {\"word\": \"\u9879\u76ee\", \"pinyin\": \"xi\u00e0ng m\u00f9\", \"trans\": \"project\"},\n    {\"word\": \"\u6a21\u62df\", \"pinyin\": \"m\u00f3 n\u01d0\", \"trans\": \"simulate\"},\n    {\"word\": \"\u4eba\u7c7b\", \"pinyin\": \"r\u00e9n l\u00e8i\", \"trans\": \"human\"},\n    {\"word\": \"\u8ba4\u77e5\", \"pinyin\": \"r\u00e8n zh\u012b\", \"trans\": \"cognition\"},\n    {\"word\": \"\u4f7f\", \"pinyin\": \"sh\u01d0\", \"trans\": \"make\"},\n    {\"word\": \"\u63d0\u4f9b\", \"pinyin\": \"t\u00ed g\u014dng\", \"trans\": \"provide\"},\n    {\"word\": \"\u6301\u7eed\", \"pinyin\": \"ch\u00ed x\u00f9\", \"trans\": \"continuous\"},\n    {\"word\": \"\u9002\u5e94\u6027\", \"pinyin\": \"sh\u00ec y\u00ecng x\u00ecng\", \"trans\": \"adaptability\"},\n    {\"word\": \"\u670d\u52a1\", \"pinyin\": \"f\u00fa w\u00f9\", \"trans\": \"service\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 344, "total_tokens": 2248, "completion_tokens": 1904}}
[15.12.2024 12:43] Response: [
    {"word": "讨论", "pinyin": "tǎo lùn", "trans": "discuss"},
    {"word": "创建", "pinyin": "chuàng jiàn", "trans": "create"},
    {"word": "能够", "pinyin": "néng gòu", "trans": "be able to"},
    {"word": "长时间", "pinyin": "cháng shí jiān", "trans": "long period of time"},
    {"word": "互动", "pinyin": "hù dòng", "trans": "interact"},
    {"word": "人工智能", "pinyin": "rén gōng zhì néng", "trans": "artificial intelligence"},
    {"word": "系统", "pinyin": "xì tǒng", "trans": "system"},
    {"word": "研究", "pinyin": "yán jiū", "trans": "research"},
    {"word": "目标", "pinyin": "mù biāo", "trans": "goal"},
    {"word": "多模态", "pinyin": "duō mó shuài", "trans": "multimodal"},
    {"word": "大型", "pinyin": "dà xíng", "trans": "large-scale"},
    {"word": "语言", "pinyin": "yǔ yán", "trans": "language"},
    {"word": "模型", "pinyin": "mó xíng", "trans": "model"},
    {"word": "开放", "pinyin": "kāi fàng", "trans": "open"},
    {"word": "世界", "pinyin": "shì jiè", "trans": "world"},
    {"word": "理解", "pinyin": "lǐ jiě", "trans": "understand"},
    {"word": "方面", "pinyin": "fāng miàn", "trans": "aspect"},
    {"word": "取得", "pinyin": "qǔ dé", "trans": "achieve"},
    {"word": "显著", "pinyin": "xiǎn zhù", "trans": "significant"},
    {"word": "进展", "pinyin": "jìn zhǎn", "trans": "progress"},
    {"word": "然而", "pinyin": "rán ér", "trans": "however"},
    {"word": "连续", "pinyin": "lián xù", "trans": "continuous"},
    {"word": "同时", "pinyin": "tóng shí", "trans": "simultaneous"},
    {"word": "感知", "pinyin": "gǎn zhī", "trans": "perception"},
    {"word": "记忆", "pinyin": "jì yì", "trans": "memory"},
    {"word": "推理", "pinyin": "tuī lǐ", "trans": "reasoning"},
    {"word": "挑战", "pinyin": "tiǎo zhàn", "trans": "challenge"},
    {"word": "未", "pinyin": "wèi", "trans": "not yet"},
    {"word": "探索", "pinyin": "tàn suǒ", "trans": "explore"},
    {"word": "受限于", "pinyin": "shòu xiàn yú", "trans": "be limited to"},
    {"word": "其", "pinyin": "qí", "trans": "its"},
    {"word": "序列", "pinyin": "xù liè", "trans": "sequence"},
    {"word": "架构", "pinyin": "jià gòu", "trans": "architecture"},
    {"word": "无法", "pinyin": "wú fǎ", "trans": "unable to"},
    {"word": "处理", "pinyin": "chǔ lǐ", "trans": "process"},
    {"word": "输入", "pinyin": "shū rù", "trans": "input"},
    {"word": "生成", "pinyin": "shēng chéng", "trans": "generate"},
    {"word": "响应", "pinyin": "xiǎng yìng", "trans": "response"},
    {"word": "引入", "pinyin": "yǐn rù", "trans": "introduce"},
    {"word": "分离", "pinyin": "fēn lí", "trans": "separate"},
    {"word": "机制", "pinyin": "jī zhì", "trans": "mechanism"},
    {"word": "实时", "pinyin": "shí shí", "trans": "real-time"},
    {"word": "视频", "pinyin": "shì pín", "trans": "video"},
    {"word": "音频", "pinyin": "yīn pín", "trans": "audio"},
    {"word": "提出", "pinyin": "tí chū", "trans": "propose"},
    {"word": "框架", "pinyin": "kuàng jià", "trans": "framework"},
    {"word": "IXC2.5-OL", "pinyin": "IXC2.5-OL", "trans": "IXC2.5-OL"},
    {"word": "包括", "pinyin": "bāo kuò", "trans": "include"},
    {"word": "关键", "pinyin": "guǎn jiàn", "trans": "key"},
    {"word": "模块", "pinyin": "mó kuài", "trans": "module"},
    {"word": "整合", "pinyin": "zhěng hé", "trans": "integrate"},
    {"word": "短期", "pinyin": "duǎn qī", "trans": "short-term"},
    {"word": "长期", "pinyin": "cháng qī", "trans": "long-term"},
    {"word": "多模态长期记忆", "pinyin": "duō mó shuài cháng qī jì yì", "trans": "multimodal long-term memory"},
    {"word": "响应查询", "pinyin": "xiǎng yìng chá xún", "trans": "respond to queries"},
    {"word": "执行", "pinyin": "zhí xíng", "trans": "execute"},
    {"word": "任务", "pinyin": "rèn wù", "trans": "task"},
    {"word": "项目", "pinyin": "xiàng mù", "trans": "project"},
    {"word": "模拟", "pinyin": "mó nǐ", "trans": "simulate"},
    {"word": "人类", "pinyin": "rén lèi", "trans": "human"},
    {"word": "认知", "pinyin": "rèn zhī", "trans": "cognition"},
    {"word": "使", "pinyin": "shǐ", "trans": "make"},
    {"word": "提供", "pinyin": "tí gōng", "trans": "provide"},
    {"word": "持续", "pinyin": "chí xù", "trans": "continuous"},
    {"word": "适应性", "pinyin": "shì yìng xìng", "trans": "adaptability"},
    {"word": "服务", "pinyin": "fú wù", "trans": "service"}
]
[15.12.2024 12:43] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章讨论了创建能够长时间与环境互动的人工智能系统的研究目标。近期，多模态大型语言模型（MLLMs）在开放世界理解方面取得了显著进展。然而，连续和同时进行的感知、记忆和推理仍然是一个未探索的挑战。当前的MLLMs受限于其序列到序列的架构，无法同时处理输入和生成响应。因此，这个项目引入了分离的感知、推理和记忆机制，使得模型能够实时处理视频和音频输入。提出的框架IXC2.5-OL包括三个关键模块：实时处理多模态信息的感知模块，整合短期和长期记忆的多模态长期记忆模块，以及响应查询和执行推理任务的推理模块。这个项目模拟了人类认知，使多模态大型语言模型能够提供持续和适应性的服务。
[15.12.2024 12:43] Mistral response. {"id": "c44d6467572c41fca5d26d778eac0c4e", "object": "chat.completion", "created": 1734266583, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the research goal of creating artificial intelligence systems capable of long-term interaction with their environment. Recently, multimodal large language models (MLLMs) have made significant progress in understanding the open world. However, continuous and simultaneous perception, memory, and reasoning remain unexplored challenges. Current MLLMs are limited by their sequence-to-sequence architecture, which prevents them from processing inputs and generating responses simultaneously. Therefore, this project introduces separate mechanisms for perception, reasoning, and memory, allowing the model to process video and audio inputs in real-time. The proposed framework, IXC2.5-OL, includes three key modules: a perception module for real-time processing of multimodal information, a multimodal long-term memory module that integrates short-term and long-term memory, and a reasoning module for responding to queries and performing reasoning tasks. This project simulates human cognition, enabling multimodal large language models to provide continuous and adaptive services."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 310, "total_tokens": 522, "completion_tokens": 212}}
[15.12.2024 12:43] Response: This article discusses the research goal of creating artificial intelligence systems capable of long-term interaction with their environment. Recently, multimodal large language models (MLLMs) have made significant progress in understanding the open world. However, continuous and simultaneous perception, memory, and reasoning remain unexplored challenges. Current MLLMs are limited by their sequence-to-sequence architecture, which prevents them from processing inputs and generating responses simultaneously. Therefore, this project introduces separate mechanisms for perception, reasoning, and memory, allowing the model to process video and audio inputs in real-time. The proposed framework, IXC2.5-OL, includes three key modules: a perception module for real-time processing of multimodal information, a multimodal long-term memory module that integrates short-term and long-term memory, and a reasoning module for responding to queries and performing reasoning tasks. This project simulates human cognition, enabling multimodal large language models to provide continuous and adaptive services.
[15.12.2024 12:43] Renaming data file.
[15.12.2024 12:43] Renaming previous data. hf_papers.json to ./d/2024-12-13.json
[15.12.2024 12:43] Saving new data file.
[15.12.2024 12:43] Generating page.
[15.12.2024 12:43] Renaming previous page.
[15.12.2024 12:43] Renaming previous data. index.html to ./d/2024-12-13.html
[15.12.2024 12:43] [Experimental] Generating Chinese page for reading.
[15.12.2024 12:43] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '创建', 'pinyin': 'chuàng jiàn', 'trans': 'create'}, {'word': '能够', 'pinyin': 'néng gòu', 'trans': 'be able to'}, {'word': '长时间', 'pinyin': 'cháng shí jiān', 'trans': 'long period of time'}, {'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interact'}, {'word': '人工智能', 'pinyin': 'rén gōng zhì néng', 'trans': 'artificial intelligence'}, {'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'goal'}, {'word': '多模态', 'pinyin': 'duō mó shuài', 'trans': 'multimodal'}, {'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'}, {'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '开放', 'pinyin': 'kāi fàng', 'trans': 'open'}, {'word': '世界', 'pinyin': 'shì jiè', 'trans': 'world'}, {'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understand'}, {'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}, {'word': '然而', 'pinyin': 'rán ér', 'trans': 'however'}, {'word': '连续', 'pinyin': 'lián xù', 'trans': 'continuous'}, {'word': '同时', 'pinyin': 'tóng shí', 'trans': 'simultaneous'}, {'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'perception'}, {'word': '记忆', 'pinyin': 'jì yì', 'trans': 'memory'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '未', 'pinyin': 'wèi', 'trans': 'not yet'}, {'word': '探索', 'pinyin': 'tàn suǒ', 'trans': 'explore'}, {'word': '受限于', 'pinyin': 'shòu xiàn yú', 'trans': 'be limited to'}, {'word': '其', 'pinyin': 'qí', 'trans': 'its'}, {'word': '序列', 'pinyin': 'xù liè', 'trans': 'sequence'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '无法', 'pinyin': 'wú fǎ', 'trans': 'unable to'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'}, {'word': '输入', 'pinyin': 'shū rù', 'trans': 'input'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '响应', 'pinyin': 'xiǎng yìng', 'trans': 'response'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': '分离', 'pinyin': 'fēn lí', 'trans': 'separate'}, {'word': '机制', 'pinyin': 'jī zhì', 'trans': 'mechanism'}, {'word': '实时', 'pinyin': 'shí shí', 'trans': 'real-time'}, {'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'}, {'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': 'IXC2.5-OL', 'pinyin': 'IXC2.5-OL', 'trans': 'IXC2.5-OL'}, {'word': '包括', 'pinyin': 'bāo kuò', 'trans': 'include'}, {'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '整合', 'pinyin': 'zhěng hé', 'trans': 'integrate'}, {'word': '短期', 'pinyin': 'duǎn qī', 'trans': 'short-term'}, {'word': '长期', 'pinyin': 'cháng qī', 'trans': 'long-term'}, {'word': '多模态长期记忆', 'pinyin': 'duō mó shuài cháng qī jì yì', 'trans': 'multimodal long-term memory'}, {'word': '响应查询', 'pinyin': 'xiǎng yìng chá xún', 'trans': 'respond to queries'}, {'word': '执行', 'pinyin': 'zhí xíng', 'trans': 'execute'}, {'word': '任务', 'pinyin': 'rèn wù', 'trans': 'task'}, {'word': '项目', 'pinyin': 'xiàng mù', 'trans': 'project'}, {'word': '模拟', 'pinyin': 'mó nǐ', 'trans': 'simulate'}, {'word': '人类', 'pinyin': 'rén lèi', 'trans': 'human'}, {'word': '认知', 'pinyin': 'rèn zhī', 'trans': 'cognition'}, {'word': '使', 'pinyin': 'shǐ', 'trans': 'make'}, {'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}, {'word': '持续', 'pinyin': 'chí xù', 'trans': 'continuous'}, {'word': '适应性', 'pinyin': 'shì yìng xìng', 'trans': 'adaptability'}, {'word': '服务', 'pinyin': 'fú wù', 'trans': 'service'}]
[15.12.2024 12:43] Renaming previous Chinese page.
[15.12.2024 12:43] Renaming previous data. zh.html to ./d/2024-12-14_zh_reading_task.html
[15.12.2024 12:43] Writing Chinese reading task.
[15.12.2024 12:43] Writing result.
[15.12.2024 12:43] Renaming log file.
[15.12.2024 12:43] Renaming previous data. log.txt to ./logs/2024-12-15_last_log.txt
