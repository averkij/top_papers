[11.12.2024 09:12] Read previous papers.
[11.12.2024 09:12] Generating top page (month).
[11.12.2024 09:12] Writing top page (month).
[11.12.2024 10:11] Read previous papers.
[11.12.2024 10:11] Get feed.
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05210
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07589
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07730
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04653
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07774
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07674
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07776
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03548
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06674
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07724
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06673
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07721
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06845
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07334
[11.12.2024 10:11] Extract page data from URL. URL: https://huggingface.co/papers/2412.07759
[11.12.2024 10:11] Extract page data from URL. URL: https://huggingface.co/papers/2412.04835
[11.12.2024 10:11] Extract page data from URL. URL: https://huggingface.co/papers/2412.05983
[11.12.2024 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07187
[11.12.2024 10:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.12.2024 10:11] No deleted papers detected.
[11.12.2024 10:11] Downloading and parsing papers (pdf, html). Total: 18.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.05210.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.05210.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.05210.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07589.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07589.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07589.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07730.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07730.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07730.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.04653.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.04653.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.04653.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07774.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07774.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07774.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07674.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07674.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07674.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07776.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07776.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07776.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.03548.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.03548.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.03548.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.06674.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.06674.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.06674.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07724.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07724.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07724.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.06673.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.06673.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.06673.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07721.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07721.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07721.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.06845.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.06845.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.06845.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07334.
[11.12.2024 10:11] Extra JSON file exists (./assets/json/2412.07334.json), skip PDF parsing.
[11.12.2024 10:11] Paper image links file exists (./assets/img_data/2412.07334.json), skip HTML parsing.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.07759.
[11.12.2024 10:11] Downloading paper 2412.07759 from http://arxiv.org/pdf/2412.07759v1...
[11.12.2024 10:11] Extracting affiliations from text.
[11.12.2024 10:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"4 2 0 2 0 1 ] . [ 1 9 5 7 7 0 . 2 1 4 2 : r 3DTRAJMASTER: MASTERING 3D TRAJECTORY FOR MULTI-ENTITY MOTION IN VIDEO GENERATION Xiao Fu1 Xian Liu1 Xintao Wang2(cid:66) Sida Peng3 Menghan Xia2 Xiaoyu Shi2 Ziyang Yuan2 Pengfei Wan2 Di Zhang2 Dahua Lin1(cid:66) 1The Chinese University of Hong Kong 2Kuaishou Technology 3Zhejiang University "
[11.12.2024 10:11] Response: ```python
["The Chinese University of Hong Kong", "Kuaishou Technology", "Zhejiang University"]
```
[11.12.2024 10:11] Deleting PDF ./assets/pdf/2412.07759.pdf.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.04835.
[11.12.2024 10:11] Downloading paper 2412.04835 from http://arxiv.org/pdf/2412.04835v1...
[11.12.2024 10:11] Extracting affiliations from text.
[11.12.2024 10:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment . Ran Tian1, Yilin Wu2, Chenfeng Xu1, Masayoshi Tomizuka1, Jitendra Malik1, Andrea Bajcsy2 Abstract Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-users visual representation and then constructs dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment. More details (e.g., videos) are at the project website. Keywords Reinforcement learning from human feedback, visuomotor policy learning, representation learning, alignment Visuomotor"
[11.12.2024 10:11] Response: ```python
[]
```
[11.12.2024 10:11] Extracting affiliations from text.
[11.12.2024 10:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment . Ran Tian1, Yilin Wu2, Chenfeng Xu1, Masayoshi Tomizuka1, Jitendra Malik1, Andrea Bajcsy2 Abstract Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-users visual representation and then constructs dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment. More details (e.g., videos) are at the project website. Keywords Reinforcement learning from human feedback, visuomotor policy learning, representation learning, alignmentVisuomotor robot policieswhich predict actions directly image observationsare being from high-dimensional revolutionized by pre-training on large-scale datasets. For example, robots like manipulators (Brohan et al. 2023; Chi et al. 2024), humanoids (Radosavovic et al. 2023), and autonomous cars (Hu et al. 2023; Tian et al. 2024b) rely on pre-trained vision encoders (Deng et al. 2009) for representing RGB images and for learning multimodal behavior policies from increasingly large observation-action teleoperation datasets (Padalkar et al. 2023). Despite this remarkable progress, these visuomotor policies do not always act in accordance with human enduser preferences. For instance, consider the scenario on the left of Figure 1 where robot manipulator is trained to imitate diverse teleoperators picking up bag of chips. At deployment, the end-user prefers the chips to remain intact. However, the manipulator frequently grasps the bag by squeezing the middlerisking damage to the chipsinstead of holding the packaging by its edges like the user prefers. foundational approach to tackle this misalignment between pre-trained policy and an end-users hard-tospecify preferences is reinforcement learning from human feedback (RLHF) (Christiano et al. 2017). By presenting the end-user with outputs generated by the pre-trained model and collecting their preference rankings, RLHF trains reward model that is then used to fine-tune the base policy, enabling it to produce outputs that better align with the end-users preferences. While RLHF has emerged as the predominant alignment mechanism in non-embodied domains such as large language models (LLMs) (Ouyang et al. 2022) and text-to-image generation models (Lee et al. 2023), it has not shown the same impact for aligning visuomotor robot policies. Fundamentally, this challenge arises because learning high-quality visual reward function requires an impractically large amount of human preference in our hardware experiments, collecting 200 feedback: preference rankings for single task takes approximately 1 day. If we hope to align generalist, pre-trained visuomotor policies, it is essential to adapt the RLHF paradigm to operate with significantly less human feedback. that Our approach is motivated by seminal work in inverse reinforcement learning (Abbeel and Ng 2004; Ziebart the desired reward et al. 2008), which states function we seek to learn via RLHF makes the robots behavior indistinguishable from the humans ideal behavior. Mathematically, the divergence between the feature distribution of the robots policy and that of the end-users optimal demonstrations (Pomerleau 1988; Dadashi et al. 2021; Sun et al. 2019; Swamy et al. 2021). However, when constructing visual reward can be modeled as this 1UC Berkeley 2Carnegie Mellon University This work has been supported in part by the Google Research Scholar Award. 4 2 0 2 ] . [ 1 5 3 8 4 0 . 2 1 4 2 : r 2 Figure 1. Representation-Aligned Preference-based Learning (RAPL), is an observation-only method for learning visual robot rewards from significantly less human preference feedback. (center) Unlike traditional reinforcement learning from human feedback, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-users visual representation. The aligned representation is used to construct an optimal transport-based visual reward for aligning the robots visuomotor policy. (left) Before alignment, the robot frequently picks up bag of chips by squeezing the middle, risking damage to the contents. (right) After alignment with our RAPL reward, the robot adheres to the end-users preference and picks up the bag by its edges. rewards, this feature matching has to occur within the endusers visual representation of the world, which is unknown to the robot priori. This implies that critical aspect of reward learning is identifying which visual features are important to the end-user. Instead of jointly learning visual features and divergence measure through human feedback (Christiano et al. 2017), our key idea is to allocate the limited human preference budget exclusively to fine-tuning pretrained vision encoders, aligning their visual representations with those of the end-user. Once the visual representation is fine-tuned, the reward function can be directly instantiated as dense feature matching using techniques such as optimal transport (Kantorovich and Rubinshtein 1958) within this aligned visual representation space (center, Figure "
[11.12.2024 10:11] Mistral response. {"id": "51032d7ed8aa4456bdae43c6e0b61c41", "object": "chat.completion", "created": 1733911914, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['UC Berkeley', 'Carnegie Mellon University']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1582, "total_tokens": 1601, "completion_tokens": 19}}
[11.12.2024 10:11] Response: ```python
['UC Berkeley', 'Carnegie Mellon University']
```
[11.12.2024 10:11] Deleting PDF ./assets/pdf/2412.04835.pdf.
[11.12.2024 10:11] Success.
[11.12.2024 10:11] Downloading and parsing paper https://huggingface.co/papers/2412.05983.
[11.12.2024 10:11] Downloading paper 2412.05983 from http://arxiv.org/pdf/2412.05983v1...
[11.12.2024 10:12] Extracting affiliations from text.
[11.12.2024 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"4 2 0 2 8 ] . [ 1 3 8 9 5 0 . 2 1 4 2 : r Chimera: Improving Generalist Model with Domain-Specific Experts Tianshuo Peng1,2,, Mingsheng Li3,, Hongbin Zhou1, Renqiu Xia1,4, Renrui Zhang2 Lei Bai1, Song Mao1, Bin Wang1, Conghui He1, Aojun Zhou2, Botian Shi1 Tao Chen3, Bo Zhang1,,(cid:66), Xiangyu Yue2,(cid:66) 1Shanghai Artificial Intelligence Laboratory, 2MMLab, The Chinese University of Hong Kong 3Fudan University, 4Shanghai Jiao Tong University * Equal Contribution, (cid:66) Corresponding Authors, Project Leader "
[11.12.2024 10:12] Response: ```python
[
    "Shanghai Artificial Intelligence Laboratory",
    "MMLab, The Chinese University of Hong Kong",
    "Fudan University",
    "Shanghai Jiao Tong University"
]
```
[11.12.2024 10:12] Deleting PDF ./assets/pdf/2412.05983.pdf.
[11.12.2024 10:12] Success.
[11.12.2024 10:12] Downloading and parsing paper https://huggingface.co/papers/2412.07187.
[11.12.2024 10:12] Extra JSON file exists (./assets/json/2412.07187.json), skip PDF parsing.
[11.12.2024 10:12] Paper image links file exists (./assets/img_data/2412.07187.json), skip HTML parsing.
[11.12.2024 10:12] Success.
[11.12.2024 10:12] Enriching papers with extra data.
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 0. Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLM...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 1. Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limi...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 2. The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectu...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 3. As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarki...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 4. We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 5. Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable at...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 6. We propose DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal c...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 7. Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produc...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 8. This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 9. We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including soc...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 10. In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically r...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 11. This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance co...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 12. Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkabl...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 13. Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis ...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 14. This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expres...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 15. Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning f...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 16. Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets domina...
[11.12.2024 10:12] ********************************************************************************
[11.12.2024 10:12] Abstract 17. Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety ...
[11.12.2024 10:12] Read previous papers.
[11.12.2024 10:12] Generating reviews via LLM API.
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#plp", "#alignment", "#benchmark", "#open_source", "#synthetic", "#training"], "emoji": "🏆", "ru": {"title": "CodeArena: новый стандарт оценки ИИ-помощников программиста", "desc": "В статье представлен новый бенчмарк CodeArena для оценки языковых моделей кода с учетом предпочтений п
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#story_generation", "#dataset", "#cv"], "emoji": "🖼️", "ru": {"title": "DiffSensei: ИИ-художник манги с контролем персонажей", "desc": "Статья представляет DiffSensei - новую систему для генерации манги с возможностью контроля нескольких персонажей. Diff
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#architecture", "#games", "#diffusion", "#training", "#video"], "emoji": "🎬", "ru": {"title": "STIV: универсальный рецепт для масштабируемой генерации видео", "desc": "Статья представляет комплексное исследование в области генерации видео, предлагая метод STIV. STIV интегрирует усло
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#diffusion", "#security", "#rag", "#cv"], "emoji": "🖼️", "ru": {"title": "Невидимые и неуязвимые: революция в защите AI-изображений", "desc": "Статья представляет новый метод водяных знаков для изображений, генерируемых искусственным интеллектом. Авторы предлагают двухэтапный подход
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video"], "emoji": "🎨", "ru": {"title": "Универсальная модель для генерации и редактирования изображений на основе видеоданных", "desc": "UniReal - это унифицированная модель для генерации и редактирования изображений. Она рассматривает различные задачи обработ
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#dataset", "#cv", "#synthetic"], "emoji": "🎨", "ru": {"title": "Точный контроль визуальных атрибутов в генерации изображений", "desc": "Статья описывает новый подход к генерации изображений с использованием глубокого обучения. Авторы создали датасет FiVA с разметкой визуальных атриб
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#training", "#video", "#optimization", "#diffusion", "#transfer_learning", "#multimodal", "#architecture"], "emoji": "🎬", "ru": {"title": "DiTFlow: Перенос движения в видео с помощью диффузионных трансформеров", "desc": "DiTFlow - это метод переноса движения из эталонного видео в но
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Токены восприятия: новый инструмент для визуальных рассуждений в мультимодальных моделях", "desc": "Статья представляет новый подход к улучшению мультимодальных языковых моделей (MLM) в 
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#small_models", "#architecture", "#optimization", "#training", "#cv"], "emoji": "🚀", "ru": {"title": "EMOv2: Новый рубеж для легковесных моделей компьютерного зрения", "desc": "Эта работа посвящена разработке эффективных по параметрам и легковесных моделей для плотных предсказаний, 
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#rag", "#hallucinations", "#dataset", "#open_source", "#benchmark", "#ethics", "#synthetic", "#multimodal"], "emoji": "🛡️", "ru": {"title": "Защитник ИИ: обеспечение безопасности языковых моделей", "desc": "Представлены модели Granite Guardian - набор инструментов для обнаружения ри
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#agi", "#dataset", "#benchmark", "#interpretability", "#training", "#optimization", "#multimodal"], "emoji": "🧠", "ru": {"title": "ILLUME: Единая мультимодальная ИИ-модель с улучшенной эффективностью обучения", "desc": "ILLUME - это унифицированная мультимодальная большая языковая м
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "🎥", "ru": {"title": "3D траектории для улучшенного контроля объектов в генерации видео", "desc": "Это исследование представляет новый подход к управлению объектами в генерации видео из изображений, называемый ObjCtrl-2.5D. Метод использует 3D траекторию вм
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#data", "#dataset", "#open_source", "#ethics", "#training", "#multimodal"], "emoji": "🔓", "ru": {"title": "Moxin 7B: Открытая LLM для прозрачных инноваций в ИИ", "desc": "В статье представлена модель Moxin 7B - полностью открытая большая языковая модель (LLM), разработанная в соотве
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#training", "#ethics", "#data", "#interpretability", "#multimodal", "#alignment", "#architecture"], "emoji": "🧠", "ru": {"title": "Фреймовое представление слов: ключ к интерпретации и контролю языковых моделей", "desc": "Статья представляет гипотезу фреймового представления для инте
[11.12.2024 10:12] Querying the API.
[11.12.2024 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster
[11.12.2024 10:12] Response: {
  "desc": "Статья представляет 3DTrajMaster - контроллер для управления динамикой множества объектов в 3D-пространстве при генерации видео. Ключевой компонент - инжектор объектов, учитывающий 3D-траектории через механизм гейтированного самовнимания. Для улучшения качества видео используются доменный адаптер и стратегия отжига при сэмплировании. Авторы также создали датасет 360-Motion для обучения модели.",
  "emoji": "🎥",
  "title": "Управление 3D-движением объектов в генерации видео"
}
[11.12.2024 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster"

[11.12.2024 10:12] Response: ```python
['3D', 'DATASET', 'VIDEO']
```
[11.12.2024 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster"

[11.12.2024 10:12] Response: ```python
[]
```
[11.12.2024 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents 3DTrajMaster, a novel approach for controlling multi-entity motions in 3D video generation. Unlike previous methods that rely on 2D control signals, 3DTrajMaster utilizes 6DoF pose sequences to accurately represent the 3D dynamics of objects. The method incorporates a gated self-attention mechanism to integrate multiple entities with their 3D trajectories, enhancing the realism of generated videos. Additionally, the authors introduce a 360-Motion Dataset to improve training data quality, leading to superior performance in both accuracy and generalization for 3D motion control.","title":"Mastering 3D Motion Control in Video Generation"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper presents 3DTrajMaster, a novel approach for controlling multi-entity motions in 3D video generation. Unlike previous methods that rely on 2D control signals, 3DTrajMaster utilizes 6DoF pose sequences to accurately represent the 3D dynamics of objects. The method incorporates a gated self-attention mechanism to integrate multiple entities with their 3D trajectories, enhancing the realism of generated videos. Additionally, the authors introduce a 360-Motion Dataset to improve training data quality, leading to superior performance in both accuracy and generalization for 3D motion control.', title='Mastering 3D Motion Control in Video Generation'))
[11.12.2024 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文旨在操控视频生成中的多实体三维运动。以往的可控视频生成方法主要依赖二维控制信号来操控物体运动，但二维信号在表达三维运动特性方面存在局限。为了解决这个问题，我们提出了3DTrajMaster，这是一种强大的控制器，可以根据用户期望的六自由度姿态序列调节三维空间中的多实体动态。我们的研究通过一个插件式的三维运动基础物体注入器，结合多个输入实体及其相应的三维轨迹，利用门控自注意力机制实现了这一目标。","title":"掌控三维运动，重塑视频生成"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文旨在操控视频生成中的多实体三维运动。以往的可控视频生成方法主要依赖二维控制信号来操控物体运动，但二维信号在表达三维运动特性方面存在局限。为了解决这个问题，我们提出了3DTrajMaster，这是一种强大的控制器，可以根据用户期望的六自由度姿态序列调节三维空间中的多实体动态。我们的研究通过一个插件式的三维运动基础物体注入器，结合多个输入实体及其相应的三维轨迹，利用门控自注意力机制实现了这一目标。', title='掌控三维运动，重塑视频生成'))
[11.12.2024 10:12] Querying the API.
[11.12.2024 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.
[11.12.2024 10:12] Response: {
  "desc": "Статья представляет метод RAPL для обучения визуальных наград с использованием меньшего количества обратной связи от человека. В отличие от традиционного обучения с подкреплением по обратной связи человека (RLHF), RAPL фокусируется на дообучении предварительно обученных энкодеров зрения для согласования с визуальным представлением конечного пользователя. Метод показал эффективность в симуляционных экспериментах и задачах манипуляции роботом. RAPL позволяет настраивать политики с использованием в 5 раз меньше реальных данных о предпочтениях человека.",
  "emoji": "🤖",
  "title": "Эффективное обучение роботов с минимальным участием человека"
}
[11.12.2024 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment."

[11.12.2024 10:12] Response: ```python
['RLHF', 'ROBOTICS', 'TRAINING']
```
[11.12.2024 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment."

[11.12.2024 10:12] Response: ```python
['ALIGNMENT']
```
[11.12.2024 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Representation-Aligned Preference-based Learning (RAPL), a novel approach to align visuomotor robot policies with human preferences using minimal feedback. RAPL improves upon traditional reinforcement learning from human feedback (RLHF) by focusing on fine-tuning pre-trained vision encoders, allowing for the construction of dense visual rewards through feature matching. The method is validated through simulations and hardware experiments, showing that it can effectively learn rewards that align with human preferences while requiring significantly less human input. Overall, RAPL represents a significant step towards efficient alignment of robot policies in various manipulation tasks.","title":"Efficiently Aligning Robot Policies with Minimal Human Feedback"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces Representation-Aligned Preference-based Learning (RAPL), a novel approach to align visuomotor robot policies with human preferences using minimal feedback. RAPL improves upon traditional reinforcement learning from human feedback (RLHF) by focusing on fine-tuning pre-trained vision encoders, allowing for the construction of dense visual rewards through feature matching. The method is validated through simulations and hardware experiments, showing that it can effectively learn rewards that align with human preferences while requiring significantly less human input. Overall, RAPL represents a significant step towards efficient alignment of robot policies in various manipulation tasks.', title='Efficiently Aligning Robot Policies with Minimal Human Feedback'))
[11.12.2024 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文提出了一种新的方法，称为基于表示对齐的偏好学习（RAPL），旨在减少对人类反馈的需求，以便更好地对齐视觉运动机器人策略。传统的强化学习方法需要大量的人类反馈来学习视觉奖励函数，而RAPL通过优化预训练的视觉编码器来实现这一目标。该方法通过特征匹配在对齐的表示空间中构建密集的视觉奖励，从而提高了学习效率。实验结果表明，RAPL能够在减少人类偏好数据的情况下，有效地对齐机器人策略。","title":"减少人类反馈，实现机器人策略对齐的突破"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='这篇论文提出了一种新的方法，称为基于表示对齐的偏好学习（RAPL），旨在减少对人类反馈的需求，以便更好地对齐视觉运动机器人策略。传统的强化学习方法需要大量的人类反馈来学习视觉奖励函数，而RAPL通过优化预训练的视觉编码器来实现这一目标。该方法通过特征匹配在对齐的表示空间中构建密集的视觉奖励，从而提高了学习效率。实验结果表明，RAPL能够在减少人类偏好数据的情况下，有效地对齐机器人策略。', title='减少人类反馈，实现机器人策略对齐的突破'))
[11.12.2024 10:12] Querying the API.
[11.12.2024 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs.
[11.12.2024 10:12] Response: {
  "desc": "Статья представляет Chimera - новый подход к улучшению крупных мультимодальных моделей (LMM) для специализированных задач. Авторы предлагают прогрессивную стратегию обучения для интеграции экспертных моделей в обобщенную LMM. Для решения проблемы несбалансированной оптимизации вводится механизм маскирования совместной работы обобщенной и специализированной моделей (GSCM). Результатом является универсальная модель, показывающая высокие результаты в задачах мультимодальных рассуждений и извлечения визуального контента в специализированных областях.",
  "emoji": "🧠",
  "title": "Chimera: Универсальная LMM с экспертными знаниями"
}
[11.12.2024 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs."

[11.12.2024 10:12] Response: ```python
['MULTIMODAL', 'TRAINING', 'CV']
```
[11.12.2024 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs."

[11.12.2024 10:12] Response: ```python
["TRANSFER_LEARNING", "OPTIMIZATION", "REASONING"]
```
[11.12.2024 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Chimera, a new approach to enhance Large Multi-modal Models (LMMs) by integrating domain-specific expert models. The authors highlight that while LMMs perform well on general tasks, they struggle with specialized tasks due to their training on broad datasets. To improve this, Chimera employs a progressive training strategy that incorporates expert features into the generalist model\'s input. Additionally, the Generalist-Specialist Collaboration Masking (GSCM) mechanism is introduced to balance optimization between the generalist and specialist models, leading to superior performance in multi-modal reasoning and visual content extraction tasks.","title":"Chimera: Bridging Generalist and Specialist Models for Enhanced Multi-modal Learning"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc="This paper presents Chimera, a new approach to enhance Large Multi-modal Models (LMMs) by integrating domain-specific expert models. The authors highlight that while LMMs perform well on general tasks, they struggle with specialized tasks due to their training on broad datasets. To improve this, Chimera employs a progressive training strategy that incorporates expert features into the generalist model's input. Additionally, the Generalist-Specialist Collaboration Masking (GSCM) mechanism is introduced to balance optimization between the generalist and specialist models, leading to superior performance in multi-modal reasoning and visual content extraction tasks.", title='Chimera: Bridging Generalist and Specialist Models for Enhanced Multi-modal Learning'))
[11.12.2024 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种名为Chimera的多模态模型，旨在提升现有大型多模态模型（LMMs）在特定领域的能力。通过引入领域专家模型的特征，Chimera采用渐进式训练策略，解决了通用模型与专家模型之间的表示差距和优化不平衡问题。特别地，文章提出了一种新的通用-专家协作掩码机制（GSCM），以优化模型性能。最终，Chimera在图表、表格、数学和文档等领域的多模态推理和视觉内容提取任务中表现出色，达到了最先进的性能。","title":"Chimera：提升多模态模型的领域专长"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文介绍了一种名为Chimera的多模态模型，旨在提升现有大型多模态模型（LMMs）在特定领域的能力。通过引入领域专家模型的特征，Chimera采用渐进式训练策略，解决了通用模型与专家模型之间的表示差距和优化不平衡问题。特别地，文章提出了一种新的通用-专家协作掩码机制（GSCM），以优化模型性能。最终，Chimera在图表、表格、数学和文档等领域的多模态推理和视觉内容提取任务中表现出色，达到了最先进的性能。', title='Chimera：提升多模态模型的领域专长'))
[11.12.2024 10:12] Using data from previous issue: {"categories": ["#data", "#healthcare", "#ethics", "#security", "#open_source", "#training"], "emoji": "🛡️", "ru": {"title": "Защита конфиденциальности в федеративном обучении с помощью гиперсетей", "desc": "В статье представлен новый подход к федеративному обучению (FL), направленный на защиту конф
[11.12.2024 10:12] Loading Chinese text from previous data.
[11.12.2024 10:12] Renaming data file.
[11.12.2024 10:12] Renaming previous data. hf_papers.json to ./d/2024-12-11.json
[11.12.2024 10:12] Saving new data file.
[11.12.2024 10:12] Generating page.
[11.12.2024 10:12] Renaming previous page.
[11.12.2024 10:12] Renaming previous data. index.html to ./d/2024-12-11.html
[11.12.2024 10:12] [Experimental] Generating Chinese page for reading.
[11.12.2024 10:12] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}, {'word': '以前', 'pinyin': 'yǐ qián', 'trans': 'before'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '基准测试', 'pinyin': 'jī zhǔn cè shì', 'trans': 'benchmark test'}, {'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'main'}, {'word': '关注', 'pinyin': 'guān zhù', 'trans': 'focus on'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '正确', 'pinyin': 'zhèng què', 'trans': 'correct'}, {'word': '片段', 'pinyin': 'piàn duàn', 'trans': 'segment'}, {'word': '忽略', 'pinyin': 'hū lüè', 'trans': 'ignore'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '弥补', 'pinyin': 'mí bǔ', 'trans': 'make up for'}, {'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'}, {'word': '人工编制', 'pinyin': 'rén gōng biān zhì', 'trans': 'artificially compiled'}, {'word': '模拟', 'pinyin': 'mó nǐ', 'trans': 'simulate'}, {'word': '真实世界', 'pinyin': 'zhēn shí shì jiè', 'trans': 'real world'}, {'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'}, {'word': '多样性', 'pinyin': 'duō yàng xìng', 'trans': 'diversity'}, {'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open source'}, {'word': '专有', 'pinyin': 'zhuān yǒu', 'trans': 'proprietary'}, {'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '强调', 'pinyin': 'qiáng diào', 'trans': 'emphasize'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}]
[11.12.2024 10:12] Renaming previous Chinese page.
[11.12.2024 10:12] Renaming previous data. zh.html to ./d/2024-12-10_zh_reading_task.html
[11.12.2024 10:12] Writing Chinese reading task.
[11.12.2024 10:12] Writing result.
[11.12.2024 10:12] Renaming log file.
[11.12.2024 10:12] Renaming previous data. log.txt to ./logs/2024-12-11_last_log.txt
