[11.12.2024 08:15] Read previous papers.
[11.12.2024 08:15] Generating top page (month).
[11.12.2024 08:15] Writing top page (month).
[11.12.2024 09:11] Read previous papers.
[11.12.2024 09:11] Get feed.
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05210
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07589
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07774
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07674
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07730
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04653
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03548
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07776
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06674
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07724
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06673
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07721
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06845
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07334
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07187
[11.12.2024 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.12.2024 09:11] No deleted papers detected.
[11.12.2024 09:11] Downloading and parsing papers (pdf, html). Total: 15.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.05210.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.05210.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.05210.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07589.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07589.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07589.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07774.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07774.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07774.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07674.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07674.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07674.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07730.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07730.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07730.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04653.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04653.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04653.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.03548.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.03548.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.03548.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07776.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07776.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07776.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.06674.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.06674.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.06674.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07724.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07724.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07724.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.06673.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.06673.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.06673.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07721.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07721.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07721.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.06845.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.06845.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.06845.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07334.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07334.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07334.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07187.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07187.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07187.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Enriching papers with extra data.
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 0. Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLM...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 1. Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limi...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 2. We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 3. Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable at...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 4. The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectu...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 5. As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarki...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 6. Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produc...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 7. We propose DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal c...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 8. This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 9. We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including soc...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 10. In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically r...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 11. This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance co...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 12. Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkabl...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 13. Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis ...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 14. Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety ...
[11.12.2024 09:11] Read previous papers.
[11.12.2024 09:11] Generating reviews via LLM API.
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#plp", "#alignment", "#benchmark", "#open_source", "#synthetic", "#training"], "emoji": "🏆", "ru": {"title": "CodeArena: новый стандарт оценки ИИ-помощников программиста", "desc": "В статье представлен новый бенчмарк CodeArena для оценки языковых моделей кода с учетом предпочтений п
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#story_generation", "#dataset", "#cv"], "emoji": "🖼️", "ru": {"title": "DiffSensei: ИИ-художник манги с контролем персонажей", "desc": "Статья представляет DiffSensei - новую систему для генерации манги с возможностью контроля нескольких персонажей. Diff
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video"], "emoji": "🎨", "ru": {"title": "Универсальная модель для генерации и редактирования изображений на основе видеоданных", "desc": "UniReal - это унифицированная модель для генерации и редактирования изображений. Она рассматривает различные задачи обработ
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#dataset", "#cv", "#synthetic"], "emoji": "🎨", "ru": {"title": "Точный контроль визуальных атрибутов в генерации изображений", "desc": "Статья описывает новый подход к генерации изображений с использованием глубокого обучения. Авторы создали датасет FiVA с разметкой визуальных атриб
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#architecture", "#games", "#diffusion", "#training", "#video"], "emoji": "🎬", "ru": {"title": "STIV: универсальный рецепт для масштабируемой генерации видео", "desc": "Статья представляет комплексное исследование в области генерации видео, предлагая метод STIV. STIV интегрирует усло
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#security", "#rag", "#cv"], "emoji": "🖼️", "ru": {"title": "Невидимые и неуязвимые: революция в защите AI-изображений", "desc": "Статья представляет новый метод водяных знаков для изображений, генерируемых искусственным интеллектом. Авторы предлагают двухэтапный подход
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Токены восприятия: новый инструмент для визуальных рассуждений в мультимодальных моделях", "desc": "Статья представляет новый подход к улучшению мультимодальных языковых моделей (MLM) в 
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#video", "#optimization", "#diffusion", "#transfer_learning", "#multimodal", "#architecture"], "emoji": "🎬", "ru": {"title": "DiTFlow: Перенос движения в видео с помощью диффузионных трансформеров", "desc": "DiTFlow - это метод переноса движения из эталонного видео в но
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#small_models", "#architecture", "#optimization", "#training", "#cv"], "emoji": "🚀", "ru": {"title": "EMOv2: Новый рубеж для легковесных моделей компьютерного зрения", "desc": "Эта работа посвящена разработке эффективных по параметрам и легковесных моделей для плотных предсказаний, 
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#rag", "#hallucinations", "#dataset", "#open_source", "#benchmark", "#ethics", "#synthetic", "#multimodal"], "emoji": "🛡️", "ru": {"title": "Защитник ИИ: обеспечение безопасности языковых моделей", "desc": "Представлены модели Granite Guardian - набор инструментов для обнаружения ри
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#agi", "#dataset", "#benchmark", "#interpretability", "#training", "#optimization", "#multimodal"], "emoji": "🧠", "ru": {"title": "ILLUME: Единая мультимодальная ИИ-модель с улучшенной эффективностью обучения", "desc": "ILLUME - это унифицированная мультимодальная большая языковая м
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "🎥", "ru": {"title": "3D траектории для улучшенного контроля объектов в генерации видео", "desc": "Это исследование представляет новый подход к управлению объектами в генерации видео из изображений, называемый ObjCtrl-2.5D. Метод использует 3D траекторию вм
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#data", "#dataset", "#open_source", "#ethics", "#training", "#multimodal"], "emoji": "🔓", "ru": {"title": "Moxin 7B: Открытая LLM для прозрачных инноваций в ИИ", "desc": "В статье представлена модель Moxin 7B - полностью открытая большая языковая модель (LLM), разработанная в соотве
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#ethics", "#data", "#interpretability", "#multimodal", "#alignment", "#architecture"], "emoji": "🧠", "ru": {"title": "Фреймовое представление слов: ключ к интерпретации и контролю языковых моделей", "desc": "Статья представляет гипотезу фреймового представления для инте
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#data", "#healthcare", "#ethics", "#security", "#open_source", "#training"], "emoji": "🛡️", "ru": {"title": "Защита конфиденциальности в федеративном обучении с помощью гиперсетей", "desc": "В статье представлен новый подход к федеративному обучению (FL), направленный на защиту конф
[11.12.2024 09:11] Trying to get texts in Chinese.
[11.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\url{https://codearenaeval.github.io/ }
[11.12.2024 09:11] Mistral response. {"id": "4b5ba27db03c4d3fb33c52dacbcfb3c8", "object": "chat.completion", "created": 1733908303, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08codeLLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u3002\u4ee5\u524d\u7684\u4ee3\u7801\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u6b63\u786e\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCodeArena\u7684\u4e25\u683c\u4eba\u5de5\u7f16\u5236\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\u5728\u5f00\u6e90\u4ee3\u7801LLMs\u548c\u4e13\u6709LLMs\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 379, "total_tokens": 556, "completion_tokens": 177}}
[11.12.2024 09:11] Response: 这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。
[11.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。
[11.12.2024 09:11] Mistral response. {"id": "822db69024714557aacddfbc0bbb2bbf", "object": "chat.completion", "created": 1733908307, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08codeLLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u3002\u4ee5\u524d\u7684\u4ee3\u7801\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u6b63\u786e\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCodeArena\u7684\u4e25\u683c\u4eba\u5de5\u7f16\u5236\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\u5728\u5f00\u6e90\u4ee3\u7801LLMs\u548c\u4e13\u6709LLMs\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng t\u01ceo l\u00f9n le d\u00e0i m\u01ce d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng (codeLLMs) z\u00e0i d\u00e0i m\u01ce sh\u0113ng ch\u00e9ng f\u0101ng mi\u00e0n de j\u00ecn zh\u00e0n. y\u01d0 qi\u00e1n de d\u00e0i m\u01ce xi\u0101ng gu\u0101n j\u012b zh\u01d4n c\u00e8 sh\u00ec zh\u01d4 y\u00e0o gu\u0101n zh\u00f9 sh\u0113ng ch\u00e9ng zh\u00e8ng qu\u00e8 de d\u00e0i m\u01ce pi\u00e0n du\u00e0n, d\u00e0n h\u016b l\u00fc\u00e8 le y\u01d4 r\u00e9n l\u00e8i pi\u0101n h\u00e0o de y\u012b zh\u00ec x\u00ecng. w\u00e8i le m\u00ed b\u01d4 zh\u00e8 y\u012b ch\u0101 j\u00f9, zu\u00f2 zh\u011b t\u00ed ch\u016b le y\u012b g\u00e8 m\u00edng w\u00e8i CodeArena de y\u00e1n g\u00e9 r\u00e9n g\u014dng bi\u0101n zh\u00ec j\u012b zh\u01d4n c\u00e8 sh\u00ec, m\u00f3 n\u01d0 zh\u0113n sh\u00ed sh\u00ec ji\u00e8 bi\u0101n m\u01ce r\u00e8n w\u00f9 de f\u00fa z\u00e0 x\u00ecng h\u00e9 du\u014d y\u00e0ng x\u00ecng. t\u014dng gu\u00f2 x\u00ec t\u01d2ng sh\u00ed y\u00e0n, zu\u00f2 zh\u011b f\u0101 xi\u00e0n z\u00e0i k\u0101i yu\u00e1n d\u00e0i m\u01ce LLMs h\u00e9 zhu\u0101n y\u01d2u LLMs zh\u012b ji\u0101n c\u00fan z\u00e0i xi\u01cen zh\u00f9 de x\u00ecng n\u00e9ng ch\u0101 j\u00f9, qi\u00e1ng di\u00e0o le r\u00e9n l\u00e8i pi\u0101n h\u00e0o y\u012b zh\u00ec x\u00ecng de zh\u00f2ng y\u00e0o x\u00ecng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 192, "total_tokens": 768, "completion_tokens": 576}}
[11.12.2024 09:11] Response: 这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。

zhè piān wén zhāng tǎo lùn le dài mǎ dà yǔ yán mó xíng (codeLLMs) zài dài mǎ shēng chéng fāng miàn de jìn zhàn. yǐ qián de dài mǎ xiāng guān jī zhǔn cè shì zhǔ yào guān zhù shēng chéng zhèng què de dài mǎ piàn duàn, dàn hū lüè le yǔ rén lèi piān hào de yī zhì xìng. wèi le mí bǔ zhè yī chā jù, zuò zhě tí chū le yī gè míng wèi CodeArena de yán gé rén gōng biān zhì jī zhǔn cè shì, mó nǐ zhēn shí shì jiè biān mǎ rèn wù de fú zà xìng hé duō yàng xìng. tōng guò xì tǒng shí yàn, zuò zhě fā xiàn zài kāi yuán dài mǎ LLMs hé zhuān yǒu LLMs zhī jiān cún zài xiǎn zhù de xìng néng chā jù, qiáng diào le rén lèi piān hào yī zhì xìng de zhòng yào xìng.
[11.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。
[11.12.2024 09:12] Mistral response. {"id": "da14a96d3c26431f8b76067656d0ad84", "object": "chat.completion", "created": 1733908319, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u8ba8\u8bba', 'pinyin': 't\u01ceo l\u00f9n', 'trans': 'discuss'},\n{'word': '\u4ee3\u7801', 'pinyin': 'd\u00e0i m\u01ce', 'trans': 'code'},\n{'word': '\u5927\u8bed\u8a00\u6a21\u578b', 'pinyin': 'd\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng', 'trans': 'large language model'},\n{'word': '\u8fdb\u5c55', 'pinyin': 'j\u00ecn zh\u01cen', 'trans': 'progress'},\n{'word': '\u4ee5\u524d', 'pinyin': 'y\u01d0 qi\u00e1n', 'trans': 'before'},\n{'word': '\u76f8\u5173', 'pinyin': 'xi\u0101ng gu\u0101n', 'trans': 'related'},\n{'word': '\u57fa\u51c6\u6d4b\u8bd5', 'pinyin': 'j\u012b zh\u01d4n c\u00e8 sh\u00ec', 'trans': 'benchmark test'},\n{'word': '\u4e3b\u8981', 'pinyin': 'zh\u01d4 y\u00e0o', 'trans': 'main'},\n{'word': '\u5173\u6ce8', 'pinyin': 'gu\u0101n zh\u00f9', 'trans': 'focus on'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ng ch\u00e9ng', 'trans': 'generate'},\n{'word': '\u6b63\u786e', 'pinyin': 'zh\u00e8ng qu\u00e8', 'trans': 'correct'},\n{'word': '\u7247\u6bb5', 'pinyin': 'pi\u00e0n du\u00e0n', 'trans': 'segment'},\n{'word': '\u5ffd\u7565', 'pinyin': 'h\u016b l\u00fc\u00e8', 'trans': 'ignore'},\n{'word': '\u4e00\u81f4\u6027', 'pinyin': 'y\u012b zh\u00ec x\u00ecng', 'trans': 'consistency'},\n{'word': '\u5f25\u8865', 'pinyin': 'm\u00ed b\u01d4', 'trans': 'make up for'},\n{'word': '\u5dee\u8ddd', 'pinyin': 'ch\u0101 j\u00f9', 'trans': 'gap'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u4e25\u683c', 'pinyin': 'y\u00e1n g\u00e9', 'trans': 'strict'},\n{'word': '\u4eba\u5de5\u7f16\u5236', 'pinyin': 'r\u00e9n g\u014dng bi\u0101n zh\u00ec', 'trans': 'artificially compiled'},\n{'word': '\u6a21\u62df', 'pinyin': 'm\u00f3 n\u01d0', 'trans': 'simulate'},\n{'word': '\u771f\u5b9e\u4e16\u754c', 'pinyin': 'zh\u0113n sh\u00ed sh\u00ec ji\u00e8', 'trans': 'real world'},\n{'word': '\u590d\u6742\u6027', 'pinyin': 'f\u00f9 z\u00e1 x\u00ecng', 'trans': 'complexity'},\n{'word': '\u591a\u6837\u6027', 'pinyin': 'du\u014d y\u00e0ng x\u00ecng', 'trans': 'diversity'},\n{'word': '\u7cfb\u7edf', 'pinyin': 'x\u00ec t\u01d2ng', 'trans': 'system'},\n{'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00ed y\u00e0n', 'trans': 'experiment'},\n{'word': '\u5f00\u6e90', 'pinyin': 'k\u0101i yu\u00e1n', 'trans': 'open source'},\n{'word': '\u4e13\u6709', 'pinyin': 'zhu\u0101n y\u01d2u', 'trans': 'proprietary'},\n{'word': '\u5b58\u5728', 'pinyin': 'c\u00fan z\u00e0i', 'trans': 'exist'},\n{'word': '\u663e\u8457', 'pinyin': 'xi\u01cen zh\u00f9', 'trans': 'significant'},\n{'word': '\u6027\u80fd', 'pinyin': 'x\u00ecng n\u00e9ng', 'trans': 'performance'},\n{'word': '\u5f3a\u8c03', 'pinyin': 'qi\u00e1ng di\u00e0o', 'trans': 'emphasize'},\n{'word': '\u91cd\u8981\u6027', 'pinyin': 'zh\u00f2ng y\u00e0o x\u00ecng', 'trans': 'importance'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 224, "total_tokens": 1143, "completion_tokens": 919}}
[11.12.2024 09:12] Response: [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},
{'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'},
{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},
{'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'},
{'word': '以前', 'pinyin': 'yǐ qián', 'trans': 'before'},
{'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'},
{'word': '基准测试', 'pinyin': 'jī zhǔn cè shì', 'trans': 'benchmark test'},
{'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'main'},
{'word': '关注', 'pinyin': 'guān zhù', 'trans': 'focus on'},
{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'},
{'word': '正确', 'pinyin': 'zhèng què', 'trans': 'correct'},
{'word': '片段', 'pinyin': 'piàn duàn', 'trans': 'segment'},
{'word': '忽略', 'pinyin': 'hū lüè', 'trans': 'ignore'},
{'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'},
{'word': '弥补', 'pinyin': 'mí bǔ', 'trans': 'make up for'},
{'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'},
{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},
{'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'},
{'word': '人工编制', 'pinyin': 'rén gōng biān zhì', 'trans': 'artificially compiled'},
{'word': '模拟', 'pinyin': 'mó nǐ', 'trans': 'simulate'},
{'word': '真实世界', 'pinyin': 'zhēn shí shì jiè', 'trans': 'real world'},
{'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'},
{'word': '多样性', 'pinyin': 'duō yàng xìng', 'trans': 'diversity'},
{'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'},
{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},
{'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open source'},
{'word': '专有', 'pinyin': 'zhuān yǒu', 'trans': 'proprietary'},
{'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'},
{'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'},
{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},
{'word': '强调', 'pinyin': 'qiáng diào', 'trans': 'emphasize'},
{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}]
[11.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章讨论了代码大语言模型（codeLLMs）在代码生成方面的进展。以前的代码相关基准测试主要关注生成正确的代码片段，但忽略了与人类偏好的一致性。为了弥补这一差距，作者提出了一个名为CodeArena的严格人工编制基准测试，模拟真实世界编码任务的复杂性和多样性。通过系统实验，作者发现在开源代码LLMs和专有LLMs之间存在显著的性能差距，强调了人类偏好一致性的重要性。
[11.12.2024 09:12] Mistral response. {"id": "70cecd1302634b2b93f17dab58d7042f", "object": "chat.completion", "created": 1733908338, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 190, "total_tokens": 307, "completion_tokens": 117}}
[11.12.2024 09:12] Response: This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences.
[11.12.2024 09:12] Renaming data file.
[11.12.2024 09:12] Renaming previous data. hf_papers.json to ./d/2024-12-11.json
[11.12.2024 09:12] Saving new data file.
[11.12.2024 09:12] Generating page.
[11.12.2024 09:12] Renaming previous page.
[11.12.2024 09:12] Renaming previous data. index.html to ./d/2024-12-11.html
[11.12.2024 09:12] [Experimental] Generating Chinese page for reading.
[11.12.2024 09:12] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}, {'word': '以前', 'pinyin': 'yǐ qián', 'trans': 'before'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '基准测试', 'pinyin': 'jī zhǔn cè shì', 'trans': 'benchmark test'}, {'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'main'}, {'word': '关注', 'pinyin': 'guān zhù', 'trans': 'focus on'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '正确', 'pinyin': 'zhèng què', 'trans': 'correct'}, {'word': '片段', 'pinyin': 'piàn duàn', 'trans': 'segment'}, {'word': '忽略', 'pinyin': 'hū lüè', 'trans': 'ignore'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '弥补', 'pinyin': 'mí bǔ', 'trans': 'make up for'}, {'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'}, {'word': '人工编制', 'pinyin': 'rén gōng biān zhì', 'trans': 'artificially compiled'}, {'word': '模拟', 'pinyin': 'mó nǐ', 'trans': 'simulate'}, {'word': '真实世界', 'pinyin': 'zhēn shí shì jiè', 'trans': 'real world'}, {'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'}, {'word': '多样性', 'pinyin': 'duō yàng xìng', 'trans': 'diversity'}, {'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open source'}, {'word': '专有', 'pinyin': 'zhuān yǒu', 'trans': 'proprietary'}, {'word': '存在', 'pinyin': 'cún zài', 'trans': 'exist'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '强调', 'pinyin': 'qiáng diào', 'trans': 'emphasize'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}]
[11.12.2024 09:12] Renaming previous Chinese page.
[11.12.2024 09:12] Renaming previous data. zh.html to ./d/2024-12-10_zh_reading_task.html
[11.12.2024 09:12] Writing Chinese reading task.
[11.12.2024 09:12] Writing result.
[11.12.2024 09:12] Renaming log file.
[11.12.2024 09:12] Renaming previous data. log.txt to ./logs/2024-12-11_last_log.txt
