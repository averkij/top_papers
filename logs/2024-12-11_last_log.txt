[11.12.2024 08:15] Read previous papers.
[11.12.2024 08:15] Generating top page (month).
[11.12.2024 08:15] Writing top page (month).
[11.12.2024 09:11] Read previous papers.
[11.12.2024 09:11] Get feed.
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05210
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07589
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07774
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07674
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07730
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04653
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03548
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07776
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06674
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07724
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06673
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07721
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.06845
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07334
[11.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.07187
[11.12.2024 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.12.2024 09:11] No deleted papers detected.
[11.12.2024 09:11] Downloading and parsing papers (pdf, html). Total: 15.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.05210.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.05210.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.05210.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07589.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07589.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07589.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07774.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07774.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07774.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07674.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07674.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07674.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07730.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07730.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07730.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.04653.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.04653.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.04653.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.03548.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.03548.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.03548.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07776.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07776.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07776.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.06674.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.06674.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.06674.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07724.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07724.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07724.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.06673.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.06673.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.06673.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07721.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07721.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07721.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.06845.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.06845.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.06845.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07334.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07334.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07334.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.07187.
[11.12.2024 09:11] Extra JSON file exists (./assets/json/2412.07187.json), skip PDF parsing.
[11.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.07187.json), skip HTML parsing.
[11.12.2024 09:11] Success.
[11.12.2024 09:11] Enriching papers with extra data.
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 0. Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLM...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 1. Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limi...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 2. We introduce UniReal, a unified framework designed to address various image generation and editing tasks. Existing solutions often vary by tasks, yet share fundamental principles: preserving consistency between inputs and outputs while capturing visual variations. Inspired by recent video generation...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 3. Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable at...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 4. The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectu...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 5. As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarki...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 6. Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produc...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 7. We propose DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal c...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 8. This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 9. We introduce the Granite Guardian models, a suite of safeguards designed to provide risk detection for prompts and responses, enabling safe and responsible use in combination with any large language model (LLM). These models offer comprehensive coverage across multiple risk dimensions, including soc...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 10. In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically r...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 11. This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance co...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 12. Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkabl...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 13. Interpretability is a key challenge in fostering trust for Large Language Models (LLMs), which stems from the complexity of extracting reasoning from model's parameters. We present the Frame Representation Hypothesis, a theoretically robust framework grounded in the Linear Representation Hypothesis ...
[11.12.2024 09:11] ********************************************************************************
[11.12.2024 09:11] Abstract 14. Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety ...
[11.12.2024 09:11] Read previous papers.
[11.12.2024 09:11] Generating reviews via LLM API.
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#plp", "#alignment", "#benchmark", "#open_source", "#synthetic", "#training"], "emoji": "ğŸ†", "ru": {"title": "CodeArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ°", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CodeArena Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#story_generation", "#dataset", "#cv"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "DiffSensei: Ğ˜Ğ˜-Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸Ğº Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DiffSensei - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. Diff
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video"], "emoji": "ğŸ¨", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "UniReal - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#dataset", "#cv", "#synthetic"], "emoji": "ğŸ¨", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ FiVA Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#architecture", "#games", "#diffusion", "#training", "#video"], "emoji": "ğŸ¬", "ru": {"title": "STIV: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ STIV. STIV Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#security", "#rag", "#cv"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "ĞĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸ Ğ½ĞµÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ğµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ AI-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#training", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLM) Ğ² 
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#video", "#optimization", "#diffusion", "#transfer_learning", "#multimodal", "#architecture"], "emoji": "ğŸ¬", "ru": {"title": "DiTFlow: ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "DiTFlow - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ½Ğ¾
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#small_models", "#architecture", "#optimization", "#training", "#cv"], "emoji": "ğŸš€", "ru": {"title": "EMOv2: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹, 
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#rag", "#hallucinations", "#dataset", "#open_source", "#benchmark", "#ethics", "#synthetic", "#multimodal"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ğ¸Ğº Ğ˜Ğ˜: Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Granite Guardian - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#agi", "#dataset", "#benchmark", "#interpretability", "#training", "#optimization", "#multimodal"], "emoji": "ğŸ§ ", "ru": {"title": "ILLUME: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "ILLUME - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "ğŸ¥", "ru": {"title": "3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ ObjCtrl-2.5D. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ¼
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#data", "#dataset", "#open_source", "#ethics", "#training", "#multimodal"], "emoji": "ğŸ”“", "ru": {"title": "Moxin 7B: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Moxin 7B - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²Ğµ
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#ethics", "#data", "#interpretability", "#multimodal", "#alignment", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²: ĞºĞ»ÑÑ‡ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚Ğµ
[11.12.2024 09:11] Using data from previous issue: {"categories": ["#data", "#healthcare", "#ethics", "#security", "#open_source", "#training"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (FL), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ ĞºĞ¾Ğ½Ñ„
[11.12.2024 09:11] Trying to get texts in Chinese.
[11.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\url{https://codearenaeval.github.io/ }
[11.12.2024 09:11] Mistral response. {"id": "4b5ba27db03c4d3fb33c52dacbcfb3c8", "object": "chat.completion", "created": 1733908303, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08codeLLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u3002\u4ee5\u524d\u7684\u4ee3\u7801\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u6b63\u786e\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCodeArena\u7684\u4e25\u683c\u4eba\u5de5\u7f16\u5236\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\u5728\u5f00\u6e90\u4ee3\u7801LLMs\u548c\u4e13\u6709LLMs\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 379, "total_tokens": 556, "completion_tokens": 177}}
[11.12.2024 09:11] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚
[11.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚
[11.12.2024 09:11] Mistral response. {"id": "822db69024714557aacddfbc0bbb2bbf", "object": "chat.completion", "created": 1733908307, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08codeLLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u3002\u4ee5\u524d\u7684\u4ee3\u7801\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u6b63\u786e\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCodeArena\u7684\u4e25\u683c\u4eba\u5de5\u7f16\u5236\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\u5728\u5f00\u6e90\u4ee3\u7801LLMs\u548c\u4e13\u6709LLMs\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng t\u01ceo l\u00f9n le d\u00e0i m\u01ce d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng (codeLLMs) z\u00e0i d\u00e0i m\u01ce sh\u0113ng ch\u00e9ng f\u0101ng mi\u00e0n de j\u00ecn zh\u00e0n. y\u01d0 qi\u00e1n de d\u00e0i m\u01ce xi\u0101ng gu\u0101n j\u012b zh\u01d4n c\u00e8 sh\u00ec zh\u01d4 y\u00e0o gu\u0101n zh\u00f9 sh\u0113ng ch\u00e9ng zh\u00e8ng qu\u00e8 de d\u00e0i m\u01ce pi\u00e0n du\u00e0n, d\u00e0n h\u016b l\u00fc\u00e8 le y\u01d4 r\u00e9n l\u00e8i pi\u0101n h\u00e0o de y\u012b zh\u00ec x\u00ecng. w\u00e8i le m\u00ed b\u01d4 zh\u00e8 y\u012b ch\u0101 j\u00f9, zu\u00f2 zh\u011b t\u00ed ch\u016b le y\u012b g\u00e8 m\u00edng w\u00e8i CodeArena de y\u00e1n g\u00e9 r\u00e9n g\u014dng bi\u0101n zh\u00ec j\u012b zh\u01d4n c\u00e8 sh\u00ec, m\u00f3 n\u01d0 zh\u0113n sh\u00ed sh\u00ec ji\u00e8 bi\u0101n m\u01ce r\u00e8n w\u00f9 de f\u00fa z\u00e0 x\u00ecng h\u00e9 du\u014d y\u00e0ng x\u00ecng. t\u014dng gu\u00f2 x\u00ec t\u01d2ng sh\u00ed y\u00e0n, zu\u00f2 zh\u011b f\u0101 xi\u00e0n z\u00e0i k\u0101i yu\u00e1n d\u00e0i m\u01ce LLMs h\u00e9 zhu\u0101n y\u01d2u LLMs zh\u012b ji\u0101n c\u00fan z\u00e0i xi\u01cen zh\u00f9 de x\u00ecng n\u00e9ng ch\u0101 j\u00f9, qi\u00e1ng di\u00e0o le r\u00e9n l\u00e8i pi\u0101n h\u00e0o y\u012b zh\u00ec x\u00ecng de zh\u00f2ng y\u00e0o x\u00ecng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 192, "total_tokens": 768, "completion_tokens": 576}}
[11.12.2024 09:11] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚

zhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ i mÇ dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (codeLLMs) zÃ i dÃ i mÇ shÄ“ng chÃ©ng fÄng miÃ n de jÃ¬n zhÃ n. yÇ qiÃ¡n de dÃ i mÇ xiÄng guÄn jÄ« zhÇ”n cÃ¨ shÃ¬ zhÇ” yÃ o guÄn zhÃ¹ shÄ“ng chÃ©ng zhÃ¨ng quÃ¨ de dÃ i mÇ piÃ n duÃ n, dÃ n hÅ« lÃ¼Ã¨ le yÇ” rÃ©n lÃ¨i piÄn hÃ o de yÄ« zhÃ¬ xÃ¬ng. wÃ¨i le mÃ­ bÇ” zhÃ¨ yÄ« chÄ jÃ¹, zuÃ² zhÄ› tÃ­ chÅ« le yÄ« gÃ¨ mÃ­ng wÃ¨i CodeArena de yÃ¡n gÃ© rÃ©n gÅng biÄn zhÃ¬ jÄ« zhÇ”n cÃ¨ shÃ¬, mÃ³ nÇ zhÄ“n shÃ­ shÃ¬ jiÃ¨ biÄn mÇ rÃ¨n wÃ¹ de fÃº zÃ  xÃ¬ng hÃ© duÅ yÃ ng xÃ¬ng. tÅng guÃ² xÃ¬ tÇ’ng shÃ­ yÃ n, zuÃ² zhÄ› fÄ xiÃ n zÃ i kÄi yuÃ¡n dÃ i mÇ LLMs hÃ© zhuÄn yÇ’u LLMs zhÄ« jiÄn cÃºn zÃ i xiÇn zhÃ¹ de xÃ¬ng nÃ©ng chÄ jÃ¹, qiÃ¡ng diÃ o le rÃ©n lÃ¨i piÄn hÃ o yÄ« zhÃ¬ xÃ¬ng de zhÃ²ng yÃ o xÃ¬ng.
[11.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚
[11.12.2024 09:12] Mistral response. {"id": "da14a96d3c26431f8b76067656d0ad84", "object": "chat.completion", "created": 1733908319, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u8ba8\u8bba', 'pinyin': 't\u01ceo l\u00f9n', 'trans': 'discuss'},\n{'word': '\u4ee3\u7801', 'pinyin': 'd\u00e0i m\u01ce', 'trans': 'code'},\n{'word': '\u5927\u8bed\u8a00\u6a21\u578b', 'pinyin': 'd\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng', 'trans': 'large language model'},\n{'word': '\u8fdb\u5c55', 'pinyin': 'j\u00ecn zh\u01cen', 'trans': 'progress'},\n{'word': '\u4ee5\u524d', 'pinyin': 'y\u01d0 qi\u00e1n', 'trans': 'before'},\n{'word': '\u76f8\u5173', 'pinyin': 'xi\u0101ng gu\u0101n', 'trans': 'related'},\n{'word': '\u57fa\u51c6\u6d4b\u8bd5', 'pinyin': 'j\u012b zh\u01d4n c\u00e8 sh\u00ec', 'trans': 'benchmark test'},\n{'word': '\u4e3b\u8981', 'pinyin': 'zh\u01d4 y\u00e0o', 'trans': 'main'},\n{'word': '\u5173\u6ce8', 'pinyin': 'gu\u0101n zh\u00f9', 'trans': 'focus on'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ng ch\u00e9ng', 'trans': 'generate'},\n{'word': '\u6b63\u786e', 'pinyin': 'zh\u00e8ng qu\u00e8', 'trans': 'correct'},\n{'word': '\u7247\u6bb5', 'pinyin': 'pi\u00e0n du\u00e0n', 'trans': 'segment'},\n{'word': '\u5ffd\u7565', 'pinyin': 'h\u016b l\u00fc\u00e8', 'trans': 'ignore'},\n{'word': '\u4e00\u81f4\u6027', 'pinyin': 'y\u012b zh\u00ec x\u00ecng', 'trans': 'consistency'},\n{'word': '\u5f25\u8865', 'pinyin': 'm\u00ed b\u01d4', 'trans': 'make up for'},\n{'word': '\u5dee\u8ddd', 'pinyin': 'ch\u0101 j\u00f9', 'trans': 'gap'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u4e25\u683c', 'pinyin': 'y\u00e1n g\u00e9', 'trans': 'strict'},\n{'word': '\u4eba\u5de5\u7f16\u5236', 'pinyin': 'r\u00e9n g\u014dng bi\u0101n zh\u00ec', 'trans': 'artificially compiled'},\n{'word': '\u6a21\u62df', 'pinyin': 'm\u00f3 n\u01d0', 'trans': 'simulate'},\n{'word': '\u771f\u5b9e\u4e16\u754c', 'pinyin': 'zh\u0113n sh\u00ed sh\u00ec ji\u00e8', 'trans': 'real world'},\n{'word': '\u590d\u6742\u6027', 'pinyin': 'f\u00f9 z\u00e1 x\u00ecng', 'trans': 'complexity'},\n{'word': '\u591a\u6837\u6027', 'pinyin': 'du\u014d y\u00e0ng x\u00ecng', 'trans': 'diversity'},\n{'word': '\u7cfb\u7edf', 'pinyin': 'x\u00ec t\u01d2ng', 'trans': 'system'},\n{'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00ed y\u00e0n', 'trans': 'experiment'},\n{'word': '\u5f00\u6e90', 'pinyin': 'k\u0101i yu\u00e1n', 'trans': 'open source'},\n{'word': '\u4e13\u6709', 'pinyin': 'zhu\u0101n y\u01d2u', 'trans': 'proprietary'},\n{'word': '\u5b58\u5728', 'pinyin': 'c\u00fan z\u00e0i', 'trans': 'exist'},\n{'word': '\u663e\u8457', 'pinyin': 'xi\u01cen zh\u00f9', 'trans': 'significant'},\n{'word': '\u6027\u80fd', 'pinyin': 'x\u00ecng n\u00e9ng', 'trans': 'performance'},\n{'word': '\u5f3a\u8c03', 'pinyin': 'qi\u00e1ng di\u00e0o', 'trans': 'emphasize'},\n{'word': '\u91cd\u8981\u6027', 'pinyin': 'zh\u00f2ng y\u00e0o x\u00ecng', 'trans': 'importance'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 224, "total_tokens": 1143, "completion_tokens": 919}}
[11.12.2024 09:12] Response: [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},
{'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'},
{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},
{'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'},
{'word': 'ä»¥å‰', 'pinyin': 'yÇ qiÃ¡n', 'trans': 'before'},
{'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'related'},
{'word': 'åŸºå‡†æµ‹è¯•', 'pinyin': 'jÄ« zhÇ”n cÃ¨ shÃ¬', 'trans': 'benchmark test'},
{'word': 'ä¸»è¦', 'pinyin': 'zhÇ” yÃ o', 'trans': 'main'},
{'word': 'å…³æ³¨', 'pinyin': 'guÄn zhÃ¹', 'trans': 'focus on'},
{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'},
{'word': 'æ­£ç¡®', 'pinyin': 'zhÃ¨ng quÃ¨', 'trans': 'correct'},
{'word': 'ç‰‡æ®µ', 'pinyin': 'piÃ n duÃ n', 'trans': 'segment'},
{'word': 'å¿½ç•¥', 'pinyin': 'hÅ« lÃ¼Ã¨', 'trans': 'ignore'},
{'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'},
{'word': 'å¼¥è¡¥', 'pinyin': 'mÃ­ bÇ”', 'trans': 'make up for'},
{'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'},
{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},
{'word': 'ä¸¥æ ¼', 'pinyin': 'yÃ¡n gÃ©', 'trans': 'strict'},
{'word': 'äººå·¥ç¼–åˆ¶', 'pinyin': 'rÃ©n gÅng biÄn zhÃ¬', 'trans': 'artificially compiled'},
{'word': 'æ¨¡æ‹Ÿ', 'pinyin': 'mÃ³ nÇ', 'trans': 'simulate'},
{'word': 'çœŸå®ä¸–ç•Œ', 'pinyin': 'zhÄ“n shÃ­ shÃ¬ jiÃ¨', 'trans': 'real world'},
{'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'},
{'word': 'å¤šæ ·æ€§', 'pinyin': 'duÅ yÃ ng xÃ¬ng', 'trans': 'diversity'},
{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'},
{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},
{'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open source'},
{'word': 'ä¸“æœ‰', 'pinyin': 'zhuÄn yÇ’u', 'trans': 'proprietary'},
{'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'},
{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},
{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},
{'word': 'å¼ºè°ƒ', 'pinyin': 'qiÃ¡ng diÃ o', 'trans': 'emphasize'},
{'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}]
[11.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆcodeLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚ä»¥å‰çš„ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç”Ÿæˆæ­£ç¡®çš„ä»£ç ç‰‡æ®µï¼Œä½†å¿½ç•¥äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºCodeArenaçš„ä¸¥æ ¼äººå·¥ç¼–åˆ¶åŸºå‡†æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œä½œè€…å‘ç°åœ¨å¼€æºä»£ç LLMså’Œä¸“æœ‰LLMsä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†äººç±»åå¥½ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚
[11.12.2024 09:12] Mistral response. {"id": "70cecd1302634b2b93f17dab58d7042f", "object": "chat.completion", "created": 1733908338, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 190, "total_tokens": 307, "completion_tokens": 117}}
[11.12.2024 09:12] Response: This article discusses the advancements in code generation by code large language models (codeLLMs). Previous code-related benchmark tests primarily focused on generating correct code snippets but overlooked consistency with human preferences. To address this gap, the authors propose a rigorous, human-crafted benchmark test called CodeArena, which simulates the complexity and diversity of real-world coding tasks. Through systematic experiments, the authors found a significant performance gap between open-source codeLLMs and proprietary LLMs, highlighting the importance of consistency with human preferences.
[11.12.2024 09:12] Renaming data file.
[11.12.2024 09:12] Renaming previous data. hf_papers.json to ./d/2024-12-11.json
[11.12.2024 09:12] Saving new data file.
[11.12.2024 09:12] Generating page.
[11.12.2024 09:12] Renaming previous page.
[11.12.2024 09:12] Renaming previous data. index.html to ./d/2024-12-11.html
[11.12.2024 09:12] [Experimental] Generating Chinese page for reading.
[11.12.2024 09:12] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇn', 'trans': 'progress'}, {'word': 'ä»¥å‰', 'pinyin': 'yÇ qiÃ¡n', 'trans': 'before'}, {'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'related'}, {'word': 'åŸºå‡†æµ‹è¯•', 'pinyin': 'jÄ« zhÇ”n cÃ¨ shÃ¬', 'trans': 'benchmark test'}, {'word': 'ä¸»è¦', 'pinyin': 'zhÇ” yÃ o', 'trans': 'main'}, {'word': 'å…³æ³¨', 'pinyin': 'guÄn zhÃ¹', 'trans': 'focus on'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'}, {'word': 'æ­£ç¡®', 'pinyin': 'zhÃ¨ng quÃ¨', 'trans': 'correct'}, {'word': 'ç‰‡æ®µ', 'pinyin': 'piÃ n duÃ n', 'trans': 'segment'}, {'word': 'å¿½ç•¥', 'pinyin': 'hÅ« lÃ¼Ã¨', 'trans': 'ignore'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': 'å¼¥è¡¥', 'pinyin': 'mÃ­ bÇ”', 'trans': 'make up for'}, {'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'ä¸¥æ ¼', 'pinyin': 'yÃ¡n gÃ©', 'trans': 'strict'}, {'word': 'äººå·¥ç¼–åˆ¶', 'pinyin': 'rÃ©n gÅng biÄn zhÃ¬', 'trans': 'artificially compiled'}, {'word': 'æ¨¡æ‹Ÿ', 'pinyin': 'mÃ³ nÇ', 'trans': 'simulate'}, {'word': 'çœŸå®ä¸–ç•Œ', 'pinyin': 'zhÄ“n shÃ­ shÃ¬ jiÃ¨', 'trans': 'real world'}, {'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'}, {'word': 'å¤šæ ·æ€§', 'pinyin': 'duÅ yÃ ng xÃ¬ng', 'trans': 'diversity'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open source'}, {'word': 'ä¸“æœ‰', 'pinyin': 'zhuÄn yÇ’u', 'trans': 'proprietary'}, {'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'å¼ºè°ƒ', 'pinyin': 'qiÃ¡ng diÃ o', 'trans': 'emphasize'}, {'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}]
[11.12.2024 09:12] Renaming previous Chinese page.
[11.12.2024 09:12] Renaming previous data. zh.html to ./d/2024-12-10_zh_reading_task.html
[11.12.2024 09:12] Writing Chinese reading task.
[11.12.2024 09:12] Writing result.
[11.12.2024 09:12] Renaming log file.
[11.12.2024 09:12] Renaming previous data. log.txt to ./logs/2024-12-11_last_log.txt
