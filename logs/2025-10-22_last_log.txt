[22.10.2025 10:14] Read previous papers.
[22.10.2025 10:14] Generating top page (month).
[22.10.2025 10:14] Writing top page (month).
[22.10.2025 11:10] Read previous papers.
[22.10.2025 11:10] Get feed.
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18866
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18135
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18701
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16880
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18692
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18876
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18726
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18849
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18855
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17722
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18250
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17519
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18795
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18775
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18873
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17045
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14264
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18554
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18489
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16505
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15600
[22.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17928
[22.10.2025 11:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.10.2025 11:10] No deleted papers detected.
[22.10.2025 11:10] Downloading and parsing papers (pdf, html). Total: 22.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18866.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18866.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18866.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18135.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18135.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18135.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18701.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18701.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18701.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.16880.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.16880.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.16880.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18692.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18692.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18692.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18876.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18876.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18876.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18726.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18726.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18726.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18849.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18849.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18849.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18855.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18855.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18855.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.17722.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.17722.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.17722.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18250.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18250.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18250.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.17519.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.17519.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.17519.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18795.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18795.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18795.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18775.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18775.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18775.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18873.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18873.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18873.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.17045.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.17045.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.17045.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.14264.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.14264.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.14264.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18554.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18554.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18554.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18489.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18489.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18489.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.16505.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.16505.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.16505.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.15600.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.15600.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.15600.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.17928.
[22.10.2025 11:10] Extra JSON file exists (./assets/json/2510.17928.json), skip PDF parsing.
[22.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.17928.json), skip HTML parsing.
[22.10.2025 11:10] Success.
[22.10.2025 11:10] Enriching papers with extra data.
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 0. LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  					AI-generated summary 				 Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effective...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 1. World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.  					AI-generated summary 				 Generative world models (WMs) can now simulate worlds with str...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 2. UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  					AI-generated summary 				 Recent progress in text-to-image (T2I) generation...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 3. Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  					AI-generated summary 				 Although large language models (LLMs) have significant potential to advance chemical d...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 4. Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  					AI-generated summary 				 Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full ...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 5. Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  					AI-generated summary 				 While Multimodal Large Language Models (...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 6. A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) have demonstrated...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 7. A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.  					AI-generated summary 				 Faithfully personalizing large language models (LLMs) to align with individu...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 8. Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  					AI-generated summary 				 We present Ring-1T, the first open-source, state-of-...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 9. MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  					AI-generated summary 				 The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. H...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 10. ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critic...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 11. A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.  					AI-generated summary 			...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 12. ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.  					AI-generated summary 				 The original CLIP text encoder is limited by a maximum input lengt...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 13. UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  					AI-generated summary 				 Recent advances in video generation have made it possible to produce visua...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 14. DSI-Bench evaluates the dynamic spatial reasoning capabilities of vision-language and visual expertise models through a benchmark of dynamic videos and annotated questions, highlighting their limitations in understanding self-motion, object motion, and relative relationships.  					AI-generated summ...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 15. The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.  					AI-generated summary 				 Video reasoning usin...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 16. AlphaQuanter, a single-agent framework using reinforcement learning, achieves top performance in automated trading by learning dynamic policies and proactively acquiring information.  					AI-generated summary 				 While Large Language Model (LLM) agents show promise in automated trading, they still...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 17. Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices.  					AI-generated summary 				 In this work, we show that it is possible to extract significant amounts of alignment training d...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 18. A system for reconstructing 4D HDR scenes from unposed LDR videos using Gaussian Splatting with two-stage optimization and temporal luminance regularization.  					AI-generated summary 				 We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes f...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 19. PRISMM-Bench evaluates the ability of large multimodal models to detect, correct, and reason over inconsistencies in scientific papers, revealing significant challenges in multimodal scientific reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) are increasingly applied to scie...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 20. Thoth, a large language model trained with the Sketch-and-Fill paradigm and structured component-based reward mechanism, generates more reliable and executable scientific protocols compared to existing models.  					AI-generated summary 				 The foundation of reproducible science lies in protocols t...
[22.10.2025 11:10] ********************************************************************************
[22.10.2025 11:10] Abstract 21. An evolutionary framework synthesizes verifiable data for language models, improving reinforcement learning and distillation across various tasks.  					AI-generated summary 				 Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforce...
[22.10.2025 11:10] Read previous papers.
[22.10.2025 11:10] Generating reviews via LLM API.
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#data", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è AI: –±—ã—Å—Ç—Ä–µ–µ, —Ç–æ—á–Ω–µ–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ", "desc": "LightMem ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è LLM, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –ê—Ç–∫–∏–Ω—Å–æ–Ω–∞-–®–∏—Ñ—Ñ—Ä–∏–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –æ—Ä
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#games", "#benchmark", "#optimization", "#dataset", "#agents"], "emoji": "üåç", "ru": {"title": "–ö–æ–≥–¥–∞ –∫—Ä–∞—Å–∏–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–æ–±–æ—Ç—É: –≤–∞–∂–Ω–∞ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å, –∞ –Ω–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É World-in-World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö world models –≤ –∑
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#science", "#multilingual", "#multimodal", "#benchmark", "#survey"], "emoji": "üé®", "ru": {"title": "–í—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "UniGenBench++ ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ text-to-image –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#science", "#reasoning", "#architecture", "#benchmark", "#optimization", "#interpretability", "#training"], "emoji": "‚öóÔ∏è", "ru": {"title": "Chem-R: LLM, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç –∫–∞–∫ —Ö–∏–º–∏–∫", "desc": "Chem-R - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è —Ö–∏–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ–±—É—á–µ–Ω–Ω–∞—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –¥–ª—è —Ä–µ—à
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#video", "#architecture", "#diffusion", "#training", "#long_context"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mixture-of-Groups Attention (MoGA) ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é Diffus
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#games", "#benchmark", "#cv"], "emoji": "üîç", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ª—é–±—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å GAR (Grasp Any Region), –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö,
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤–∞–∂–Ω–µ–µ –ø–æ–ª–Ω–æ—Ç—ã –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ IF-VidCap –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏–Ω—Å
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Critique-Post-Edit –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#optimization", "#training", "#open_source"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∏–ª–ª–∏–æ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö: –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –º–æ—â–Ω–æ–≥–æ AI-–º—ã—à–ª–µ–Ω–∏—è", "desc": "Ring-1T ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è thinking-–º–æ–¥–µ–ª—å —Å —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#multimodal", "#science", "#video", "#benchmark", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MT-Video-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#training", "#data", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤: —Å–∞–º–æ–º–æ–¥—É–ª—è—Ü–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ssToken - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è supervised fine-tuning –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#video", "#architecture", "#data", "#training", "#optimization", "#open_source"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ–≥—Ä–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–ø—Ç–∏–º–∏–∑
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#alignment", "#training", "#multimodal", "#long_context"], "emoji": "üéì", "ru": {"title": "–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ CLIP —Å LLM —á–µ—Ä–µ–∑ curriculum learning", "desc": "ProCLIP —É–ª—É—á—à–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ CLIP, –∑–∞–º–µ–Ω—è—è –µ—ë —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#video", "#games", "#optimization", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ù–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ 4K —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "UltraGen - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –≤–ø–µ—Ä–≤—ã–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤ –≤—ã—Å–æ–∫–æ–º —Ä–∞
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#cv", "#reasoning", "#benchmark", "#3d"], "emoji": "üé•", "ru": {"title": "–ö–æ–≥–¥–∞ AI —Ç–µ—Ä—è–µ—Ç—Å—è –≤ –¥–≤–∏–∂–µ–Ω–∏–∏: —Ç–µ—Å—Ç –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DSI-Bench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è AI-–º–æ–¥–µ–ª–µ–π –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –î–∞—Ç–∞—Å–µ—Ç 
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#reasoning", "#video", "#training", "#multimodal", "#inference", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç V-Reason ‚Äî –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Large Multimodal Models –Ω–∞–¥ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#games", "#optimization", "#open_source", "#rl", "#training", "#agents", "#interpretability"], "emoji": "üìà", "ru": {"title": "–û–¥–∏–Ω —É–º–Ω—ã–π –∞–≥–µ–Ω—Ç –≤–º–µ—Å—Ç–æ —Ö–∞–æ—Å–∞: RL-—Ç—Ä–µ–π–¥–µ—Ä —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ–π –ª–æ–≥–∏–∫–æ–π", "desc": "AlphaQuanter ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω–æ–≥–æ AI-–∞
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#hallucinations", "#rlhf", "#long_context", "#training", "#alignment", "#data"], "emoji": "üîì", "ru": {"title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: —Å–∫—Ä—ã—Ç—ã–µ —Ä–∏—Å–∫–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏–∑ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä—ë
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#cv"], "emoji": "üé•", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 4D HDR —Å—Ü–µ–Ω –∏–∑ LDR –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ–∑ –∫–∞–º–µ—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Mono4DGS-HDR –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 4D HDR —Å—Ü–µ–Ω –∏–∑ –Ω–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö LDR –≤–∏–¥–µ–æ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian 
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning", "#science"], "emoji": "üî¨", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ AI –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –≤ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ PRISMM-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞—Ö–æ–¥–∏—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–æ—Ç–∏–≤–æ
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#open_source", "#optimization", "#data", "#science", "#benchmark", "#agents"], "emoji": "üß™", "ru": {"title": "–ù–∞—É—á–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ç –∏–¥–µ–∏ –¥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Thoth ‚Äî –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã—Ö –Ω–∞—É—á–Ω—ã—Ö
[22.10.2025 11:10] Using data from previous issue: {"categories": ["#hallucinations", "#synthetic", "#training", "#reinforcement_learning", "#agents", "#data", "#dataset", "#rl"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ 
[22.10.2025 11:10] Renaming data file.
[22.10.2025 11:10] Renaming previous data. hf_papers.json to ./d/2025-10-22.json
[22.10.2025 11:10] Saving new data file.
[22.10.2025 11:10] Generating page.
[22.10.2025 11:10] Renaming previous page.
[22.10.2025 11:10] Renaming previous data. index.html to ./d/2025-10-22.html
[22.10.2025 11:10] Writing result.
[22.10.2025 11:10] Renaming log file.
[22.10.2025 11:10] Renaming previous data. log.txt to ./logs/2025-10-22_last_log.txt
