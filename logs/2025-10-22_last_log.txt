[22.10.2025 11:10] Read previous papers.
[22.10.2025 11:10] Generating top page (month).
[22.10.2025 11:10] Writing top page (month).
[22.10.2025 12:23] Read previous papers.
[22.10.2025 12:23] Get feed.
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18866
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18135
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18701
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16880
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18692
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18876
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18726
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18849
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18855
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17722
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18250
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17519
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18795
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18775
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18873
[22.10.2025 12:23] Extract page data from URL. URL: https://huggingface.co/papers/2510.18019
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17045
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16505
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14264
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18554
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18489
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15600
[22.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17928
[22.10.2025 12:23] Extract page data from URL. URL: https://huggingface.co/papers/2510.15862
[22.10.2025 12:23] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.10.2025 12:23] No deleted papers detected.
[22.10.2025 12:23] Downloading and parsing papers (pdf, html). Total: 24.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18866.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18866.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18866.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18135.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18135.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18135.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18701.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18701.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18701.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.16880.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.16880.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.16880.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18692.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18692.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18692.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18876.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18876.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18876.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18726.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18726.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18726.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18849.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18849.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18849.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18855.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18855.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18855.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.17722.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.17722.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.17722.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18250.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18250.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18250.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.17519.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.17519.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.17519.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18795.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18795.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18795.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18775.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18775.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18775.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18873.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18873.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18873.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18019.
[22.10.2025 12:23] Downloading paper 2510.18019 from http://arxiv.org/pdf/2510.18019v1...
[22.10.2025 12:23] Extracting affiliations from text.
[22.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Is Multilingual LLM Watermarking Truly Multilingual? Simple Back-Translation Solution Asim Mohamed African Institute for Mathematical Sciences amohamed@aimsammi.org Martin Gubri Parameter Lab martin.gubri@parameterlab.de 5 2 0 2 0 2 ] . [ 1 9 1 0 8 1 . 0 1 5 2 : r a "
[22.10.2025 12:23] Response: ```python
["African Institute for Mathematical Sciences", "Parameter Lab"]
```
[22.10.2025 12:23] Deleting PDF ./assets/pdf/2510.18019.pdf.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.17045.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.17045.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.17045.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.16505.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.16505.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.16505.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.14264.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.14264.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.14264.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18554.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18554.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18554.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.18489.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.18489.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.18489.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.15600.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.15600.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.15600.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.17928.
[22.10.2025 12:23] Extra JSON file exists (./assets/json/2510.17928.json), skip PDF parsing.
[22.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.17928.json), skip HTML parsing.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.15862.
[22.10.2025 12:23] Downloading paper 2510.15862 from http://arxiv.org/pdf/2510.15862v3...
[22.10.2025 12:23] Extracting affiliations from text.
[22.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold Yi Wan*, Jiuqi Wang*, Liam Li, Jinsong Liu, Ruihao Zhu, Zheqing Zhu October 22, 2025 Abstract Tool-augmented large language models (LLMs) are emerging as deep research agentssystems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, 7B-parameter deep research agent built under unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. chainof-thoughtdriven multi-call reasoning scaffold further enhances robustness through adaptive recovery from tool failures and self-verification on generated answers. At test time, multiple research threads are executed independently and synthesized to produce the best answer. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under Apache 2.0 license at https://github.com/Pokee-AI/PokeeResearchOSS. 5 2 0 2 1 2 ] A . [ 3 2 6 8 5 1 . 0 1 5 2 : r Figure 1: Performance on HLE, GAIA and BrowseComp among 7B-scale deep research models. With their ability to use tools for complex, open-domain tasks, tool-augmented large language models (LLMs) are increasingly deployed as deep research agents, i.e., systems capable of gathering, reasoning over, and 1 Figure 2: Performance on 7 QA Benchmarks among 7B-scale deep research models. synthesizing evid"
[22.10.2025 12:23] Response: ```python
[]
```
[22.10.2025 12:23] Extracting affiliations from text.
[22.10.2025 12:23] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold Yi Wan*, Jiuqi Wang*, Liam Li, Jinsong Liu, Ruihao Zhu, Zheqing ZhuOctober 22, 2025 Abstract Tool-augmented large language models (LLMs) are emerging as deep research agentssystems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, 7B-parameter deep research agent built under unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. chainof-thoughtdriven multi-call reasoning scaffold further enhances robustness through adaptive recovery from tool failures and self-verification on generated answers. At test time, multiple research threads are executed independently and synthesized to produce the best answer. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under Apache 2.0 license at https://github.com/Pokee-AI/PokeeResearchOSS. 5 2 0 2 1 2 ] A . [ 3 2 6 8 5 1 . 0 1 5 2 : r Figure 1: Performance on HLE, GAIA and BrowseComp among 7B-scale deep research models.With their ability to use tools for complex, open-domain tasks, tool-augmented large language models (LLMs) are increasingly deployed as deep research agents, i.e., systems capable of gathering, reasoning over, and 1 Figure 2: Performance on 7 QA Benchmarks among 7B-scale deep research models. synthesizing evidence from diverse sources to provide well-grounded answers to sophisticated queries. typical deep research workflow requires an agent to decompose the users query, identify relevant external information, and synthesize it into coherent, verifiable response. Recent progress in deep research agents has been driven by two parallel directions: (1) the development of smaller, cost-efficient LLM backbones suitable for scalable deployment [SJM+25, JZY+25, SQG+25, GFX+25, ZFH+25, WZA+25]; and (2) the design of training recipes that interleave reasoning with external actions [TT25, Tea25, LJD+25]. Despite these advances, existing agents remain constrained by several key limitations. First, most training pipelines still depend on token-overlap metrics such as F1 or ROUGE [WZA+25, DCL+25, MHF+25, LZY+25], which correlate weakly with human judgments of usefulness, factual grounding, and instruction adherence. Moreover, current agents exhibit brittle tool-use behavior: single malformed function call, fetch error, or transient API failure can derail an entire trajectory, with little opportunity for self-correction [SWY+25, XHJ+25, SMPN25]. In addition, plausible research threads can miss critical information, producing misleading inferences and therefore incorrect answers, especially for challenging questions. To address these challenges, we introduce PokeeResearch-7B, 7-billion-parameter deep research agent developed under unified reinforcement learning framework optimized for reliability, human alignment, and practical deployment. Our contributions center on three core innovations: Human Value-Driven Training Pipeline: We enhance agent alignment with Reinforcement Learning from AI Feedback (RLAIF) instantiated with the REINFORCE Leave-One-Out (RLOO) algorithm [KvHW19, ACG+24]. RLAIF provides rich, LLM-based reward signals calibrated to human values, including factual accuracy, citation faithfulness, and instruction compliance. RLOO offers unbiased gradient estimation in an on-policy fashion for policy improvement. This algorithm distinguishes itself from algorithms that are only approximately on-policy and follow biased gradient direction, such as the PPO family of algorithms. Robust Reasoning Scaffold: To enhance the robustness of our deep research agent, we employ three complementary techniques. Self-correction: Rather than blindly executing potentially malformed tool calls, the agent proactively diagnoses errors and suggests corrections. Self-verification: The agent incorporates an answer verification step that filters out easily detectable false outputs. Research threads synthesis: To improve answer accuracy on challenging questions at test time, the agent launches multiple independent 2 research threads in parallel. These research threads are jointly analyzed and synthesized by the agent to produce the final answer. Together, these design choices yield deep research agent that (1) optimizes directly for human-salient dimensions of answer quality rather than surface overlap; and (2) maintains robustness across imperfect models through adaptive, verifiable reasoning. At 7B scale, PokeeResearch-7B demonstrates that carefully engineered reinforcement learning and reasoning scaffolds can produce research-grade agents that are both cost-efficient and resilient in open-domain settings. We show in 10 benchmarks that PokeeResearch-7B presents state-of-the-art performance among 7B-size deep research agents.In this section, we give review of existing benchmarks for deep research and summarize recent progress in building deep research agents. Information Seeking Benchmarks. The development of sophisticated information retrieval agents has necessitated increasingly complex evaluation benchmarks. Traditional datasets such as Natural Questions [KPR+19] and TriviaQA [JCWZ17] primarily assess single-hop reasoning, which can often be addressed using parametric knowledge or simple query sequences. Their multi-step extensions, including HotpotQA [YQZ+18] and Musique [TBKS22], evaluate structured multi-hop reasoning but remain insufficiently challenging for advanced agents. Recent evaluation frameworks have evolved to present more nuanced challenges: GAIA [MFW+23] introduces real-world complexity requiring sophisticated reasoning chains, while WebWalkerQA [WYJ+25] emphasizes dynamic web navigation skills. The BrowseComp suite [WSP+25, ZLY+25] establishes standardized metrics for web browsing competency across multiple languages. At the frontier o"
[22.10.2025 12:23] Mistral response. {"id": "ad33e3c916e546899611723a2c635101", "created": 1761135812, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1430, "total_tokens": 1441, "completion_tokens": 11}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Pokee-AI\"]\n```"}}]}
[22.10.2025 12:23] Response: ```python
["Pokee-AI"]
```
[22.10.2025 12:23] Deleting PDF ./assets/pdf/2510.15862.pdf.
[22.10.2025 12:23] Success.
[22.10.2025 12:23] Enriching papers with extra data.
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 0. LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  					AI-generated summary 				 Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effective...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 1. World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.  					AI-generated summary 				 Generative world models (WMs) can now simulate worlds with str...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 2. UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  					AI-generated summary 				 Recent progress in text-to-image (T2I) generation...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 3. Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  					AI-generated summary 				 Although large language models (LLMs) have significant potential to advance chemical d...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 4. Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  					AI-generated summary 				 Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full ...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 5. Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  					AI-generated summary 				 While Multimodal Large Language Models (...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 6. A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) have demonstrated...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 7. A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.  					AI-generated summary 				 Faithfully personalizing large language models (LLMs) to align with individu...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 8. Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  					AI-generated summary 				 We present Ring-1T, the first open-source, state-of-...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 9. MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  					AI-generated summary 				 The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. H...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 10. ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critic...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 11. A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.  					AI-generated summary 			...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 12. ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.  					AI-generated summary 				 The original CLIP text encoder is limited by a maximum input lengt...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 13. UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  					AI-generated summary 				 Recent advances in video generation have made it possible to produce visua...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 14. DSI-Bench evaluates the dynamic spatial reasoning capabilities of vision-language and visual expertise models through a benchmark of dynamic videos and annotated questions, highlighting their limitations in understanding self-motion, object motion, and relative relationships.  					AI-generated summ...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 15. STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.  					AI-generated summary 				 Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, y...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 16. The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.  					AI-generated summary 				 Video reasoning usin...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 17. PRISMM-Bench evaluates the ability of large multimodal models to detect, correct, and reason over inconsistencies in scientific papers, revealing significant challenges in multimodal scientific reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) are increasingly applied to scie...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 18. AlphaQuanter, a single-agent framework using reinforcement learning, achieves top performance in automated trading by learning dynamic policies and proactively acquiring information.  					AI-generated summary 				 While Large Language Model (LLM) agents show promise in automated trading, they still...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 19. Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices.  					AI-generated summary 				 In this work, we show that it is possible to extract significant amounts of alignment training d...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 20. A system for reconstructing 4D HDR scenes from unposed LDR videos using Gaussian Splatting with two-stage optimization and temporal luminance regularization.  					AI-generated summary 				 We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes f...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 21. Thoth, a large language model trained with the Sketch-and-Fill paradigm and structured component-based reward mechanism, generates more reliable and executable scientific protocols compared to existing models.  					AI-generated summary 				 The foundation of reproducible science lies in protocols t...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 22. An evolutionary framework synthesizes verifiable data for language models, improving reinforcement learning and distillation across various tasks.  					AI-generated summary 				 Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforce...
[22.10.2025 12:23] ********************************************************************************
[22.10.2025 12:23] Abstract 23. PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.  					AI-generated summary 				 Tool-augmented large language models (LLMs) are emerging as deep research agent...
[22.10.2025 12:23] Read previous papers.
[22.10.2025 12:23] Generating reviews via LLM API.
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#data", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è AI: –±—ã—Å—Ç—Ä–µ–µ, —Ç–æ—á–Ω–µ–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ", "desc": "LightMem ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è LLM, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –ê—Ç–∫–∏–Ω—Å–æ–Ω–∞-–®–∏—Ñ—Ñ—Ä–∏–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –æ—Ä
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#games", "#benchmark", "#optimization", "#dataset", "#agents"], "emoji": "üåç", "ru": {"title": "–ö–æ–≥–¥–∞ –∫—Ä–∞—Å–∏–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–æ–±–æ—Ç—É: –≤–∞–∂–Ω–∞ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å, –∞ –Ω–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É World-in-World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö world models –≤ –∑
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#science", "#multilingual", "#multimodal", "#benchmark", "#survey"], "emoji": "üé®", "ru": {"title": "–í—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "UniGenBench++ ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ text-to-image –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#dataset", "#science", "#reasoning", "#architecture", "#benchmark", "#optimization", "#interpretability", "#training"], "emoji": "‚öóÔ∏è", "ru": {"title": "Chem-R: LLM, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç –∫–∞–∫ —Ö–∏–º–∏–∫", "desc": "Chem-R - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è —Ö–∏–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ–±—É—á–µ–Ω–Ω–∞—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –¥–ª—è —Ä–µ—à
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#video", "#architecture", "#diffusion", "#training", "#long_context"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mixture-of-Groups Attention (MoGA) ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é Diffus
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#games", "#benchmark", "#cv"], "emoji": "üîç", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ª—é–±—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å GAR (Grasp Any Region), –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö,
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤–∞–∂–Ω–µ–µ –ø–æ–ª–Ω–æ—Ç—ã –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ IF-VidCap –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏–Ω—Å
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Critique-Post-Edit –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#optimization", "#training", "#open_source"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∏–ª–ª–∏–æ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö: –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –º–æ—â–Ω–æ–≥–æ AI-–º—ã—à–ª–µ–Ω–∏—è", "desc": "Ring-1T ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è thinking-–º–æ–¥–µ–ª—å —Å —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#multimodal", "#science", "#video", "#benchmark", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MT-Video-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#training", "#data", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤: —Å–∞–º–æ–º–æ–¥—É–ª—è—Ü–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ssToken - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è supervised fine-tuning –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#video", "#architecture", "#data", "#training", "#optimization", "#open_source"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ–≥—Ä–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–ø—Ç–∏–º–∏–∑
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#dataset", "#alignment", "#training", "#multimodal", "#long_context"], "emoji": "üéì", "ru": {"title": "–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ CLIP —Å LLM —á–µ—Ä–µ–∑ curriculum learning", "desc": "ProCLIP —É–ª—É—á—à–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ CLIP, –∑–∞–º–µ–Ω—è—è –µ—ë —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#video", "#games", "#optimization", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ù–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ 4K —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "UltraGen - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –≤–ø–µ—Ä–≤—ã–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤ –≤—ã—Å–æ–∫–æ–º —Ä–∞
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#cv", "#reasoning", "#benchmark", "#3d"], "emoji": "üé•", "ru": {"title": "–ö–æ–≥–¥–∞ AI —Ç–µ—Ä—è–µ—Ç—Å—è –≤ –¥–≤–∏–∂–µ–Ω–∏–∏: —Ç–µ—Å—Ç –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DSI-Bench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è AI-–º–æ–¥–µ–ª–µ–π –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –î–∞—Ç–∞—Å–µ—Ç 
[22.10.2025 12:23] Querying the API.
[22.10.2025 12:23] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.  					AI-generated summary 				 Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.
[22.10.2025 12:23] Response: ```json
{
  "title": "–í–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –¥–ª—è LLM —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –≤—Å–µ—Ö —è–∑—ã–∫–∞—Ö —Å –æ–±—Ä–∞—Ç–Ω—ã–º –ø–µ—Ä–µ–≤–æ–¥–æ–º",
  "desc": "–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –¥–ª—è LLM —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, –Ω–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —Ç–µ—Ä—è—é—Ç –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –ø—Ä–∏ –∞—Ç–∞–∫–∞—Ö —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º –Ω–∞ —è–∑—ã–∫–∏ —Å–æ —Å—Ä–µ–¥–Ω–∏–º–∏ –∏ –Ω–∏–∑–∫–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ü—Ä–æ–±–ª–µ–º–∞ –∫—Ä–æ–µ—Ç—Å—è –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–≥–¥–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –º–∞–ª–æ –ø–æ–ª–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ú–µ—Ç–æ–¥ STEAM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Ö –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞. –†–µ—à–µ–Ω–∏–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –ª—é–±—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤, —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ 17 —è–∑—ã–∫–∞—Ö –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—É—é –∑–∞—â–∏—Ç—É —Ç–µ–∫—Å—Ç–æ–≤ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —è–∑—ã–∫–∞.",
  "emoji": "üåê",
  "watermark_strength": "0.19"
}
```
[22.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.  					AI-generated summary 				 Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages."

[22.10.2025 12:23] Response: ```python
['MULTILINGUAL', 'DATASET']
```
[22.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.  					AI-generated summary 				 Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages."

[22.10.2025 12:23] Response: ```python
['WATERMARKING', 'LOW_RESOURCE']
```
[22.10.2025 12:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces STEAM, a new method for improving the robustness of multilingual watermarking in AI-generated text. Current watermarking techniques struggle with medium- and low-resource languages, often failing due to issues with semantic clustering and limited tokenizer vocabularies. STEAM uses back-translation to enhance the detection of watermarks, ensuring they remain effective even after translation. The method shows significant performance improvements across 17 languages, making watermarking more reliable and fair in diverse linguistic contexts.","title":"STEAM: Strengthening Multilingual Watermarking with Back-Translation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces STEAM, a new method for improving the robustness of multilingual watermarking in AI-generated text. Current watermarking techniques struggle with medium- and low-resource languages, often failing due to issues with semantic clustering and limited tokenizer vocabularies. STEAM uses back-translation to enhance the detection of watermarks, ensuring they remain effective even after translation. The method shows significant performance improvements across 17 languages, making watermarking more reliable and fair in diverse linguistic contexts.', title='STEAM: Strengthening Multilingual Watermarking with Back-Translation'))
[22.10.2025 12:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"STEAMÊòØ‰∏ÄÁßçÂü∫‰∫éÂõûËØëÁöÑÊ£ÄÊµãÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∫Â§öËØ≠Ë®ÄÊ∞¥Âç∞ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÁé∞ÊúâÁöÑÂ§öËØ≠Ë®ÄÊ∞¥Âç∞ÊñπÊ≥ïÂú®‰∏≠‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äË°®Áé∞‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ËØ≠‰πâËÅöÁ±ªÂ§±Ë¥•„ÄÇSTEAMÈÄöËøáÊÅ¢Â§çÁøªËØëËøáÁ®ã‰∏≠‰∏¢Â§±ÁöÑÊ∞¥Âç∞Âº∫Â∫¶ÔºåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ï‰∏é‰ªª‰ΩïÊ∞¥Âç∞ÊäÄÊúØÂÖºÂÆπÔºåÈÄÇÁî®‰∫é‰∏çÂêåÁöÑÂàÜËØçÂô®ÂíåËØ≠Ë®ÄÔºå‰∏îÊòì‰∫éÊâ©Â±ïÂà∞Êñ∞ËØ≠Ë®Ä„ÄÇ","title":"STEAMÔºöÊèêÂçáÂ§öËØ≠Ë®ÄÊ∞¥Âç∞È≤ÅÊ£íÊÄßÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='STEAMÊòØ‰∏ÄÁßçÂü∫‰∫éÂõûËØëÁöÑÊ£ÄÊµãÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∫Â§öËØ≠Ë®ÄÊ∞¥Âç∞ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÁé∞ÊúâÁöÑÂ§öËØ≠Ë®ÄÊ∞¥Âç∞ÊñπÊ≥ïÂú®‰∏≠‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏äË°®Áé∞‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ËØ≠‰πâËÅöÁ±ªÂ§±Ë¥•„ÄÇSTEAMÈÄöËøáÊÅ¢Â§çÁøªËØëËøáÁ®ã‰∏≠‰∏¢Â§±ÁöÑÊ∞¥Âç∞Âº∫Â∫¶ÔºåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ï‰∏é‰ªª‰ΩïÊ∞¥Âç∞ÊäÄÊúØÂÖºÂÆπÔºåÈÄÇÁî®‰∫é‰∏çÂêåÁöÑÂàÜËØçÂô®ÂíåËØ≠Ë®ÄÔºå‰∏îÊòì‰∫éÊâ©Â±ïÂà∞Êñ∞ËØ≠Ë®Ä„ÄÇ', title='STEAMÔºöÊèêÂçáÂ§öËØ≠Ë®ÄÊ∞¥Âç∞È≤ÅÊ£íÊÄßÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#reasoning", "#video", "#training", "#multimodal", "#inference", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç V-Reason ‚Äî –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Large Multimodal Models –Ω–∞–¥ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning", "#science"], "emoji": "üî¨", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ AI –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –≤ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ PRISMM-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞—Ö–æ–¥–∏—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–æ—Ç–∏–≤–æ
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#games", "#optimization", "#open_source", "#rl", "#training", "#agents", "#interpretability"], "emoji": "üìà", "ru": {"title": "–û–¥–∏–Ω —É–º–Ω—ã–π –∞–≥–µ–Ω—Ç –≤–º–µ—Å—Ç–æ —Ö–∞–æ—Å–∞: RL-—Ç—Ä–µ–π–¥–µ—Ä —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ–π –ª–æ–≥–∏–∫–æ–π", "desc": "AlphaQuanter ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω–æ–≥–æ AI-–∞
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#hallucinations", "#rlhf", "#long_context", "#training", "#alignment", "#data"], "emoji": "üîì", "ru": {"title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: —Å–∫—Ä—ã—Ç—ã–µ —Ä–∏—Å–∫–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏–∑ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä—ë
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#cv"], "emoji": "üé•", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 4D HDR —Å—Ü–µ–Ω –∏–∑ LDR –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ–∑ –∫–∞–º–µ—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Mono4DGS-HDR –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 4D HDR —Å—Ü–µ–Ω –∏–∑ –Ω–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö LDR –≤–∏–¥–µ–æ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian 
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#dataset", "#training", "#open_source", "#optimization", "#data", "#science", "#benchmark", "#agents"], "emoji": "üß™", "ru": {"title": "–ù–∞—É—á–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ç –∏–¥–µ–∏ –¥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Thoth ‚Äî –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã—Ö –Ω–∞—É—á–Ω—ã—Ö
[22.10.2025 12:23] Using data from previous issue: {"categories": ["#hallucinations", "#synthetic", "#training", "#reinforcement_learning", "#agents", "#data", "#dataset", "#rl"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ 
[22.10.2025 12:23] Querying the API.
[22.10.2025 12:23] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.  					AI-generated summary 				 Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS.
[22.10.2025 12:23] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PokeeResearch-7B ‚Äî –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π AI-–∞–≥–µ–Ω—Ç —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reinforcement learning –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∏ –¥–∞–Ω–Ω—ã–º–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é RLAIF (–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç AI) –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç chain-of-thought —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≤—ã–∑–æ–≤–∞–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–æ–π –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫. –ù–∞ 10 –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π –º–∞—Å—à—Ç–∞–±–∞ 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.",
  "emoji": "üî¨",
  "title": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–≥–µ–Ω—Ç –Ω–∞ 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ–ø–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ reinforcement learning"
}
```
[22.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.  					AI-generated summary 				 Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS."

[22.10.2025 12:23] Response: ```python
['AGENTS', 'RL', 'RLHF', 'BENCHMARK', 'TRAINING']
```
[22.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.  					AI-generated summary 				 Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at https://github.com/Pokee-AI/PokeeResearchOSS."

[22.10.2025 12:23] Response: ```python
['AGI', 'ALIGNMENT', 'REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[22.10.2025 12:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PokeeResearch-7B is a 7 billion parameter deep research agent that utilizes reinforcement learning and chain-of-thought reasoning to improve its performance and reliability. It addresses limitations in existing AI agents by employing a novel Reinforcement Learning from AI Feedback (RLAIF) approach, which enhances factual accuracy and adherence to instructions without requiring extensive annotations. The model\'s multi-call reasoning framework allows it to verify its own outputs and recover from errors, making it more robust in handling complex queries. With state-of-the-art results on various benchmarks, PokeeResearch-7B demonstrates the effectiveness of combining advanced reinforcement learning techniques with structured reasoning in AI research applications.","title":"Empowering Research with Robust AI: Introducing PokeeResearch-7B"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="PokeeResearch-7B is a 7 billion parameter deep research agent that utilizes reinforcement learning and chain-of-thought reasoning to improve its performance and reliability. It addresses limitations in existing AI agents by employing a novel Reinforcement Learning from AI Feedback (RLAIF) approach, which enhances factual accuracy and adherence to instructions without requiring extensive annotations. The model's multi-call reasoning framework allows it to verify its own outputs and recover from errors, making it more robust in handling complex queries. With state-of-the-art results on various benchmarks, PokeeResearch-7B demonstrates the effectiveness of combining advanced reinforcement learning techniques with structured reasoning in AI research applications.", title='Empowering Research with Robust AI: Introducing PokeeResearch-7B'))
[22.10.2025 12:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PokeeResearch-7B ÊòØ‰∏Ä‰∏™Êã•Êúâ 70 ‰∫øÂèÇÊï∞ÁöÑÊ∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÔºåÈááÁî®Âº∫ÂåñÂ≠¶‰π†ÂíåÊÄùÁª¥ÈìæÊé®ÁêÜÊù•ÊèêÈ´òÂÖ∂È≤ÅÊ£íÊÄßÂíåÂØπÈΩêÊÄß„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊó†Ê≥®ÈáäÁöÑ AI ÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ËøõË°åËÆ≠ÁªÉÔºå‰ºòÂåñÁ≠ñÁï•‰ª•ÊçïÊçâ‰∫ãÂÆûÂáÜÁ°ÆÊÄßÂíåÂºïÁî®Âø†ÂÆûÂ∫¶„ÄÇÂÆÉËøòÈÄöËøáÂ§öÊ¨°Êé®ÁêÜÁöÑÊÄùÁª¥ÈìæÈ©±Âä®ÔºåÂ¢ûÂº∫‰∫ÜËá™ÊàëÈ™åËØÅÂíå‰ªéÂ∑•ÂÖ∑Â§±Ë¥•‰∏≠Ëá™ÈÄÇÂ∫îÊÅ¢Â§çÁöÑËÉΩÂäõ„ÄÇPokeeResearch-7B Âú® 10 ‰∏™ÊµÅË°åÁöÑÊ∑±Â∫¶Á†îÁ©∂Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÁ≤æÂøÉËÆæËÆ°ÁöÑÂº∫ÂåñÂ≠¶‰π†ÂíåÊé®ÁêÜÂèØ‰ª•‰∫ßÁîüÈ´òÊïà‰∏îÂÖ∑ÊúâÁ†îÁ©∂Á∫ßÂà´ÁöÑ AI ‰ª£ÁêÜ„ÄÇ","title":"Âº∫ÂåñÂ≠¶‰π†‰∏éÊé®ÁêÜËÆæËÆ°ÁöÑÂÆåÁæéÁªìÂêà"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PokeeResearch-7B ÊòØ‰∏Ä‰∏™Êã•Êúâ 70 ‰∫øÂèÇÊï∞ÁöÑÊ∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÔºåÈááÁî®Âº∫ÂåñÂ≠¶‰π†ÂíåÊÄùÁª¥ÈìæÊé®ÁêÜÊù•ÊèêÈ´òÂÖ∂È≤ÅÊ£íÊÄßÂíåÂØπÈΩêÊÄß„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊó†Ê≥®ÈáäÁöÑ AI ÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ËøõË°åËÆ≠ÁªÉÔºå‰ºòÂåñÁ≠ñÁï•‰ª•ÊçïÊçâ‰∫ãÂÆûÂáÜÁ°ÆÊÄßÂíåÂºïÁî®Âø†ÂÆûÂ∫¶„ÄÇÂÆÉËøòÈÄöËøáÂ§öÊ¨°Êé®ÁêÜÁöÑÊÄùÁª¥ÈìæÈ©±Âä®ÔºåÂ¢ûÂº∫‰∫ÜËá™ÊàëÈ™åËØÅÂíå‰ªéÂ∑•ÂÖ∑Â§±Ë¥•‰∏≠Ëá™ÈÄÇÂ∫îÊÅ¢Â§çÁöÑËÉΩÂäõ„ÄÇPokeeResearch-7B Âú® 10 ‰∏™ÊµÅË°åÁöÑÊ∑±Â∫¶Á†îÁ©∂Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÁ≤æÂøÉËÆæËÆ°ÁöÑÂº∫ÂåñÂ≠¶‰π†ÂíåÊé®ÁêÜÂèØ‰ª•‰∫ßÁîüÈ´òÊïà‰∏îÂÖ∑ÊúâÁ†îÁ©∂Á∫ßÂà´ÁöÑ AI ‰ª£ÁêÜ„ÄÇ', title='Âº∫ÂåñÂ≠¶‰π†‰∏éÊé®ÁêÜËÆæËÆ°ÁöÑÂÆåÁæéÁªìÂêà'))
[22.10.2025 12:24] Renaming data file.
[22.10.2025 12:24] Renaming previous data. hf_papers.json to ./d/2025-10-22.json
[22.10.2025 12:24] Saving new data file.
[22.10.2025 12:24] Generating page.
[22.10.2025 12:24] Renaming previous page.
[22.10.2025 12:24] Renaming previous data. index.html to ./d/2025-10-22.html
[22.10.2025 12:24] Writing result.
[22.10.2025 12:24] Renaming log file.
[22.10.2025 12:24] Renaming previous data. log.txt to ./logs/2025-10-22_last_log.txt
