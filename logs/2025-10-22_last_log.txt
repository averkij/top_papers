[22.10.2025 03:41] Read previous papers.
[22.10.2025 03:41] Generating top page (month).
[22.10.2025 03:41] Writing top page (month).
[22.10.2025 04:15] Read previous papers.
[22.10.2025 04:15] Get feed.
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18866
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18135
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18701
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18692
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16880
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18726
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18849
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18855
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18876
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18795
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17722
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17519
[22.10.2025 04:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.18250
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18775
[22.10.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17045
[22.10.2025 04:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.10.2025 04:15] No deleted papers detected.
[22.10.2025 04:15] Downloading and parsing papers (pdf, html). Total: 15.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18866.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18866.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18866.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18135.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18135.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18135.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18701.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18701.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18701.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18692.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18692.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18692.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.16880.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.16880.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.16880.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18726.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18726.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18726.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18849.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18849.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18849.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18855.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18855.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18855.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18876.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18876.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18876.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18795.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18795.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18795.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.17722.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.17722.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.17722.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.17519.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.17519.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.17519.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18250.
[22.10.2025 04:15] Downloading paper 2510.18250 from http://arxiv.org/pdf/2510.18250v1...
[22.10.2025 04:15] Extracting affiliations from text.
[22.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 0 5 2 8 1 . 0 1 5 2 : r rethink@sjtu SSTOKEN: SELF-MODULATED AND SEMANTIC-AWARE TOKEN SELECTION FOR LLM FINE-TUNING Xiaohan Qin1,2, Xiaoxing Wang1, Ning Liao1, Cancheng Zhang1, Xiangdong Zhang1, Mingquan Feng1, Jingzhi Wang1, Junchi Yan1,2, 1Shanghai Jiao Tong University, 2Shanghai Innovation Institute https://github.com/jianke0604/ssToken "
[22.10.2025 04:15] Response: ["Shanghai Jiao Tong University", "Shanghai Innovation Institute"]
[22.10.2025 04:15] Deleting PDF ./assets/pdf/2510.18250.pdf.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18775.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18775.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18775.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.17045.
[22.10.2025 04:15] Extra JSON file exists (./assets/json/2510.17045.json), skip PDF parsing.
[22.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.17045.json), skip HTML parsing.
[22.10.2025 04:15] Success.
[22.10.2025 04:15] Enriching papers with extra data.
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 0. LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  					AI-generated summary 				 Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effective...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 1. World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.  					AI-generated summary 				 Generative world models (WMs) can now simulate worlds with str...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 2. UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  					AI-generated summary 				 Recent progress in text-to-image (T2I) generation...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 3. Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  					AI-generated summary 				 Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full ...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 4. Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  					AI-generated summary 				 Although large language models (LLMs) have significant potential to advance chemical d...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 5. A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) have demonstrated...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 6. A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.  					AI-generated summary 				 Faithfully personalizing large language models (LLMs) to align with individu...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 7. Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  					AI-generated summary 				 We present Ring-1T, the first open-source, state-of-...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 8. Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  					AI-generated summary 				 While Multimodal Large Language Models (...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 9. ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.  					AI-generated summary 				 The original CLIP text encoder is limited by a maximum input lengt...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 10. MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  					AI-generated summary 				 The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. H...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 11. A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.  					AI-generated summary 			...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 12. ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critic...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 13. UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  					AI-generated summary 				 Recent advances in video generation have made it possible to produce visua...
[22.10.2025 04:15] ********************************************************************************
[22.10.2025 04:15] Abstract 14. The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.  					AI-generated summary 				 Video reasoning usin...
[22.10.2025 04:15] Read previous papers.
[22.10.2025 04:15] Generating reviews via LLM API.
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#data", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è AI: –±—ã—Å—Ç—Ä–µ–µ, —Ç–æ—á–Ω–µ–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ", "desc": "LightMem ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è LLM, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –ê—Ç–∫–∏–Ω—Å–æ–Ω–∞-–®–∏—Ñ—Ñ—Ä–∏–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –æ—Ä
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#games", "#benchmark", "#optimization", "#dataset", "#agents"], "emoji": "üåç", "ru": {"title": "–ö–æ–≥–¥–∞ –∫—Ä–∞—Å–∏–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–æ–±–æ—Ç—É: –≤–∞–∂–Ω–∞ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å, –∞ –Ω–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É World-in-World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö world models –≤ –∑
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#science", "#multilingual", "#multimodal", "#benchmark", "#survey"], "emoji": "üé®", "ru": {"title": "–í—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "UniGenBench++ ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ text-to-image –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#video", "#architecture", "#diffusion", "#training", "#long_context"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mixture-of-Groups Attention (MoGA) ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é Diffus
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#dataset", "#science", "#reasoning", "#architecture", "#benchmark", "#optimization", "#interpretability", "#training"], "emoji": "‚öóÔ∏è", "ru": {"title": "Chem-R: LLM, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç –∫–∞–∫ —Ö–∏–º–∏–∫", "desc": "Chem-R - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è —Ö–∏–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ–±—É—á–µ–Ω–Ω–∞—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –¥–ª—è —Ä–µ—à
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤–∞–∂–Ω–µ–µ –ø–æ–ª–Ω–æ—Ç—ã –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ IF-VidCap –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏–Ω—Å
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Critique-Post-Edit –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#optimization", "#training", "#open_source"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∏–ª–ª–∏–æ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö: –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –º–æ—â–Ω–æ–≥–æ AI-–º—ã—à–ª–µ–Ω–∏—è", "desc": "Ring-1T ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è thinking-–º–æ–¥–µ–ª—å —Å —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#games", "#benchmark", "#cv"], "emoji": "üîç", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ª—é–±—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å GAR (Grasp Any Region), –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö,
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#dataset", "#alignment", "#training", "#multimodal", "#long_context"], "emoji": "üéì", "ru": {"title": "–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ CLIP —Å LLM —á–µ—Ä–µ–∑ curriculum learning", "desc": "ProCLIP —É–ª—É—á—à–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ CLIP, –∑–∞–º–µ–Ω—è—è –µ—ë —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#science", "#video", "#benchmark", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MT-Video-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#video", "#architecture", "#data", "#training", "#optimization", "#open_source"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ–≥—Ä–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–ø—Ç–∏–º–∏–∑
[22.10.2025 04:15] Querying the API.
[22.10.2025 04:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency.
[22.10.2025 04:15] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ssToken - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è supervised fine-tuning –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è —Å–∞–º–æ–π –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π reference –º–æ–¥–µ–ª–∏, –≤—ã—á–∏—Å–ª—è—è —Ä–∞–∑–Ω–∏—Ü—É loss –º–µ–∂–¥—É —Ç–µ–∫—É—â–µ–π –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π –º–æ–¥–µ–ª–∏. –ü–æ–º–∏–º–æ loss-based –º–µ—Ç—Ä–∏–∫–∏, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç semantic-aware –º–µ—Ç—Ä–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ attention, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –æ–±–æ–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã token-level —Å–µ–ª–µ–∫—Ü–∏–∏ –∏ –æ–±—ã—á–Ω—ã–π fine-tuning –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.",
  "emoji": "üéØ",
  "title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤: —Å–∞–º–æ–º–æ–¥—É–ª—è—Ü–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning LLM"
}
```
[22.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency."

[22.10.2025 04:15] Response: ```python
['TRAINING', 'DATA']
```
[22.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency."

[22.10.2025 04:15] Response: ```python
["OPTIMIZATION"]
```
[22.10.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces ssToken, a novel approach for selecting tokens during the supervised fine-tuning of large language models. It addresses limitations of existing methods by using a self-modulated signal derived from the model\'s own history, allowing for adaptive token selection without needing an additional reference model. Additionally, ssToken incorporates a semantic-aware metric that evaluates token importance based on their meaning, rather than just loss metrics. The results show that ssToken significantly improves performance and efficiency compared to traditional token selection methods.","title":"Enhancing Token Selection with ssToken for Better Language Model Fine-Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces ssToken, a novel approach for selecting tokens during the supervised fine-tuning of large language models. It addresses limitations of existing methods by using a self-modulated signal derived from the model's own history, allowing for adaptive token selection without needing an additional reference model. Additionally, ssToken incorporates a semantic-aware metric that evaluates token importance based on their meaning, rather than just loss metrics. The results show that ssToken significantly improves performance and efficiency compared to traditional token selection methods.", title='Enhancing Token Selection with ssToken for Better Language Model Fine-Tuning'))
[22.10.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ssTokenÊòØ‰∏ÄÁßçËá™ÊàëË∞ÉËäÇÂíåËØ≠‰πâÊÑüÁü•ÁöÑ‰ª§ÁâåÈÄâÊã©ÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁõëÁù£ÂæÆË∞É„ÄÇÂÆÉÈÄöËøáËá™ÈÄÇÂ∫îÈÄâÊã©‰ª§ÁâåÂπ∂Êèê‰æõË°•ÂÖÖÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåssTokenÂà©Áî®ÂéÜÂè≤Ê®°ÂûãËÆ°ÁÆóÊØè‰∏™‰ª§ÁâåÁöÑÊçüÂ§±Â∑ÆÂºÇÔºå‰Ωú‰∏∫Ëá™ÊàëË∞ÉËäÇ‰ø°Âè∑Ôºå‰ºòÂåñ‰ª§ÁâåÈÄâÊã©ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåssTokenÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËØ≠‰πâÊÑüÁü•‰ª§ÁâåÈáçË¶ÅÊÄßËØÑ‰º∞ÊåáÊ†áÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÈÄâÊã©ÁöÑÊúâÊïàÊÄß„ÄÇ","title":"Ëá™ÊàëË∞ÉËäÇ‰∏éËØ≠‰πâÊÑüÁü•ÁöÑ‰ª§ÁâåÈÄâÊã©Êñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ssTokenÊòØ‰∏ÄÁßçËá™ÊàëË∞ÉËäÇÂíåËØ≠‰πâÊÑüÁü•ÁöÑ‰ª§ÁâåÈÄâÊã©ÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁõëÁù£ÂæÆË∞É„ÄÇÂÆÉÈÄöËøáËá™ÈÄÇÂ∫îÈÄâÊã©‰ª§ÁâåÂπ∂Êèê‰æõË°•ÂÖÖÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºåssTokenÂà©Áî®ÂéÜÂè≤Ê®°ÂûãËÆ°ÁÆóÊØè‰∏™‰ª§ÁâåÁöÑÊçüÂ§±Â∑ÆÂºÇÔºå‰Ωú‰∏∫Ëá™ÊàëË∞ÉËäÇ‰ø°Âè∑Ôºå‰ºòÂåñ‰ª§ÁâåÈÄâÊã©ËøáÁ®ã„ÄÇÊ≠§Â§ñÔºåssTokenÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËØ≠‰πâÊÑüÁü•‰ª§ÁâåÈáçË¶ÅÊÄßËØÑ‰º∞ÊåáÊ†áÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜÈÄâÊã©ÁöÑÊúâÊïàÊÄß„ÄÇ', title='Ëá™ÊàëË∞ÉËäÇ‰∏éËØ≠‰πâÊÑüÁü•ÁöÑ‰ª§ÁâåÈÄâÊã©Êñ∞ÊñπÊ≥ï'))
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#video", "#games", "#optimization", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ù–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ 4K —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "UltraGen - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –≤–ø–µ—Ä–≤—ã–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤ –≤—ã—Å–æ–∫–æ–º —Ä–∞
[22.10.2025 04:15] Using data from previous issue: {"categories": ["#reasoning", "#video", "#training", "#multimodal", "#inference", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç V-Reason ‚Äî –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Large Multimodal Models –Ω–∞–¥ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏
[22.10.2025 04:15] Renaming data file.
[22.10.2025 04:15] Renaming previous data. hf_papers.json to ./d/2025-10-22.json
[22.10.2025 04:15] Saving new data file.
[22.10.2025 04:15] Generating page.
[22.10.2025 04:15] Renaming previous page.
[22.10.2025 04:15] Renaming previous data. index.html to ./d/2025-10-22.html
[22.10.2025 04:15] Writing result.
[22.10.2025 04:15] Renaming log file.
[22.10.2025 04:15] Renaming previous data. log.txt to ./logs/2025-10-22_last_log.txt
