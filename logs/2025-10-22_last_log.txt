[22.10.2025 12:24] Read previous papers.
[22.10.2025 12:24] Generating top page (month).
[22.10.2025 12:24] Writing top page (month).
[22.10.2025 13:28] Read previous papers.
[22.10.2025 13:28] Get feed.
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18866
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18135
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18701
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16880
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18692
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18876
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18726
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18849
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18855
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17722
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18250
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18019
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17519
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18795
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18775
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18873
[22.10.2025 13:28] Extract page data from URL. URL: https://huggingface.co/papers/2510.17699
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17045
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16505
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14264
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18554
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18489
[22.10.2025 13:28] Extract page data from URL. URL: https://huggingface.co/papers/2510.18234
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15600
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17928
[22.10.2025 13:28] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15862
[22.10.2025 13:28] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.10.2025 13:28] No deleted papers detected.
[22.10.2025 13:28] Downloading and parsing papers (pdf, html). Total: 26.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18866.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18866.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18866.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18135.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18135.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18135.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18701.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18701.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18701.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.16880.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.16880.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.16880.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18692.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18692.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18692.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18876.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18876.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18876.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18726.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18726.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18726.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18849.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18849.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18849.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18855.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18855.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18855.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.17722.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.17722.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.17722.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18250.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18250.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18250.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18019.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18019.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18019.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.17519.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.17519.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.17519.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18795.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18795.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18795.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18775.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18775.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18775.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18873.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18873.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18873.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.17699.
[22.10.2025 13:28] Downloading paper 2510.17699 from http://arxiv.org/pdf/2510.17699v1...
[22.10.2025 13:28] Extracting affiliations from text.
[22.10.2025 13:28] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 9 9 6 7 1 . 0 1 5 2 : r Preprint. GAS: IMPROVING DISCRETIZATION OF DIFFUSION ODES VIA GENERALIZED ADVERSARIAL SOLVER Aleksandr Oganov,1,2,, Ilya Bykov,1, Eva Neudachina,1, Mishan Aliev1, Alexander Tolmachev, Alexander Sidorov, Aleksandr Zuev, Andrey Okhotin1, Denis Rakitin1, Aibek Alanov1 1HSE University, Russia 2Lomonosov Moscow State University, Russia "
[22.10.2025 13:28] Response: ```python
["HSE University, Russia", "Lomonosov Moscow State University, Russia"]
```
[22.10.2025 13:28] Deleting PDF ./assets/pdf/2510.17699.pdf.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.17045.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.17045.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.17045.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.16505.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.16505.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.16505.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.14264.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.14264.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.14264.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18554.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18554.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18554.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18489.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.18489.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.18489.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.18234.
[22.10.2025 13:28] Downloading paper 2510.18234 from http://arxiv.org/pdf/2510.18234v1...
[22.10.2025 13:28] Extracting affiliations from text.
[22.10.2025 13:28] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DeepSeek-OCR: Contexts Optical Compression Haoran Wei, Yaofeng Sun, Yukun Li DeepSeek-AI "
[22.10.2025 13:28] Response: []
[22.10.2025 13:28] Extracting affiliations from text.
[22.10.2025 13:28] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DeepSeek-OCR: Contexts Optical Compression Haoran Wei, Yaofeng Sun, Yukun Li DeepSeek-AIWe present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., compression ratio < 10), the model can achieve decoding (OCR) precision of 97%. Even at compression ratio of 20, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR. 5 2 0 2 1 2 ] . [ 1 4 3 2 8 1 . 0 1 5 2 : r (a) Compression on Fox benchmark (b) Performance on Omnidocbench Figure 1 Figure (a) shows the compression ratio (number of text tokens in ground truth/number of vision tokens model used) testing on Fox [21] benchmark; Figure (b) shows performance comparisons on OmniDocBench [27]. DeepSeek-OCR can achieve state-of-the-art performance among end-to-end models enjoying the fewest vision tokens.1 Introduction 2 Related Works 2.1 Typical Vision Encoders in VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 End-to-end OCR Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Methodology 3.1 Architecture . 3.2 DeepEncoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Architecture of DeepEncoder . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Multiple resolution support . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 The MoE Decoder . 3.4 Data Engine . . . . . . . . 3.4.1 OCR 1.0 data . 3.4.2 OCR 2.0 data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 General vision data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.4 Text-only data . 3.5 Training Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Training DeepEncoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.2 Training DeepSeek-OCR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Evaluation 4.1 Vision-text Compression Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 OCR Practical Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Qualitative Study . . . 4.3.1 Deep parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.2 Multilingual recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.3 General vision understanding . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Discussion 6 Conclusion 3 4 4 5 5 5 5 7 7 7 8 9 9 10 10 10 10 12 12 16 17 18 2 1. Introduction Current Large Language Models (LLMs) face significant computational challenges when processing long textual content due to quadratic scaling with sequence length. We explore potential solution: leveraging visual modality as an efficient compression medium for textual information. single image containing document text can represent rich information using substantially fewer tokens than the equivalent digital text, suggesting that optical compression through vision tokens could achieve much higher compression ratios. This insight motivates us to reexamine vision-language models (VLMs) from an LLM-centric perspective, focusing on how vision encoders can enhance LLMs efficiency in processing textual information rather than basic VQA [12, 16, 24, 32, 41] what humans excel at. OCR tasks, as an intermediate modality bridging vision and language, provide an ideal testbed for this visiontext compression paradigm, as they establish natural compression-decompression mapping between visual and textual representations while offering quantitative evaluation metrics. Accordingly, we present DeepSeek-OCR, VLM designed as preliminary proof-of-concept for efficient vision-text compression. Our work makes three primary contributions: First, we provide comprehensive quantitative analysis of vision-text token compression ratios. Our method achieves 96%+ OCR decoding precision at 9-10 text compression, 90% at 10-12 compression, and 60% at 20 compression on Fox [21] benchmarks featuring diverse document layouts (with actual accuracy being even higher when accounting for formatting differences between output and ground truth), as shown in Figure 1(a). The results demonstrate that compact language models can effectively learn to decode compressed visual representations, suggesting that larger LLMs could readily acquire similar capabilities through appropriate pretraining design. Second, we introduce DeepEncoder, novel architecture that maintains low activation memory and minimal vision tokens even with high-resolution inputs. It serially connects window attention and global attention encoder components through 16 convolutional compressor. This design ensures that the window attention component processes large number of vision tokens, while the compressor reduces vision tokens before they enter the dense global attention component, achieving effective memory and token compression. Third, we develop DeepSeek-OCR based on DeepEncoder and DeepSeek3B-MoE [19, 20]. As shown in Figure 1(b), it achieves state-of-the-art performance within end-to-end models on OmniDocBench while "
[22.10.2025 13:28] Mistral response. {"id": "34bc5a41534f4eb4ad207fd7a0b1e40b", "created": 1761139717, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1990, "total_tokens": 2001, "completion_tokens": 11}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"DeepSeek-AI\"]\n```"}}]}
[22.10.2025 13:28] Response: ```python
["DeepSeek-AI"]
```
[22.10.2025 13:28] Deleting PDF ./assets/pdf/2510.18234.pdf.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.15600.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.15600.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.15600.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.17928.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.17928.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.17928.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Downloading and parsing paper https://huggingface.co/papers/2510.15862.
[22.10.2025 13:28] Extra JSON file exists (./assets/json/2510.15862.json), skip PDF parsing.
[22.10.2025 13:28] Paper image links file exists (./assets/img_data/2510.15862.json), skip HTML parsing.
[22.10.2025 13:28] Success.
[22.10.2025 13:28] Enriching papers with extra data.
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 0. LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  					AI-generated summary 				 Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effective...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 1. World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.  					AI-generated summary 				 Generative world models (WMs) can now simulate worlds with str...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 2. UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  					AI-generated summary 				 Recent progress in text-to-image (T2I) generation...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 3. Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  					AI-generated summary 				 Although large language models (LLMs) have significant potential to advance chemical d...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 4. Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  					AI-generated summary 				 Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full ...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 5. Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  					AI-generated summary 				 While Multimodal Large Language Models (...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 6. A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) have demonstrated...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 7. A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.  					AI-generated summary 				 Faithfully personalizing large language models (LLMs) to align with individu...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 8. Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  					AI-generated summary 				 We present Ring-1T, the first open-source, state-of-...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 9. MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  					AI-generated summary 				 The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. H...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 10. ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critic...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 11. STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.  					AI-generated summary 				 Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, y...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 12. A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.  					AI-generated summary 			...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 13. ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.  					AI-generated summary 				 The original CLIP text encoder is limited by a maximum input lengt...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 14. UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  					AI-generated summary 				 Recent advances in video generation have made it possible to produce visua...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 15. DSI-Bench evaluates the dynamic spatial reasoning capabilities of vision-language and visual expertise models through a benchmark of dynamic videos and annotated questions, highlighting their limitations in understanding self-motion, object motion, and relative relationships.  					AI-generated summ...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 16. The Generalized Adversarial Solver improves diffusion model sampling efficiency and quality by combining a simple ODE solver parameterization with adversarial training.  					AI-generated summary 				 While diffusion models achieve state-of-the-art generation quality, they still suffer from computat...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 17. The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.  					AI-generated summary 				 Video reasoning usin...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 18. PRISMM-Bench evaluates the ability of large multimodal models to detect, correct, and reason over inconsistencies in scientific papers, revealing significant challenges in multimodal scientific reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) are increasingly applied to scie...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 19. AlphaQuanter, a single-agent framework using reinforcement learning, achieves top performance in automated trading by learning dynamic policies and proactively acquiring information.  					AI-generated summary 				 While Large Language Model (LLM) agents show promise in automated trading, they still...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 20. Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices.  					AI-generated summary 				 In this work, we show that it is possible to extract significant amounts of alignment training d...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 21. A system for reconstructing 4D HDR scenes from unposed LDR videos using Gaussian Splatting with two-stage optimization and temporal luminance regularization.  					AI-generated summary 				 We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes f...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 22. DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.  					AI-generated summary 				 We present DeepSeek-OCR as an initial investigation into the feasibility of compressing lo...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 23. Thoth, a large language model trained with the Sketch-and-Fill paradigm and structured component-based reward mechanism, generates more reliable and executable scientific protocols compared to existing models.  					AI-generated summary 				 The foundation of reproducible science lies in protocols t...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 24. An evolutionary framework synthesizes verifiable data for language models, improving reinforcement learning and distillation across various tasks.  					AI-generated summary 				 Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforce...
[22.10.2025 13:28] ********************************************************************************
[22.10.2025 13:28] Abstract 25. PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.  					AI-generated summary 				 Tool-augmented large language models (LLMs) are emerging as deep research agent...
[22.10.2025 13:28] Read previous papers.
[22.10.2025 13:28] Generating reviews via LLM API.
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#data", "#training", "#long_context"], "emoji": "ðŸ§ ", "ru": {"title": "Ð§ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð´Ð»Ñ AI: Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ, Ñ‚Ð¾Ñ‡Ð½ÐµÐµ, ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½ÐµÐµ", "desc": "LightMem â€” ÑÑ‚Ð¾ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð´Ð»Ñ LLM, Ð²Ð´Ð¾Ñ…Ð½Ð¾Ð²Ð»Ñ‘Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚Ð¸ ÐÑ‚ÐºÐ¸Ð½ÑÐ¾Ð½Ð°-Ð¨Ð¸Ñ„Ñ„Ñ€Ð¸Ð½Ð°. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¾Ñ€
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#games", "#benchmark", "#optimization", "#dataset", "#agents"], "emoji": "ðŸŒ", "ru": {"title": "ÐšÐ¾Ð³Ð´Ð° ÐºÑ€Ð°ÑÐ¸Ð²Ð°Ñ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ° Ð½Ðµ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ñ€Ð¾Ð±Ð¾Ñ‚Ñƒ: Ð²Ð°Ð¶Ð½Ð° ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ð¾ÑÑ‚ÑŒ, Ð° Ð½Ðµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñƒ World-in-World Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… world models Ð² Ð·
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#science", "#multilingual", "#multimodal", "#benchmark", "#survey"], "emoji": "ðŸŽ¨", "ru": {"title": "Ð’ÑÐµÑÑ‚Ð¾Ñ€Ð¾Ð½Ð½ÑÑ Ð¾Ñ†ÐµÐ½ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ", "desc": "UniGenBench++ â€” ÑÑ‚Ð¾ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ text-to-image Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÑƒÑŽ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#dataset", "#science", "#reasoning", "#architecture", "#benchmark", "#optimization", "#interpretability", "#training"], "emoji": "âš—ï¸", "ru": {"title": "Chem-R: LLM, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´Ð°ÐµÑ‚ ÐºÐ°Ðº Ñ…Ð¸Ð¼Ð¸Ðº", "desc": "Chem-R - ÑÑ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ…Ð¸Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ, Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð°Ñ Ð² Ñ‚Ñ€Ð¸ ÑÑ‚Ð°Ð¿Ð° Ð´Ð»Ñ Ñ€ÐµÑˆ
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#video", "#architecture", "#diffusion", "#training", "#long_context"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Mixture-of-Groups Attention (MoGA) â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Diffus
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#games", "#benchmark", "#cv"], "emoji": "ðŸ”", "ru": {"title": "ÐŸÐ¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð»ÑŽÐ±Ñ‹Ñ… Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð¾Ð² Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ GAR (Grasp Any Region), ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð¾Ð² Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÑ…,
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#video"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð¡Ð»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼ Ð²Ð°Ð¶Ð½ÐµÐµ Ð¿Ð¾Ð»Ð½Ð¾Ñ‚Ñ‹ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº IF-VidCap Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¼ Ð¸Ð½Ñ
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "âœï¸", "ru": {"title": "ÐšÑ€Ð¸Ñ‚Ð¸ÐºÐ° Ð¸ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ: Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿ÑƒÑ‚ÑŒ Ðº Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ LLM", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Critique-Post-Edit Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð´ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#optimization", "#training", "#open_source"], "emoji": "ðŸ§ ", "ru": {"title": "Ð¢Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð´Ð»Ñ Ð²ÑÐµÑ…: Ð´ÐµÐ¼Ð¾ÐºÑ€Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ñ‰Ð½Ð¾Ð³Ð¾ AI-Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ", "desc": "Ring-1T â€” ÑÑ‚Ð¾ Ð¿ÐµÑ€Ð²Ð°Ñ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð°Ñ thinking-Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ð°
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#multimodal", "#science", "#video", "#benchmark", "#open_source"], "emoji": "ðŸŽ¬", "ru": {"title": "ÐœÐ½Ð¾Ð³Ð¾Ñ…Ð¾Ð´Ð¾Ð²Ñ‹Ðµ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¸: Ð½Ð¾Ð²Ñ‹Ð¹ Ñ€ÑƒÐ±ÐµÐ¶ Ð² Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ AI", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ MT-Video-Bench â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (MLLM) Ð² Ð¼Ð½Ð¾Ð³Ð¾Ñ…Ð¾Ð´Ð¾Ð²
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#training", "#data", "#optimization"], "emoji": "ðŸŽ¯", "ru": {"title": "Ð£Ð¼Ð½Ñ‹Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: ÑÐ°Ð¼Ð¾Ð¼Ð¾Ð´ÑƒÐ»ÑÑ†Ð¸Ñ Ð¸ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÐ° Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ fine-tuning LLM", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ssToken - Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð²Ñ‹Ð±Ð¾Ñ€Ñƒ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ supervised fine-tuning Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐœÐµÑ‚Ð¾Ð´ Ð¸ÑÐ¿
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#watermarking", "#low_resource"], "emoji": "ðŸŒ", "ru": {"title": "Ð’Ð¾Ð´ÑÐ½Ñ‹Ðµ Ð·Ð½Ð°ÐºÐ¸ Ð´Ð»Ñ LLM Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð½Ð° Ð²ÑÐµÑ… ÑÐ·Ñ‹ÐºÐ°Ñ… Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ð¼ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð¼", "desc": "Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð²Ð¾Ð´ÑÐ½Ñ‹Ñ… Ð·Ð½Ð°ÐºÐ¾Ð² Ð´Ð»Ñ LLM ÑƒÑ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ°Ñ…, Ð½Ð¾ Ð½Ð° Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ Ñ‚ÐµÑ€ÑÑŽÑ‚ Ð½Ð°Ð´
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#video", "#architecture", "#data", "#training", "#optimization", "#open_source"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ð³Ñ€Ð¾Ð¼Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾, Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#dataset", "#alignment", "#training", "#multimodal", "#long_context"], "emoji": "ðŸŽ“", "ru": {"title": "ÐŸÐ¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ð¾Ðµ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ CLIP Ñ LLM Ñ‡ÐµÑ€ÐµÐ· curriculum learning", "desc": "ProCLIP ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ CLIP, Ð·Ð°Ð¼ÐµÐ½ÑÑ ÐµÑ‘ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ ÑÐ½ÐºÐ¾Ð´ÐµÑ€ Ð½Ð° ÑÐ¼Ð±ÐµÐ´Ð´ÐµÑ€ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ LLM Ð´
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#video", "#games", "#optimization", "#architecture", "#diffusion"], "emoji": "ðŸŽ¬", "ru": {"title": "ÐÐ°Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ð² 4K Ñ‡ÐµÑ€ÐµÐ· Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ", "desc": "UltraGen - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð²Ð¿ÐµÑ€Ð²Ñ‹Ðµ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð¸Ð´ÐµÐ¾ Ð² Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¼ Ñ€Ð°
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#cv", "#reasoning", "#benchmark", "#3d"], "emoji": "ðŸŽ¥", "ru": {"title": "ÐšÐ¾Ð³Ð´Ð° AI Ñ‚ÐµÑ€ÑÐµÑ‚ÑÑ Ð² Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¸: Ñ‚ÐµÑÑ‚ Ð½Ð° Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð°", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ DSI-Bench â€” Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ AI-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÑÑ…. Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ 
[22.10.2025 13:28] Querying the API.
[22.10.2025 13:28] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The Generalized Adversarial Solver improves diffusion model sampling efficiency and quality by combining a simple ODE solver parameterization with adversarial training.  					AI-generated summary 				 While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS.
[22.10.2025 13:28] Response: ```json
{
  "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Generalized Adversarial Solver â€” Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ ÑÐµÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸Ð·Ð°Ñ†Ð¸ÑŽ ODE-ÑÐ¾Ð»Ð²ÐµÑ€Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ‚ÐµÑ…Ð½Ð¸Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÑÐ¾ÐºÑ€Ð°Ñ‚Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹ Ñ Ð´ÐµÑÑÑ‚ÐºÐ¾Ð² Ð´Ð¾ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… ÑˆÐ°Ð³Ð¾Ð². ÐšÐ»ÑŽÑ‡ÐµÐ²Ð°Ñ Ð¸Ð´ÐµÑ Ð·Ð°ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ÑÑ Ð² ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸ Ñ adversarial Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð°Ñ€Ñ‚ÐµÑ„Ð°ÐºÑ‚Ð¾Ð² Ð¸ Ð»ÑƒÑ‡ÑˆÐµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐœÐµÑ‚Ð¾Ð´ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð½Ð°Ð´ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¼Ð¸ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°Ð¼Ð¸ Ð¿Ñ€Ð¸ ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼Ñ‹Ñ… Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ñ€ÐµÑÑƒÑ€ÑÐ°Ñ….",
  "emoji": "âš¡",
  "title": "Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ ÑÐµÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· adversarial Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¾Ð»Ð²ÐµÑ€Ð°"
}
```
[22.10.2025 13:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Generalized Adversarial Solver improves diffusion model sampling efficiency and quality by combining a simple ODE solver parameterization with adversarial training.  					AI-generated summary 				 While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS."

[22.10.2025 13:28] Response: ```python
['TRAINING', 'ARCHITECTURE']
```
[22.10.2025 13:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Generalized Adversarial Solver improves diffusion model sampling efficiency and quality by combining a simple ODE solver parameterization with adversarial training.  					AI-generated summary 				 While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS."

[22.10.2025 13:28] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[22.10.2025 13:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Generalized Adversarial Solver enhances the efficiency and quality of sampling in diffusion models by integrating a straightforward ODE solver parameterization with adversarial training techniques. This approach addresses the high computational costs associated with traditional sampling methods by significantly reducing the number of function evaluations needed. Unlike previous methods that require complex training strategies, our method maintains fine-grained detail without additional training complications. The combination of distillation loss and adversarial training in the Generalized Adversarial Solver leads to improved performance and reduced artifacts in generated outputs.","title":"Efficient and Detailed Sampling with Generalized Adversarial Solver"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The Generalized Adversarial Solver enhances the efficiency and quality of sampling in diffusion models by integrating a straightforward ODE solver parameterization with adversarial training techniques. This approach addresses the high computational costs associated with traditional sampling methods by significantly reducing the number of function evaluations needed. Unlike previous methods that require complex training strategies, our method maintains fine-grained detail without additional training complications. The combination of distillation loss and adversarial training in the Generalized Adversarial Solver leads to improved performance and reduced artifacts in generated outputs.', title='Efficient and Detailed Sampling with Generalized Adversarial Solver'))
[22.10.2025 13:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¹¿ä¹‰å¯¹æŠ—æ±‚è§£å™¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡åž‹çš„é‡‡æ ·æ•ˆçŽ‡å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç®€å•çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨å‚æ•°åŒ–å’Œå¯¹æŠ—è®­ç»ƒï¼Œé¿å…äº†å¤æ‚çš„è®­ç»ƒæŠ€å·§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¹¿ä¹‰å¯¹æŠ—æ±‚è§£å™¨åœ¨ä¿æŒç»†èŠ‚çš„åŒæ—¶ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨èµ„æºé™åˆ¶ä¸‹çš„è¡¨çŽ°ä¼˜äºŽçŽ°æœ‰çš„æ±‚è§£å™¨è®­ç»ƒæ–¹æ³•ã€‚","title":"æå‡æ‰©æ•£æ¨¡åž‹é‡‡æ ·æ•ˆçŽ‡ä¸Žè´¨é‡çš„å¹¿ä¹‰å¯¹æŠ—æ±‚è§£å™¨"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¹¿ä¹‰å¯¹æŠ—æ±‚è§£å™¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡åž‹çš„é‡‡æ ·æ•ˆçŽ‡å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç®€å•çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ±‚è§£å™¨å‚æ•°åŒ–å’Œå¯¹æŠ—è®­ç»ƒï¼Œé¿å…äº†å¤æ‚çš„è®­ç»ƒæŠ€å·§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¹¿ä¹‰å¯¹æŠ—æ±‚è§£å™¨åœ¨ä¿æŒç»†èŠ‚çš„åŒæ—¶ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼Œè¯¥æ–¹æ³•åœ¨èµ„æºé™åˆ¶ä¸‹çš„è¡¨çŽ°ä¼˜äºŽçŽ°æœ‰çš„æ±‚è§£å™¨è®­ç»ƒæ–¹æ³•ã€‚', title='æå‡æ‰©æ•£æ¨¡åž‹é‡‡æ ·æ•ˆçŽ‡ä¸Žè´¨é‡çš„å¹¿ä¹‰å¯¹æŠ—æ±‚è§£å™¨'))
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#reasoning", "#video", "#training", "#multimodal", "#inference", "#optimization"], "emoji": "ðŸŽ¯", "ru": {"title": "Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑÐ¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸ÑŽ Ð±ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ V-Reason â€” Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Large Multimodal Models Ð½Ð°Ð´ Ð²Ð¸Ð´ÐµÐ¾ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning", "#science"], "emoji": "ðŸ”¬", "ru": {"title": "ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° AI Ð½Ð° Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð½ÐµÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ð¹ Ð² Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… ÑÑ‚Ð°Ñ‚ÑŒÑÑ…", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº PRISMM-Bench Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ Ð¸ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÑÑ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#games", "#optimization", "#open_source", "#rl", "#training", "#agents", "#interpretability"], "emoji": "ðŸ“ˆ", "ru": {"title": "ÐžÐ´Ð¸Ð½ ÑƒÐ¼Ð½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚ Ð²Ð¼ÐµÑÑ‚Ð¾ Ñ…Ð°Ð¾ÑÐ°: RL-Ñ‚Ñ€ÐµÐ¹Ð´ÐµÑ€ Ñ Ð¿Ñ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ð¾Ð¹ Ð»Ð¾Ð³Ð¸ÐºÐ¾Ð¹", "desc": "AlphaQuanter â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ñ‚Ð¾Ñ€Ð³Ð¾Ð²Ð»Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐµÐ´Ð¸Ð½Ð¾Ð³Ð¾ AI-Ð°
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#hallucinations", "#rlhf", "#long_context", "#training", "#alignment", "#data"], "emoji": "ðŸ”“", "ru": {"title": "Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸: ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ñ€Ð¸ÑÐºÐ¸ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ð¸Ð· Ð¿Ð¾ÑÑ‚-Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ð±ÑŠÑ‘
[22.10.2025 13:28] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#cv"], "emoji": "ðŸŽ¥", "ru": {"title": "Ð ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ 4D HDR ÑÑ†ÐµÐ½ Ð¸Ð· LDR Ð²Ð¸Ð´ÐµÐ¾ Ð±ÐµÐ· Ð¿Ð¾Ð· ÐºÐ°Ð¼ÐµÑ€Ñ‹", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Mono4DGS-HDR Ð´Ð»Ñ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ 4D HDR ÑÑ†ÐµÐ½ Ð¸Ð· Ð½ÐµÑƒÐ¿Ð¾Ñ€ÑÐ´Ð¾Ñ‡ÐµÐ½Ð½Ñ‹Ñ… LDR Ð²Ð¸Ð´ÐµÐ¾. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ð°Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Gaussian 
[22.10.2025 13:28] Querying the API.
[22.10.2025 13:28] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.  					AI-generated summary 				 We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR.
[22.10.2025 13:28] Response: ```json
{
  "desc": "DeepSeek-OCR Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑÐ¶Ð°Ñ‚Ð¸ÑŽ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð¿Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ 2D-Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· DeepEncoder Ð´Ð»Ñ ÐºÐ¾Ð¼Ð¿Ñ€ÐµÑÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ð´ÐµÐºÐ¾Ð´ÐµÑ€Ð° DeepSeek3B-MoE-A570M. ÐŸÑ€Ð¸ ÑÑ‚ÐµÐ¿ÐµÐ½Ð¸ ÑÐ¶Ð°Ñ‚Ð¸Ñ Ð¼ÐµÐ½ÐµÐµ 10x Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ OCR 97%, Ð° Ð¿Ñ€Ð¸ 20x ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¾ÐºÐ¾Ð»Ð¾ 60% Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸. ÐÐ° Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐµ OmniDocBench ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ GOT-OCR2.0 Ð¸ MinerU2.0, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¼ÐµÐ½ÑŒÑˆÐµ vision tokens Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ ÑÐ²Ñ‹ÑˆÐµ 200 Ñ‚Ñ‹ÑÑÑ‡ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ð² Ð´ÐµÐ½ÑŒ Ð½Ð° Ð¾Ð´Ð½Ð¾Ð¹ A100.",
  "emoji": "ðŸ”",
  "title": "Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð² 20 Ñ€Ð°Ð· Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ"
}
```
[22.10.2025 13:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.  					AI-generated summary 				 We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR."

[22.10.2025 13:28] Response: ```python
['DATASET', 'DATA', 'CV', 'TRAINING']
```
[22.10.2025 13:28] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.  					AI-generated summary 				 We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR."

[22.10.2025 13:28] Response: ```python
["LONG_CONTEXT", "OPEN_SOURCE"]
```
[22.10.2025 13:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeepSeek-OCR is a novel approach that utilizes optical 2D mapping to effectively compress long contexts in document processing. It features two main components: DeepEncoder, which optimizes the processing of high-resolution inputs while minimizing activations, and DeepSeek3B-MoE-A570M as the decoder. The model achieves impressive OCR precision, reaching 97% accuracy with a compression ratio of less than 10x, and maintains around 60% accuracy even at 20x compression. This method not only enhances OCR performance but also shows potential for applications in historical document analysis and large language model training.","title":"Efficient Context Compression for High-Precision OCR"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeepSeek-OCR is a novel approach that utilizes optical 2D mapping to effectively compress long contexts in document processing. It features two main components: DeepEncoder, which optimizes the processing of high-resolution inputs while minimizing activations, and DeepSeek3B-MoE-A570M as the decoder. The model achieves impressive OCR precision, reaching 97% accuracy with a compression ratio of less than 10x, and maintains around 60% accuracy even at 20x compression. This method not only enhances OCR performance but also shows potential for applications in historical document analysis and large language model training.', title='Efficient Context Compression for High-Precision OCR'))
[22.10.2025 13:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeepSeek-OCR æ˜¯ä¸€ç§åˆ©ç”¨å…‰å­¦äºŒç»´æ˜ å°„æ¥åŽ‹ç¼©é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡çš„æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰çš„ç²¾åº¦ã€‚è¯¥æ¨¡åž‹ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šDeepEncoder ä½œä¸ºæ ¸å¿ƒå¼•æ“Žï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨çŽ‡è¾“å…¥ä¸‹ä¿æŒä½Žæ¿€æ´»ï¼ŒåŒæ—¶å®žçŽ°é«˜åŽ‹ç¼©æ¯”ï¼Œä»Žè€Œå‡å°‘è§†è§‰æ ‡è®°çš„æ•°é‡ã€‚å®žéªŒè¡¨æ˜Žï¼Œå½“æ–‡æœ¬æ ‡è®°æ•°é‡ä¸è¶…è¿‡è§†è§‰æ ‡è®°çš„åå€æ—¶ï¼Œæ¨¡åž‹çš„è§£ç ç²¾åº¦å¯è¾¾åˆ° 97%ã€‚æ­¤å¤–ï¼ŒDeepSeek-OCR åœ¨å®žé™…åº”ç”¨ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œèƒ½å¤Ÿä»¥è¾ƒå°‘çš„è§†è§‰æ ‡è®°ç”Ÿæˆå¤§é‡è®­ç»ƒæ•°æ®ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ–‡æ¡£å¤„ç†ä¸­çš„å®žé™…ä»·å€¼ã€‚","title":"DeepSeek-OCRï¼šé«˜æ•ˆåŽ‹ç¼©ä¸Žç²¾å‡†è¯†åˆ«çš„ç»“åˆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeepSeek-OCR æ˜¯ä¸€ç§åˆ©ç”¨å…‰å­¦äºŒç»´æ˜ å°„æ¥åŽ‹ç¼©é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡çš„æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰çš„ç²¾åº¦ã€‚è¯¥æ¨¡åž‹ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šDeepEncoder ä½œä¸ºæ ¸å¿ƒå¼•æ“Žï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨çŽ‡è¾“å…¥ä¸‹ä¿æŒä½Žæ¿€æ´»ï¼ŒåŒæ—¶å®žçŽ°é«˜åŽ‹ç¼©æ¯”ï¼Œä»Žè€Œå‡å°‘è§†è§‰æ ‡è®°çš„æ•°é‡ã€‚å®žéªŒè¡¨æ˜Žï¼Œå½“æ–‡æœ¬æ ‡è®°æ•°é‡ä¸è¶…è¿‡è§†è§‰æ ‡è®°çš„åå€æ—¶ï¼Œæ¨¡åž‹çš„è§£ç ç²¾åº¦å¯è¾¾åˆ° 97%ã€‚æ­¤å¤–ï¼ŒDeepSeek-OCR åœ¨å®žé™…åº”ç”¨ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œèƒ½å¤Ÿä»¥è¾ƒå°‘çš„è§†è§‰æ ‡è®°ç”Ÿæˆå¤§é‡è®­ç»ƒæ•°æ®ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ–‡æ¡£å¤„ç†ä¸­çš„å®žé™…ä»·å€¼ã€‚', title='DeepSeek-OCRï¼šé«˜æ•ˆåŽ‹ç¼©ä¸Žç²¾å‡†è¯†åˆ«çš„ç»“åˆ'))
[22.10.2025 13:29] Using data from previous issue: {"categories": ["#dataset", "#training", "#open_source", "#optimization", "#data", "#science", "#benchmark", "#agents"], "emoji": "ðŸ§ª", "ru": {"title": "ÐÐ°ÑƒÑ‡Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ñ‹ Ð¾Ñ‚ Ð¸Ð´ÐµÐ¸ Ð´Ð¾ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Thoth â€” Ð±Ð¾Ð»ÑŒÑˆÑƒÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ…
[22.10.2025 13:29] Using data from previous issue: {"categories": ["#hallucinations", "#synthetic", "#training", "#reinforcement_learning", "#agents", "#data", "#dataset", "#rl"], "emoji": "ðŸ§¬", "ru": {"title": "Ð­Ð²Ð¾Ð»ÑŽÑ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ ÑÐ¸Ð½Ñ‚ÐµÐ· Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ ÑÐ²Ð¾Ð»ÑŽÑ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° 
[22.10.2025 13:29] Using data from previous issue: {"categories": ["#rlhf", "#training", "#rl", "#open_source", "#reasoning", "#agents", "#agi", "#optimization", "#alignment", "#benchmark"], "emoji": "ðŸ”¬", "ru": {"title": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð°Ð³ÐµÐ½Ñ‚ Ð½Ð° 7B Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ñ‚Ð¾Ð¿Ð¾Ð²Ñ‹Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· reinforcement learning", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»
[22.10.2025 13:29] Renaming data file.
[22.10.2025 13:29] Renaming previous data. hf_papers.json to ./d/2025-10-22.json
[22.10.2025 13:29] Saving new data file.
[22.10.2025 13:29] Generating page.
[22.10.2025 13:29] Renaming previous page.
[22.10.2025 13:29] Renaming previous data. index.html to ./d/2025-10-22.html
[22.10.2025 13:29] Writing result.
[22.10.2025 13:29] Renaming log file.
[22.10.2025 13:29] Renaming previous data. log.txt to ./logs/2025-10-22_last_log.txt
