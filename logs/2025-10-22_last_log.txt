[22.10.2025 21:09] Read previous papers.
[22.10.2025 21:09] Generating top page (month).
[22.10.2025 21:09] Writing top page (month).
[22.10.2025 22:10] Read previous papers.
[22.10.2025 22:10] Get feed.
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18866
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18135
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18701
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18121
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16880
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18692
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18876
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18726
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18855
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17699
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18849
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18019
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17722
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18775
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18250
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17519
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18795
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18234
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18873
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18632
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14264
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18554
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18489
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17045
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16505
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07581
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18081
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15600
[22.10.2025 22:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.14463
[22.10.2025 22:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.17862
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18087
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15710
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15136
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13982
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17928
[22.10.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15862
[22.10.2025 22:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.10.2025 22:10] No deleted papers detected.
[22.10.2025 22:10] Downloading and parsing papers (pdf, html). Total: 36.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18866.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18866.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18866.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18135.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18135.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18135.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18701.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18701.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18701.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18121.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18121.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18121.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.16880.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.16880.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.16880.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18692.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18692.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18692.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18876.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18876.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18876.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18726.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18726.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18726.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18855.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18855.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18855.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.17699.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.17699.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.17699.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18849.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18849.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18849.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18019.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18019.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18019.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.17722.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.17722.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.17722.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18775.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18775.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18775.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18250.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18250.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18250.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.17519.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.17519.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.17519.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18795.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18795.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18795.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18234.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18234.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18234.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18873.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18873.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18873.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18632.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18632.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18632.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.14264.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.14264.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.14264.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18554.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18554.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18554.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18489.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18489.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18489.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.17045.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.17045.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.17045.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.16505.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.16505.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.16505.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.07581.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.07581.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.07581.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18081.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18081.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18081.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.15600.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.15600.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.15600.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.14463.
[22.10.2025 22:10] Downloading paper 2510.14463 from http://arxiv.org/pdf/2510.14463v1...
[22.10.2025 22:10] Extracting affiliations from text.
[22.10.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration Thomas Katraouras Dept. of Electrical and Computer Engineering University of Thessaly Volos, Greece tkatraouras@uth.gr Dimitrios Rafailidis Dept. of Electrical and Computer Engineering University of Thessaly Volos, Greece draf@uth.gr 5 2 0 2 6 1 ] . [ 1 3 6 4 4 1 . 0 1 5 2 : r AbstractImage quality is critical factor in delivering visually appealing content on web platforms. However, images often suffer from degradation due to lossy operations applied by online social networks (OSNs), negatively affecting user experience. Image restoration is the process of recovering clean high-quality image from given degraded input. Recently, multi-task (all-inone) image restoration models have gained significant attention, due to their ability to simultaneously handle different types of image degradations. However, these models often come with an excessively high number of trainable parameters, making them computationally inefficient. In this paper, we propose strategy for compressing multi-task image restoration models. We aim to discover highly sparse subnetworks within overparameterized deep models that can match or even surpass the performance of their dense counterparts. The proposed model, namely MIR-L, utilizes an iterative pruning strategy that removes low-magnitude weights across multiple rounds, while resetting the remaining weights to their original initialization. This iterative process is important for the multi-task image restoration models optimization, effectively uncovering winning tickets that maintain or exceed state-of-the-art performance at high sparsity levels. Experimental evaluation on benchmark datasets for the deraining, dehazing, and denoising tasks shows that MIR-L retains only 10% of the trainable parameters while maintaining high image restoration performance. Our code, datasets and pre-trained models are made publicly available at https://github.com/Thomkat/MIR-L. Inde"
[22.10.2025 22:10] Response: ```python
["Dept. of Electrical and Computer Engineering University of Thessaly Volos, Greece"]
```
[22.10.2025 22:10] Deleting PDF ./assets/pdf/2510.14463.pdf.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.17862.
[22.10.2025 22:10] Downloading paper 2510.17862 from http://arxiv.org/pdf/2510.17862v1...
[22.10.2025 22:10] Extracting affiliations from text.
[22.10.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 2 6 8 7 1 . 0 1 5 2 : r When Correct Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents? Yibo Peng1,, James Song2,, Lei Li3,, Xinyu Yang1, Mihai Christodorescu4, Ravi Mangal5, Corina Pasareanu1, Haizhong Zheng1, Beidi Chen1 1Carnegie Mellon University 2University of Michigan, Ann Arbor 3Peking University 4Google 5Colorado State University {yibop, xinyuya2, pcoria, haizhonz, beidic}@andrew.cmu.edu, shxjames@umich.edu, lilei2021@stu.pku.edu.cn, christodorescu@google.com, ravi.mangal@colostate.edu Equal contributions. Code agents are increasingly trusted to autonomously fix bugs on platforms such as GitHub, yet their security evaluation focuses almost exclusively on functional correctness. In this paper, we reveal novel type of threat to real-world code-agents: Functionally Correct yet Vulnerable (FCV) patches, which pass all test cases but contain vulnerable code. With our proposed FCV-Attack, which can be deliberately crafted by malicious attackers or implicitly introduced by benign developers, we show that SOTA LLMs (e.g., ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench, the attack only requires black-box access and single query to the code agent to perform the attack. For example, for CWE-538 (information exposure vulnerability), the FCV-Attack attains an attack success rate of 40.7% on GPT-5 MINI + OPENHANDS. Our results reveal an important security threat overlooked by current evaluation paradigms and urge the development of security-aware defenses for code agents. Github: https://github.com/Infini-AI-Lab/FCV Website: https://infini-ai-lab.github.io/FCV Agentic coding, in which LLM-based agents (Wang et al., 2025b; Yang et al., 2024; Team et al., 2025b; Gao et al., 2024, 2025; Ma et al., 2024; Xia et al., 2024; mini-swe-agent, 2025) autonomously read, generate, test, and submit code, has emerged as transfo"
[22.10.2025 22:10] Response: ```python
[
    "Carnegie Mellon University",
    "University of Michigan, Ann Arbor",
    "Peking University",
    "Google",
    "Colorado State University"
]
```
[22.10.2025 22:10] Deleting PDF ./assets/pdf/2510.17862.pdf.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.18087.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.18087.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.18087.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.15710.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.15710.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.15710.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.15136.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.15136.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.15136.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.13982.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.13982.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.13982.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.17928.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.17928.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.17928.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2510.15862.
[22.10.2025 22:10] Extra JSON file exists (./assets/json/2510.15862.json), skip PDF parsing.
[22.10.2025 22:10] Paper image links file exists (./assets/img_data/2510.15862.json), skip HTML parsing.
[22.10.2025 22:10] Success.
[22.10.2025 22:10] Enriching papers with extra data.
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 0. LightMem, a memory system inspired by human memory, enhances LLMs by efficiently managing historical interaction information, improving accuracy and reducing computational costs.  					AI-generated summary 				 Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effective...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 1. World-in-World evaluates generative world models in closed-loop environments, emphasizing task success over visual quality and revealing insights into controllability, data scaling, and compute allocation.  					AI-generated summary 				 Generative world models (WMs) can now simulate worlds with str...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 2. UniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.  					AI-generated summary 				 Recent progress in text-to-image (T2I) generation...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 3. CAD, a technique for long-context large language model training, improves throughput and balance by decoupling and distributing core attention computations.  					AI-generated summary 				 We present core attention disaggregation (CAD), a technique that improves long-context large language model tra...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 4. Chem-R, a three-phase trained Chemical Reasoning model, achieves superior performance on chemical tasks by integrating core knowledge, expert reasoning, and multi-task optimization.  					AI-generated summary 				 Although large language models (LLMs) have significant potential to advance chemical d...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 5. Mixture-of-Groups Attention (MoGA) enables efficient long video generation by addressing the quadratic scaling issue of full attention in Diffusion Transformers.  					AI-generated summary 				 Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full ...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 6. Grasp Any Region (GAR) enhances region-level visual understanding by integrating global contexts and modeling interactions, achieving advanced reasoning and outperforming existing models in captioning and video reference tasks.  					AI-generated summary 				 While Multimodal Large Language Models (...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 7. A new benchmark, IF-VidCap, evaluates video captioning models on instruction-following capabilities, revealing that top-tier open-source models are closing the performance gap with proprietary models.  					AI-generated summary 				 Although Multimodal Large Language Models (MLLMs) have demonstrated...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 8. Ring-1T, a trillion-parameter open-source thinking model, addresses training challenges with IcePop, C3PO++, and ASystem, achieving top results across benchmarks and democratizing large-scale reasoning intelligence.  					AI-generated summary 				 We present Ring-1T, the first open-source, state-of-...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 9. The Generalized Adversarial Solver improves diffusion model sampling efficiency and quality by combining a simple ODE solver parameterization with adversarial training.  					AI-generated summary 				 While diffusion models achieve state-of-the-art generation quality, they still suffer from computat...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 10. A Critique-Post-Edit framework enhances personalization of large language models by integrating a multi-dimensional reward model and a self-revision mechanism, outperforming standard methods.  					AI-generated summary 				 Faithfully personalizing large language models (LLMs) to align with individu...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 11. STEAM, a back-translation-based detection method, enhances multilingual watermarking robustness across various languages by addressing semantic clustering failures.  					AI-generated summary 				 Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, y...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 12. MT-Video-Bench evaluates MLLMs in multi-turn video dialogues, assessing perceptivity and interactivity across diverse domains.  					AI-generated summary 				 The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. H...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 13. UltraGen, a novel video generation framework, enables efficient high-resolution video synthesis using a hierarchical dual-branch attention architecture and spatially compressed global modeling.  					AI-generated summary 				 Recent advances in video generation have made it possible to produce visua...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 14. ssToken, a self-modulated and semantic-aware token selection approach, enhances supervised fine-tuning of large language models by adaptively selecting tokens and providing complementary semantic information, outperforming existing methods.  					AI-generated summary 				 Data quality plays a critic...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 15. A training framework for large-scale video generation models optimizes data processing, model architecture, training strategy, and infrastructure, resulting in a model that matches state-of-the-art performance and is open-sourced with Megatron-Core-based training code.  					AI-generated summary 			...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 16. ProCLIP enhances CLIP's text processing capabilities by aligning its image encoder with an LLM-based embedder through curriculum learning and contrastive tuning, preserving CLIP's pretrained knowledge.  					AI-generated summary 				 The original CLIP text encoder is limited by a maximum input lengt...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 17. DeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.  					AI-generated summary 				 We present DeepSeek-OCR as an initial investigation into the feasibility of compressing lo...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 18. DSI-Bench evaluates the dynamic spatial reasoning capabilities of vision-language and visual expertise models through a benchmark of dynamic videos and annotated questions, highlighting their limitations in understanding self-motion, object motion, and relative relationships.  					AI-generated summ...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 19. 3DThinker is a framework that enhances multimodal reasoning by integrating 3D spatial understanding from images without requiring 3D prior input or labeled data.  					AI-generated summary 				 Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide ra...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 20. AlphaQuanter, a single-agent framework using reinforcement learning, achieves top performance in automated trading by learning dynamic policies and proactively acquiring information.  					AI-generated summary 				 While Large Language Model (LLM) agents show promise in automated trading, they still...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 21. Extracting alignment training data from post-trained models using embedding models reveals significant semantic similarities and potential risks in distillation practices.  					AI-generated summary 				 In this work, we show that it is possible to extract significant amounts of alignment training d...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 22. A system for reconstructing 4D HDR scenes from unposed LDR videos using Gaussian Splatting with two-stage optimization and temporal luminance regularization.  					AI-generated summary 				 We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes f...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 23. The paper proposes V-Reason, a method that tunes the behavior of Large Multimodal Models during inference using entropy-based optimization, improving video reasoning accuracy and efficiency without reinforcement learning or supervised fine-tuning.  					AI-generated summary 				 Video reasoning usin...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 24. PRISMM-Bench evaluates the ability of large multimodal models to detect, correct, and reason over inconsistencies in scientific papers, revealing significant challenges in multimodal scientific reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) are increasingly applied to scie...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 25. Expanded Action space (ExpA) with ExpA Reinforcement Learning (EARL) enhances Large Language Models (LLMs) by decoupling environment interactions from language, improving performance in multi-turn interactions and contingent planning tasks.  					AI-generated summary 				 Large Language Models (LLMs...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 26. Any-Depth Alignment (ADA) is an inference-time defense that enhances the safety of Large Language Models (LLMs) by reintroducing alignment tokens mid-stream, ensuring robust protection against adversarial attacks without altering the model's parameters.  					AI-generated summary 				 Large Language...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 27. Thoth, a large language model trained with the Sketch-and-Fill paradigm and structured component-based reward mechanism, generates more reliable and executable scientific protocols compared to existing models.  					AI-generated summary 				 The foundation of reproducible science lies in protocols t...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 28. MIR-L, a compressed multi-task image restoration model, achieves high performance with significantly reduced parameters through iterative pruning and weight resetting.  					AI-generated summary 				 Image quality is a critical factor in delivering visually appealing content on web platforms. Howeve...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 29. FCV-Attack exploits functionally correct yet vulnerable patches in code agents, demonstrating a significant security threat to current evaluation methods.  					AI-generated summary 				 Code agents are increasingly trusted to autonomously fix bugs on platforms such as GitHub, yet their security eva...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 30. Planned diffusion combines autoregressive and diffusion models to achieve faster text generation with minimal quality loss.  					AI-generated summary 				 A central challenge in large language model inference is the trade-off between generation speed and output quality. Autoregressive models produc...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 31. A unified multimodal medical model integrates image understanding and generation, enhancing performance across various medical vision-language tasks.  					AI-generated summary 				 Medical diagnostic applications require models that can process multimodal medical inputs (images, patient histories, ...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 32. A Bidirectional LSTM outperforms classical and deep learning baselines in forecasting weekly terrorism incidents using the Global Terrorism Database.  					AI-generated summary 				 We study short-horizon forecasting of weekly terrorism incident counts using the Global Terrorism Database (GTD, 1970-...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 33. Emerging architectures combining LLMs with multi-agent dynamics offer new possibilities for modeling complex, open-ended environments, but require addressing challenges like stability, diversity, and scalability.  					AI-generated summary 				 What if artificial agents could not just communicate, b...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 34. An evolutionary framework synthesizes verifiable data for language models, improving reinforcement learning and distillation across various tasks.  					AI-generated summary 				 Reliable verifiable data has become a key driver of capability gains in modern language models, enabling stable reinforce...
[22.10.2025 22:10] ********************************************************************************
[22.10.2025 22:10] Abstract 35. PokeeResearch-7B, a 7B-parameter deep research agent, achieves state-of-the-art performance using reinforcement learning and chain-of-thought reasoning to enhance robustness and alignment.  					AI-generated summary 				 Tool-augmented large language models (LLMs) are emerging as deep research agent...
[22.10.2025 22:10] Read previous papers.
[22.10.2025 22:10] Generating reviews via LLM API.
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#data", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è AI: –±—ã—Å—Ç—Ä–µ–µ, —Ç–æ—á–Ω–µ–µ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ", "desc": "LightMem ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è LLM, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –ê—Ç–∫–∏–Ω—Å–æ–Ω–∞-–®–∏—Ñ—Ñ—Ä–∏–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –æ—Ä
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#games", "#benchmark", "#optimization", "#dataset", "#agents"], "emoji": "üåç", "ru": {"title": "–ö–æ–≥–¥–∞ –∫—Ä–∞—Å–∏–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–æ–±–æ—Ç—É: –≤–∞–∂–Ω–∞ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å, –∞ –Ω–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É World-in-World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö world models –≤ –∑
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#science", "#multilingual", "#multimodal", "#benchmark", "#survey"], "emoji": "üé®", "ru": {"title": "–í—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "UniGenBench++ ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ text-to-image –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#training", "#architecture"], "emoji": "üîÄ", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –≤–ª–∞—Å—Ç–≤—É–π –Ω–∞–¥ –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ—Ö–Ω–∏–∫—É CAD (core attention disaggregation), –∫–æ—Ç–æ—Ä–∞—è —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–ª–∏–Ω–Ω—ã
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#science", "#reasoning", "#architecture", "#benchmark", "#optimization", "#interpretability", "#training"], "emoji": "‚öóÔ∏è", "ru": {"title": "Chem-R: LLM, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç –∫–∞–∫ —Ö–∏–º–∏–∫", "desc": "Chem-R - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è —Ö–∏–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ–±—É—á–µ–Ω–Ω–∞—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –¥–ª—è —Ä–µ—à
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#video", "#architecture", "#diffusion", "#training", "#long_context"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mixture-of-Groups Attention (MoGA) ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é Diffus
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#games", "#benchmark", "#cv"], "emoji": "üîç", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ª—é–±—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å GAR (Grasp Any Region), –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö,
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤–∞–∂–Ω–µ–µ –ø–æ–ª–Ω–æ—Ç—ã –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ IF-VidCap –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏–Ω—Å
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#architecture", "#benchmark", "#optimization", "#training", "#open_source"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∏–ª–ª–∏–æ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö: –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –º–æ—â–Ω–æ–≥–æ AI-–º—ã—à–ª–µ–Ω–∏—è", "desc": "Ring-1T ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è thinking-–º–æ–¥–µ–ª—å —Å —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#training", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ adversarial –æ–±—É—á–µ–Ω–∏–µ —Å–æ–ª–≤–µ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Generalized Adversarial Solver ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Critique-Post-Edit –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#watermarking", "#low_resource"], "emoji": "üåê", "ru": {"title": "–í–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –¥–ª—è LLM —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –≤—Å–µ—Ö —è–∑—ã–∫–∞—Ö —Å –æ–±—Ä–∞—Ç–Ω—ã–º –ø–µ—Ä–µ–≤–æ–¥–æ–º", "desc": "–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –¥–ª—è LLM —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, –Ω–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —Ç–µ—Ä—è—é—Ç –Ω–∞–¥
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#multimodal", "#science", "#video", "#benchmark", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è AI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MT-Video-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#video", "#games", "#optimization", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ù–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ 4K —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "UltraGen - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –≤–ø–µ—Ä–≤—ã–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–¥–µ–æ –≤ –≤—ã—Å–æ–∫–æ–º —Ä–∞
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#training", "#data", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤: —Å–∞–º–æ–º–æ–¥—É–ª—è—Ü–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ssToken - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è supervised fine-tuning –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#video", "#architecture", "#data", "#training", "#optimization", "#open_source"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ–≥—Ä–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–ø—Ç–∏–º–∏–∑
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#alignment", "#training", "#multimodal", "#long_context"], "emoji": "üéì", "ru": {"title": "–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ CLIP —Å LLM —á–µ—Ä–µ–∑ curriculum learning", "desc": "ProCLIP —É–ª—É—á—à–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ CLIP, –∑–∞–º–µ–Ω—è—è –µ—ë —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#long_context", "#open_source", "#cv", "#training", "#data"], "emoji": "üîç", "ru": {"title": "–°–∂–∞—Ç–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ 20 —Ä–∞–∑ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è", "desc": "DeepSeek-OCR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏—á–µ—Å–∫–æ–µ 2D-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –°–∏—Å
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#cv", "#reasoning", "#benchmark", "#3d"], "emoji": "üé•", "ru": {"title": "–ö–æ–≥–¥–∞ AI —Ç–µ—Ä—è–µ—Ç—Å—è –≤ –¥–≤–∏–∂–µ–Ω–∏–∏: —Ç–µ—Å—Ç –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DSI-Bench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è AI-–º–æ–¥–µ–ª–µ–π –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –î–∞—Ç–∞—Å–µ—Ç 
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#3d", "#reasoning", "#multimodal"], "emoji": "üßä", "ru": {"title": "–ú—ã—à–ª–µ–Ω–∏–µ –≤ 3D: AI —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –±–µ–∑ —è–≤–Ω—ã—Ö —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç 3DThinker ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#games", "#optimization", "#open_source", "#rl", "#training", "#agents", "#interpretability"], "emoji": "üìà", "ru": {"title": "–û–¥–∏–Ω —É–º–Ω—ã–π –∞–≥–µ–Ω—Ç –≤–º–µ—Å—Ç–æ —Ö–∞–æ—Å–∞: RL-—Ç—Ä–µ–π–¥–µ—Ä —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ–π –ª–æ–≥–∏–∫–æ–π", "desc": "AlphaQuanter ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω–æ–≥–æ AI-–∞
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#hallucinations", "#rlhf", "#long_context", "#training", "#alignment", "#data"], "emoji": "üîì", "ru": {"title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: —Å–∫—Ä—ã—Ç—ã–µ —Ä–∏—Å–∫–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏–∑ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä—ë
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#cv"], "emoji": "üé•", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 4D HDR —Å—Ü–µ–Ω –∏–∑ LDR –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ–∑ –∫–∞–º–µ—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Mono4DGS-HDR –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 4D HDR —Å—Ü–µ–Ω –∏–∑ –Ω–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö LDR –≤–∏–¥–µ–æ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian 
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#reasoning", "#video", "#training", "#multimodal", "#inference", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç V-Reason ‚Äî –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Large Multimodal Models –Ω–∞–¥ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning", "#science"], "emoji": "üî¨", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ AI –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –≤ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ PRISMM-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞—Ö–æ–¥–∏—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–æ—Ç–∏–≤–æ
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#rl", "#optimization"], "emoji": "üéÆ", "ru": {"title": "–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π: –∫–æ–≥–¥–∞ LLM —É–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ ExpA, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ —Å—á—ë—Ç –æ—Ç–¥–µ–ª–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#alignment", "#inference", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM –Ω–∞ –ª—é–±–æ–π –≥–ª—É–±–∏–Ω–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ LLM –æ–±–ª–∞–¥–∞—é—Ç ¬´–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º¬ª (shallow alignment) ‚Äî –æ–Ω–∏ –æ—Ç–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤—Ä–µ–¥–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã 
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#open_source", "#optimization", "#data", "#science", "#benchmark", "#agents"], "emoji": "üß™", "ru": {"title": "–ù–∞—É—á–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ç –∏–¥–µ–∏ –¥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Thoth ‚Äî –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã—Ö –Ω–∞—É—á–Ω—ã—Ö
[22.10.2025 22:10] Querying the API.
[22.10.2025 22:10] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MIR-L, a compressed multi-task image restoration model, achieves high performance with significantly reduced parameters through iterative pruning and weight resetting.  					AI-generated summary 				 Image quality is a critical factor in delivering visually appealing content on web platforms. However, images often suffer from degradation due to lossy operations applied by online social networks (OSNs), negatively affecting user experience. Image restoration is the process of recovering a clean high-quality image from a given degraded input. Recently, multi-task (all-in-one) image restoration models have gained significant attention, due to their ability to simultaneously handle different types of image degradations. However, these models often come with an excessively high number of trainable parameters, making them computationally inefficient. In this paper, we propose a strategy for compressing multi-task image restoration models. We aim to discover highly sparse subnetworks within overparameterized deep models that can match or even surpass the performance of their dense counterparts. The proposed model, namely MIR-L, utilizes an iterative pruning strategy that removes low-magnitude weights across multiple rounds, while resetting the remaining weights to their original initialization. This iterative process is important for the multi-task image restoration model's optimization, effectively uncovering "winning tickets" that maintain or exceed state-of-the-art performance at high sparsity levels. Experimental evaluation on benchmark datasets for the deraining, dehazing, and denoising tasks shows that MIR-L retains only 10% of the trainable parameters while maintaining high image restoration performance. Our code, datasets and pre-trained models are made publicly available at https://github.com/Thomkat/MIR-L.
[22.10.2025 22:10] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MIR-L ‚Äî —Å–∂–∞—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (—É–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∂–¥—è, –¥—ã–º–∫–∏ –∏ —à—É–º–∞). –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –æ–±—Ä–µ–∑–∫—É (pruning), —É–¥–∞–ª—è—è –≤–µ—Å–∞ —Å –º–∞–ª–æ–π –≤–µ–ª–∏—á–∏–Ω–æ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞—è –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫ –∏—Å—Ö–æ–¥–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–µ—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –±–æ–ª—å—à–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ ¬´winning tickets¬ª –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ–≥–æ 10% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. MIR-L —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏ multi-task –º–æ–¥–µ–ª–µ–π –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –¥–µ–ª–∞—è –∏—Ö –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.",
  "emoji": "‚úÇÔ∏è",
  "title": "–û–±—Ä–µ–∑–∞–µ–º 90% –≤–µ—Å–æ–≤ ‚Äî –∫–∞—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞—ë—Ç—Å—è!"
}
```
[22.10.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MIR-L, a compressed multi-task image restoration model, achieves high performance with significantly reduced parameters through iterative pruning and weight resetting.  					AI-generated summary 				 Image quality is a critical factor in delivering visually appealing content on web platforms. However, images often suffer from degradation due to lossy operations applied by online social networks (OSNs), negatively affecting user experience. Image restoration is the process of recovering a clean high-quality image from a given degraded input. Recently, multi-task (all-in-one) image restoration models have gained significant attention, due to their ability to simultaneously handle different types of image degradations. However, these models often come with an excessively high number of trainable parameters, making them computationally inefficient. In this paper, we propose a strategy for compressing multi-task image restoration models. We aim to discover highly sparse subnetworks within overparameterized deep models that can match or even surpass the performance of their dense counterparts. The proposed model, namely MIR-L, utilizes an iterative pruning strategy that removes low-magnitude weights across multiple rounds, while resetting the remaining weights to their original initialization. This iterative process is important for the multi-task image restoration model's optimization, effectively uncovering "winning tickets" that maintain or exceed state-of-the-art performance at high sparsity levels. Experimental evaluation on benchmark datasets for the deraining, dehazing, and denoising tasks shows that MIR-L retains only 10% of the trainable parameters while maintaining high image restoration performance. Our code, datasets and pre-trained models are made publicly available at https://github.com/Thomkat/MIR-L."

[22.10.2025 22:10] Response: ```python
['DATASET', 'INFERENCE', 'CV', 'TRAINING']
```
[22.10.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MIR-L, a compressed multi-task image restoration model, achieves high performance with significantly reduced parameters through iterative pruning and weight resetting.  					AI-generated summary 				 Image quality is a critical factor in delivering visually appealing content on web platforms. However, images often suffer from degradation due to lossy operations applied by online social networks (OSNs), negatively affecting user experience. Image restoration is the process of recovering a clean high-quality image from a given degraded input. Recently, multi-task (all-in-one) image restoration models have gained significant attention, due to their ability to simultaneously handle different types of image degradations. However, these models often come with an excessively high number of trainable parameters, making them computationally inefficient. In this paper, we propose a strategy for compressing multi-task image restoration models. We aim to discover highly sparse subnetworks within overparameterized deep models that can match or even surpass the performance of their dense counterparts. The proposed model, namely MIR-L, utilizes an iterative pruning strategy that removes low-magnitude weights across multiple rounds, while resetting the remaining weights to their original initialization. This iterative process is important for the multi-task image restoration model's optimization, effectively uncovering "winning tickets" that maintain or exceed state-of-the-art performance at high sparsity levels. Experimental evaluation on benchmark datasets for the deraining, dehazing, and denoising tasks shows that MIR-L retains only 10% of the trainable parameters while maintaining high image restoration performance. Our code, datasets and pre-trained models are made publicly available at https://github.com/Thomkat/MIR-L."

[22.10.2025 22:10] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[22.10.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MIR-L is a multi-task image restoration model designed to improve image quality while using fewer parameters. It employs an iterative pruning technique to eliminate less important weights, allowing the model to maintain high performance with only 10% of the original parameters. This approach helps in finding efficient subnetworks that can handle various image degradation tasks like deraining, dehazing, and denoising. The results demonstrate that MIR-L can achieve state-of-the-art performance despite its reduced complexity, making it suitable for deployment in resource-constrained environments.","title":"Efficient Image Restoration with MIR-L: Less is More!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MIR-L is a multi-task image restoration model designed to improve image quality while using fewer parameters. It employs an iterative pruning technique to eliminate less important weights, allowing the model to maintain high performance with only 10% of the original parameters. This approach helps in finding efficient subnetworks that can handle various image degradation tasks like deraining, dehazing, and denoising. The results demonstrate that MIR-L can achieve state-of-the-art performance despite its reduced complexity, making it suitable for deployment in resource-constrained environments.', title='Efficient Image Restoration with MIR-L: Less is More!'))
[22.10.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MIR-LÊòØ‰∏ÄÁßçÂéãÁº©ÁöÑÂ§ö‰ªªÂä°ÂõæÂÉèÊÅ¢Â§çÊ®°ÂûãÔºåÈÄöËøáËø≠‰ª£Ââ™ÊûùÂíåÊùÉÈáçÈáçÁΩÆÊòæËëóÂáèÂ∞ëÂèÇÊï∞Êï∞ÈáèÔºåÂêåÊó∂ÂÆûÁé∞È´òÊÄßËÉΩ„ÄÇÂõæÂÉèÊÅ¢Â§çÊó®Âú®‰ªéÈôçË¥®ÁöÑËæìÂÖ•‰∏≠ÊÅ¢Â§çÂá∫È´òË¥®ÈáèÁöÑÂõæÂÉèÔºåÂ∞§ÂÖ∂Âú®Á§æ‰∫§ÁΩëÁªú‰∏≠Â∞§‰∏∫ÈáçË¶Å„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂèëÁé∞Á®ÄÁñèÂ≠êÁΩëÁªúÔºåËÉΩÂ§üÂú®ÂèÇÊï∞Á®ÄÁñèÁöÑÊÉÖÂÜµ‰∏ãÔºå‰øùÊåÅÊàñË∂ÖË∂äÂØÜÈõÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMIR-LÂú®‰ªÖ‰øùÁïô10%ÂèØËÆ≠ÁªÉÂèÇÊï∞ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰æùÁÑ∂ËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÂéªÈõ®„ÄÅÂéªÈõæÂíåÂéªÂô™Á≠âÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°„ÄÇ","title":"È´òÊïàÂéãÁº©ÁöÑÂ§ö‰ªªÂä°ÂõæÂÉèÊÅ¢Â§çÊ®°ÂûãMIR-L"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MIR-LÊòØ‰∏ÄÁßçÂéãÁº©ÁöÑÂ§ö‰ªªÂä°ÂõæÂÉèÊÅ¢Â§çÊ®°ÂûãÔºåÈÄöËøáËø≠‰ª£Ââ™ÊûùÂíåÊùÉÈáçÈáçÁΩÆÊòæËëóÂáèÂ∞ëÂèÇÊï∞Êï∞ÈáèÔºåÂêåÊó∂ÂÆûÁé∞È´òÊÄßËÉΩ„ÄÇÂõæÂÉèÊÅ¢Â§çÊó®Âú®‰ªéÈôçË¥®ÁöÑËæìÂÖ•‰∏≠ÊÅ¢Â§çÂá∫È´òË¥®ÈáèÁöÑÂõæÂÉèÔºåÂ∞§ÂÖ∂Âú®Á§æ‰∫§ÁΩëÁªú‰∏≠Â∞§‰∏∫ÈáçË¶Å„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂèëÁé∞Á®ÄÁñèÂ≠êÁΩëÁªúÔºåËÉΩÂ§üÂú®ÂèÇÊï∞Á®ÄÁñèÁöÑÊÉÖÂÜµ‰∏ãÔºå‰øùÊåÅÊàñË∂ÖË∂äÂØÜÈõÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMIR-LÂú®‰ªÖ‰øùÁïô10%ÂèØËÆ≠ÁªÉÂèÇÊï∞ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰æùÁÑ∂ËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÂéªÈõ®„ÄÅÂéªÈõæÂíåÂéªÂô™Á≠âÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°„ÄÇ', title='È´òÊïàÂéãÁº©ÁöÑÂ§ö‰ªªÂä°ÂõæÂÉèÊÅ¢Â§çÊ®°ÂûãMIR-L'))
[22.10.2025 22:10] Querying the API.
[22.10.2025 22:10] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FCV-Attack exploits functionally correct yet vulnerable patches in code agents, demonstrating a significant security threat to current evaluation methods.  					AI-generated summary 				 Code agents are increasingly trusted to autonomously fix bugs on platforms such as GitHub, yet their security evaluation focuses almost exclusively on functional correctness. In this paper, we reveal a novel type of threat to real-world code agents: Functionally Correct yet Vulnerable (FCV) patches, which pass all test cases but contain vulnerable code. With our proposed FCV-Attack, which can be deliberately crafted by malicious attackers or implicitly introduced by benign developers, we show that SOTA LLMs (e.g., ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench, the attack only requires black-box access and a single query to the code agent to perform the attack. For example, for CWE-538 (information exposure vulnerability), the FCV-Attack attains an attack success rate of 40.7% on GPT-5 Mini + OpenHands. Our results reveal an important security threat overlooked by current evaluation paradigms and urge the development of security-aware defenses for code agents.
[22.10.2025 22:10] Response: ```json
{
  "title": "–ö–æ–≥–¥–∞ –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ —É—è–∑–≤–∏–º: –Ω–æ–≤–∞—è —É–≥—Ä–æ–∑–∞ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –Ω–æ–≤—ã–π —Ç–∏–ø –∞—Ç–∞–∫ –Ω–∞ code-–∞–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç –±–∞–≥–∏ –≤ –∫–æ–¥–µ. –ê—Ç–∞–∫–∞ FCV-Attack —Å–æ–∑–¥–∞—ë—Ç –ø–∞—Ç—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM (ChatGPT, Claude) –∏ –∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã (SWE-agent, OpenHands) –æ–∫–∞–∑–∞–ª–∏—Å—å —É—è–∑–≤–∏–º—ã –∫ —Ç–∞–∫–∏–º –∞—Ç–∞–∫–∞–º ‚Äî –¥–ª—è —É—Å–ø–µ—à–Ω–æ–π –∞—Ç–∞–∫–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ black-box —Ä–µ–∂–∏–º–µ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–±–µ–ª –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–∞—Ö –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.",
  "emoji": "ü¶†"
}
```
[22.10.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FCV-Attack exploits functionally correct yet vulnerable patches in code agents, demonstrating a significant security threat to current evaluation methods.  					AI-generated summary 				 Code agents are increasingly trusted to autonomously fix bugs on platforms such as GitHub, yet their security evaluation focuses almost exclusively on functional correctness. In this paper, we reveal a novel type of threat to real-world code agents: Functionally Correct yet Vulnerable (FCV) patches, which pass all test cases but contain vulnerable code. With our proposed FCV-Attack, which can be deliberately crafted by malicious attackers or implicitly introduced by benign developers, we show that SOTA LLMs (e.g., ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench, the attack only requires black-box access and a single query to the code agent to perform the attack. For example, for CWE-538 (information exposure vulnerability), the FCV-Attack attains an attack success rate of 40.7% on GPT-5 Mini + OpenHands. Our results reveal an important security threat overlooked by current evaluation paradigms and urge the development of security-aware defenses for code agents."

[22.10.2025 22:10] Response: ```python
['AGENTS', 'SECURITY']
```
[22.10.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FCV-Attack exploits functionally correct yet vulnerable patches in code agents, demonstrating a significant security threat to current evaluation methods.  					AI-generated summary 				 Code agents are increasingly trusted to autonomously fix bugs on platforms such as GitHub, yet their security evaluation focuses almost exclusively on functional correctness. In this paper, we reveal a novel type of threat to real-world code agents: Functionally Correct yet Vulnerable (FCV) patches, which pass all test cases but contain vulnerable code. With our proposed FCV-Attack, which can be deliberately crafted by malicious attackers or implicitly introduced by benign developers, we show that SOTA LLMs (e.g., ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench, the attack only requires black-box access and a single query to the code agent to perform the attack. For example, for CWE-538 (information exposure vulnerability), the FCV-Attack attains an attack success rate of 40.7% on GPT-5 Mini + OpenHands. Our results reveal an important security threat overlooked by current evaluation paradigms and urge the development of security-aware defenses for code agents."

[22.10.2025 22:10] Response: ```python
["SECURITY"]
```
[22.10.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces the FCV-Attack, which targets code agents that are functionally correct but contain hidden vulnerabilities. These vulnerabilities can pass all standard tests, making them a significant security risk. The authors demonstrate that state-of-the-art language models and agent frameworks are susceptible to this type of attack, requiring only a single query to exploit the vulnerabilities. This research highlights the need for improved security evaluations that go beyond functional correctness to protect against such threats.","title":"Unmasking Hidden Vulnerabilities in Code Agents with FCV-Attack"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces the FCV-Attack, which targets code agents that are functionally correct but contain hidden vulnerabilities. These vulnerabilities can pass all standard tests, making them a significant security risk. The authors demonstrate that state-of-the-art language models and agent frameworks are susceptible to this type of attack, requiring only a single query to exploit the vulnerabilities. This research highlights the need for improved security evaluations that go beyond functional correctness to protect against such threats.', title='Unmasking Hidden Vulnerabilities in Code Agents with FCV-Attack'))
[22.10.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂÆâÂÖ®Â®ÅËÉÅÔºåÁß∞‰∏∫ÂäüËÉΩÊ≠£Á°Æ‰ΩÜËÑÜÂº±ÔºàFCVÔºâË°•‰∏Å„ÄÇËøô‰∫õË°•‰∏ÅÂú®ÂäüËÉΩ‰∏äÊòØÊ≠£Á°ÆÁöÑÔºåËÉΩÂ§üÈÄöËøáÊâÄÊúâÊµãËØïÁî®‰æãÔºå‰ΩÜÂÆûÈôÖ‰∏äÂç¥ÂåÖÂê´ÂÆâÂÖ®ÊºèÊ¥û„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑFCV-AttackÂèØ‰ª•Ë¢´ÊÅ∂ÊÑèÊîªÂáªËÄÖÁ≤æÂøÉËÆæËÆ°ÔºåÊàñËÄÖÁî±Êó†ÂÆ≥ÁöÑÂºÄÂèëËÄÖÊó†ÊÑè‰∏≠ÂºïÂÖ•„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑ‰ª£Á†Å‰ª£ÁêÜÔºàÂ¶ÇChatGPTÂíåClaudeÔºâÂú®Èù¢ÂØπËøôÁßçÂ®ÅËÉÅÊó∂Â≠òÂú®ÊòéÊòæÁöÑËÑÜÂº±ÊÄßÔºå‰∫üÈúÄÂºÄÂèëÂÆâÂÖ®Èò≤Âæ°Êé™ÊñΩ„ÄÇ","title":"ÂäüËÉΩÊ≠£Á°Æ‰ΩÜËÑÜÂº±Ôºö‰ª£Á†Å‰ª£ÁêÜÁöÑÊñ∞ÂÆâÂÖ®Â®ÅËÉÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂÆâÂÖ®Â®ÅËÉÅÔºåÁß∞‰∏∫ÂäüËÉΩÊ≠£Á°Æ‰ΩÜËÑÜÂº±ÔºàFCVÔºâË°•‰∏Å„ÄÇËøô‰∫õË°•‰∏ÅÂú®ÂäüËÉΩ‰∏äÊòØÊ≠£Á°ÆÁöÑÔºåËÉΩÂ§üÈÄöËøáÊâÄÊúâÊµãËØïÁî®‰æãÔºå‰ΩÜÂÆûÈôÖ‰∏äÂç¥ÂåÖÂê´ÂÆâÂÖ®ÊºèÊ¥û„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑFCV-AttackÂèØ‰ª•Ë¢´ÊÅ∂ÊÑèÊîªÂáªËÄÖÁ≤æÂøÉËÆæËÆ°ÔºåÊàñËÄÖÁî±Êó†ÂÆ≥ÁöÑÂºÄÂèëËÄÖÊó†ÊÑè‰∏≠ÂºïÂÖ•„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑ‰ª£Á†Å‰ª£ÁêÜÔºàÂ¶ÇChatGPTÂíåClaudeÔºâÂú®Èù¢ÂØπËøôÁßçÂ®ÅËÉÅÊó∂Â≠òÂú®ÊòéÊòæÁöÑËÑÜÂº±ÊÄßÔºå‰∫üÈúÄÂºÄÂèëÂÆâÂÖ®Èò≤Âæ°Êé™ÊñΩ„ÄÇ', title='ÂäüËÉΩÊ≠£Á°Æ‰ΩÜËÑÜÂº±Ôºö‰ª£Á†Å‰ª£ÁêÜÁöÑÊñ∞ÂÆâÂÖ®Â®ÅËÉÅ'))
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#diffusion", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç planned diffusion ‚Äî –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#games", "#optimization", "#science", "#dataset", "#benchmark", "#healthcare"], "emoji": "üè•", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è AI-–º–æ–¥–µ–ª—å: –æ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–Ω–∏–º–∫–æ–≤ –¥–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ UniMedVL ‚Äî 
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#ethics", "#training", "#data", "#dataset", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è LSTM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç–µ—Ä–∞–∫—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#agents", "#alignment", "#games", "#agi", "#benchmark", "#architecture"], "emoji": "üå±", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π –∫ –∂–∏–≤—ã–º AI-—ç–∫–æ—Å–∏—Å—Ç–µ–º–∞–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –Ω–æ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–µ LLM —Å –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö, –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ä–µ–¥
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#hallucinations", "#synthetic", "#training", "#reinforcement_learning", "#agents", "#data", "#dataset", "#rl"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ 
[22.10.2025 22:10] Using data from previous issue: {"categories": ["#rlhf", "#training", "#rl", "#open_source", "#reasoning", "#agents", "#agi", "#optimization", "#alignment", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–≥–µ–Ω—Ç –Ω–∞ 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ–ø–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ reinforcement learning", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª
[22.10.2025 22:10] Renaming data file.
[22.10.2025 22:10] Renaming previous data. hf_papers.json to ./d/2025-10-22.json
[22.10.2025 22:10] Saving new data file.
[22.10.2025 22:10] Generating page.
[22.10.2025 22:10] Renaming previous page.
[22.10.2025 22:10] Renaming previous data. index.html to ./d/2025-10-22.html
[22.10.2025 22:10] Writing result.
[22.10.2025 22:10] Renaming log file.
[22.10.2025 22:10] Renaming previous data. log.txt to ./logs/2025-10-22_last_log.txt
