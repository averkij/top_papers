[04.07.2025 07:13] Read previous papers.
[04.07.2025 07:13] Generating top page (month).
[04.07.2025 07:13] Writing top page (month).
[04.07.2025 08:16] Read previous papers.
[04.07.2025 08:16] Get feed.
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02813
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02025
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02592
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01352
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23918
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02652
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02754
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02694
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02726
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02092
[04.07.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01663
[04.07.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2507.01004
[04.07.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2506.22813
[04.07.2025 08:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.07.2025 08:16] No deleted papers detected.
[04.07.2025 08:16] Downloading and parsing papers (pdf, html). Total: 13.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02813.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02813.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02813.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02025.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02025.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02025.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02592.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02592.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02592.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.01352.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.01352.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.01352.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2506.23918.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2506.23918.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2506.23918.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02652.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02652.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02652.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02754.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02754.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02754.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02694.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02694.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02694.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02726.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02726.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02726.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.02092.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.02092.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.02092.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.01663.
[04.07.2025 08:16] Extra JSON file exists (./assets/json/2507.01663.json), skip PDF parsing.
[04.07.2025 08:16] Paper image links file exists (./assets/img_data/2507.01663.json), skip HTML parsing.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2507.01004.
[04.07.2025 08:16] Downloading paper 2507.01004 from http://arxiv.org/pdf/2507.01004v2...
[04.07.2025 08:16] Extracting affiliations from text.
[04.07.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Yuhong Chou1 , Ruijie Zhu3, Xinyi Wan4, Tianjian Li2, Congying Chu5, , Zehao Liu1 , Jibin Wu1 1The Hong Kong Polytechnic University, 2TikTok, 3UC Santa Cruz 4National University of Singapore, 5Institute of Automation, Chinese Academy of Sciences Qian Liu2 , Zejun Ma Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training model with 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on single device. At the heart of ZeCO lies All-Scan, new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 devices with an 8M sequence length, ZeCO achieves 60% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths. Date: July 1, 2025 Correspondence: "
[04.07.2025 08:16] Response: ```python
[
    "The Hong Kong Polytechnic University",
    "TikTok",
    "UC Santa Cruz",
    "National University of Singapore",
    "Institute of Automation, Chinese Academy of Sciences"
]
```
[04.07.2025 08:16] Deleting PDF ./assets/pdf/2507.01004.pdf.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2506.22813.
[04.07.2025 08:16] Downloading paper 2506.22813 from http://arxiv.org/pdf/2506.22813v1...
[04.07.2025 08:16] Extracting affiliations from text.
[04.07.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models Zhuojun Ding1, Wei Wei*1 and Chenghao Fan1 1 School of Computer Science & Technology, Huazhong University of Science and Technology {dingzj, weiw}@hust.edu.cn, facicofan@gmail.com 5 2 0 2 8 2 ] . [ 1 3 1 8 2 2 . 6 0 5 2 : r a "
[04.07.2025 08:16] Response: ```python
["School of Computer Science & Technology, Huazhong University of Science and Technology"]
```
[04.07.2025 08:16] Deleting PDF ./assets/pdf/2506.22813.pdf.
[04.07.2025 08:16] Success.
[04.07.2025 08:16] Enriching papers with extra data.
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 0. Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction ...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 1. We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure predicti...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 2. Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 3. Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorpo...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 4. Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental "semantic gap" between rich pe...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 5. Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: th...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 6. Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-b...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 7. Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 8. Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains univers...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 9. Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable d...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 10. Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the correspond...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 11. Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these wo...
[04.07.2025 08:16] ********************************************************************************
[04.07.2025 08:16] Abstract 12. Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model...
[04.07.2025 08:16] Read previous papers.
[04.07.2025 08:16] Generating reviews via LLM API.
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#open_source", "#3d", "#games", "#multimodal", "#diffusion"], "emoji": "🏙️", "ru": {"title": "Генерация 3D-сцен с языковым пониманием по нескольким изображениям", "desc": "LangScene-X - это новая генеративная система для восстановления и понимания 3D-сцен с открытым словарем на осно
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#architecture", "#training", "#healthcare"], "emoji": "🧬", "ru": {"title": "IntFold: Гибкий инструмент для точного прогнозирования биомолекулярных структур", "desc": "IntFold - это новая управляемая базовая модель для предсказания как общих, так и специализированных биомолекулярных 
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents", "#rl"], "emoji": "🧭", "ru": {"title": "WebSailor: навигация в океане неопределенности для ИИ", "desc": "Статья представляет новый метод обучения языковых моделей под названием WebSailor. Он направлен на преодоление когнитивных ог
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#open_source", "#rlhf", "#training", "#alignment", "#data", "#dataset"], "emoji": "🤖", "ru": {"title": "Синергия человека и ИИ для создания передовых моделей вознаграждения", "desc": "Статья представляет новый набор данных предпочтений SynPref-40M, содержащий 40 миллионов пар предпо
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning", "#alignment", "#survey"], "emoji": "🧠", "ru": {"title": "От мышления об изображениях к мышлению изображениями: новая эра мультимодального ИИ", "desc": "Эта статья описывает новую парадигму в мультимодальном искусственном интеллекте, где визу
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#multimodal", "#agents"], "emoji": "🧠", "ru": {"title": "HiRA: Иерархический подход к сложному информационному поиску", "desc": "Статья представляет HiRA - иерархическую систему для сложных информационных запросов. HiRA разделяет 
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#reasoning", "#math", "#training"], "emoji": "🧠", "ru": {"title": "Повышение эффективности языковых моделей с помощью 2-симплициального внимания", "desc": "В статье исследуется архитектура 2-симплициального трансформера, обобщающая стандартное внима
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#dataset", "#benchmark", "#synthetic", "#science"], "emoji": "🔬", "ru": {"title": "ИИ на страже научной объективности: LLM как помощник в рецензировании", "desc": "Статья посвящена использованию больших языковых моделей (LLM) для помощи в процессе рецензирован
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#benchmark", "#agents", "#rl"], "emoji": "🧠", "ru": {"title": "Самогенерируемые цели улучшают автоматическое доказательство теорем", "desc": "Статья представляет новый подход к автоматическому доказательству теорем с использованием больших языковых мод
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#inference", "#training", "#optimization", "#architecture"], "emoji": "🧠", "ru": {"title": "EBTs: Обучение мышлению через неконтролируемое обучение", "desc": "Эта статья представляет новый класс моделей - Energy-Based Transformers (EBTs), которые обучаются проверять со
[04.07.2025 08:16] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#rl"], "emoji": "🔄", "ru": {"title": "AsyncFlow: Асинхронное обучение с подкреплением для больших языковых моделей", "desc": "AsyncFlow - это новая асинхронная потоковая система обучения с подкреплением для эффективной пост-обработки бо
[04.07.2025 08:16] Querying the API.
[04.07.2025 08:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.
[04.07.2025 08:17] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[04.07.2025 08:17] Querying the API.
[04.07.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.
[04.07.2025 08:17] Response: {
  "desc": "Статья представляет новый подход SaM для адаптации моделей извлечения информации к различным предметным областям. Вместо обучения единой модели, SaM динамически выбирает и объединяет предобученные экспертные модели на этапе вывода. Этот метод улучшает обобщение на новые домены без дополнительного обучения и обеспечивает масштабируемость. Эксперименты показывают, что SaM превосходит единую модель в среднем на 10% по различным бенчмаркам.",
  "emoji": "🧩",
  "title": "Динамическое объединение экспертов для адаптивного извлечения информации"
}
[04.07.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework."

[04.07.2025 08:17] Response: ```python
['TRAINING', 'BENCHMARK', 'MULTIMODAL']
```
[04.07.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework."

[04.07.2025 08:17] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION']
```
[04.07.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces the SaM framework, which enhances the performance of large language models in information extraction tasks by dynamically selecting and merging expert models tailored to specific domains. Instead of training a single model for all domains, SaM identifies domain-specific experts based on their relevance and performance, allowing for better adaptation to target tasks. This approach not only improves generalization across various domains but also offers scalability by enabling the addition or removal of experts without retraining. Experimental results show that the SaM framework outperforms traditional unified models by an average of 10%, highlighting its effectiveness in optimizing task-specific performance.","title":"Dynamic Expert Selection for Enhanced Domain Adaptation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces the SaM framework, which enhances the performance of large language models in information extraction tasks by dynamically selecting and merging expert models tailored to specific domains. Instead of training a single model for all domains, SaM identifies domain-specific experts based on their relevance and performance, allowing for better adaptation to target tasks. This approach not only improves generalization across various domains but also offers scalability by enabling the addition or removal of experts without retraining. Experimental results show that the SaM framework outperforms traditional unified models by an average of 10%, highlighting its effectiveness in optimizing task-specific performance.', title='Dynamic Expert Selection for Enhanced Domain Adaptation'))
[04.07.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"监督微调（SFT）广泛应用于将大型语言模型（LLM）与信息提取（IE）任务对齐，例如命名实体识别（NER）。然而，标注这些细粒度标签和训练特定领域的模型成本高昂。现有方法通常在多个领域训练统一模型，但这种方法缺乏适应性和可扩展性，因为并非所有训练数据都能惠及目标领域。我们提出了SaM框架，在推理时动态选择和合并专家模型，从而提高了跨领域的泛化能力，而无需额外训练。","title":"动态选择与合并专家模型，提升跨领域性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='监督微调（SFT）广泛应用于将大型语言模型（LLM）与信息提取（IE）任务对齐，例如命名实体识别（NER）。然而，标注这些细粒度标签和训练特定领域的模型成本高昂。现有方法通常在多个领域训练统一模型，但这种方法缺乏适应性和可扩展性，因为并非所有训练数据都能惠及目标领域。我们提出了SaM框架，在推理时动态选择和合并专家模型，从而提高了跨领域的泛化能力，而无需额外训练。', title='动态选择与合并专家模型，提升跨领域性能'))
[04.07.2025 08:17] Renaming data file.
[04.07.2025 08:17] Renaming previous data. hf_papers.json to ./d/2025-07-04.json
[04.07.2025 08:17] Saving new data file.
[04.07.2025 08:17] Generating page.
[04.07.2025 08:17] Renaming previous page.
[04.07.2025 08:17] Renaming previous data. index.html to ./d/2025-07-04.html
[04.07.2025 08:17] Writing result.
[04.07.2025 08:17] Renaming log file.
[04.07.2025 08:17] Renaming previous data. log.txt to ./logs/2025-07-04_last_log.txt
