[04.07.2025 19:09] Read previous papers.
[04.07.2025 19:09] Generating top page (month).
[04.07.2025 19:09] Writing top page (month).
[04.07.2025 20:12] Read previous papers.
[04.07.2025 20:12] Get feed.
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02592
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02813
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02321
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02025
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01352
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23918
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02652
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02726
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02754
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02694
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02778
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02092
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01663
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01004
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22813
[04.07.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21546
[04.07.2025 20:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.07.2025 20:12] No deleted papers detected.
[04.07.2025 20:12] Downloading and parsing papers (pdf, html). Total: 16.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02592.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02592.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02592.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02813.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02813.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02813.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02321.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02321.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02321.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02025.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02025.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02025.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.01352.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.01352.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.01352.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2506.23918.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2506.23918.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2506.23918.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02652.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02652.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02652.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02726.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02726.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02726.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02754.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02754.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02754.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02694.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02694.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02694.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02778.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02778.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02778.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.02092.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.02092.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.02092.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.01663.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.01663.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.01663.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2507.01004.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2507.01004.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2507.01004.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2506.22813.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2506.22813.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2506.22813.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2506.21546.
[04.07.2025 20:12] Extra JSON file exists (./assets/json/2506.21546.json), skip PDF parsing.
[04.07.2025 20:12] Paper image links file exists (./assets/img_data/2506.21546.json), skip HTML parsing.
[04.07.2025 20:12] Success.
[04.07.2025 20:12] Enriching papers with extra data.
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 0. Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 1. Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction ...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 2. InnerControl enforces spatial consistency across all diffusion steps by training lightweight convolutional probes to improve control fidelity and generation quality in text-to-image diffusion models.  					AI-generated summary 				 Despite significant progress in text-to-image diffusion models, achi...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 3. We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure predicti...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 4. Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorpo...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 5. Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental "semantic gap" between rich pe...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 6. Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: th...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 7. Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains univers...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 8. Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-b...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 9. Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 10. Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending "Wait" notably improves their ability to correct errors in their outputs.  					AI-generated summary 				 Although larg...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 11. Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable d...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 12. Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the correspond...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 13. A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  					AI-generated summary 				 Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs)...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 14. Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model...
[04.07.2025 20:12] ********************************************************************************
[04.07.2025 20:12] Abstract 15. HalluSegBench provides a benchmark for evaluating hallucinations in vision-language segmentation models by analyzing counterfactual scene edits.  					AI-generated summary 				 Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these mo...
[04.07.2025 20:12] Read previous papers.
[04.07.2025 20:12] Generating reviews via LLM API.
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents", "#rl"], "emoji": "üß≠", "ru": {"title": "WebSailor: –Ω–∞–≤–∏–≥–∞—Ü–∏—è –≤ –æ–∫–µ–∞–Ω–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º WebSailor. –û–Ω –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –æ–≥
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#open_source", "#3d", "#games", "#multimodal", "#diffusion"], "emoji": "üèôÔ∏è", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-—Å—Ü–µ–Ω —Å —è–∑—ã–∫–æ–≤—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º", "desc": "LangScene-X - —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D-—Å—Ü–µ–Ω —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º –Ω–∞ –æ—Å–Ω–æ
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#alignment", "#training"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "InnerControl - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#healthcare"], "emoji": "üß¨", "ru": {"title": "IntFold: –ì–∏–±–∫–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –±–∏–æ–º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä", "desc": "IntFold - —ç—Ç–æ –Ω–æ–≤–∞—è —É–ø—Ä–∞–≤–ª—è–µ–º–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–∫ –æ–±—â–∏—Ö, —Ç–∞–∫ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–∏–æ–º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö 
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#open_source", "#rlhf", "#training", "#alignment", "#data", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ –ò–ò –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π SynPref-40M, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 40 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –ø—Ä–µ–¥–ø–æ
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning", "#alignment", "#survey"], "emoji": "üß†", "ru": {"title": "–û—Ç –º—ã—à–ª–µ–Ω–∏—è –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∫ –º—ã—à–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: –Ω–æ–≤–∞—è —ç—Ä–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ, –≥–¥–µ –≤–∏–∑—É
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#multimodal", "#agents"], "emoji": "üß†", "ru": {"title": "HiRA: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–ª–æ–∂–Ω–æ–º—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HiRA - –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. HiRA —Ä–∞–∑–¥–µ–ª—è–µ—Ç 
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#benchmark", "#agents", "#rl"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ —Ü–µ–ª–∏ —É–ª—É—á—à–∞—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç–µ–æ—Ä–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É —Ç–µ–æ—Ä–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#reasoning", "#math", "#training"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é 2-—Å–∏–º–ø–ª–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ 2-—Å–∏–º–ø–ª–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –æ–±–æ–±—â–∞—é—â–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#dataset", "#benchmark", "#synthetic", "#science"], "emoji": "üî¨", "ru": {"title": "–ò–ò –Ω–∞ —Å—Ç—Ä–∞–∂–µ –Ω–∞—É—á–Ω–æ–π –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: LLM –∫–∞–∫ –ø–æ–º–æ—â–Ω–∏–∫ –≤ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#alignment", "#benchmark", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è: –∫–ª—é—á –∫ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç, —á—Ç–æ —É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–∞ —Å —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ—à
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#reasoning", "#inference", "#training", "#optimization", "#architecture"], "emoji": "üß†", "ru": {"title": "EBTs: –û–±—É—á–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—é —á–µ—Ä–µ–∑ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–µ–π - Energy-Based Transformers (EBTs), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–æ
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#rl"], "emoji": "üîÑ", "ru": {"title": "AsyncFlow: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "AsyncFlow - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–æ—Ç–æ–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context", "#optimization"], "emoji": "üöÄ", "ru": {"title": "ZeCO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–ª—å—Ç—Ä–∞–¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏", "desc": "ZeCO - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –Ω—É–ª–µ–≤—ã–º–∏ –Ω–∞–∫–ª–∞–¥–Ω—ã–º–∏ —Ä–∞—Å—Ö–æ–¥–∞–º–∏ –Ω–∞ –∫–æ–º–º
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#transfer_learning", "#training", "#multimodal"], "emoji": "üß©", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ SaM –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫ —Ä–∞–∑–ª
[04.07.2025 20:12] Using data from previous issue: {"categories": ["#reasoning", "#hallucinations", "#benchmark", "#dataset"], "emoji": "üîç", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ò–ò —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", "desc": "HalluSegBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º–æ–¥–µ–ª—è—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–∞. –û–Ω –≤–∫–ª—é—á
[04.07.2025 20:12] Renaming data file.
[04.07.2025 20:12] Renaming previous data. hf_papers.json to ./d/2025-07-04.json
[04.07.2025 20:12] Saving new data file.
[04.07.2025 20:12] Generating page.
[04.07.2025 20:12] Renaming previous page.
[04.07.2025 20:12] Renaming previous data. index.html to ./d/2025-07-04.html
[04.07.2025 20:12] Writing result.
[04.07.2025 20:12] Renaming log file.
[04.07.2025 20:12] Renaming previous data. log.txt to ./logs/2025-07-04_last_log.txt
