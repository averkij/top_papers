[04.07.2025 08:17] Read previous papers.
[04.07.2025 08:17] Generating top page (month).
[04.07.2025 08:17] Writing top page (month).
[04.07.2025 09:13] Read previous papers.
[04.07.2025 09:13] Get feed.
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02813
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02025
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01352
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02592
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23918
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02652
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02754
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02694
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02726
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.02092
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22813
[04.07.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01663
[04.07.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2507.01004
[04.07.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2507.02778
[04.07.2025 09:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.07.2025 09:13] No deleted papers detected.
[04.07.2025 09:13] Downloading and parsing papers (pdf, html). Total: 14.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02813.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02813.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02813.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02025.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02025.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02025.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.01352.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.01352.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.01352.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02592.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02592.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02592.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.23918.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2506.23918.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2506.23918.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02652.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02652.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02652.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02754.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02754.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02754.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02694.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02694.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02694.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02726.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02726.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02726.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02092.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.02092.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.02092.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.22813.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2506.22813.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2506.22813.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.01663.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.01663.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.01663.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.01004.
[04.07.2025 09:13] Extra JSON file exists (./assets/json/2507.01004.json), skip PDF parsing.
[04.07.2025 09:13] Paper image links file exists (./assets/img_data/2507.01004.json), skip HTML parsing.
[04.07.2025 09:13] Success.
[04.07.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2507.02778.
[04.07.2025 09:13] Downloading paper 2507.02778 from http://arxiv.org/pdf/2507.02778v1...
[04.07.2025 09:13] Extracting affiliations from text.
[04.07.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 ] . [ 1 8 7 7 2 0 . 7 0 5 2 : r SELF-CORRECTION BENCH: REVEALING AND ADDRESSING THE SELF-CORRECTION BLIND SPOT IN LLMS PREPRINT Ken Tsui Independent kenhktsui@gmail.com July 4, "
[04.07.2025 09:13] Response: []
[04.07.2025 09:13] Extracting affiliations from text.
[04.07.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 ] . [ 1 8 7 7 2 0 . 7 0 5 2 : r SELF-CORRECTION BENCH: REVEALING AND ADDRESSING THE SELF-CORRECTION BLIND SPOT IN LLMS PREPRINT Ken Tsui Independent kenhktsui@gmail.com July 4,Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit systematic Self-Correction Blind Spot - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending Wait reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness. have not failed. Ive just found 10,000 ways that wont work. Thomas A. EdisonLarge Language Models (LLMs) have rapidly advanced natural language processing, achieving state-of-the-art results on diverse range of tasks (OpenAI et al., 2024; Anthropic, 2024; Gemini Team, 2025; Yang et al., 2025; Meta, 2025; DeepSeek-AI et al., 2025a). However, despite their impressive capabilities, LLMs are known to exhibit unpredictable failures and generate inaccurate information (Huang et al., 2025; Bang et al., 2023; Shi et al., 2023; Nezhurina et al., 2025). particularly concerning issue is the tendency of LLMs to make seemingly random errors even on simple tasks, despite possessing the underlying knowledge required for correct solutions. This raises fundamental questions about their reliability and trustworthiness, hindering their deployment in critical applications. Observing LLM self-correction behavior in natural settings is challenging due to their inherent accuracy; the rarity of naturally occurring errors makes systematic evaluation challenging. To address this limitation, we construct SelfCorrection Bench by systematically injecting error into the LLM reasoning traces. This allows us to create controlled environments for testing self-correction and reliably quantifying performance using dedicated benchmarks. This approach also allows us to simulate real-world imperfections and assess the robustness of LLMs in realistic scenarios. Our results reveal that LLMs are not good at self-correction, not because of knowledge. We refer to the phenomenon as Self-Correction Blind Spot. We further demonstrate that appending simple Wait and other correction markers can induce self-correction performance without finetuning, suggesting that the core issue is not knowledge deficiency, but rather lack of activation for self-correction. We provide behavioral explanation for why Wait works, supported by systematic analysis of Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT correction marker patterns and post training data. We also analyze why the RL trained reasoning model does not have the blind spot. Our contributions are as follows. Discover and quantify the Self-Correction Blind Spot - LLMs systematically fail to correct their own error while succeeding on identical external error (64.5% average across 14 models) Establish systematic evaluation methodology - Self-Correction Bench with controlled error injection across three complexity levels, enabling fair cross-model comparison Human demonstrations rarely include self-correction sequences, unlike RL models that learn error-correction through outcome feedback Demonstrate simple yet effective intervention - Appending Wait reduces blind spots by 89.3%, with behavioral explanation supported by correction marker analysisIntrinsic Self-correction in LLMs: Recent work has shown that intrinsic self-correction improves performance by prompting the same LLM to generate feedback on their own responses (Kim et al., 2023; Shinn et al., 2023; Madaan et al., 2023; Kamoi et al., 2024). In particular, it indicates that LLMs struggle to rectify mistakes because the quality of the models self-feedback is bounded by existing knowledge. (Kim et al., 2023) and (Shinn et al., 2023) use an oracle label to guide feedback. However, without access to the oracle label Huang et al. (2024) has identified limitations in the LLMs ability to self-correct error, while Tyen et al. (2024) demonstrates that poor self-correction performance is due to the inability to localize rather than correct an error. Most works require multiple-step prompting, while our focus is on single-pass inference, in which LLM is expected to self-correct in one completion. We further study self-correction from cognitive perspective rather than the limitation due to model knowledge. Recent work understands why some LLMs thrive and some do not in outcome-based reinforcement learning (RL) from cognitive point of view (Gandhi et al., 2025). The study uses supervised finetuning (SFT) to elicit backtracking behavior, while our focus is on self-correction, including backtracking, and we use correction markers to elicit such cognitive behavior without finetuning. Kumar et al. (2024) tackles self-correction with RL, leads to significantly positive intrinsic self-correction, while DeepSeek-AI et al. (2025a)s training does not explicitly single out self-correction but instead rely on ground truth to provide signal. They observe that the aha moment when LLM learns to allocate more time to re-evaluate its initial approach (i.e. self correction). Prompt Injection for Evaluation: Traditional prompt injection technique focuses on adversarial scenarios in which attackers inject malicious instruction or prefix to manipulate LLM output (Wei et al., 2023; Liu et al., 2024). However, controlled error injection has not been systematically explored to test the self-correction capability. Lanham et al. (2023) injects mistakes into reasoning chains to evaluate the faithfulness of chain-of-thought reasoning. However, their focus is on measuring co"
[04.07.2025 09:13] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "invalid_request_error", "param": null, "code": null}
[04.07.2025 09:13] Failed to download and parse paper https://huggingface.co/papers/2507.02778: 'choices'
[04.07.2025 09:13] Enriching papers with extra data.
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 0. Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction ...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 1. We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure predicti...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 2. Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorpo...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 3. Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 4. Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental "semantic gap" between rich pe...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 5. Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: th...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 6. Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-b...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 7. Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 8. Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains univers...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 9. Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable d...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 10. Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 11. Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the correspond...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 12. A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  					AI-generated summary 				 Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs)...
[04.07.2025 09:13] ********************************************************************************
[04.07.2025 09:13] Abstract 13. Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending "Wait" notably improves their ability to correct errors in their outputs.  					AI-generated summary 				 Although larg...
[04.07.2025 09:13] Read previous papers.
[04.07.2025 09:13] Generating reviews via LLM API.
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#open_source", "#3d", "#games", "#multimodal", "#diffusion"], "emoji": "🏙️", "ru": {"title": "Генерация 3D-сцен с языковым пониманием по нескольким изображениям", "desc": "LangScene-X - это новая генеративная система для восстановления и понимания 3D-сцен с открытым словарем на осно
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#architecture", "#training", "#healthcare"], "emoji": "🧬", "ru": {"title": "IntFold: Гибкий инструмент для точного прогнозирования биомолекулярных структур", "desc": "IntFold - это новая управляемая базовая модель для предсказания как общих, так и специализированных биомолекулярных 
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#open_source", "#rlhf", "#training", "#alignment", "#data", "#dataset"], "emoji": "🤖", "ru": {"title": "Синергия человека и ИИ для создания передовых моделей вознаграждения", "desc": "Статья представляет новый набор данных предпочтений SynPref-40M, содержащий 40 миллионов пар предпо
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#agents", "#rl"], "emoji": "🧭", "ru": {"title": "WebSailor: навигация в океане неопределенности для ИИ", "desc": "Статья представляет новый метод обучения языковых моделей под названием WebSailor. Он направлен на преодоление когнитивных ог
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#reasoning", "#alignment", "#survey"], "emoji": "🧠", "ru": {"title": "От мышления об изображениях к мышлению изображениями: новая эра мультимодального ИИ", "desc": "Эта статья описывает новую парадигму в мультимодальном искусственном интеллекте, где визу
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rag", "#benchmark", "#multimodal", "#agents"], "emoji": "🧠", "ru": {"title": "HiRA: Иерархический подход к сложному информационному поиску", "desc": "Статья представляет HiRA - иерархическую систему для сложных информационных запросов. HiRA разделяет 
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#reasoning", "#math", "#training"], "emoji": "🧠", "ru": {"title": "Повышение эффективности языковых моделей с помощью 2-симплициального внимания", "desc": "В статье исследуется архитектура 2-симплициального трансформера, обобщающая стандартное внима
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#dataset", "#benchmark", "#synthetic", "#science"], "emoji": "🔬", "ru": {"title": "ИИ на страже научной объективности: LLM как помощник в рецензировании", "desc": "Статья посвящена использованию больших языковых моделей (LLM) для помощи в процессе рецензирован
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#benchmark", "#agents", "#rl"], "emoji": "🧠", "ru": {"title": "Самогенерируемые цели улучшают автоматическое доказательство теорем", "desc": "Статья представляет новый подход к автоматическому доказательству теорем с использованием больших языковых мод
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#inference", "#training", "#optimization", "#architecture"], "emoji": "🧠", "ru": {"title": "EBTs: Обучение мышлению через неконтролируемое обучение", "desc": "Эта статья представляет новый класс моделей - Energy-Based Transformers (EBTs), которые обучаются проверять со
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#transfer_learning", "#training", "#multimodal"], "emoji": "🧩", "ru": {"title": "Динамическое объединение экспертов для адаптивного извлечения информации", "desc": "Статья представляет новый подход SaM для адаптации моделей извлечения информации к разл
[04.07.2025 09:13] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#rl"], "emoji": "🔄", "ru": {"title": "AsyncFlow: Асинхронное обучение с подкреплением для больших языковых моделей", "desc": "AsyncFlow - это новая асинхронная потоковая система обучения с подкреплением для эффективной пост-обработки бо
[04.07.2025 09:13] Querying the API.
[04.07.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  					AI-generated summary 				 Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.
[04.07.2025 09:13] Response: {
    "desc": "ZeCO - это новый метод параллелизма последовательностей с нулевыми накладными расходами на коммуникацию, разработанный для эффективного обучения больших языковых моделей с ультрадлинными последовательностями на нескольких устройствах. В основе ZeCO лежит новый примитив коллективной коммуникации All-Scan, который обеспечивает каждому рангу SP точное начальное состояние оператора при минимальных затратах на коммуникацию. Теоретически доказана оптимальность ZeCO, показывающая, что он вносит лишь незначительные временные и пространственные накладные расходы. Эмпирически продемонстрировано, что на 256 GPU с длиной последовательности 8M ZeCO достигает ускорения на 60% по сравнению с современными методами SP.",
    "emoji": "🚀",
    "title": "ZeCO: Революция в обучении языковых моделей с ультрадлинными последовательностями"
}
[04.07.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  					AI-generated summary 				 Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths."

[04.07.2025 09:13] Response: ```python
['TRAINING', 'ARCHITECTURE']
```
[04.07.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  					AI-generated summary 				 Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths."

[04.07.2025 09:13] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[04.07.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents ZeCO, a novel sequence parallelism method that eliminates communication overhead during the training of large language models (LLMs) with ultra-long sequences. By utilizing linear attention mechanisms, ZeCO allows for efficient processing of sequences up to 1 million tokens across multiple devices without the typical bottlenecks caused by communication delays. The core innovation, All-Scan, enables each device to access the necessary operator state with minimal communication, resulting in near-linear scalability for long sequence training. Empirical results show that ZeCO significantly outperforms existing methods, achieving a 60% speedup on 256 GPUs with an 8M sequence length, paving the way for training next-generation LLMs.","title":"ZeCO: Revolutionizing Long Sequence Training with Zero Communication Overhead"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents ZeCO, a novel sequence parallelism method that eliminates communication overhead during the training of large language models (LLMs) with ultra-long sequences. By utilizing linear attention mechanisms, ZeCO allows for efficient processing of sequences up to 1 million tokens across multiple devices without the typical bottlenecks caused by communication delays. The core innovation, All-Scan, enables each device to access the necessary operator state with minimal communication, resulting in near-linear scalability for long sequence training. Empirical results show that ZeCO significantly outperforms existing methods, achieving a 60% speedup on 256 GPUs with an 8M sequence length, paving the way for training next-generation LLMs.', title='ZeCO: Revolutionizing Long Sequence Training with Zero Communication Overhead'))
[04.07.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种新的零通信开销序列并行方法ZeCO，旨在高效训练具有超长序列的大型语言模型。ZeCO通过引入All-Scan这一新的集体通信原语，显著减少了设备间的通信开销，从而实现了接近线性的可扩展性。与传统的序列并行方法相比，ZeCO在多个设备上处理1M序列时的训练时间与在单个设备上处理16k序列的时间相当。实验结果表明，ZeCO在256个GPU上处理8M序列时，相较于现有的最佳序列并行方法实现了60%的速度提升。","title":"ZeCO：高效训练超长序列的大型语言模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种新的零通信开销序列并行方法ZeCO，旨在高效训练具有超长序列的大型语言模型。ZeCO通过引入All-Scan这一新的集体通信原语，显著减少了设备间的通信开销，从而实现了接近线性的可扩展性。与传统的序列并行方法相比，ZeCO在多个设备上处理1M序列时的训练时间与在单个设备上处理16k序列的时间相当。实验结果表明，ZeCO在256个GPU上处理8M序列时，相较于现有的最佳序列并行方法实现了60%的速度提升。', title='ZeCO：高效训练超长序列的大型语言模型'))
[04.07.2025 09:13] Querying the API.
[04.07.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending "Wait" notably improves their ability to correct errors in their outputs.  					AI-generated summary 				 Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.
[04.07.2025 09:13] Response: {
  "desc": "Исследование представляет методологию Self-Correction Bench для измерения способности больших языковых моделей (LLM) к самокоррекции. Авторы обнаружили, что обучение преимущественно на безошибочных ответах приводит к "слепому пятну самокоррекции" - неспособности моделей исправлять собственные ошибки. Тестирование 14 моделей показало среднюю частоту появления слепого пятна 64.5%. Интересно, что простое добавление слова "Wait" снижает этот показатель на 89.3%, что указывает на наличие способности к самокоррекции, требующей активации.",
  "emoji": "🔍",
  "title": "Преодоление слепого пятна самокоррекции в языковых моделях"
}
[04.07.2025 09:13] Error. Failed to parse JSON from LLM. {
  "desc": "Исследование представляет методологию Self-Correction Bench для измерения способности больших языковых моделей (LLM) к самокоррекции. Авторы обнаружили, что обучение преимущественно на безошибочных ответах приводит к "слепому пятну самокоррекции" - неспособности моделей исправлять собственные ошибки. Тестирование 14 моделей показало среднюю частоту появления слепого пятна 64.5%. Интересно, что простое добавление слова "Wait" снижает этот показатель на 89.3%, что указывает на наличие способности к самокоррекции, требующей активации.",
  "emoji": "🔍",
  "title": "Преодоление слепого пятна самокоррекции в языковых моделях"
}
[04.07.2025 09:13] Fallback to OpenAI.
[04.07.2025 09:13] Response: ParsedChatCompletionMessage[ArticleFull](content='{"desc":"Исследование выявляет, что у больших языковых моделей (LLM) есть проблема с самокоррекцией, так как они обучаются на данных без ошибок. Это приводит к тому, что модели не могут исправлять собственные ошибки, хотя и могут находить ошибки в пользовательском вводе. Введение фразы \\"Wait\\" помогает моделям активировать способность к самокоррекции, что значительно уменьшает количество ошибок. Работа подчеркивает важность улучшения надежности LLM через обучение на данных с ошибками и их исправлениями.","emoji":"🔍","title":"Самокоррекция: ключ к надежности LLM"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=ArticleFull(desc='Исследование выявляет, что у больших языковых моделей (LLM) есть проблема с самокоррекцией, так как они обучаются на данных без ошибок. Это приводит к тому, что модели не могут исправлять собственные ошибки, хотя и могут находить ошибки в пользовательском вводе. Введение фразы "Wait" помогает моделям активировать способность к самокоррекции, что значительно уменьшает количество ошибок. Работа подчеркивает важность улучшения надежности LLM через обучение на данных с ошибками и их исправлениями.', emoji='🔍', title='Самокоррекция: ключ к надежности LLM'))
[04.07.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending "Wait" notably improves their ability to correct errors in their outputs.  					AI-generated summary 				 Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness."

[04.07.2025 09:13] Response: ```python
['BENCHMARK', 'TRAINING']
```
[04.07.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending "Wait" notably improves their ability to correct errors in their outputs.  					AI-generated summary 				 Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness."

[04.07.2025 09:13] Response: ```python
["HALLUCINATIONS", "REASONING", "ALIGNMENT"]
```
[04.07.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Self-Correction Bench, a framework designed to measure the self-correction capabilities of large language models (LLMs). It identifies a significant issue known as the \'Self-Correction Blind Spot\', where LLMs fail to correct errors in their own outputs despite being able to recognize errors in user inputs. The study reveals that this blind spot is largely due to the training data, which often consists of error-free examples rather than sequences that include error corrections. Notably, the simple addition of the word \'Wait\' to prompts can significantly enhance the models\' self-correction abilities, indicating that these capabilities can be activated with the right cues.","title":"Unlocking Self-Correction in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Self-Correction Bench, a framework designed to measure the self-correction capabilities of large language models (LLMs). It identifies a significant issue known as the 'Self-Correction Blind Spot', where LLMs fail to correct errors in their own outputs despite being able to recognize errors in user inputs. The study reveals that this blind spot is largely due to the training data, which often consists of error-free examples rather than sequences that include error corrections. Notably, the simple addition of the word 'Wait' to prompts can significantly enhance the models' self-correction abilities, indicating that these capabilities can be activated with the right cues.", title='Unlocking Self-Correction in Language Models'))
[04.07.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文介绍了自我纠正基准（Self-Correction Bench），用于测量大型语言模型（LLM）在自我纠正方面的盲点。研究发现，主要在无错误的响应上进行训练会导致模型在自身输出中无法纠正相同的错误。通过对14个模型进行测试，发现平均有64.5%的盲点率。简单地在输出中添加“等待”一词可以将盲点减少89.3%，这表明模型具备自我纠正的能力，但需要激活。","title":"激活自我纠正，提升语言模型的可靠性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文介绍了自我纠正基准（Self-Correction Bench），用于测量大型语言模型（LLM）在自我纠正方面的盲点。研究发现，主要在无错误的响应上进行训练会导致模型在自身输出中无法纠正相同的错误。通过对14个模型进行测试，发现平均有64.5%的盲点率。简单地在输出中添加“等待”一词可以将盲点减少89.3%，这表明模型具备自我纠正的能力，但需要激活。', title='激活自我纠正，提升语言模型的可靠性'))
[04.07.2025 09:13] Renaming data file.
[04.07.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-07-04.json
[04.07.2025 09:13] Saving new data file.
[04.07.2025 09:13] Generating page.
[04.07.2025 09:13] Renaming previous page.
[04.07.2025 09:13] Renaming previous data. index.html to ./d/2025-07-04.html
[04.07.2025 09:13] Writing result.
[04.07.2025 09:13] Renaming log file.
[04.07.2025 09:13] Renaming previous data. log.txt to ./logs/2025-07-04_last_log.txt
