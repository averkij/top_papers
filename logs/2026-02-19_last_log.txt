[19.02.2026 06:53] Read previous papers.
[19.02.2026 06:53] Generating top page (month).
[19.02.2026 06:53] Writing top page (month).
[19.02.2026 07:45] Read previous papers.
[19.02.2026 07:45] Get feed.
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12675
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16705
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14979
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14080
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16301
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15922
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15989
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16682
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16493
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16666
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07345
[19.02.2026 07:45] Extract page data from URL. URL: https://huggingface.co/papers/2602.15927
[19.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08392
[19.02.2026 07:45] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2026 07:45] No deleted papers detected.
[19.02.2026 07:45] Downloading and parsing papers (pdf, html). Total: 13.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.12675.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.12675.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.12675.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.16705.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.16705.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.16705.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.14979.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.14979.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.14979.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.14080.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.14080.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.14080.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.16301.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.16301.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.16301.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.15922.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.15922.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.15922.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.15989.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.15989.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.15989.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.16682.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.16682.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.16682.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.16493.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.16493.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.16493.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.16666.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.16666.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.16666.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.07345.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.07345.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.07345.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.15927.
[19.02.2026 07:45] Downloading paper 2602.15927 from https://arxiv.org/pdf/2602.15927v1...
[19.02.2026 07:45] Extracting affiliations from text.
[19.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 7 2 9 5 1 . 2 0 6 2 : r Visual Memory Injection Attacks for Multi-Turn Conversations Christian Schlarmann 1 Matthias Hein "
[19.02.2026 07:45] Response: ```python
[]
```
[19.02.2026 07:45] Extracting affiliations from text.
[19.02.2026 07:45] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 7 2 9 5 1 . 2 0 6 2 : r Visual Memory Injection Attacks for Multi-Turn Conversations Christian Schlarmann 1 Matthias Heinlarge Generative vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads manipulated image to the web/social media. benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives triggering prompt, the LVLM outputs specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after long multiturn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code on GitHub. 1. Introduction The success of generative large vision-language models (LVLMs) (Alayrac et al., 2022; Awadalla et al., 2023; Liu et al., 2023; Bai et al., 2025a; An et al., 2025) has led to their broad adoption and deployment (Achiam et al., 2023; Gemini Team, 2023; Anthropic, 2024; Liu et al., 2024a). These models can process images as well as text inputs and generate natural language responses, all in multi-turn conversation setting. As part of online chatbots, millions of users interact with them daily. This scale makes LVLMs increasingly attractive targets for malicious parties, who could exploit model weaknesses to inflict widespread harm. 1Tubingen AI Center, University of Tubingen, Germany. Correspondence to: Christian Schlarmann <christian.schlarmann@unituebingen.de>. Figure 1. Visual Memory Injection. An adversary manipulates an image via VMI with small perturbation and uploads it online. When an unsuspecting user shares this image in LVLM conversation, the model behaves normally for several conversation turns. However, when the user asks about trigger topic (stock advice), the model outputs the injected target (buy GameStop stock). Prior work (Schlarmann & Hein, 2023; Bailey et al., 2024) has demonstrated that an attacker can add small visual perturbations to input images in order to force LVLMs into outputting given target string. This allows malicious third parties to harm honest users by forcing LVLMs to output false information. However, these studies are limited to single-turn interactions, meaning the context consists of single user prompt and the influence of the attack beyond the first prompt is not considered. In practice, however, 1 Visual Memory Injection Attacks for Multi-Turn Conversations users often interact with LVLMs in multi-turn fashion (Liu et al., 2024b). In this work, we therefore develop an attack that is tailored to the multi-turn conversation setting. extensively (Szegedy et al., 2014; Goodfellow et al., 2015). large body of work has focussed on improving attack algorithms (Carlini & Wagner, 2017; Croce & Hein, 2020). In multi-turn chats, an image that was once provided to an LVLM usually remains in the context for the duration of the conversation (Bai et al., 2025a). In subsequent conversation turns, the LVLM thereby continues to process the image, potentially influencing its output. We show that an adversary can manipulate an image so that LVLMs exhibit target behavior (e.g. recommending product or stock) even after over 25 unrelated conversation turns. Crucially, we use benign anchoring technique that causes the behavior to only be triggered on topic-related prompts (e.g. Which stock should buy?). On unrelated prompts the model behaves normally, thus raising no suspicion in the user. We call this attack Visual Memory Injection (VMI). VMI enables concerning applications as shown in Section 5: adversarial marketing campaigns could manipulate product recommendations, malicious actors could influence political opinions during election periods, and fraudulent schemes could push specific financial advice. The scalability of the attack (one adversarial image can affect many users) combined with its stealthy nature makes Visual Memory Injection attacks significant threat that warrants careful study and the development of appropriate defenses. As illustrated in Fig. 1, the attack remains effective even after several conversation turns, demonstrating remarkable persistence across extended dialogues. Our contributions can be summarized as follows: 1) We introduce Visual Memory Injections, novel attack scenario for multi-turn LVLM conversations, where an adversary exploits the persistent visual context to inject targeted malicious behavior triggered only by specific topics, while the model behaves normally otherwise. 2) We propose our attack VMI which has two key components: (i) benign anchoring, which jointly optimizes for helpful first-turn output alongside the n-th turn malicious target response, preventing model degeneration; and (ii) context-cycling, which varies context lengths during optimization, making the attack persist across conversation lengths. 3) We provide comprehensive evaluation of VMI on three recent open-weight LVLMs across multiple attack targets, demonstrating effectiveness even after long conversations and transferability to unseen prompts, contexts, and even to fine-tuned variants of source LVLMs. 2. Related Work Adversarial attacks against LVLMs. The visual input modality of LVLMs has been shown to provide attack surface for jailbreaking (Qi et al., 2024; Carlini et al., 2023; Shayegani et al., 2024) and targeted attacks in single-turn settings (Schlarmann & Hein, 2023; Zhao et al., 2023; Bagdasaryan et al., 2023; Bailey et al., 2024; Miao et al., 2025). The transferability of targeted attacks across prompts in single-turn setting has been investigated by Luo et al. (2024). Lu et al. (2024) propose test-time backdoor attack, where malicious user plants visual backdoor. They evaluate this attack in single-turn settings. In contrast, our work focusses on benign users being harmed by malicious third party in multi-turn conversation sett"
[19.02.2026 07:45] Mistral response. {"id": "133e4df263894f66a0868fab1eafee21", "created": 1771487117, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1516, "total_tokens": 1535, "completion_tokens": 19, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Tubingen AI Center, University of Tubingen, Germany\"]\n```"}}]}
[19.02.2026 07:45] Response: ```python
["Tubingen AI Center, University of Tubingen, Germany"]
```
[19.02.2026 07:45] Deleting PDF ./assets/pdf/2602.15927.pdf.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.08392.
[19.02.2026 07:45] Extra JSON file exists (./assets/json/2602.08392.json), skip PDF parsing.
[19.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.08392.json), skip HTML parsing.
[19.02.2026 07:45] Success.
[19.02.2026 07:45] Enriching papers with extra data.
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 0. SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.  					AI-generated summary 				 Sparse-Linear Attention (SLA) combines sparse and linear attention to acc...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 1. HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.  					AI-generated summary 				 Visual loco-manipulation of arbitrary objects in the wil...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 2. RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.  					AI-generated summary 				 Despite rapid progress in multimodal foundation models, embodied inte...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 3. LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.  					AI-generated summary 				 Standard factuality evaluations of LLMs treat all e...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 4. Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.  					AI-generated summary 				 Achieving cooperation among self-interested agents remains a fundamental challenge in multi-a...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 5. DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.  					AI-generated summary 				 State-of-the-art Vision-Language-Action (VLA) models excel at sema...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 6. A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.  					AI-generated summary 				 We introduce SAM 3D Body (3DB), a promptable model for single...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 7. SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.  					AI-generated summary 				 A core aspect of human p...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 8. Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.  					AI-generated summary 				 Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval oft...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 9. Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.  					AI-generated summary 				 AI agents are increasingly deploy...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 10. Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape.  					AI-generated summary 				 Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its s...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 11. Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  					AI-generated summary 				 Generative large vision-language models (LVLMs) have...
[19.02.2026 07:45] ********************************************************************************
[19.02.2026 07:45] Abstract 12. BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI,...
[19.02.2026 07:45] Read previous papers.
[19.02.2026 07:45] Generating reviews via LLM API.
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#inference", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SLA2, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ-–ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –¥–∏—Ñ
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#robotics", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ —Ä–æ–±–æ—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª–æ–≤–∫–∏–º: —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ + –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã", "desc": "HERO ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#open_source", "#transfer_learning", "#multimodal", "#reasoning", "#benchmark", "#training", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–∞", "desc": "RynnBrain ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø—Ä–æ—Å—Ç
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#hallucinations", "#dataset", "#reasoning", "#benchmark"], "emoji": "üîë", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –µ—Å—Ç—å, –Ω–æ –∫–ª—é—á–∏ –ø–æ—Ç–µ—Ä—è–Ω—ã: –∫–∞–∫ LLM –∫–æ–¥–∏—Ä—É—é—Ç —Ñ–∞–∫—Ç—ã, –Ω–æ –Ω–µ –º–æ–≥—É—Ç –∏—Ö –≤—Å–ø–æ–º–Ω–∏—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –∫–æ–¥–∏—Ä—É—é—Ç –ø–æ—á—Ç–∏ –≤—Å–µ —Ñ–∞–∫—Ç—ã (9
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#rl", "#agents", "#games", "#reasoning", "#architecture"], "emoji": "ü§ù", "ru": {"title": "–ö–æ–æ–ø–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã) –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–≥–µ–Ω—Ç–∞–º –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#multimodal", "#training", "#robotics", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–î–∏–Ω–∞–º–∏–∫–∞ –≤–º–µ—Å—Ç–æ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º", "desc": "DreamZero ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π (World Action Model)
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#data", "#architecture", "#dataset"], "emoji": "üßç", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç SAM 3D Body (3DB) ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#video", "#survey", "#benchmark", "#multimodal", "#reasoning", "#dataset"], "emoji": "üëì", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∏—Ä–∞ —á–µ—Ä–µ–∑ –≥–ª–∞–∑–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª—è", "desc": "SAW-Bench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö foundation models –ø–æ–Ω–∏–º–∞—Ç—å –æ–∫—Ä—É–∂
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#agents", "#security", "#benchmark", "#multimodal", "#open_source", "#long_context", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–∞–¥–µ–∂–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –∏ –±–æ—Ä—å–±–∞ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–≥–µ–Ω—Ç Mu
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∫—Ä–∏—Ç–∏–∫—É–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É —É—Å–ø–µ—Ö–∞ –∏ –Ω–µ –≤—ã—è–≤–ª—è—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏. –ê
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#training", "#benchmark", "#video", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –∑–æ–Ω –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Adaptive Matching Distillation (AMD) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ
[19.02.2026 07:45] Querying the API.
[19.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  					AI-generated summary 				 Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection
[19.02.2026 07:45] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Ç–∞–∫—É Visual Memory Injection, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∫—Ä—ã—Ç–Ω–æ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ vision-language –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Å–∫—Ä—ã—Ç—ã–µ –≤–æ–∑–º—É—â–µ–Ω–∏—è. –ê—Ç–∞–∫–∞ —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –≤—ã–≥–ª—è–¥–µ–ª–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø—Ä–∏ –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö, –Ω–æ –≤—ã–¥–∞–≤–∞–ª–∞ —Ü–µ–ª–µ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–º —Ç—Ä–∏–≥–≥–µ—Ä-–∑–∞–ø—Ä–æ—Å–µ. –û—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –∞—Ç–∞–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–∞–∂–µ –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ—Ç—É—Ä–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö open-weight LVLMs –∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø–æ–¥–æ–±–Ω—ã–º –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è–º.",
  "emoji": "üé≠",
  "title": "–°–∫—Ä—ã—Ç–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —á–µ—Ä–µ–∑ –≤–æ–∑–º—É—â—ë–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö"
}
```
[19.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  					AI-generated summary 				 Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection"

[19.02.2026 07:45] Response: ```python
['CV', 'MULTIMODAL', 'BENCHMARK']
```
[19.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  					AI-generated summary 				 Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection"

[19.02.2026 07:45] Response: ```python
['SECURITY', 'LONG_CONTEXT', 'OPEN_SOURCE']
```
[19.02.2026 07:45] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper introduces a new type of attack called Visual Memory Injection (VMI) that targets generative vision-language models (LVLMs). The attack allows an adversary to manipulate the model\'s responses by using specially crafted images that only trigger specific outputs when certain prompts are given during multi-turn conversations. Unlike previous attacks that worked in single-turn scenarios, VMI remains effective even after multiple interactions, making it a significant threat. The authors demonstrate the feasibility of this attack on various open-weight LVLMs, highlighting the need for improved security measures in these models.","title":"Stealthy Manipulation of LVLMs through Visual Memory Injection"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a new type of attack called Visual Memory Injection (VMI) that targets generative vision-language models (LVLMs). The attack allows an adversary to manipulate the model's responses by using specially crafted images that only trigger specific outputs when certain prompts are given during multi-turn conversations. Unlike previous attacks that worked in single-turn scenarios, VMI remains effective even after multiple interactions, making it a significant threat. The authors demonstrate the feasibility of this attack on various open-weight LVLMs, highlighting the need for improved security measures in these models.", title='Stealthy Manipulation of LVLMs through Visual Memory Injection'))
[19.02.2026 07:45] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"ËßÜËßâËÆ∞ÂøÜÊ≥®ÂÖ•ÊîªÂáªÔºàVMIÔºâÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÊîªÂáªÊñπÂºèÔºåËÉΩÂ§üÂú®Â§öËΩÆÂØπËØù‰∏≠ÈÄöËøáÊìçÊéßÂõæÂÉèÊù•ÈöêÁßòÂú∞ÊìçÁ∫µÁîüÊàêÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMÔºâ„ÄÇÊîªÂáªËÄÖ‰∏ä‰º†ÁªèËøáÊìçÊéßÁöÑÂõæÂÉèÔºåÁî®Êà∑‰∏ãËΩΩÂêé‰Ωú‰∏∫ËæìÂÖ•‰ΩøÁî®ÔºåLVLMÂú®Ê≠£Â∏∏ÊèêÁ§∫‰∏ãË°®Áé∞Ê≠£Â∏∏Ôºå‰ΩÜÂú®ÁâπÂÆöËß¶ÂèëÊèêÁ§∫‰∏ã‰ºöËæìÂá∫È¢ÑËÆæÁöÑÁõÆÊ†á‰ø°ÊÅØÔºå‰ªéËÄåÂΩ±ÂìçÁî®Êà∑ÁöÑÂÜ≥Á≠ñ„ÄÇ‰∏é‰ª•ÂæÄÂè™ÂÖ≥Ê≥®ÂçïËΩÆÊîªÂáªÁöÑÁ†îÁ©∂‰∏çÂêåÔºåVMIÂú®ÈïøÊó∂Èó¥ÁöÑÂ§öËΩÆÂØπËØù‰∏≠‰æùÁÑ∂ÊúâÊïàÔºåÂ±ïÁ§∫‰∫ÜÂ§ßËßÑÊ®°ÊìçÊéßÁî®Êà∑ÁöÑÂèØËÉΩÊÄß„ÄÇËØ•Á†îÁ©∂ÂëºÂêÅÂØπLVLMËøõË°åÊõ¥Â•ΩÁöÑÂÆâÂÖ®ÊÄßÂ¢ûÂº∫Ôºå‰ª•ÊäµÂæ°Ê≠§Á±ªÊîªÂáª„ÄÇ","title":"ËßÜËßâËÆ∞ÂøÜÊ≥®ÂÖ•ÔºöÂ§öËΩÆÂØπËØù‰∏≠ÁöÑÈöêÁßòÊìçÊéß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËßÜËßâËÆ∞ÂøÜÊ≥®ÂÖ•ÊîªÂáªÔºàVMIÔºâÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÊîªÂáªÊñπÂºèÔºåËÉΩÂ§üÂú®Â§öËΩÆÂØπËØù‰∏≠ÈÄöËøáÊìçÊéßÂõæÂÉèÊù•ÈöêÁßòÂú∞ÊìçÁ∫µÁîüÊàêÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMÔºâ„ÄÇÊîªÂáªËÄÖ‰∏ä‰º†ÁªèËøáÊìçÊéßÁöÑÂõæÂÉèÔºåÁî®Êà∑‰∏ãËΩΩÂêé‰Ωú‰∏∫ËæìÂÖ•‰ΩøÁî®ÔºåLVLMÂú®Ê≠£Â∏∏ÊèêÁ§∫‰∏ãË°®Áé∞Ê≠£Â∏∏Ôºå‰ΩÜÂú®ÁâπÂÆöËß¶ÂèëÊèêÁ§∫‰∏ã‰ºöËæìÂá∫È¢ÑËÆæÁöÑÁõÆÊ†á‰ø°ÊÅØÔºå‰ªéËÄåÂΩ±ÂìçÁî®Êà∑ÁöÑÂÜ≥Á≠ñ„ÄÇ‰∏é‰ª•ÂæÄÂè™ÂÖ≥Ê≥®ÂçïËΩÆÊîªÂáªÁöÑÁ†îÁ©∂‰∏çÂêåÔºåVMIÂú®ÈïøÊó∂Èó¥ÁöÑÂ§öËΩÆÂØπËØù‰∏≠‰æùÁÑ∂ÊúâÊïàÔºåÂ±ïÁ§∫‰∫ÜÂ§ßËßÑÊ®°ÊìçÊéßÁî®Êà∑ÁöÑÂèØËÉΩÊÄß„ÄÇËØ•Á†îÁ©∂ÂëºÂêÅÂØπLVLMËøõË°åÊõ¥Â•ΩÁöÑÂÆâÂÖ®ÊÄßÂ¢ûÂº∫Ôºå‰ª•ÊäµÂæ°Ê≠§Á±ªÊîªÂáª„ÄÇ', title='ËßÜËßâËÆ∞ÂøÜÊ≥®ÂÖ•ÔºöÂ§öËΩÆÂØπËØù‰∏≠ÁöÑÈöêÁßòÊìçÊéß'))
[19.02.2026 07:45] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#multimodal", "#reasoning", "#hallucinations"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ –¥–≤–µ —Ä—É–∫–∏ –ª—É—á—à–µ, —á–µ–º –æ–¥–Ω–∞: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "BiManiBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[19.02.2026 07:45] Renaming data file.
[19.02.2026 07:45] Renaming previous data. hf_papers.json to ./d/2026-02-19.json
[19.02.2026 07:45] Saving new data file.
[19.02.2026 07:45] Generating page.
[19.02.2026 07:45] Renaming previous page.
[19.02.2026 07:45] Renaming previous data. index.html to ./d/2026-02-19.html
[19.02.2026 07:45] Writing result.
[19.02.2026 07:45] Renaming log file.
[19.02.2026 07:45] Renaming previous data. log.txt to ./logs/2026-02-19_last_log.txt
