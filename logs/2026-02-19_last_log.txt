[19.02.2026 12:44] Read previous papers.
[19.02.2026 12:44] Generating top page (month).
[19.02.2026 12:44] Writing top page (month).
[19.02.2026 14:05] Read previous papers.
[19.02.2026 14:05] Get feed.
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12675
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16705
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14979
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14080
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15989
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16301
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16008
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15922
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16666
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16493
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07345
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16682
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15927
[19.02.2026 14:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08392
[19.02.2026 14:05] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2026 14:05] No deleted papers detected.
[19.02.2026 14:05] Downloading and parsing papers (pdf, html). Total: 14.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.12675.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.12675.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.12675.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.16705.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.16705.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.16705.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.14979.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.14979.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.14979.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.14080.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.14080.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.14080.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.15989.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.15989.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.15989.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.16301.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.16301.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.16301.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.16008.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.16008.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.16008.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.15922.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.15922.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.15922.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.16666.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.16666.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.16666.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.16493.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.16493.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.16493.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.07345.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.07345.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.07345.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.16682.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.16682.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.16682.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.15927.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.15927.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.15927.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Downloading and parsing paper https://huggingface.co/papers/2602.08392.
[19.02.2026 14:05] Extra JSON file exists (./assets/json/2602.08392.json), skip PDF parsing.
[19.02.2026 14:05] Paper image links file exists (./assets/img_data/2602.08392.json), skip HTML parsing.
[19.02.2026 14:05] Success.
[19.02.2026 14:05] Enriching papers with extra data.
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 0. SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.  					AI-generated summary 				 Sparse-Linear Attention (SLA) combines sparse and linear attention to acc...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 1. HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.  					AI-generated summary 				 Visual loco-manipulation of arbitrary objects in the wil...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 2. RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.  					AI-generated summary 				 Despite rapid progress in multimodal foundation models, embodied inte...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 3. LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.  					AI-generated summary 				 Standard factuality evaluations of LLMs treat all e...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 4. A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.  					AI-generated summary 				 We introduce SAM 3D Body (3DB), a promptable model for single...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 5. Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.  					AI-generated summary 				 Achieving cooperation among self-interested agents remains a fundamental challenge in multi-a...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 6. MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.  					AI-generated summary 				 We introduce the Massive Audio Embedding Benchmark (MAEB...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 7. DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.  					AI-generated summary 				 State-of-the-art Vision-Language-Action (VLA) models excel at sema...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 8. Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.  					AI-generated summary 				 AI agents are increasingly deploy...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 9. Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.  					AI-generated summary 				 Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval oft...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 10. Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape.  					AI-generated summary 				 Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its s...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 11. SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.  					AI-generated summary 				 A core aspect of human p...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 12. Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  					AI-generated summary 				 Generative large vision-language models (LVLMs) have...
[19.02.2026 14:05] ********************************************************************************
[19.02.2026 14:05] Abstract 13. BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI,...
[19.02.2026 14:05] Read previous papers.
[19.02.2026 14:05] Generating reviews via LLM API.
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#inference", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SLA2, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ-–ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –¥–∏—Ñ
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#robotics", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ —Ä–æ–±–æ—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª–æ–≤–∫–∏–º: —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ + –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã", "desc": "HERO ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#open_source", "#transfer_learning", "#multimodal", "#reasoning", "#benchmark", "#training", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–∞", "desc": "RynnBrain ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø—Ä–æ—Å—Ç
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#hallucinations", "#dataset", "#reasoning", "#benchmark"], "emoji": "üîë", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –µ—Å—Ç—å, –Ω–æ –∫–ª—é—á–∏ –ø–æ—Ç–µ—Ä—è–Ω—ã: –∫–∞–∫ LLM –∫–æ–¥–∏—Ä—É—é—Ç —Ñ–∞–∫—Ç—ã, –Ω–æ –Ω–µ –º–æ–≥—É—Ç –∏—Ö –≤—Å–ø–æ–º–Ω–∏—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –∫–æ–¥–∏—Ä—É—é—Ç –ø–æ—á—Ç–∏ –≤—Å–µ —Ñ–∞–∫—Ç—ã (9
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#data", "#architecture", "#dataset"], "emoji": "üßç", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç SAM 3D Body (3DB) ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#rl", "#agents", "#games", "#reasoning", "#architecture"], "emoji": "ü§ù", "ru": {"title": "–ö–æ–æ–ø–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã) –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–≥–µ–Ω—Ç–∞–º –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#dataset", "#audio", "#multimodal", "#open_source", "#survey", "#multilingual", "#low_resource", "#benchmark"], "emoji": "üéµ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∞—É–¥–∏–æ–º–æ–¥–µ–ª–µ–π: –Ω–µ—Ç –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –Ω–∞ –≤—Å–µ —Å–ª—É—á–∞–∏ –∂–∏–∑–Ω–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Massive Audio Embedding Benchmark (MAEB) ‚Äî –∫—Ä—É–ø–Ω
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#multimodal", "#training", "#robotics", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–î–∏–Ω–∞–º–∏–∫–∞ –≤–º–µ—Å—Ç–æ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º", "desc": "DreamZero ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π (World Action Model)
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∫—Ä–∏—Ç–∏–∫—É–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É —É—Å–ø–µ—Ö–∞ –∏ –Ω–µ –≤—ã—è–≤–ª—è—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏. –ê
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#agents", "#security", "#benchmark", "#multimodal", "#open_source", "#long_context", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–∞–¥–µ–∂–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –∏ –±–æ—Ä—å–±–∞ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–≥–µ–Ω—Ç Mu
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#training", "#benchmark", "#video", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –∑–æ–Ω –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Adaptive Matching Distillation (AMD) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#video", "#survey", "#benchmark", "#multimodal", "#reasoning", "#dataset"], "emoji": "üëì", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∏—Ä–∞ —á–µ—Ä–µ–∑ –≥–ª–∞–∑–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª—è", "desc": "SAW-Bench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö foundation models –ø–æ–Ω–∏–º–∞—Ç—å –æ–∫—Ä—É–∂
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#multimodal", "#benchmark", "#cv", "#security"], "emoji": "üé≠", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —á–µ—Ä–µ–∑ –≤–æ–∑–º—É—â—ë–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Ç–∞–∫—É Visual Memory Injection, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∫—Ä—ã—Ç–Ω–æ –º–∞–Ω–∏–ø
[19.02.2026 14:05] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#multimodal", "#reasoning", "#hallucinations"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ –¥–≤–µ —Ä—É–∫–∏ –ª—É—á—à–µ, —á–µ–º –æ–¥–Ω–∞: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "BiManiBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[19.02.2026 14:05] Renaming data file.
[19.02.2026 14:05] Renaming previous data. hf_papers.json to ./d/2026-02-19.json
[19.02.2026 14:05] Saving new data file.
[19.02.2026 14:05] Generating page.
[19.02.2026 14:05] Renaming previous page.
[19.02.2026 14:05] Renaming previous data. index.html to ./d/2026-02-19.html
[19.02.2026 14:05] Writing result.
[19.02.2026 14:05] Renaming log file.
[19.02.2026 14:05] Renaming previous data. log.txt to ./logs/2026-02-19_last_log.txt
