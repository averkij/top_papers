[19.02.2026 09:38] Read previous papers.
[19.02.2026 09:38] Generating top page (month).
[19.02.2026 09:38] Writing top page (month).
[19.02.2026 10:36] Read previous papers.
[19.02.2026 10:36] Get feed.
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12675
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16705
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14979
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14080
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15989
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16301
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15922
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16666
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07345
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16682
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16493
[19.02.2026 10:36] Extract page data from URL. URL: https://huggingface.co/papers/2602.16008
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15927
[19.02.2026 10:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08392
[19.02.2026 10:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2026 10:36] No deleted papers detected.
[19.02.2026 10:36] Downloading and parsing papers (pdf, html). Total: 14.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.12675.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.12675.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.12675.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.16705.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.16705.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.16705.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.14979.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.14979.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.14979.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.14080.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.14080.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.14080.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.15989.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.15989.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.15989.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.16301.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.16301.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.16301.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.15922.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.15922.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.15922.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.16666.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.16666.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.16666.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.07345.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.07345.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.07345.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.16682.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.16682.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.16682.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.16493.
[19.02.2026 10:36] Extra JSON file exists (./assets/json/2602.16493.json), skip PDF parsing.
[19.02.2026 10:36] Paper image links file exists (./assets/img_data/2602.16493.json), skip HTML parsing.
[19.02.2026 10:36] Success.
[19.02.2026 10:36] Downloading and parsing paper https://huggingface.co/papers/2602.16008.
[19.02.2026 10:36] Downloading paper 2602.16008 from https://arxiv.org/pdf/2602.16008v1...
[19.02.2026 10:36] Extracting affiliations from text.
[19.02.2026 10:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MAEB: Massive Audio Embedding Benchmark Adnan El Assadi 1 Isaac Chung 2 Chenghao Xiao 3 Roman Solomatin 4 5 Animesh Jha 6 Rahul Chand 6 Silky Singh 6 Kaitlyn Wang 6 Ali Sartaz Khan 6 Marc Moussa Nasser 6 Sufen Fong 6 Pengfei He 6 Alan Xiao 6 Ayush Sunil Munot 7 Aditya Shrivastava 8 Artem Gazizov 9 Niklas Muennighoff 6 Kenneth Enevoldsen "
[19.02.2026 10:36] Response: ```python
[]
```
[19.02.2026 10:36] Extracting affiliations from text.
[19.02.2026 10:36] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MAEB: Massive Audio Embedding Benchmark Adnan El Assadi 1 Isaac Chung 2 Chenghao Xiao 3 Roman Solomatin 4 5 Animesh Jha 6 Rahul Chand 6 Silky Singh 6 Kaitlyn Wang 6 Ali Sartaz Khan 6 Marc Moussa Nasser 6 Sufen Fong 6 Pengfei He 6 Alan Xiao 6 Ayush Sunil Munot 7 Aditya Shrivastava 8 Artem Gazizov 9 Niklas Muennighoff 6 Kenneth EnevoldsenWe introduce the Massive Audio Embedding Benchmark (MAEB), large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and leaderboard at https://github.com/ embeddings-benchmark/mteb. 6 2 0 2 7 1 ] . [ 1 8 0 0 6 1 . 2 0 6 2 : r 1. Introduction Audio and speech representations support diverse applications such as voice assistants and music recommendation systems. However, evaluation protocols for audio embedding models vary significantly, spanning speech recognition, 1Carleton University 2Zendesk 3Durham University 4MIRAI 5SaluteDevices 6Stanford University 7Indian Institute of Technology, Kharagpur 8Capital One 9Harvard University 10Aarhus University. Correspondence to: Adnan El Assadi <adnanelassadi@cmail.carleton.ca>. Preprint. February 19, 2026. 1 zero-shot classification, and audio-text retrieval. Existing audio benchmarks often focus on specific tasks (e.g., vocal sound classification (Gong et al., 2022)) or narrow domains (e.g., environmental sounds (Piczak, 2015)) while often ignoring others, limiting insight into how well embeddings transfer across different applications. Without unified evaluation framework, the field remains fragmented, making it difficult to compare models or track meaningful progress across the full landscape of audio tasks. Additionally, the absence of integrated development and maintenance infrastructure has led to stagnation in existing benchmarks, with many becoming outdated as the field rapidly evolves. We introduce the Massive Audio Embedding Benchmark (MAEB) to provide unified, comprehensive evaluation protocol to spur the fields advancement toward universal audio embedding models. Building on the success of MTEB (Muennighoff et al., 2023), MMTEB (Enevoldsen et al., 2025), and MIEB (Xiao et al., 2025b), which have unified and expanded evaluation of embedding models for text and image through continual development and community maintenance, we extend this proven framework to the audio domain. MAEB spans 30 audio tasks grouped into 7 categories. Aligning with MTEBs approach, we include Classification, Zero-shot Classification, Clustering, Pair Classification, Retrieval, and Reranking tasks adapted for audio data. Notably, we consider audio-specific aspects such as multilingual audio understanding, long-form audio processing, and cross-modal audio-text tasks that have been largely absent from prior audio benchmarks. Beyond traditional speech recognition tasks, we emphasize comprehensive audio understanding capabilities through: 1) Diverse acoustic domains, including speech, music, environmental sounds, and bioacoustics; 2) Cross-modal abilities, particularly in zeroshot settings leveraging text descriptions; 3) Complex recognition tasks requiring fine-grained audio understanding; 4) Multilingual audio processing across various languages and dialects. To ensure efficient evaluation and broader adoption, MAEB allows for evaluation of small audio-only model in 2 GPU hours while not compromising on coverage. We also MAEB: Massive Audio Embedding Benchmark Figure 1. overview of task types and example subtypes in MAEB+. Values in parentheses denote numbers for MAEB. provide MAEB(audio), 19-task audio-only subset for evaluating audio-only models, and MAEB+, our full unfiltered collection of 98 tasks. Additionally, we provide modular architecture that simplifies the addition of new audio models and datasets, ensuring that MAEB can evolve with the rapidly advancing field of audio representation learning. Our evaluation of 53 models reveals that no single model dominates across all audio domains; each excels in specific areas while underperforming in others. Preliminary evidence from four Audio LLMs suggests that MAEB encoder quality may correlate with downstream Audio LLM performance (R2 = 0.86, = 4; see Figure 3), validating the benchmarks relevance for multimodal audio understanding. To summarize, MAEB makes the following key contributions: 1. We provide the first comprehensive benchmark for audio embeddings that spans multiple domains, languages, and task types, 2. We establish baseline evaluations using representative set of 53 models, revealing strengths and weaknesses across different audio understanding capabilities, 3. We identify critical areas where current models struggle, particularly in multilingual contexts and crossmodal understanding, providing clear directions for future research, 4. We create flexible, extensible framework that enables the audio research community to standardize evaluation practices and track progress more effectively. 2. MAEB MAEB is fully integrated into the MTEB ecosystem (Muennighoff et al., 2023), extending its unified evaluation framework to the audio modality alongside text (Enevoldsen et al., 2025) and image (Xiao et al., 2025b) embeddings. This integration provides several advantages: (1) tried-and-tested implementations with standardized metrics and evaluation protocols validated across thousands of submissions; (2) extensibility through minimal interface that allows adding new models or tasks with minimal code changes; (3) reproducibility via versioned code and"
[19.02.2026 10:37] Mistral response. {"id": "69ea905f81f24ce89e43598f4a625797", "created": 1771497418, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1492, "total_tokens": 1565, "completion_tokens": 73, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Carleton University\",\n    \"Zendesk\",\n    \"Durham University\",\n    \"MIRAI\",\n    \"SaluteDevices\",\n    \"Stanford University\",\n    \"Indian Institute of Technology, Kharagpur\",\n    \"Capital One\",\n    \"Harvard University\",\n    \"Aarhus University\"\n]\n```"}}]}
[19.02.2026 10:37] Response: ```python
[
    "Carleton University",
    "Zendesk",
    "Durham University",
    "MIRAI",
    "SaluteDevices",
    "Stanford University",
    "Indian Institute of Technology, Kharagpur",
    "Capital One",
    "Harvard University",
    "Aarhus University"
]
```
[19.02.2026 10:37] Deleting PDF ./assets/pdf/2602.16008.pdf.
[19.02.2026 10:37] Success.
[19.02.2026 10:37] Downloading and parsing paper https://huggingface.co/papers/2602.15927.
[19.02.2026 10:37] Extra JSON file exists (./assets/json/2602.15927.json), skip PDF parsing.
[19.02.2026 10:37] Paper image links file exists (./assets/img_data/2602.15927.json), skip HTML parsing.
[19.02.2026 10:37] Success.
[19.02.2026 10:37] Downloading and parsing paper https://huggingface.co/papers/2602.08392.
[19.02.2026 10:37] Extra JSON file exists (./assets/json/2602.08392.json), skip PDF parsing.
[19.02.2026 10:37] Paper image links file exists (./assets/img_data/2602.08392.json), skip HTML parsing.
[19.02.2026 10:37] Success.
[19.02.2026 10:37] Enriching papers with extra data.
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 0. SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.  					AI-generated summary 				 Sparse-Linear Attention (SLA) combines sparse and linear attention to acc...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 1. HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.  					AI-generated summary 				 Visual loco-manipulation of arbitrary objects in the wil...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 2. RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.  					AI-generated summary 				 Despite rapid progress in multimodal foundation models, embodied inte...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 3. LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.  					AI-generated summary 				 Standard factuality evaluations of LLMs treat all e...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 4. A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.  					AI-generated summary 				 We introduce SAM 3D Body (3DB), a promptable model for single...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 5. Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.  					AI-generated summary 				 Achieving cooperation among self-interested agents remains a fundamental challenge in multi-a...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 6. DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.  					AI-generated summary 				 State-of-the-art Vision-Language-Action (VLA) models excel at sema...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 7. Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.  					AI-generated summary 				 AI agents are increasingly deploy...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 8. Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape.  					AI-generated summary 				 Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its s...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 9. SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.  					AI-generated summary 				 A core aspect of human p...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 10. Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.  					AI-generated summary 				 Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval oft...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 11. MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.  					AI-generated summary 				 We introduce the Massive Audio Embedding Benchmark (MAEB...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 12. Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  					AI-generated summary 				 Generative large vision-language models (LVLMs) have...
[19.02.2026 10:37] ********************************************************************************
[19.02.2026 10:37] Abstract 13. BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI,...
[19.02.2026 10:37] Read previous papers.
[19.02.2026 10:37] Generating reviews via LLM API.
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#inference", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SLA2, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ-–ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –¥–∏—Ñ
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#robotics", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ —Ä–æ–±–æ—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª–æ–≤–∫–∏–º: —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ + –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã", "desc": "HERO ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#open_source", "#transfer_learning", "#multimodal", "#reasoning", "#benchmark", "#training", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–∞", "desc": "RynnBrain ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø—Ä–æ—Å—Ç
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#hallucinations", "#dataset", "#reasoning", "#benchmark"], "emoji": "üîë", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –µ—Å—Ç—å, –Ω–æ –∫–ª—é—á–∏ –ø–æ—Ç–µ—Ä—è–Ω—ã: –∫–∞–∫ LLM –∫–æ–¥–∏—Ä—É—é—Ç —Ñ–∞–∫—Ç—ã, –Ω–æ –Ω–µ –º–æ–≥—É—Ç –∏—Ö –≤—Å–ø–æ–º–Ω–∏—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –∫–æ–¥–∏—Ä—É—é—Ç –ø–æ—á—Ç–∏ –≤—Å–µ —Ñ–∞–∫—Ç—ã (9
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#data", "#architecture", "#dataset"], "emoji": "üßç", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç SAM 3D Body (3DB) ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#rl", "#agents", "#games", "#reasoning", "#architecture"], "emoji": "ü§ù", "ru": {"title": "–ö–æ–æ–ø–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã) –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–≥–µ–Ω—Ç–∞–º –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#multimodal", "#training", "#robotics", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–î–∏–Ω–∞–º–∏–∫–∞ –≤–º–µ—Å—Ç–æ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º", "desc": "DreamZero ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π (World Action Model)
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∫—Ä–∏—Ç–∏–∫—É–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É —É—Å–ø–µ—Ö–∞ –∏ –Ω–µ –≤—ã—è–≤–ª—è—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏. –ê
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#training", "#benchmark", "#video", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –∑–æ–Ω –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Adaptive Matching Distillation (AMD) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#video", "#survey", "#benchmark", "#multimodal", "#reasoning", "#dataset"], "emoji": "üëì", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∏—Ä–∞ —á–µ—Ä–µ–∑ –≥–ª–∞–∑–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª—è", "desc": "SAW-Bench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö foundation models –ø–æ–Ω–∏–º–∞—Ç—å –æ–∫—Ä—É–∂
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#agents", "#security", "#benchmark", "#multimodal", "#open_source", "#long_context", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–∞–¥–µ–∂–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –∏ –±–æ—Ä—å–±–∞ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–≥–µ–Ω—Ç Mu
[19.02.2026 10:37] Querying the API.
[19.02.2026 10:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.  					AI-generated summary 				 We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.
[19.02.2026 10:37] Response: ```json
{
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Massive Audio Embedding Benchmark (MAEB) ‚Äî –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª–µ–µ 50 –º–æ–¥–µ–ª–µ–π –Ω–∞ 30 –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏, –º—É–∑—ã–∫–∏ –∏ –∑–≤—É–∫–æ–≤ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –Ω–∞ 100+ —è–∑—ã–∫–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–æ –≤—Å–µ—Ö –∑–∞–¥–∞—á–∞—Ö: –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã–µ –∞—É–¥–∏–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —à—É–º–æ–≤, –Ω–æ –ø–ª–æ—Ö–æ –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Ä–µ—á–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∞—É–¥–∏–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –Ω–∞ MAEB –∏ –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∞—É–¥–∏–æ. –ë–µ–Ω—á–º–∞—Ä–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ —ç–∫–æ—Å–∏—Å—Ç–µ–º—É MTEB –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç 98 –∑–∞–¥–∞—á —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –ª–∏–¥–µ—Ä–±–æ—Ä–¥–æ–º –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞—É–¥–∏–æ.",
  "emoji": "üéµ",
  "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∞—É–¥–∏–æ–º–æ–¥–µ–ª–µ–π: –Ω–µ—Ç –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –Ω–∞ –≤—Å–µ —Å–ª—É—á–∞–∏ –∂–∏–∑–Ω–∏"
}
```
[19.02.2026 10:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.  					AI-generated summary 				 We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb."

[19.02.2026 10:37] Response: ```python
["BENCHMARK", "AUDIO", "DATASET", "MULTILINGUAL", "MULTIMODAL"]
```
[19.02.2026 10:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.  					AI-generated summary 				 We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb."

[19.02.2026 10:37] Response: ```python
['SURVEY', 'OPEN_SOURCE', 'LOW_RESOURCE']
```
[19.02.2026 10:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"The Massive Audio Embedding Benchmark (MAEB) is a comprehensive evaluation framework that assesses over 50 audio models across 30 diverse tasks, including speech recognition, music classification, and environmental sound identification. The study reveals that no single model excels in all areas; for instance, contrastive audio-text models perform well in environmental sound tasks but struggle with multilingual speech tasks. Additionally, the research highlights a trade-off where models that are strong in acoustic understanding often underperform in linguistic tasks, indicating a need for specialized models. MAEB also shows a strong correlation between the performance of audio encoders and their effectiveness in audio large language models, providing a valuable resource for future research in audio processing.","title":"Unlocking Audio Insights: MAEB\'s Diverse Benchmarking Power"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The Massive Audio Embedding Benchmark (MAEB) is a comprehensive evaluation framework that assesses over 50 audio models across 30 diverse tasks, including speech recognition, music classification, and environmental sound identification. The study reveals that no single model excels in all areas; for instance, contrastive audio-text models perform well in environmental sound tasks but struggle with multilingual speech tasks. Additionally, the research highlights a trade-off where models that are strong in acoustic understanding often underperform in linguistic tasks, indicating a need for specialized models. MAEB also shows a strong correlation between the performance of audio encoders and their effectiveness in audio large language models, providing a valuable resource for future research in audio processing.', title="Unlocking Audio Insights: MAEB's Diverse Benchmarking Power"))
[19.02.2026 10:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"MAEBÔºàÂ§ßËßÑÊ®°Èü≥È¢ëÂµåÂÖ•Âü∫ÂáÜÔºâÊòØ‰∏Ä‰∏™ËØÑ‰º∞50Â§ö‰∏™Ê®°ÂûãÂú®30‰∏™‰ªªÂä°‰∏äÁöÑË°®Áé∞ÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜËØ≠Èü≥„ÄÅÈü≥‰πêÂíåÁéØÂ¢ÉÂ£∞Èü≥Á≠âÈ¢ÜÂüü„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊ≤°ÊúâÂçï‰∏ÄÊ®°ÂûãÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÈü≥È¢ë-ÊñáÊú¨ÂØπÊØîÊ®°ÂûãÂú®ÁéØÂ¢ÉÂ£∞Èü≥ÂàÜÁ±ª‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Â§öËØ≠Ë®ÄËØ≠Èü≥‰ªªÂä°‰∏≠Ë°®Áé∞Êé•ËøëÈöèÊú∫„ÄÇÊ®°ÂûãÂú®Â£∞Â≠¶ÁêÜËß£ÊñπÈù¢Ë°®Áé∞‰ºòÁßÄÁöÑÊÉÖÂÜµ‰∏ãÔºåÂæÄÂæÄÂú®ËØ≠Ë®Ä‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇMAEBÊó®Âú®‰øùÊåÅ‰ªªÂä°Â§öÊ†∑ÊÄßÔºåÂêåÊó∂Èôç‰ΩéËØÑ‰º∞ÊàêÊú¨ÔºåÂπ∂‰∏éMTEBÁîüÊÄÅÁ≥ªÁªüÈõÜÊàêÔºåÂÆûÁé∞ÊñáÊú¨„ÄÅÂõæÂÉèÂíåÈü≥È¢ëÊ®°ÊÄÅÁöÑÁªü‰∏ÄËØÑ‰º∞„ÄÇ","title":"Èü≥È¢ëÊ®°ÂûãËØÑ‰º∞Êñ∞Âü∫ÂáÜÔºöMAEB"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MAEBÔºàÂ§ßËßÑÊ®°Èü≥È¢ëÂµåÂÖ•Âü∫ÂáÜÔºâÊòØ‰∏Ä‰∏™ËØÑ‰º∞50Â§ö‰∏™Ê®°ÂûãÂú®30‰∏™‰ªªÂä°‰∏äÁöÑË°®Áé∞ÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜËØ≠Èü≥„ÄÅÈü≥‰πêÂíåÁéØÂ¢ÉÂ£∞Èü≥Á≠âÈ¢ÜÂüü„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊ≤°ÊúâÂçï‰∏ÄÊ®°ÂûãÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÈü≥È¢ë-ÊñáÊú¨ÂØπÊØîÊ®°ÂûãÂú®ÁéØÂ¢ÉÂ£∞Èü≥ÂàÜÁ±ª‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Â§öËØ≠Ë®ÄËØ≠Èü≥‰ªªÂä°‰∏≠Ë°®Áé∞Êé•ËøëÈöèÊú∫„ÄÇÊ®°ÂûãÂú®Â£∞Â≠¶ÁêÜËß£ÊñπÈù¢Ë°®Áé∞‰ºòÁßÄÁöÑÊÉÖÂÜµ‰∏ãÔºåÂæÄÂæÄÂú®ËØ≠Ë®Ä‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇMAEBÊó®Âú®‰øùÊåÅ‰ªªÂä°Â§öÊ†∑ÊÄßÔºåÂêåÊó∂Èôç‰ΩéËØÑ‰º∞ÊàêÊú¨ÔºåÂπ∂‰∏éMTEBÁîüÊÄÅÁ≥ªÁªüÈõÜÊàêÔºåÂÆûÁé∞ÊñáÊú¨„ÄÅÂõæÂÉèÂíåÈü≥È¢ëÊ®°ÊÄÅÁöÑÁªü‰∏ÄËØÑ‰º∞„ÄÇ', title='Èü≥È¢ëÊ®°ÂûãËØÑ‰º∞Êñ∞Âü∫ÂáÜÔºöMAEB'))
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#multimodal", "#benchmark", "#cv", "#security"], "emoji": "üé≠", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —á–µ—Ä–µ–∑ –≤–æ–∑–º—É—â—ë–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Ç–∞–∫—É Visual Memory Injection, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∫—Ä—ã—Ç–Ω–æ –º–∞–Ω–∏–ø
[19.02.2026 10:37] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#multimodal", "#reasoning", "#hallucinations"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ –¥–≤–µ —Ä—É–∫–∏ –ª—É—á—à–µ, —á–µ–º –æ–¥–Ω–∞: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "BiManiBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[19.02.2026 10:37] Renaming data file.
[19.02.2026 10:37] Renaming previous data. hf_papers.json to ./d/2026-02-19.json
[19.02.2026 10:37] Saving new data file.
[19.02.2026 10:37] Generating page.
[19.02.2026 10:37] Renaming previous page.
[19.02.2026 10:37] Renaming previous data. index.html to ./d/2026-02-19.html
[19.02.2026 10:37] Writing result.
[19.02.2026 10:37] Renaming log file.
[19.02.2026 10:37] Renaming previous data. log.txt to ./logs/2026-02-19_last_log.txt
