[18.02.2026 23:22] Read previous papers.
[18.02.2026 23:22] Generating top page (month).
[18.02.2026 23:22] Writing top page (month).
[19.02.2026 01:22] Read previous papers.
[19.02.2026 01:22] Get feed.
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14111
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12670
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15763
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14299
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12279
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15112
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15547
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14486
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15772
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15322
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15200
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15449
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15156
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15620
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15278
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15382
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15327
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13964
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12978
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11389
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09653
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07854
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14364
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12235
[19.02.2026 01:22] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10210
[19.02.2026 01:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2026 01:22] No deleted papers detected.
[19.02.2026 01:22] Downloading and parsing papers (pdf, html). Total: 25.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.14111.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.14111.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.14111.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.12670.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.12670.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.12670.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15763.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15763.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15763.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.14299.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.14299.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.14299.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.12279.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.12279.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.12279.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15112.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15112.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15112.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15547.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15547.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15547.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.14486.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.14486.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.14486.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15772.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15772.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15772.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15322.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15322.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15322.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15200.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15200.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15200.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15449.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15449.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15449.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15156.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15156.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15156.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15620.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15620.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15620.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15278.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15278.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15278.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15382.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15382.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15382.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.15327.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.15327.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.15327.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.13964.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.13964.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.13964.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.12978.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.12978.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.12978.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.11389.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.11389.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.11389.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.09653.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.09653.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.09653.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.07854.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.07854.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.07854.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.14364.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.14364.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.14364.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.12235.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.12235.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.12235.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Downloading and parsing paper https://huggingface.co/papers/2602.10210.
[19.02.2026 01:22] Extra JSON file exists (./assets/json/2602.10210.json), skip PDF parsing.
[19.02.2026 01:22] Paper image links file exists (./assets/img_data/2602.10210.json), skip HTML parsing.
[19.02.2026 01:22] Success.
[19.02.2026 01:22] Enriching papers with extra data.
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 0. Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.  					AI-generated summary 				 Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural ne...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 1. SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.  					A...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 2. GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.  					AI-generated summary 				 We present GLM-5, a next-generation foundation model designed to transition ...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 3. Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.  					AI-generated summary 				 As large language model agents increasingly populate networked environments...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 4. UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities.  					AI-generated summary 				 Unified models can handle both multimodal understanding and generation ...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 5. ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.  					AI-generated summary 				 We introduce ResearchGym, a benchmark an...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 6. Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization.  					AI-generated summary 				 Text embedding models are widely used f...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 7. Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.  					AI-generated summary 				 The Platonic Representation Hypothesis suggests that representations from neural networks are convergin...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 8. The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.  					AI-generated summary 				 Current research in mu...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 9. Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.  					AI-generated summary 				 Training...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 10. COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.  					AI-generated summary 				 Post-training compressi...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 11. Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure.  					AI-generated summary 				 Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmical...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 12. Panini enables efficient and accurate language model reasoning through a non-parametric continual learning framework that uses generative semantic workspaces to store and retrieve knowledge, achieving superior performance with reduced computational overhead.  					AI-generated summary 				 Language ...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 13. Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance.  					AI-generated summary 				 Reinforcement Learning...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 14. Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns.  					AI-generated summary 				 The web is littered with images, once created for human consumption and now...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 15. A Vision Wormhole framework enables efficient, model-agnostic communication in multi-agent systems by using visual-language models to transfer reasoning states through a shared latent space, reducing computational overhead while maintaining reasoning accuracy.  					AI-generated summary 				 Multi-A...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 16. Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks.  					AI-generated summary 				 For deploying foundation models, practitioners increasingly need prescriptive ...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 17. HLE-Verified presents a validated and revised version of the HLE benchmark with improved reliability through expert review and model-based checks, demonstrating significant accuracy improvements in language model evaluations.  					AI-generated summary 				 Humanity's Last Exam (HLE) has become a wi...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 18. Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution.  					AI-generated summary 				 Action chunking enables Vision Language Action (VLA) models to run in r...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 19. C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.  					AI-generated summary 				 World models require robust relatio...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 20. A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning.  					AI-generated summary 				 Although large language models (LLMs) demonstrate expert-level medical k...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 21. ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization.  					AI-generated summary 				 Pred...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 22. Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks.  					AI-generated summary 				 Clawdbot is a self-ho...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 23. Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors.  					AI-generated summary 				 Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), espe...
[19.02.2026 01:22] ********************************************************************************
[19.02.2026 01:22] Abstract 24. HybridRAG-Bench evaluates retrieval-intensive multi-hop reasoning in large language models by combining unstructured text and structured knowledge graphs from recent scientific literature, providing a contamination-aware benchmark that distinguishes genuine retrieval and reasoning from parametric re...
[19.02.2026 01:22] Read previous papers.
[19.02.2026 01:22] Generating reviews via LLM API.
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#interpretability"], "emoji": "üîç", "ru": {"title": "–•–æ—Ä–æ—à–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–∏–Ω–Ω—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ (SAE) –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –ø—É—Ç–µ–º —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#agents", "#benchmark"], "emoji": "üõ†Ô∏è", "ru": {"title": "–£–º–µ–ª—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏: –ø–æ—á–µ–º—É –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∑–Ω–∞–Ω–∏—è, —á–µ–º —Å–æ–∑–¥–∞—é—Ç –∏—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SkillsBench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –∞–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#inference", "#agents", "#training", "#alignment", "#optimization", "#architecture", "#plp", "#open_source", "#reasoning", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "GLM-5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–æ–¥–µ–ª—å
[19.02.2026 01:22] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –±–µ–∑ —Å–æ—Ü–∏—É–º–∞: –ø–æ—á–µ–º—É –ò–ò-–æ–±—â–µ—Å—Ç–≤–∞ –Ω–µ —Å—Ö–æ–¥—è—Ç—Å—è –∫ –∫–æ–Ω—Å–µ–Ω—Å—É—Å—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–µ—Ç–µ–≤—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è, –ø–æ–¥–≤–µ—Ä–≥–∞—é—Ç—Å—è –ª–∏ –æ–Ω–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –ø–æ–¥–æ–±–Ω–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–∏—Å—Ç–µ–º
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#multimodal", "#training", "#agents", "#inference"], "emoji": "üîÑ", "ru": {"title": "–ú—ã—Å–ª—è—â–∏–µ –º–æ–¥–µ–ª–∏: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "UniT ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–¥–∏–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#benchmark", "#science", "#agents", "#long_context", "#dataset"], "emoji": "üß™", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö AI –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö", "desc": "ResearchGym –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–µ–Ω—á–º–∞—Ä–∫ –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–∫–≤–æ–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#training", "#multilingual", "#small_models"], "emoji": "üì¶", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#interpretability"], "emoji": "üèõÔ∏è", "ru": {"title": "–û—Ç –ü–ª–∞—Ç–æ–Ω–∞ –∫ –ê—Ä–∏—Å—Ç–æ—Ç–µ–ª—é: –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Å–µ–¥—Å—Ç–≤–æ –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≥–∏–ø–æ—Ç–µ–∑–∞ –æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∫ –µ–¥–∏–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#reasoning", "#multimodal"], "emoji": "üîÑ", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ Reason-Reflect-Refine (R3), –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "üéØ", "ru": {"title": "–°–ª—É—á–∞–π–Ω–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π ‚Äî –ø—Ä–æ—Å—Ç–æ–π –ø—É—Ç—å –∫ –ª—É—á—à–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ
[19.02.2026 01:22] Using data from previous issue: {"categories": [], "emoji": "üóúÔ∏è", "ru": {"title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "COMPOT ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π Transformer –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–∞—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏ –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#training", "#rl", "#plp", "#dataset"], "emoji": "üéì", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É—Å–∏–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ TAROT –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º reinforcement fine-tuning
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#rag", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Panini –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–ù–∞—Ö–æ–¥–∏ –∏ –∏—Å–∫–ª—é—á–∞–π: –∫–∞–∫ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ fine-tuning –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ –∑–∞–¥–∞—á–∏ –≤—ã–±–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#transfer_learning", "#multimodal", "#reasoning", "#agents", "#optimization"], "emoji": "üåÄ", "ru": {"title": "–¢–µ–ª–µ–ø–∞—Ç–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç–∞–ª", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Vision Wormhole ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –∞–≥–µ–Ω
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#training", "#optimization", "#science"], "emoji": "üìà", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –±—é–¥–∂–µ—Ç", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–∏–ª—å
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#dataset"], "emoji": "‚úÖ", "ru": {"title": "–ß–∏—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Äî —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ HLE-Verified ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞ HLE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –≤ –æ
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#robotics", "#cv", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ì–ª–∞–¥–∫–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º", "desc": "Legato ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Vision Language Action, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤. –û—Å–Ω–æ–≤–Ω–∞
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#multimodal", "#open_source", "#robotics"], "emoji": "üß©", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ", "desc": "C-JEPA ‚Äî —ç—Ç–æ –æ–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–µ—Ç–æ–¥ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–≤
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#dataset", "#training", "#healthcare", "#rlhf", "#alignment", "#reasoning", "#science", "#small_models"], "emoji": "üè•", "ru": {"title": "–ö–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ LLM —á–µ—Ä–µ–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤—Ä–∞—á–∞–º–∏ –ø—Ä–∏–º–µ—Ä—ã –∏ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#video", "#benchmark", "#architecture", "#3d"], "emoji": "üé¨", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—è –∫–∞–º–µ—Ä—ã –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –≤–∏–¥–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤—Ä–µ–º–µ–Ω–∏", "desc": "ViewRope ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏ –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#security", "#alignment"], "emoji": "‚öîÔ∏è", "ru": {"title": "–£—è–∑–≤–∏–º–æ—Å—Ç—å AI-–∞–≥–µ–Ω—Ç–æ–≤: –∫–æ–≥–¥–∞ —è—Å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∞–∂–Ω–µ–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—â–∞–µ–º–æ–≥–æ AI-–∞–≥–µ–Ω—Ç–∞ Clawdbot, –∫–æ—Ç–æ—Ä
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#benchmark", "#optimization", "#inference"], "emoji": "üóúÔ∏è", "ru": {"title": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—Ä–æ—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–∂–∞—Ç–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä–æ–µ –ø
[19.02.2026 01:22] Using data from previous issue: {"categories": ["#science", "#open_source", "#rag", "#graphs", "#reasoning", "#dataset", "#benchmark"], "emoji": "üîÄ", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: —á–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ RAG-—Å–∏—Å—Ç–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ HybridRAG-Bench ‚Äî –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞
[19.02.2026 01:22] Renaming data file.
[19.02.2026 01:22] Renaming previous data. hf_papers.json to ./d/2026-02-19.json
[19.02.2026 01:22] Saving new data file.
[19.02.2026 01:22] Generating page.
[19.02.2026 01:22] Renaming previous page.
[19.02.2026 01:22] Renaming previous data. index.html to ./d/2026-02-19.html
[19.02.2026 01:22] Writing result.
[19.02.2026 01:22] Renaming log file.
[19.02.2026 01:22] Renaming previous data. log.txt to ./logs/2026-02-19_last_log.txt
