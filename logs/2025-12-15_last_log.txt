[15.12.2025 08:35] Read previous papers.
[15.12.2025 08:35] Generating top page (month).
[15.12.2025 08:35] Writing top page (month).
[15.12.2025 09:35] Read previous papers.
[15.12.2025 09:35] Get feed.
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11558
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11749
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11799
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08269
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10411
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11253
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11792
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11464
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.06818
[15.12.2025 09:35] Extract page data from URL. URL: https://huggingface.co/papers/2512.10605
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11393
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10715
[15.12.2025 09:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.06951
[15.12.2025 09:35] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.12.2025 09:35] No deleted papers detected.
[15.12.2025 09:35] Downloading and parsing papers (pdf, html). Total: 13.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.11558.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.11558.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.11558.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.11749.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.11749.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.11749.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.11799.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.11799.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.11799.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.08269.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.08269.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.08269.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.10411.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.10411.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.10411.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.11253.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.11253.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.11253.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.11792.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.11792.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.11792.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.11464.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.11464.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.11464.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.06818.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.06818.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.06818.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.10605.
[15.12.2025 09:35] Downloading paper 2512.10605 from https://arxiv.org/pdf/2512.10605v1...
[15.12.2025 09:35] Extracting affiliations from text.
[15.12.2025 09:35] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 1 LEO-RobotAgent: General-purpose Robotic Agent for Language-driven Embodied Operator Lihuang Chen, Xiangyu Luo, and Jun Meng 5 2 0 D 1 1 ] . [ 1 5 0 6 0 1 . 2 1 5 2 : r AbstractWe propose LEO-RobotAgent, general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEORobotAgent framework is designed with streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates human-robot interaction mechanism, enabling the algorithm to collaborate with humans like partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent. Index TermsAutonomous task planning, LLMs-based agent, intelligence, embodied intelligence, general-purpose human-robot collaboration. robotic I. INTRODUCTION ARGE language models (LLMs) and robotics are two highly prominent fields in current techno"
[15.12.2025 09:35] Response: ```python
[]
```
[15.12.2025 09:35] Extracting affiliations from text.
[15.12.2025 09:35] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 1 LEO-RobotAgent: General-purpose Robotic Agent for Language-driven Embodied Operator Lihuang Chen, Xiangyu Luo, and Jun Meng 5 2 0 D 1 1 ] . [ 1 5 0 6 0 1 . 2 1 5 2 : r AbstractWe propose LEO-RobotAgent, general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEORobotAgent framework is designed with streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates human-robot interaction mechanism, enabling the algorithm to collaborate with humans like partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent. Index TermsAutonomous task planning, LLMs-based agent, intelligence, embodied intelligence, general-purpose human-robot collaboration. robotic I. INTRODUCTION ARGE language models (LLMs) and robotics are two highly prominent fields in current technology. In recent years, growing number of outstanding works have begun to focus on the integration of these two domains. Endowed with powerful natural language understanding and logical reasoning capabilities, LLMs hold considerable research value in the field of robot task planning. In the traditional robotics industry, developers need to manually design corresponding programs with specific logic for individual tasks. These programs suffer from poor flexibility and lack generalization. As task complexity and details increase, the program architecture inevitably becomes cumbersome and overcomplicated. The emergence of LLMs has made us realize that this entity with strong reasoning capabilities can effectively replace the diverse and complex logics that we previously embedded in programs Lihuang Chen, Xiangyu Luo, and Jun Meng are with Zhejiang lihuangchen@zju.edu.cn; University, Hangzhou 310000, China (e-mail: 22360566@zju.edu.cn; junmeng@zju.edu.cn). Corresponding author: Jun Meng. This work was supported by the Pioneer and Leading Goose R&D Program of Zhejiang Province (No. 2024C01170), the National Natural Science Foundation of China (No. 52475033) and the Robotics Institute of Zhejiang University under Grant K11801. manually. Indeed, works over the past two years have incorporated LLMs into robot planning to varying degrees. References [1] and [2] present and summarize numerous studies that have verified on different robot platforms, such as unmanned aerial vehicles (UAVs) and robotic arms, that LLMs can effectively enhance the planning performance of specific tasks, make program design more intuitive, and allow direct human-robot interaction. Meanwhile, intelligent agent architectures represented by the ReAct framework [3] have emerged and achieved remarkable development in the field of LLMs, aiming to address LLM task planning and execution problems in pure software applications. This is an exciting emerging subfield, indicating that LLMs also have the potential to be applied in robotics. This would allow LLMs, as the brain, to truly possess the body and tools to control robots performing various physical tasks. In this letter, we conduct in-depth research on the application of intelligent agent frameworks in the robotics domain. We propose self-designed, streamlined, and efficient generalpurpose robotic agent framework, enabling LLMs to independently perform behaviors such as planning, action execution, and reflective adjustment based on user requirements and task information, ultimately achieving self-iterative control of robots to complete tasks. Human users can optionally intervene during task execution to implement interactive behaviors through experiments, we of varying degrees. Furthermore, present the usage suggestions of different technologies for task deployment and human-robot communication from the perspective of ordinary users in this framework. Inspired by existing excellent works, we test and compare the strengths and weaknesses of agent frameworks with different core principles and structures in our designed task scenarios, hoping to provide valuable references for researchers. In summary, our main contributions are as follows: agent intelligent 1) We developed an elegantly structured general-purpose robotic framework called LEORobotAgent. The framework is equipped with humanrobot interaction capabilities. We applied several prompt engineering techniques from the LLMs domain to explore the boundary of the task planning capabilities of LLM within this framework and identified the current problems and challenges faced by LLMs in complex task planning. 2) We provide complete program system built around this framework. The system features simple and easily configurable environment with visual and monitorable operation processes. Users can register various tools in the frameworks toolset module to meet customized PREPRINT VERSION. ACCEPTED TO NOTHING YET. PREPRINT VERSION. 2 3) We verified that requirements. With high flexibility, the framework effectively lowers the threshold for human-robot interaction. this framework can be applied to different types of robots to accomplish tasks of varying complexity across diverse scenarios, exhibiting general applicability and high generalization. II. RELATED WORK 1) Large Models for Auxiliary Tasks: Large Models have been applied to varying degrees across diverse single scenarios or robot types. They can integrate semantic information from multiple sources, suc"
[15.12.2025 09:35] Mistral response. {"id": "867be5054060447d8e24ca0a3e2d67ad", "created": 1765791338, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1330, "total_tokens": 1341, "completion_tokens": 11}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Zhejiang University\"]\n```"}}]}
[15.12.2025 09:35] Response: ```python
["Zhejiang University"]
```
[15.12.2025 09:35] Deleting PDF ./assets/pdf/2512.10605.pdf.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.11393.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.11393.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.11393.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.10715.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.10715.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.10715.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Downloading and parsing paper https://huggingface.co/papers/2512.06951.
[15.12.2025 09:35] Extra JSON file exists (./assets/json/2512.06951.json), skip PDF parsing.
[15.12.2025 09:35] Paper image links file exists (./assets/img_data/2512.06951.json), skip HTML parsing.
[15.12.2025 09:35] Success.
[15.12.2025 09:35] Enriching papers with extra data.
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 0. DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.  					AI-generated summary 				 Reliable interpretation of multimodal data in dent...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 1. SVG-T2I, a scaled SVG framework, enables high-quality text-to-image synthesis directly in the Visual Foundation Model feature domain, achieving competitive performance in generative tasks.  					AI-generated summary 				 Visual generation grounded in Visual Foundation Model (VFM) representations off...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 2. V-RGBX is an end-to-end framework for intrinsic-aware video editing that combines video inverse rendering, photorealistic synthesis, and keyframe-based editing to produce consistent and physically plausible edits.  					AI-generated summary 				 Large-scale video generation models have shown remarka...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 3. EgoX framework generates egocentric videos from exocentric inputs using video diffusion models with LoRA adaptation, unified conditioning, and geometry-guided self-attention for coherence and visual fidelity.  					AI-generated summary 				 Egocentric perception enables humans to experience and unde...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 4. Sliding Window Attention Adaptation (SWAA) enables Transformer-based Large Language Models (LLMs) to use sliding window attention without retraining, recovering long-context performance through a combination of adaptation techniques.  					AI-generated summary 				 The self-attention mechanism in Tr...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 5. PersonaLive is a diffusion-based framework for real-time portrait animation that enhances speed and efficiency through multi-stage training, hybrid implicit signals, appearance distillation, and autoregressive micro-chunk streaming.  					AI-generated summary 				 Current diffusion-based portrait an...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 6. SAM2VideoX improves realistic motion generation in video models by integrating structure-preserving priors from an autoregressive model into a bidirectional diffusion model with novel feature fusion and local alignment techniques.  					AI-generated summary 				 Reality is a dance between rigid cons...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 7. MetaCanvas leverages multimodal large language models as latent-space planners to enhance precise and structured image and video generation, outperforming global-conditioning methods.  					AI-generated summary 				 Multimodal learning has rapidly advanced visual understanding, largely via multimoda...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 8. MeshSplatting, a mesh-based reconstruction method, enhances novel view synthesis by optimizing geometry and appearance through differentiable rendering, improving quality and efficiency over existing techniques.  					AI-generated summary 				 Primitive-based splatting methods like 3D Gaussian Splat...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 9. A general-purpose language-driven framework for robots, LEO-RobotAgent, enhances human-robot interaction and task planning using large language models across various robot types and tasks.  					AI-generated summary 				 We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent ...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 10. A model learns to parallelize tasks from a single egocentric video by addressing spatial and object conflicts, achieving improved action coverage and reduced collisions.  					AI-generated summary 				 Humans can intuitively parallelise complex activities, but can a model learn this from observing a...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 11. Uncertainty estimation in landmark-based segmentation of chest X-rays using hybrid neural network architectures improves reliability and robustness in clinical deployment.  					AI-generated summary 				 Uncertainty estimation is essential for the safe clinical deployment of medical image segmentati...
[15.12.2025 09:35] ********************************************************************************
[15.12.2025 09:35] Abstract 12. A vision-action policy using correlated noise for flow matching and learnable mixed-layer attention wins the 2025 BEHAVIOR Challenge with high performance across diverse household tasks.  					AI-generated summary 				 We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challen...
[15.12.2025 09:35] Read previous papers.
[15.12.2025 09:35] Generating reviews via LLM API.
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#healthcare", "#benchmark", "#multimodal", "#science", "#reasoning", "#open_source", "#synthetic", "#training", "#rl", "#dataset"], "emoji": "ü¶∑", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏", "desc": "DentalGPT ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#open_source", "#training", "#diffusion", "#architecture"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ Visual Foundation Model", "desc": "SVG-T2I ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∫
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–≤–æ–π—Å—Ç–≤ —Å—Ü–µ–Ω—ã", "desc": "V-RGBX ‚Äî —ç—Ç–æ —Å–∫–≤–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: –æ–±—Ä–∞—Ç–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –≤–∏–¥–µ–æ –¥–ª—è 
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#training", "#architecture", "#diffusion", "#video", "#multimodal", "#transfer_learning"], "emoji": "üëÅÔ∏è", "ru": {"title": "–û—Ç –≤–∑–≥–ª—è–¥–∞ —Å–±–æ–∫—É –∫ –≤–∑–≥–ª—è–¥—É –∏–∑–Ω—É—Ç—Ä–∏: –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ", "desc": "EgoX ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –æ—Ç —Ç—Ä–µ—Ç—å–µ–≥–æ –ª–∏—Ü–∞ –≤
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#optimization"], "emoji": "ü™ü", "ru": {"title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –≤–Ω–∏–º–∞–Ω–∏—è (SWAA) –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –±—ã
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#training", "#inference", "#3d", "#video"], "emoji": "üé¨", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –≤ 7-22 —Ä–∞–∑–∞", "desc": "PersonaLive ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#training", "#optimization", "#diffusion", "#architecture", "#video"], "emoji": "üé¨", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ –¥–≤–∏–∂–µ–Ω–∏–∏: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SAM2VideoX, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –¥–≤–∏–∂
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#diffusion", "#video"], "emoji": "üé®", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MetaCanvas ‚Äî –ª–µ–≥–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "üé®", "ru": {"title": "–°–µ—Ç–∫–∏ –≤–º–µ—Å—Ç–æ –æ–±–ª–∞–∫–æ–≤: –º–æ—Å—Ç –º–µ–∂–¥—É –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º –∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–æ–π", "desc": "MeshSplatting ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ç–æ—á–µ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ —Å–µ—Ç–æ–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤. –ü–æ–¥—Ö–æ
[15.12.2025 09:35] Querying the API.
[15.12.2025 09:35] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A general-purpose language-driven framework for robots, LEO-RobotAgent, enhances human-robot interaction and task planning using large language models across various robot types and tasks.  					AI-generated summary 				 We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.
[15.12.2025 09:35] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç LEO-RobotAgent ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ä–æ–±–æ—Ç–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –±–ª–∞–≥–æ–¥–∞—Ä—è –º–æ–¥—É–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏ –≥–∏–±–∫–æ–π —Å–∏—Å—Ç–µ–º–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, —Ñ–æ–∫—É—Å–∏—Ä—É—é—â–∏—Ö—Å—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–∏–ø–∞—Ö —Ä–æ–±–æ—Ç–æ–≤ –∏ –∑–∞–¥–∞—á–∞—Ö, –¥–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ–±–ª–∞–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å—é –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö ‚Äî –±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫–∞—Ö, –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–∞—Ö –∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–∞—Ö. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º —á–µ–ª–æ–≤–µ–∫–æ-—Ä–æ–±–æ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –º–∞—à–∏–Ω–æ–π –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.",
  "emoji": "ü§ñ",
  "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
```
[15.12.2025 09:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A general-purpose language-driven framework for robots, LEO-RobotAgent, enhances human-robot interaction and task planning using large language models across various robot types and tasks.  					AI-generated summary 				 We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent."

[15.12.2025 09:35] Response: ```python
["AGENTS", "ROBOTICS"]
```
[15.12.2025 09:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A general-purpose language-driven framework for robots, LEO-RobotAgent, enhances human-robot interaction and task planning using large language models across various robot types and tasks.  					AI-generated summary 				 We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent."

[15.12.2025 09:35] Response: ```python
['OPEN_SOURCE']
```
[15.12.2025 09:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LEO-RobotAgent is a versatile framework that uses large language models (LLMs) to improve how robots interact with humans and plan tasks. It allows different types of robots to handle complex and unpredictable tasks in various environments. The framework is designed to be efficient and robust, making it easier for robots to understand human intentions and collaborate effectively. By simplifying the structure of task planning, LEO-RobotAgent enhances the adaptability of robots across multiple platforms, such as UAVs and robotic arms.","title":"Empowering Robots with Language for Smarter Interaction and Task Planning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LEO-RobotAgent is a versatile framework that uses large language models (LLMs) to improve how robots interact with humans and plan tasks. It allows different types of robots to handle complex and unpredictable tasks in various environments. The framework is designed to be efficient and robust, making it easier for robots to understand human intentions and collaborate effectively. By simplifying the structure of task planning, LEO-RobotAgent enhances the adaptability of robots across multiple platforms, such as UAVs and robotic arms.', title='Empowering Robots with Language for Smarter Interaction and Task Planning'))
[15.12.2025 09:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LEO-RobotAgentÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑËØ≠Ë®ÄÈ©±Âä®Êô∫ËÉΩ‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫‰∫∫Êú∫‰∫§‰∫íÂíå‰ªªÂä°ËßÑÂàí„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊù•Êìç‰Ωú‰∏çÂêåÁ±ªÂûãÁöÑÊú∫Âô®‰∫∫ÔºåÂÆåÊàêÂ§çÊùÇÁöÑ‰ªªÂä°„ÄÇÂÆÉÂÖ∑ÊúâÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÅÈ≤ÅÊ£íÊÄßÂíåÈ´òÊïàÊÄßÔºåËÉΩÂ§üÈôç‰Ωé‰∫∫Êú∫‰∫§‰∫íÁöÑÈó®Êßõ„ÄÇÈÄöËøáÊ®°ÂùóÂåñÁöÑÂ∑•ÂÖ∑ÈõÜÔºåLEO-RobotAgentÂèØ‰ª•ÁÅµÊ¥ªË∞ÉÁî®ÂêÑÁßçÂ∑•ÂÖ∑ÔºåÈÄÇÂ∫î‰∏çÂêåÁöÑÈúÄÊ±Ç„ÄÇ","title":"ÈÄöÁî®ËØ≠Ë®ÄÈ©±Âä®ÁöÑÊú∫Âô®‰∫∫Êô∫ËÉΩÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LEO-RobotAgentÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑËØ≠Ë®ÄÈ©±Âä®Êô∫ËÉΩ‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫‰∫∫Êú∫‰∫§‰∫íÂíå‰ªªÂä°ËßÑÂàí„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊù•Êìç‰Ωú‰∏çÂêåÁ±ªÂûãÁöÑÊú∫Âô®‰∫∫ÔºåÂÆåÊàêÂ§çÊùÇÁöÑ‰ªªÂä°„ÄÇÂÆÉÂÖ∑ÊúâÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÅÈ≤ÅÊ£íÊÄßÂíåÈ´òÊïàÊÄßÔºåËÉΩÂ§üÈôç‰Ωé‰∫∫Êú∫‰∫§‰∫íÁöÑÈó®Êßõ„ÄÇÈÄöËøáÊ®°ÂùóÂåñÁöÑÂ∑•ÂÖ∑ÈõÜÔºåLEO-RobotAgentÂèØ‰ª•ÁÅµÊ¥ªË∞ÉÁî®ÂêÑÁßçÂ∑•ÂÖ∑ÔºåÈÄÇÂ∫î‰∏çÂêåÁöÑÈúÄÊ±Ç„ÄÇ', title='ÈÄöÁî®ËØ≠Ë®ÄÈ©±Âä®ÁöÑÊú∫Âô®‰∫∫Êô∫ËÉΩÊ°ÜÊû∂'))
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#video", "#cv", "#multimodal"], "emoji": "üë•", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –∏–∑ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∑–∞–¥–∞—á—É –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –∏–∑ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞. –ú–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Vision-Language Model –æ–±—É—á–∞
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#cv", "#healthcare", "#architecture", "#dataset", "#benchmark"], "emoji": "ü´Å", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "–†–∞–±–æ—Ç–∞ –∏–∑—É—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏—Ö –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤ –Ω–∞ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≥—Ä–∞–º–º–∞—Ö –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏ —Å
[15.12.2025 09:35] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#cv", "#training", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à—É–º –¥–ª—è –≥–ª–∞–¥–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–º –≤ –¥–æ–º–∞—à–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-–º–æ—Ç–æ—Ä–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–∏–≥—Ä–∞–ª–∞ –∫–æ–Ω–∫—É—Ä—Å
[15.12.2025 09:35] Renaming data file.
[15.12.2025 09:35] Renaming previous data. hf_papers.json to ./d/2025-12-15.json
[15.12.2025 09:35] Saving new data file.
[15.12.2025 09:35] Generating page.
[15.12.2025 09:35] Renaming previous page.
[15.12.2025 09:35] Renaming previous data. index.html to ./d/2025-12-15.html
[15.12.2025 09:35] Writing result.
[15.12.2025 09:35] Renaming log file.
[15.12.2025 09:35] Renaming previous data. log.txt to ./logs/2025-12-15_last_log.txt
