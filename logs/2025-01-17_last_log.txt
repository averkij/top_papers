[17.01.2025 02:07] Read previous papers.
[17.01.2025 02:07] Generating top page (month).
[17.01.2025 02:07] Writing top page (month).
[17.01.2025 03:12] Read previous papers.
[17.01.2025 03:12] Get feed.
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08365
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08828
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08983
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08994
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.09019
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07783
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.09012
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08809
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08970
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04693
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.19412
[17.01.2025 03:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.01.2025 03:12] No deleted papers detected.
[17.01.2025 03:12] Downloading and parsing papers (pdf, html). Total: 11.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08365.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08365.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08365.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08828.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08828.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08828.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08983.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08983.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08983.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08994.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08994.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08994.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.09019.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.09019.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.09019.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.07783.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.07783.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.07783.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.09012.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.09012.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.09012.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08809.
[17.01.2025 03:12] Downloading paper 2501.08809 from http://arxiv.org/pdf/2501.08809v1...
[17.01.2025 03:12] Failed to download and parse paper https://huggingface.co/papers/2501.08809: 'LTChar' object is not iterable
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08970.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08970.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08970.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.04693.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.04693.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.04693.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2412.19412.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2412.19412.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2412.19412.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Enriching papers with extra data.
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 0. Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal lands...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 1. Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance ...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 2. 3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distort...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 3. Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of repre...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 4. The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 5. Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address ...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 6. We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 7. In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 8. We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptograph...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 9. Interacting with the world is a multi-sensory experience: achieving effective general-purpose interaction requires making use of all available modalities -- including vision, touch, and audio -- to fill in gaps from partial observation. For example, when vision is occluded reaching into a bag, a rob...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 10. Image matching for both cross-view and cross-modality plays a critical role in multimodal perception. In practice, the modality gap caused by different imaging systems/styles poses great challenges to the matching task. Existing works try to extract invariant features for specific modalities and tra...
[17.01.2025 03:12] Read previous papers.
[17.01.2025 03:12] Generating reviews via LLM API.
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#open_source", "#ethics", "#data", "#dataset"], "emoji": "üìö", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ò–ò: –≤—ã–∑–æ–≤—ã –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–∞–≤–æ–æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–π. –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —é—Ä–∏–¥–∏—á
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "üîç", "ru": {"title": "MMDocIR: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMDocIR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–µ –∑–∞–¥–∞—á–∏: –ø–æ–∏—Å–∫ –Ω–∞ —É
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#3d", "#dataset"], "emoji": "üèôÔ∏è", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 4D-–≥–æ—Ä–æ–¥–æ–≤ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –¥–∏–Ω–∞–º–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏–∫–∏", "desc": "CityDreamer4D - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö 4D-–≥–æ—Ä–æ–¥–æ–≤. –û–Ω–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞) –∏ —Å
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#video", "#diffusion", "#architecture"], "emoji": "üé¨", "ru": {"title": "RepVideo: —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RepVideo - —É–ª—É—á—à–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –æ–±
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#benchmark", "#video", "#long_context", "#diffusion"], "emoji": "üêç", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –≤–∏–¥–µ–æ: Ouroboros-Diffusion –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Ouroboros-Di
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Parameter-Inverted Image Pyramid Networks (PIIP). PIIP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#artificial intelligence", "#reasoning", "#hallucinations", "#multimodal", "#benchmark", "#dataset"], "emoji": "üé®", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —É—á–∏—Ç—Å—è –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#audio", "#story_generation", "#multimodal", "#dataset"], "emoji": "üéµ", "ru": {"title": "XMusic: –ò–ò-–∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º–∏ —ç–º–æ—Ü–∏—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç XMusic - –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–π –º—É–∑—ã–∫–∏, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#data", "#ethics", "#architecture", "#security", "#inference"], "emoji": "üîê", "ru": {"title": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –ø–æ—Å—Ä–µ–¥–Ω–∏–∫ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Trusted Capa
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#robotics", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ú—É–ª—å—Ç–∏—Å–µ–Ω—Å–æ—Ä–Ω—ã–π –ò–ò: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑—Ä–µ–Ω–∏—è, –æ—Å—è–∑–∞–Ω–∏—è –∏ –∑–≤—É–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ä–æ–±–æ—Ç–æ–≤ —Å –º–∏—Ä–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FuSe - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–æ–±–æ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º—É
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal", "#open_source", "#synthetic"], "emoji": "üîÄ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MINIMA - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö
[17.01.2025 03:12] Loading Chinese text from previous data.
[17.01.2025 03:12] Renaming data file.
[17.01.2025 03:12] Renaming previous data. hf_papers.json to ./d/2025-01-17.json
[17.01.2025 03:12] Saving new data file.
[17.01.2025 03:12] Generating page.
[17.01.2025 03:12] Renaming previous page.
[17.01.2025 03:12] Renaming previous data. index.html to ./d/2025-01-17.html
[17.01.2025 03:12] [Experimental] Generating Chinese page for reading.
[17.01.2025 03:12] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Ê£ÄÁ¥¢', 'pinyin': 'ji«én su«í', 'trans': 'retrieval'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'ËØÜÂà´', 'pinyin': 'sh√≠ bi√©', 'trans': 'recognize'}, {'word': 'Â∏ÉÂ±Ä', 'pinyin': 'b√π ji√∫', 'trans': 'layout'}, {'word': 'Â∞ΩÁÆ°', 'pinyin': 'j√¨n gu«én', 'trans': 'although'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'Á≥ªÁªü', 'pinyin': 'x√¨ t«íng', 'trans': 'system'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'È°µÈù¢Á∫ß', 'pinyin': 'y√® mi√†n j√≠', 'trans': 'page-level'}, {'word': 'Ê†áÊ≥®', 'pinyin': 'biƒÅo zh√π', 'trans': 'annotation'}, {'word': 'ËµÑÊ∫ê', 'pinyin': 'zƒ´ yu√°n', 'trans': 'resource'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'perform'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'}, {'word': '‰ΩøÁî®', 'pinyin': 'sh«ê y√≤ng', 'trans': 'use'}, {'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]
[17.01.2025 03:12] Renaming previous Chinese page.
[17.01.2025 03:12] Renaming previous data. zh.html to ./d/2025-01-16_zh_reading_task.html
[17.01.2025 03:12] Writing Chinese reading task.
[17.01.2025 03:12] Writing result.
[17.01.2025 03:12] Renaming log file.
[17.01.2025 03:12] Renaming previous data. log.txt to ./logs/2025-01-17_last_log.txt
