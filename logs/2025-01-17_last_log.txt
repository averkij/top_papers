[17.01.2025 02:07] Read previous papers.
[17.01.2025 02:07] Generating top page (month).
[17.01.2025 02:07] Writing top page (month).
[17.01.2025 03:12] Read previous papers.
[17.01.2025 03:12] Get feed.
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08365
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08828
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08983
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08994
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.09019
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07783
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.09012
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08809
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08970
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04693
[17.01.2025 03:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.19412
[17.01.2025 03:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.01.2025 03:12] No deleted papers detected.
[17.01.2025 03:12] Downloading and parsing papers (pdf, html). Total: 11.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08365.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08365.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08365.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08828.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08828.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08828.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08983.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08983.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08983.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08994.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08994.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08994.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.09019.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.09019.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.09019.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.07783.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.07783.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.07783.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.09012.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.09012.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.09012.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08809.
[17.01.2025 03:12] Downloading paper 2501.08809 from http://arxiv.org/pdf/2501.08809v1...
[17.01.2025 03:12] Failed to download and parse paper https://huggingface.co/papers/2501.08809: 'LTChar' object is not iterable
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.08970.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.08970.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.08970.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.04693.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2501.04693.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2501.04693.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2412.19412.
[17.01.2025 03:12] Extra JSON file exists (./assets/json/2412.19412.json), skip PDF parsing.
[17.01.2025 03:12] Paper image links file exists (./assets/img_data/2412.19412.json), skip HTML parsing.
[17.01.2025 03:12] Success.
[17.01.2025 03:12] Enriching papers with extra data.
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 0. Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal lands...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 1. Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance ...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 2. 3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distort...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 3. Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of repre...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 4. The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 5. Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address ...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 6. We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 7. In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 8. We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptograph...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 9. Interacting with the world is a multi-sensory experience: achieving effective general-purpose interaction requires making use of all available modalities -- including vision, touch, and audio -- to fill in gaps from partial observation. For example, when vision is occluded reaching into a bag, a rob...
[17.01.2025 03:12] ********************************************************************************
[17.01.2025 03:12] Abstract 10. Image matching for both cross-view and cross-modality plays a critical role in multimodal perception. In practice, the modality gap caused by different imaging systems/styles poses great challenges to the matching task. Existing works try to extract invariant features for specific modalities and tra...
[17.01.2025 03:12] Read previous papers.
[17.01.2025 03:12] Generating reviews via LLM API.
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#open_source", "#ethics", "#data", "#dataset"], "emoji": "📚", "ru": {"title": "Открытые данные для ответственного ИИ: вызовы и перспективы", "desc": "Статья рассматривает проблему обучения больших языковых моделей (LLM) на данных без разрешения правообладателей. Анализируются юридич
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "🔍", "ru": {"title": "MMDocIR: Новый стандарт для мультимодального поиска документов", "desc": "Статья представляет новый бенчмарк MMDocIR для оценки систем мультимодального поиска документов. Бенчмарк включает две задачи: поиск на у
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#3d", "#dataset"], "emoji": "🏙️", "ru": {"title": "Композиционная генерация 4D-городов с разделением динамики и статики", "desc": "CityDreamer4D - это генеративная модель для создания неограниченных 4D-городов. Она разделяет генерацию динамических объектов (например, транспорта) и с
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#video", "#diffusion", "#architecture"], "emoji": "🎬", "ru": {"title": "RepVideo: стабильные представления для качественной генерации видео", "desc": "Статья представляет RepVideo - улучшенную систему представлений для диффузионных моделей генерации видео на основе текста. Авторы об
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#benchmark", "#video", "#long_context", "#diffusion"], "emoji": "🐍", "ru": {"title": "Бесконечное видео: Ouroboros-Diffusion для непрерывной генерации согласованного контента", "desc": "Эта статья представляет новый метод генерации видео произвольной длины под названием Ouroboros-Di
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#cv"], "emoji": "🔍", "ru": {"title": "Эффективные многомасштабные сети для точного визуального восприятия", "desc": "Статья представляет новую архитектуру нейронных сетей под названием Parameter-Inverted Image Pyramid Networks (PIIP). PIIP использует 
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#artificial intelligence", "#reasoning", "#hallucinations", "#multimodal", "#benchmark", "#dataset"], "emoji": "🎨", "ru": {"title": "Искусственный интеллект учится оценивать искусство", "desc": "Исследование посвящено использованию мультимодальных языковых моделей (MLLM) для оценки 
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#audio", "#story_generation", "#multimodal", "#dataset"], "emoji": "🎵", "ru": {"title": "XMusic: ИИ-композитор нового поколения с управляемыми эмоциями", "desc": "Статья представляет XMusic - генерализованный фреймворк для генерации символической музыки, поддерживающий различные тип
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#data", "#ethics", "#architecture", "#security", "#inference"], "emoji": "🔐", "ru": {"title": "Машинное обучение как доверенный посредник для безопасных вычислений", "desc": "Статья представляет новый подход к безопасным вычислениям с использованием машинного обучения - Trusted Capa
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#robotics", "#reasoning"], "emoji": "🤖", "ru": {"title": "Мультисенсорный ИИ: объединение зрения, осязания и звука для улучшения взаимодействия роботов с миром", "desc": "Статья представляет FuSe - новый подход к обучению роботов, использующий му
[17.01.2025 03:12] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal", "#open_source", "#synthetic"], "emoji": "🔀", "ru": {"title": "Универсальное сопоставление изображений через масштабирование данных", "desc": "Статья представляет MINIMA - универсальную систему сопоставления изображений для различных кросс-модальных
[17.01.2025 03:12] Loading Chinese text from previous data.
[17.01.2025 03:12] Renaming data file.
[17.01.2025 03:12] Renaming previous data. hf_papers.json to ./d/2025-01-17.json
[17.01.2025 03:12] Saving new data file.
[17.01.2025 03:12] Generating page.
[17.01.2025 03:12] Renaming previous page.
[17.01.2025 03:12] Renaming previous data. index.html to ./d/2025-01-17.html
[17.01.2025 03:12] [Experimental] Generating Chinese page for reading.
[17.01.2025 03:12] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '检索', 'pinyin': 'jiǎn suǒ', 'trans': 'retrieval'}, {'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '识别', 'pinyin': 'shí bié', 'trans': 'recognize'}, {'word': '布局', 'pinyin': 'bù jiú', 'trans': 'layout'}, {'word': '尽管', 'pinyin': 'jìn guǎn', 'trans': 'although'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '页面级', 'pinyin': 'yè miàn jí', 'trans': 'page-level'}, {'word': '标注', 'pinyin': 'biāo zhù', 'trans': 'annotation'}, {'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resource'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]
[17.01.2025 03:12] Renaming previous Chinese page.
[17.01.2025 03:12] Renaming previous data. zh.html to ./d/2025-01-16_zh_reading_task.html
[17.01.2025 03:12] Writing Chinese reading task.
[17.01.2025 03:12] Writing result.
[17.01.2025 03:12] Renaming log file.
[17.01.2025 03:12] Renaming previous data. log.txt to ./logs/2025-01-17_last_log.txt
