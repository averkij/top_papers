[07.02.2025 06:15] Read previous papers.
[07.02.2025 06:15] Generating top page (month).
[07.02.2025 06:15] Writing top page (month).
[07.02.2025 07:10] Read previous papers.
[07.02.2025 07:10] Get feed.
[07.02.2025 07:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.03032
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04306
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.03621
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04153
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.02358
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.03544
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04128
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04328
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.03860
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04295
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04299
[07.02.2025 07:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.04322
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04235
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04270
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.03639
[07.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04296
[07.02.2025 07:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.02.2025 07:10] No deleted papers detected.
[07.02.2025 07:10] Downloading and parsing papers (pdf, html). Total: 16.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.03032.
[07.02.2025 07:10] Downloading paper 2502.03032 from http://arxiv.org/pdf/2502.03032v2...
[07.02.2025 07:10] Extracting affiliations from text.
[07.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 2 2 3 0 3 0 . 2 0 5 2 : r Daniil Laptev 1 Nikita Balagansky 1 2 Yaroslav Aksenov 1 Daniil Gavrilov "
[07.02.2025 07:10] Response: []
[07.02.2025 07:10] Extracting affiliations from text.
[07.02.2025 07:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 2 2 3 0 3 0 . 2 0 5 2 : r Daniil Laptev 1 Nikita Balagansky 1 2 Yaroslav Aksenov 1 Daniil GavrilovWe introduce new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined interlayer feature links. By using data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of causal, crosslayer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models. modules (MLP, attention, and residual) at each layer, capturing how features originate, propagate, or vanish throughout the model in form of flow graphs. 1. Cross-Layer Feature Evolution. Using the pretrained SAEs that can isolate interpretable monosemantic directions, we utilize information obtained from cosine similarity between their decoder weights to track how these directions evolve or appear across layers. This reveals distinct patterns of feature birth and refinement not seen in single-layer analyses. 2. Mechanistic Properties of Flow Graph. By building flow graph, we uncover an evolutionary pathway, which is also an internal circuit-like computational pathway, where MLP and attention modules introduce new features to already existing ones or change them. 3. Multi-Layer Model Steering. We show that flow graphs can improve the quality of model steering by targeting multiple SAE features at once, and also offer better understanding of the steering outcome. This framework provides the first demonstration of such multi-layer steering via SAE features. 1. Introduction Large language models (LLMs) excel at generating coherent text but remain largely opaque in how they store and transform semantic information. Previous research has revealed that neural networks often encode concepts as linear directions within hidden representations (Mikolov et al., 2013), and that sparse autoencoders (SAEs) can disentangle these directions into monosemantic features in the case of LLMs (Bricken et al., 2023; Cunningham et al., 2023). Yet, most methods analyze single layer or focus solely on the residual stream, leaving the multi-layer nature of feature emergence and transformation underexplored (Balagansky et al., 2024; Balcells et al., 2024). In this paper, we propose data-free approach, based on cosine similarity, that aligns SAE features across multiple 1T-Tech 2Moscow Institute of Physics and Technology. Correspondence to: Nikita Balagansky <n.n.balaganskiy@tbank.ru>. Preprint Our method helps to discover the lifespan of SAE features, understand their evolution across layers, and shed light on how they might form computational circuits, thereby enabling more precise control over model behavior. 2. Preliminaries 2.1. Linear representation hypothesis To understand how models encode and process the information they learn, one can examine the geometric structure of their hidden representations and weights. Research has shown (Mikolov et al., 2013; Marks & Tegmark, 2024; Gurnee & Tegmark, 2024; Engels et al., 2024) that linear directions carry semantically meaningful information and may be used by models to represent learned concepts. Observations of this kind led to the development of the linear representation hypothesis, which can be stated as follows. Hidden states Rd can be represented as sparse linear combinations of features Rd that lie in linear subspaces 1 Analyze Feature Flow to Enhance Interpretation and Steering in Language Models Rd. The impact of each feature is encoded by its magnitude . The total number of these linear subspaces with unique semantics greatly exceeds d, forcing the model to build an overcomplete basis in the feature space embedded in Rd. During forward pass, the model typically uses only small fraction of them. These subspaces are usually onedimensional lines, but more complex structures can appear (Engels et al., 2024).1 2.2. SAE and Transcoders To retrieve such linear directions, Sparse Autoencoders (SAEs) (Bricken et al., 2023; Cunningham et al., 2023) were introduced. They decompose the models hidden state into sparse weighted sum of interpretable features. Let (P ) = {F (P ) {1, ..., D}}, where is the dictionary size, be collection of one-dimensional features learned by an SAE at position in the model (e.g., after the MLP block). Then the SAE can be represented as = σ(Wench + benc), ˆh = Wdecz + bdec, where Wenc, benc, Wdec, bdec are SAE parameters, σ() is nonlinear activation function, Rd is models hidden state, RF is the feature activation, and ˆh is the SAEs reconstruction of the hidden state. Sparse autoencoders are usually trained to reconstruct model hidden states while enforcing sparse feature activations: = Lrec(h, ˆh) + Lreg(z). Typically, Lrec = ˆh2 2, while Lreg(z) is an l0 proxy. The choice of activation function σ() is crucial for achieving the desired representation properties. JumpReLU (Rajamanoharan et al., 2024) introduces threshold parameter θ RF that controls how large each pre-activation must be for the feature to become active: σ(z) = H(z θ), where is the Heaviside function. Top-K (Makhzani & Frey, 2014), allows one to control the desired level of sparsity in an SAE by fixing k: σ(h) = topk(W + b). Instead of taking the top-k per sample, BatchTopK selects the top activations over all samples in the batch (Bussmann et al., 2024). 1There is distinction between the weak and strong LRH. The strong version posits that there are only linear representations, while the weak version says that representations are mostly linear and one-dimensional. Transcoders (Jermyn et al., 2024) are very similar to SAEs, but they reconstruct different target. Typically, they are trained as interpretable approximations of MLPs: ˆhpost = TC(hpre), Lrec = hpost ˆhpost2, where hpre is the pre-MLP hidden state, hpost is the postMLP hidden state, and ˆhpost is the transcoders prediction. 2.3. Features On Different Layers Interconnectio"
[07.02.2025 07:10] Mistral response. {"id": "b326322ebf6b4d9eb9412d4448210c9e", "object": "chat.completion", "created": 1738912239, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['1T-Tech', 'Moscow Institute of Physics and Technology']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1705, "total_tokens": 1729, "completion_tokens": 24}}
[07.02.2025 07:10] Response: ```python
['1T-Tech', 'Moscow Institute of Physics and Technology']
```
[07.02.2025 07:10] Deleting PDF ./assets/pdf/2502.03032.pdf.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04306.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04306.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04306.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.03621.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.03621.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.03621.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04153.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04153.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04153.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.02358.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.02358.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.02358.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.03544.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.03544.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.03544.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04128.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04128.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04128.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04328.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04328.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04328.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.03860.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.03860.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.03860.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04295.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04295.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04295.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04299.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04299.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04299.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04322.
[07.02.2025 07:10] Downloading paper 2502.04322 from http://arxiv.org/pdf/2502.04322v1...
[07.02.2025 07:10] Extracting affiliations from text.
[07.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 2 2 3 4 0 . 2 0 5 2 : r SPEAK EASY: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions Content Warning: This paper contains examples of harmful language. Yik Siu Chan * 1 Narutatsu Ri * 2 Yuxin Xiao * 3 Marzyeh Ghassemi "
[07.02.2025 07:10] Response: []
[07.02.2025 07:10] Extracting affiliations from text.
[07.02.2025 07:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 2 2 3 4 0 . 2 0 5 2 : r SPEAK EASY: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions Content Warning: This paper contains examples of harmful language. Yik Siu Chan * 1 Narutatsu Ri * 2 Yuxin Xiao * 3 Marzyeh GhassemiDespite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informativetwo attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HARMSCORE, jailbreak metric that measures how effectively an LLM response enables harmful actions, and SPEAK EASY, simple multi-step, multilingual attack framework. Notably, by incorporating SPEAK EASY into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HARMSCORE in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.1 1. Introduction Recent advancements in large language models (LLMs) have driven their widespread adoption across various domains (Achiam et al., 2023; Anthropic, 2023; Touvron et al., 2023), serving variety of individuals from highly skilled experts to non-technical, everyday users (Bommasani et al., 2021). To ensure safe deployment, significant efforts have been made to align these models (Bai et al., 2022a;b; Ganguli et al., 2022; Markov et al., 2023). However, these *Equal contribution 1Brown University 2Columbia University 3Massachusetts Institute of Technology. Correspondence to: Yik Siu Chan <yik siu chan@brown.edu>. 1Our code is available at https://github.com/ yiksiu-chan/SpeakEasy. 1 efforts face ongoing challenges from jailbreaks (Jin et al., 2024; Wei et al., 2024), adversarial attacks that aim to breach LLMs safety mechanisms and induce harmful responses, which pose societal risks when used by malicious actors (Hendrycks et al., 2023). Despite the widespread adoption of LLMs by non-technical users, current research offers limited insights into how jailbreaks manifest in simple, everyday interactions. Existing jailbreak methods typically require technical understanding of models internal mechanisms (Zou et al., 2023) or extensive engineering efforts (Chao et al., 2023; Mehrotra et al., 2023). These settings, however, may not accurately reflect real-world scenarios where an average user attempts to misuse LLMs for malicious purposes (NPR, 2025). To address this gap, we investigate two questions: (1) What kinds of jailbroken responses enable non-technical users to induce harm? (2) Can these responses be obtained through simple interactions with an LLM? To answer the first question, we identify four attributes (Xing et al., 2017; Cho et al., 2019; Ganguli et al., 2022) potentially related to harmfulness and curate synthetic dataset in which each example demonstrates combination of these attributes. Through human evaluation, we determine actionability and informativeness as key attributes in inducing harm when the jailbroken response is followed by individuals without specialized knowledge. On this basis, we introduce HARMSCORE, metric that explicitly measures the aforementioned attributes and provides more fine-grained assessment of jailbreak harmfulness than commonly used measures of success (e.g., Attack Success Rate (ASR) (Ganguli et al., 2022; Mazeika et al., 2024; Wei et al., 2024)). Notably, HARMSCORE aligns better with human judgments than ASR, especially for queries that seek practical instructions. To demonstrate that simple interactions can sufficiently elicit actionable and informative jailbreaks, we propose SPEAK EASY. In contrast to other jailbreak frameworks, SPEAK EASY emulates two types of human-LLM interactions commonly observed in real-world examples (Deng et al., 2024b; Zhao et al., 2024; Zheng et al., 2024): multi-step reasoning and multilingual querying. Given harmful query, users can decompose it into multiple seemingly innocuous subqueries (Dua et al., 2022; Kojima et al., 2022; Wei et al., 2022), which more easily circumvent safety guardrails. They can SPEAK EASY: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions further exploit the multilingual knowledge (Ahuja et al., 2023) and vulnerabilities (Yong et al., 2023; Deng et al., 2024a) in LLMs by translating the subqueries to various languages. Using these two tools, non-technical user can generate harmful response to the original malicious query by selecting the most actionable and informative responses to subqueries. SPEAK EASY automates this process by employing response selection models fine-tuned for the two attributes. Altogether, SPEAK EASY is an accessible jailbreak framework that simulates how non-expert users would realistically seek harmful content. To systematically evaluate our proposed framework, we target safety-aligned proprietary and open-source multilingual LLMs including GPT-4o (OpenAI, 2024), Qwen2-72BInstruct (Yang et al., 2024), and Llama-3.3-70B-Instruct (Grattafiori et al., 2024), across four jailbreak benchmarks (Zou et al., 2023; Mazeika et al., 2024; Xie et al., 2024; Han et al., 2024). Notably, SPEAK EASY increases the ASR of GPT-4o from 0.092 to 0.555 on average across four benchmarks, and its HARMSCORE from 0.180 to 0.759, all through simple inference easily accessible to users. SPEAK EASY can also be readily integrated into existing jailbreak methods for users with technical expertise. Incorporating SPEAK EASY into GCG-T (Zou et al., 2023) and TAPT (Mehrotra et al., 2023) significantly outperforms their vanilla baselines, yielding an average absolute increase of 0.313 in ASR and 0.398 in HARMSCORE across LLMs and benchmarks. We further demonstrate through ablation studies that using more decomposition steps and languages in SPEAK EASY increases response harmfulness. We summarize our contributions in this paper as follows: We identify actionability and informativeness "
[07.02.2025 07:10] Mistral response. {"id": "049fef37011e4f1baaa7a43318887531", "object": "chat.completion", "created": 1738912248, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\"Brown University\", \"Columbia University\", \"Massachusetts Institute of Technology\"]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1819, "total_tokens": 1837, "completion_tokens": 18}}
[07.02.2025 07:10] Response: ["Brown University", "Columbia University", "Massachusetts Institute of Technology"]
[07.02.2025 07:10] Deleting PDF ./assets/pdf/2502.04322.pdf.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04235.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04235.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04235.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04270.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04270.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04270.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.03639.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.03639.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.03639.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.04296.
[07.02.2025 07:10] Extra JSON file exists (./assets/json/2502.04296.json), skip PDF parsing.
[07.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.04296.json), skip HTML parsing.
[07.02.2025 07:10] Success.
[07.02.2025 07:10] Enriching papers with extra data.
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 0. We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, tr...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 1. Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representat...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 2. We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene ...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 3. Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we pr...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 4. Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify ...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 5. We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems invol...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 6. Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring se...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 7. Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 8. Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM ...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 9. Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has receiv...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 10. This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in mod...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 11. Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 12. Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottlene...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 13. As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In pr...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 14. We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion mod...
[07.02.2025 07:10] ********************************************************************************
[07.02.2025 07:10] Abstract 15. We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maint...
[07.02.2025 07:10] Read previous papers.
[07.02.2025 07:10] Generating reviews via LLM API.
[07.02.2025 07:10] Querying the API.
[07.02.2025 07:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.
[07.02.2025 07:10] Response: {
  "desc": "Представлен новый подход к систематическому отображению признаков, обнаруженных разреженным автоэнкодером, между последовательными слоями больших языковых моделей. Метод использует технику косинусного сходства без данных для отслеживания эволюции признаков на каждом этапе. Это позволяет создавать подробные графы потоков развития признаков, обеспечивая детальную интерпретируемость и механистическое понимание вычислений модели. Исследование демонстрирует, как межслойные карты признаков могут использоваться для прямого управления поведением модели путем усиления или подавления выбранных признаков.",
  "emoji": "🧠",
  "title": "Прозрачное управление языковыми моделями через межслойный анализ признаков"
}
[07.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models."

[07.02.2025 07:10] Response: ```python
["ARCHITECTURE", "TRAINING"]
```
[07.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models."

[07.02.2025 07:10] Response: ```python
['INTERPRETABILITY']
```
[07.02.2025 07:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel method for analyzing features in large language models using sparse autoencoders. It employs a data-free cosine similarity approach to track the evolution of features across different layers of the model. The resulting flow graphs provide detailed insights into how features change and interact during processing. Additionally, the method allows for targeted manipulation of model behavior by adjusting specific features, enhancing interpretability and control in text generation tasks.","title":"Mapping and Manipulating Features in Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper presents a novel method for analyzing features in large language models using sparse autoencoders. It employs a data-free cosine similarity approach to track the evolution of features across different layers of the model. The resulting flow graphs provide detailed insights into how features change and interact during processing. Additionally, the method allows for targeted manipulation of model behavior by adjusting specific features, enhancing interpretability and control in text generation tasks.', title='Mapping and Manipulating Features in Language Models'))
[07.02.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新方法，系统地映射稀疏自编码器在大型语言模型中各层发现的特征。我们使用无数据的余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。这种方法生成了特征演变的细粒度流图，增强了模型计算的可解释性和机制洞察力。我们还展示了如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成中的主题控制。","title":"特征映射：引导大型语言模型的新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文提出了一种新方法，系统地映射稀疏自编码器在大型语言模型中各层发现的特征。我们使用无数据的余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。这种方法生成了特征演变的细粒度流图，增强了模型计算的可解释性和机制洞察力。我们还展示了如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成中的主题控制。', title='特征映射：引导大型语言模型的新方法'))
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#optimization", "#agents", "#rlhf", "#small_models"], "emoji": "🚀", "ru": {"title": "ScoreFlow: эффективная оптимизация мультиагентных систем на основе ИИ", "desc": "В статье представлен новый фреймворк ScoreFlow для оптимизации рабочих процессов мультиаг
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#inference", "#video", "#multimodal", "#synthetic", "#diffusion"], "emoji": "🎬", "ru": {"title": "Генерация динамического контента в видео по текстовым инструкциям", "desc": "Авторы представляют метод для дополнения реальных видео новым динамическим контентом на основе текстовых инс
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#open_source", "#training", "#benchmark", "#alignment", "#rlhf"], "emoji": "🧠", "ru": {"title": "UltraIF: простой метод для обучения LLM следовать сложным инструкциям", "desc": "Исследователи представили подход UltraIF для улучшения способности больших языковых моделей (LLM) следова
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv", "#benchmark"], "emoji": "🤖", "ru": {"title": "Универсальный подход к генерации и редактированию движений человека", "desc": "Статья представляет новый подход к генерации и редактированию движений человека в компьютерной графике и компьютерном зрении
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#math", "#training", "#architecture", "#optimization", "#synthetic"], "emoji": "🧠", "ru": {"title": "ИИ превосходит человека в олимпиадной геометрии", "desc": "AlphaGeometry2 - это улучшенная версия системы для решения олимпиадных задач по геометрии, превзо
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#audio", "#training", "#optimization"], "emoji": "🗣️", "ru": {"title": "Llasa: масштабируемый синтез речи на основе единой языковой модели", "desc": "В статье представлена новая система синтеза речи Llasa, использующая одноуровневый векторный квантователь
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#audio", "#open_source", "#video", "#training", "#multimodal"], "emoji": "🦉", "ru": {"title": "Ola: Прогрессивное обучение для создания мощной омнимодальной модели", "desc": "В статье представлена Ola - омнимодальная языковая модель, способная понимать изображения, видео и аудио на 
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#math", "#long_context", "#dataset", "#reasoning"], "emoji": "🧠", "ru": {"title": "Самообучение ИИ сложным рассуждениям без подсказок", "desc": "Эта статья представляет новый подход к обучению больших языковых моделей (LLM) способности генерировать длинные
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training"], "emoji": "🧬", "ru": {"title": "Интегрированная оптимизация формы и содержания промптов повышает эффективность LLM", "desc": "Статья представляет новую методологию оптимизации промптов для больших языковых моделей (LLM), называемую CFPO. 
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#multimodal", "#video", "#diffusion", "#3d", "#games"], "emoji": "🎬", "ru": {"title": "MotionCanvas: Интуитивное проектирование видеокадров с контролем движения", "desc": "Эта статья представляет метод MotionCanvas, позволяющий пользователям проектировать кинематографические видеока
[07.02.2025 07:11] Querying the API.
[07.02.2025 07:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.
[07.02.2025 07:11] Response: {
  "desc": "Статья посвящена проблеме уязвимостей больших языковых моделей (LLM) к атакам, направленным на обход систем безопасности. Авторы предлагают новую метрику HarmScore для оценки эффективности вредоносных ответов LLM и разрабатывают фреймворк атаки Speak Easy, основанный на многошаговом многоязычном взаимодействии. Исследование показывает, что простые паттерны взаимодействия могут быть легко использованы злоумышленниками для вредоносных целей. Результаты демонстрируют значительное увеличение успешности атак и показателя HarmScore при применении Speak Easy к различным LLM.",

  "emoji": "🕵️",

  "title": "Простое взаимодействие - скрытая угроза безопасности языковых моделей"
}
[07.02.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions."

[07.02.2025 07:11] Response: ```python
["MULTILINGUAL", "BENCHMARK", "DATASET"]
```
[07.02.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions."

[07.02.2025 07:11] Response: ```python
['SECURITY', 'ETHICS']
```
[07.02.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can lead to harmful behavior. It explores whether these attacks are useful for average users and if safety issues arise in typical human-LLM interactions. The authors introduce HarmScore, a metric to evaluate how well LLM responses can facilitate harmful actions, and Speak Easy, a framework for executing multi-step, multilingual attacks. Their findings indicate that malicious users can exploit common interaction patterns, significantly increasing the success rate of harmful actions.","title":"Uncovering Hidden Vulnerabilities in Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can lead to harmful behavior. It explores whether these attacks are useful for average users and if safety issues arise in typical human-LLM interactions. The authors introduce HarmScore, a metric to evaluate how well LLM responses can facilitate harmful actions, and Speak Easy, a framework for executing multi-step, multilingual attacks. Their findings indicate that malicious users can exploit common interaction patterns, significantly increasing the success rate of harmful actions.', title='Uncovering Hidden Vulnerabilities in Language Models'))
[07.02.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"尽管已经进行了广泛的安全对齐工作，大型语言模型（LLMs）仍然容易受到越狱攻击，这会引发有害行为。本文探讨了两个关键问题：越狱响应是否真正帮助普通用户实施有害行为，以及在更常见的人类与LLM的简单互动中是否存在安全漏洞。我们提出了HarmScore，这是一种评估LLM响应如何有效促进有害行为的指标，并介绍了Speak Easy，一个简单的多步骤、多语言攻击框架。研究表明，恶意用户可以轻松利用常见的互动模式来实现有害意图。","title":"揭示大型语言模型的安全漏洞"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='尽管已经进行了广泛的安全对齐工作，大型语言模型（LLMs）仍然容易受到越狱攻击，这会引发有害行为。本文探讨了两个关键问题：越狱响应是否真正帮助普通用户实施有害行为，以及在更常见的人类与LLM的简单互动中是否存在安全漏洞。我们提出了HarmScore，这是一种评估LLM响应如何有效促进有害行为的指标，并介绍了Speak Easy，一个简单的多步骤、多语言攻击框架。研究表明，恶意用户可以轻松利用常见的互动模式来实现有害意图。', title='揭示大型语言模型的安全漏洞'))
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#optimization", "#data", "#dataset", "#training", "#synthetic"], "emoji": "🚀", "ru": {"title": "MAGA: преодоление ограничений данных для масштабирования языковых моделей", "desc": "Статья представляет метод MAGA для синтеза разнообразных и контекстуально богатых данных для предобуче
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#rlhf", "#training", "#alignment", "#optimization"], "emoji": "🎯", "ru": {"title": "PILAF: точное согласование ИИ с человеческими ценностями", "desc": "Статья представляет новый метод обучения языковых моделей с учетом человеческих ценностей - PILAF (Policy-Interpolated Learning for
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#diffusion", "#video", "#3d"], "emoji": "🎥", "ru": {"title": "3D-осведомленная генерация видео с улучшенной геометрией и динамикой", "desc": "Представлена новая система генерации видео, объединяющая трехмерную геометрию и динамическое восприятие. Двумерные видео дополняются трехмерн
[07.02.2025 07:11] Using data from previous issue: {"categories": ["#games", "#robotics", "#dataset", "#video", "#synthetic"], "emoji": "🤖", "ru": {"title": "HMA: Быстрое и качественное моделирование видео для обучения роботов", "desc": "Статья представляет новый метод Heterogeneous Masked Autoregression (HMA) для моделирования динамики видео действ
[07.02.2025 07:11] Loading Chinese text from previous data.
[07.02.2025 07:11] Renaming data file.
[07.02.2025 07:11] Renaming previous data. hf_papers.json to ./d/2025-02-07.json
[07.02.2025 07:11] Saving new data file.
[07.02.2025 07:11] Generating page.
[07.02.2025 07:11] Renaming previous page.
[07.02.2025 07:11] Renaming previous data. index.html to ./d/2025-02-07.html
[07.02.2025 07:11] [Experimental] Generating Chinese page for reading.
[07.02.2025 07:11] Chinese vocab [{'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '社会行为', 'pinyin': 'shè huì xíng wéi', 'trans': 'social behavior'}, {'word': '产生', 'pinyin': 'chǎn shēng', 'trans': 'generate'}, {'word': '传统', 'pinyin': 'chuán tǒng', 'trans': 'traditional'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '难以', 'pinyin': 'nán yǐ', 'trans': 'difficult to'}, {'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'}, {'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'}, {'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'}, {'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'}, {'word': '代理', 'pinyin': 'dài lǐ', 'trans': 'agent'}, {'word': '模拟', 'pinyin': 'mó nǐ', 'trans': 'simulate'}, {'word': '非理性', 'pinyin': 'fēi lǐ xìng', 'trans': 'irrational'}, {'word': '因素', 'pinyin': 'yīn sù', 'trans': 'factor'}, {'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': 'TwinMarket', 'pinyin': 'TwinMarket', 'trans': 'TwinMarket'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '社会经济系统', 'pinyin': 'shè huì jīng jì xì tǒng', 'trans': 'socio-economic system'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '个体行为', 'pinyin': 'gè tǐ xíng wéi', 'trans': 'individual behavior'}, {'word': '引发', 'pinyin': 'yǐn fā', 'trans': 'trigger'}, {'word': '群体行为', 'pinyin': 'qún tǐ xíng wéi', 'trans': 'group behavior'}, {'word': '突现现象', 'pinyin': 'tū xiàn xiàn xiàng', 'trans': 'emergent phenomenon'}]
[07.02.2025 07:11] Renaming previous Chinese page.
[07.02.2025 07:11] Renaming previous data. zh.html to ./d/2025-02-06_zh_reading_task.html
[07.02.2025 07:11] Writing Chinese reading task.
[07.02.2025 07:11] Writing result.
[07.02.2025 07:11] Renaming log file.
[07.02.2025 07:11] Renaming previous data. log.txt to ./logs/2025-02-07_last_log.txt
