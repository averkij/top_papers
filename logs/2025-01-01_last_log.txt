[01.01.2025 08:13] Read previous papers.
[01.01.2025 08:13] Generating top page (month).
[01.01.2025 08:13] Writing top page (month).
[01.01.2025 09:10] Read previous papers.
[01.01.2025 09:10] Get feed.
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18525
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20070
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20422
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20993
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21037
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21079
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21187
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20005
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20631
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21206
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21140
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21139
[01.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21199
[01.01.2025 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.01.2025 09:10] No deleted papers detected.
[01.01.2025 09:10] Downloading and parsing papers (pdf, html). Total: 13.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.18525.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.18525.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.18525.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20070.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.20070.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.20070.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20422.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.20422.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.20422.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20993.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.20993.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.20993.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21037.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.21037.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.21037.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21079.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.21079.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.21079.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21187.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.21187.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.21187.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20005.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.20005.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.20005.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20631.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.20631.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.20631.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21206.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.21206.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.21206.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21140.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.21140.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.21140.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21139.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.21139.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.21139.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21199.
[01.01.2025 09:10] Extra JSON file exists (./assets/json/2412.21199.json), skip PDF parsing.
[01.01.2025 09:10] Paper image links file exists (./assets/img_data/2412.21199.json), skip HTML parsing.
[01.01.2025 09:10] Success.
[01.01.2025 09:10] Enriching papers with extra data.
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 0. Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In thi...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 1. Multimodal large language models (MLLMs) hold significant potential in the medical field, but their capabilities are often limited by insufficient data in certain medical domains, highlighting the need for understanding what kinds of images can be used by MLLMs for generalization. Current research s...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 2. Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry o...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 3. The rapid evolution of large language models (LLMs) has unlocked their capabilities in advanced reasoning tasks like mathematical problem-solving, code generation, and legal analysis. Central to this progress are inference-time reasoning algorithms, which refine outputs by exploring multiple solutio...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 4. We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks st...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 5. As a verified need, consistent editing across in-the-wild images remains a technical challenge arising from various unmanageable factors, like object poses, lighting conditions, and photography environments. Edicho steps in with a training-free solution based on diffusion models, featuring a fundame...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 6. The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a criti...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 7. We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 8. Recently, "visual o1" began to enter people's vision, with expectations that this slow-thinking design can solve visual reasoning tasks, especially geometric math problems. However, the reality is that current LVLMs (Large Vision Language Models) can hardly even accurately copy a geometric figure, l...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 9. We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve thi...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 10. Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM techn...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 11. We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising a codebase with an executable runtime environment, unit tests, and a task specified in natural language. We use SWE-Gym to tra...
[01.01.2025 09:10] ********************************************************************************
[01.01.2025 09:10] Abstract 12. We introduce self-invoking code generation, a new task designed to evaluate the progressive reasoning and problem-solving capabilities of LLMs. In this task, models are presented with a base problem and a related, more complex problem. They must solve the base problem and then utilize its solution t...
[01.01.2025 09:10] Read previous papers.
[01.01.2025 09:10] Generating reviews via LLM API.
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#cv", "#multimodal", "#transfer_learning"], "emoji": "ğŸ”¬", "ru": {"title": "Ğ›Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ - ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#healthcare", "#open_source", "#multimodal", "#transfer_learning"], "emoji": "ğŸ©º", "ru": {"title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ - ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ MLLM", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¼
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#games", "#diffusion", "#video", "#3d"], "emoji": "ğŸ­", "ru": {"title": "ĞĞ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#inference"], "emoji": "ğŸ§ ", "ru": {"title": "Dynasor: ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LLM-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Dynasor, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#audio", "#open_source", "#benchmark", "#alignment", "#rlhf", "#small_models"], "emoji": "ğŸµ", "ru": {"title": "TangoFlux: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°", "desc": "TangoFlux - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ (Text-to-Audio, TT
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#open_source", "#inference"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Edicho: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Edicho - Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#math", "#inference"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ±Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ (overthinking) Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° OpenAI o1 Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#agents", "#open_source", "#benchmark", "#multimodal", "#science"], "emoji": "ğŸ§ ", "ru": {"title": "OneKE: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²", "desc": "OneKE - ÑÑ‚Ğ¾ Ğ´Ğ¾ĞºĞµÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ ÑÑ…ĞµĞ¼Ğ¾Ğ¹. ĞĞ½Ğ° ÑĞ¿Ğ¾
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#cv", "#math", "#reasoning"], "emoji": "ğŸ”", "ru": {"title": "ĞœĞµĞ´Ğ»ĞµĞ½Ğ½ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ»ÑƒÑ‡ÑˆĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ' (slow perception) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#3d", "#cv", "#dataset", "#synthetic"], "emoji": "ğŸ­", "ru": {"title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ñ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€Ñ‚ Ğ»Ğ¸Ñ†Ğ°", "desc": "PERSE - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ°. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ 
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#data", "#training", "#low_resource", "#transfer_learning", "#dataset", "#open_source", "#multilingual"], "emoji": "ğŸŒ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ 
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#agents", "#training"], "emoji": "ğŸ¤–", "ru": {"title": "SWE-Gym: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ", "desc": "SWE-Gym - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 2438 ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° P
[01.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#training", "#benchmark"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ ĞºĞ¾Ğ´: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ¾Ğ´Ğ°. Ğ’ Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´
[01.01.2025 09:10] Trying to get texts in Chinese.
[01.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In this paper, we explore the idea that CV adopts discrete and terminological task definitions (\eg, ``image segmentation''), which may be a key barrier to zero-shot task generalization. Our hypothesis is that without truly understanding previously-seen tasks--due to these terminological definitions--deep models struggle to generalize to novel tasks. To verify this, we introduce Explanatory Instructions, which provide an intuitive way to define CV task objectives through detailed linguistic transformations from input images to outputs. We create a large-scale dataset comprising 12 million ``image input to explanatory instruction to output'' triplets, and train an auto-regressive-based vision-language model (AR-based VLM) that takes both images and explanatory instructions as input. By learning to follow these instructions, the AR-based VLM achieves instruction-level zero-shot capabilities for previously-seen tasks and demonstrates strong zero-shot generalization for unseen CV tasks. Code and dataset will be openly available on our GitHub repository.
[01.01.2025 09:10] Mistral response. {"id": "d5d85dcb255e4417b83ea2c621ec11bb", "object": "chat.completion", "created": 1735722647, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u89c2\u5bdf\u5230\u7684\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u3002\u4f5c\u8005\u63d0\u51fa\uff0cCV \u4f7f\u7528\u79bb\u6563\u548c\u672f\u8bed\u5316\u7684\u4efb\u52a1\u5b9a\u4e49\uff08\u5982\u201c\u56fe\u50cf\u5206\u5272\u201d\uff09\uff0c\u8fd9\u53ef\u80fd\u662f\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u7684\u4e3b\u8981\u969c\u788d\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e00\u70b9\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u89e3\u91ca\u6027\u6307\u4ee4\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u8bed\u8a00\u8f6c\u6362\u4ece\u8f93\u5165\u56fe\u50cf\u5230\u8f93\u51fa\u6765\u5b9a\u4e49 CV \u4efb\u52a1\u76ee\u6807\u3002\u4ed6\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 1200 \u4e07\u4e2a\u201c\u56fe\u50cf\u8f93\u5165\u5230\u89e3\u91ca\u6027\u6307\u4ee4\u5230\u8f93\u51fa\u201d\u4e09\u5143\u7ec4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u81ea\u56de\u5f52\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08AR-based VLM\uff09\uff0c\u8be5\u6a21\u578b\u4ee5\u56fe\u50cf\u548c\u89e3\u91ca\u6027\u6307\u4ee4\u4f5c\u4e3a\u8f93\u5165\u3002\u901a\u8fc7\u5b66\u4e60\u9075\u5faa\u8fd9\u4e9b\u6307\u4ee4\uff0cAR-based VLM \u5b9e\u73b0\u4e86\u6307\u4ee4\u7ea7\u522b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684 CV \u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 GitHub \u5b58\u50a8\u5e93\u4e2d\u516c\u5f00\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 312, "total_tokens": 618, "completion_tokens": 306}}
[01.01.2025 09:10] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å°šæœªå®Œå…¨å®ç°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è§‚å¯Ÿåˆ°çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–ã€‚ä½œè€…æå‡ºï¼ŒCV ä½¿ç”¨ç¦»æ•£å’Œæœ¯è¯­åŒ–çš„ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä¸»è¦éšœç¢ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…å¼•å…¥äº†è§£é‡Šæ€§æŒ‡ä»¤ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºæ¥å®šä¹‰ CV ä»»åŠ¡ç›®æ ‡ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 1200 ä¸‡ä¸ªâ€œå›¾åƒè¾“å…¥åˆ°è§£é‡Šæ€§æŒ‡ä»¤åˆ°è¾“å‡ºâ€ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆAR-based VLMï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥å›¾åƒå’Œè§£é‡Šæ€§æŒ‡ä»¤ä½œä¸ºè¾“å…¥ã€‚é€šè¿‡å­¦ä¹ éµå¾ªè¿™äº›æŒ‡ä»¤ï¼ŒAR-based VLM å®ç°äº†æŒ‡ä»¤çº§åˆ«çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ CV ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨ GitHub å­˜å‚¨åº“ä¸­å…¬å¼€ã€‚
[01.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å°šæœªå®Œå…¨å®ç°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è§‚å¯Ÿåˆ°çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–ã€‚ä½œè€…æå‡ºï¼ŒCV ä½¿ç”¨ç¦»æ•£å’Œæœ¯è¯­åŒ–çš„ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä¸»è¦éšœç¢ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…å¼•å…¥äº†è§£é‡Šæ€§æŒ‡ä»¤ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºæ¥å®šä¹‰ CV ä»»åŠ¡ç›®æ ‡ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 1200 ä¸‡ä¸ªâ€œå›¾åƒè¾“å…¥åˆ°è§£é‡Šæ€§æŒ‡ä»¤åˆ°è¾“å‡ºâ€ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆAR-based VLMï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥å›¾åƒå’Œè§£é‡Šæ€§æŒ‡ä»¤ä½œä¸ºè¾“å…¥ã€‚é€šè¿‡å­¦ä¹ éµå¾ªè¿™äº›æŒ‡ä»¤ï¼ŒAR-based VLM å®ç°äº†æŒ‡ä»¤çº§åˆ«çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ CV ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨ GitHub å­˜å‚¨åº“ä¸­å…¬å¼€ã€‚
[01.01.2025 09:11] Mistral response. {"id": "e6f7829b72ff41fb88f4e35947180728", "object": "chat.completion", "created": 1735722655, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u89c2\u5bdf\u5230\u7684\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u3002\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le j\u00ecsu\u00e0nj\u012b sh\u00ecju\u00e9 (CV) sh\u00e0ng w\u00e8i qu\u00e1nxi\u00e0n sh\u00edxi\u00e0n z\u00ecr\u00e1n y\u01d4y\u00e1n ch\u01d4l\u01d0 (NLP) zh\u014dng gu\u0101nch\u00e1 d\u00e0o de l\u00edng y\u00e0ngb\u01cen r\u00e8nw\u00f9 f\u00e0nhu\u00e0.\n\n\u4f5c\u8005\u63d0\u51fa\uff0cCV \u4f7f\u7528\u79bb\u6563\u548c\u672f\u8bed\u5316\u7684\u4efb\u52a1\u5b9a\u4e49\uff08\u5982\u201c\u56fe\u50cf\u5206\u5272\u201d\uff09\uff0c\u8fd9\u53ef\u80fd\u662f\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u7684\u4e3b\u8981\u969c\u788d\u3002\nZu\u00f2zh\u011b t\u00edch\u016b, CV sh\u01d0y\u00f2ng l\u00ed s\u00e0n h\u00e9 sh\u00f9y\u01d4 hu\u00e0 de r\u00e8nw\u00f9 d\u00ecngy\u00ec (r\u00fa \u201ct\u00faxi\u00e0ng f\u0113ng\u0113\u201d), zh\u00e8 k\u011bn\u00e9ng sh\u00ec l\u00edng y\u00e0ngb\u01cen r\u00e8nw\u00f9 f\u00e0nhu\u00e0 de zh\u01d4y\u00e0o zh\u00e0ng'\u00e0i.\n\n\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e00\u70b9\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u89e3\u91ca\u6027\u6307\u4ee4\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u8bed\u8a00\u8f6c\u6362\u4ece\u8f93\u5165\u56fe\u50cf\u5230\u8f93\u51fa\u6765\u5b9a\u4e49 CV \u4efb\u52a1\u76ee\u6807\u3002\nW\u00e8ile y\u00e0nzh\u00e8ng zh\u00e8 y\u012b di\u01cen, zu\u00f2zh\u011b y\u01d0nr\u00f9 le ji\u011bsh\u00ec x\u00ecng zh\u01d0l\u00ecng, t\u014dnggu\u00f2 xi\u00e1ngx\u00ec de y\u01d4y\u00e1n zhu\u01cenhu\u00e0n c\u00f3ng sh\u016br\u00f9 t\u00faxi\u00e0ng d\u00e0o sh\u016bch\u016b l\u00e1i d\u00ecngy\u00ec CV r\u00e8nw\u00f9 m\u00f9bi\u0101o.\n\n\u4ed6\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 1200 \u4e07\u4e2a\u201c\u56fe\u50cf\u8f93\u5165\u5230\u89e3\u91ca\u6027\u6307\u4ee4\u5230\u8f93\u51fa\u201d\u4e09\u5143\u7ec4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u81ea\u56de\u5f52\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08AR-based VLM\uff09\uff0c\u8be5\u6a21\u578b\u4ee5\u56fe\u50cf\u548c\u89e3\u91ca\u6027\u6307\u4ee4\u4f5c\u4e3a\u8f93\u5165\u3002\nT\u0101men chu\u00e0ngji\u00e0n le y\u012bg\u00e8 b\u0101oh\u00e1n 1200 w\u00e0n g\u00e8 \u201ct\u00faxi\u00e0ng sh\u016br\u00f9 d\u00e0o ji\u011bsh\u00ec x\u00ecng zh\u01d0l\u00ecng d\u00e0o sh\u016bch\u016b\u201d s\u0101nyu\u00e1nz\u01d4 de d\u00e0 gu\u012bm\u00f3 sh\u00f9j\u00f9j\u00ed, b\u00ecng x\u00f9nli\u00e0n le y\u012bg\u00e8 z\u00ec hu\u00edgu\u012b sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng (AR-based VLM), g\u0101i m\u00f3x\u00edng y\u01d0 t\u00faxi\u00e0ng h\u00e9 ji\u011bsh\u00ec x\u00ecng zh\u01d0l\u00ecng zu\u00f2w\u00e9i sh\u016br\u00f9.\n\n\u901a\u8fc7\u5b66\u4e60\u9075\u5faa\u8fd9\u4e9b\u6307\u4ee4\uff0cAR-based VLM \u5b9e\u73b0\u4e86\u6307\u4ee4\u7ea7\u522b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684 CV \u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002\nT\u014dnggu\u00f2 xu\u00e9x\u00ed z\u016bnx\u00fan zh\u00e8xi\u0113 zh\u01d0l\u00ecng, AR-based VLM sh\u00edxi\u00e0n le zh\u01d0l\u00ecng j\u00edbi\u00e9 de l\u00edng y\u00e0ngb\u01cen n\u00e9ngl\u00ec, b\u00ecng z\u00e0i w\u00e8i ji\u00e0ngu\u00f2 de CV r\u00e8nw\u00f9 sh\u00e0ng zh\u01censh\u00ec le qi\u00e1ngd\u00e0 de l\u00edng y\u00e0ngb\u01cen f\u00e0nhu\u00e0.\n\n\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 GitHub \u5b58\u50a8\u5e93\u4e2d\u516c\u5f00\u3002\nD\u00e0im\u01ce h\u00e9 sh\u00f9j\u00f9j\u00ed ji\u0101ng z\u00e0i GitHub c\u00fanch\u016bk\u00f9 zh\u014dng g\u014dngk\u0101i."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 321, "total_tokens": 1253, "completion_tokens": 932}}
[01.01.2025 09:11] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å°šæœªå®Œå…¨å®ç°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è§‚å¯Ÿåˆ°çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–ã€‚
ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le jÃ¬suÃ njÄ« shÃ¬juÃ© (CV) shÃ ng wÃ¨i quÃ¡nxiÃ n shÃ­xiÃ n zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ (NLP) zhÅng guÄnchÃ¡ dÃ o de lÃ­ng yÃ ngbÇn rÃ¨nwÃ¹ fÃ nhuÃ .

ä½œè€…æå‡ºï¼ŒCV ä½¿ç”¨ç¦»æ•£å’Œæœ¯è¯­åŒ–çš„ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä¸»è¦éšœç¢ã€‚
ZuÃ²zhÄ› tÃ­chÅ«, CV shÇyÃ²ng lÃ­ sÃ n hÃ© shÃ¹yÇ” huÃ  de rÃ¨nwÃ¹ dÃ¬ngyÃ¬ (rÃº â€œtÃºxiÃ ng fÄ“ngÄ“â€), zhÃ¨ kÄ›nÃ©ng shÃ¬ lÃ­ng yÃ ngbÇn rÃ¨nwÃ¹ fÃ nhuÃ  de zhÇ”yÃ o zhÃ ng'Ã i.

ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…å¼•å…¥äº†è§£é‡Šæ€§æŒ‡ä»¤ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºæ¥å®šä¹‰ CV ä»»åŠ¡ç›®æ ‡ã€‚
WÃ¨ile yÃ nzhÃ¨ng zhÃ¨ yÄ« diÇn, zuÃ²zhÄ› yÇnrÃ¹ le jiÄ›shÃ¬ xÃ¬ng zhÇlÃ¬ng, tÅngguÃ² xiÃ¡ngxÃ¬ de yÇ”yÃ¡n zhuÇnhuÃ n cÃ³ng shÅ«rÃ¹ tÃºxiÃ ng dÃ o shÅ«chÅ« lÃ¡i dÃ¬ngyÃ¬ CV rÃ¨nwÃ¹ mÃ¹biÄo.

ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 1200 ä¸‡ä¸ªâ€œå›¾åƒè¾“å…¥åˆ°è§£é‡Šæ€§æŒ‡ä»¤åˆ°è¾“å‡ºâ€ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆAR-based VLMï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥å›¾åƒå’Œè§£é‡Šæ€§æŒ‡ä»¤ä½œä¸ºè¾“å…¥ã€‚
TÄmen chuÃ ngjiÃ n le yÄ«gÃ¨ bÄohÃ¡n 1200 wÃ n gÃ¨ â€œtÃºxiÃ ng shÅ«rÃ¹ dÃ o jiÄ›shÃ¬ xÃ¬ng zhÇlÃ¬ng dÃ o shÅ«chÅ«â€ sÄnyuÃ¡nzÇ” de dÃ  guÄ«mÃ³ shÃ¹jÃ¹jÃ­, bÃ¬ng xÃ¹nliÃ n le yÄ«gÃ¨ zÃ¬ huÃ­guÄ« shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng (AR-based VLM), gÄi mÃ³xÃ­ng yÇ tÃºxiÃ ng hÃ© jiÄ›shÃ¬ xÃ¬ng zhÇlÃ¬ng zuÃ²wÃ©i shÅ«rÃ¹.

é€šè¿‡å­¦ä¹ éµå¾ªè¿™äº›æŒ‡ä»¤ï¼ŒAR-based VLM å®ç°äº†æŒ‡ä»¤çº§åˆ«çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ CV ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚
TÅngguÃ² xuÃ©xÃ­ zÅ«nxÃºn zhÃ¨xiÄ“ zhÇlÃ¬ng, AR-based VLM shÃ­xiÃ n le zhÇlÃ¬ng jÃ­biÃ© de lÃ­ng yÃ ngbÇn nÃ©nglÃ¬, bÃ¬ng zÃ i wÃ¨i jiÃ nguÃ² de CV rÃ¨nwÃ¹ shÃ ng zhÇnshÃ¬ le qiÃ¡ngdÃ  de lÃ­ng yÃ ngbÇn fÃ nhuÃ .

ä»£ç å’Œæ•°æ®é›†å°†åœ¨ GitHub å­˜å‚¨åº“ä¸­å…¬å¼€ã€‚
DÃ imÇ hÃ© shÃ¹jÃ¹jÃ­ jiÄng zÃ i GitHub cÃºnchÅ«kÃ¹ zhÅng gÅngkÄi.
[01.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å°šæœªå®Œå…¨å®ç°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è§‚å¯Ÿåˆ°çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–ã€‚ä½œè€…æå‡ºï¼ŒCV ä½¿ç”¨ç¦»æ•£å’Œæœ¯è¯­åŒ–çš„ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä¸»è¦éšœç¢ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…å¼•å…¥äº†è§£é‡Šæ€§æŒ‡ä»¤ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºæ¥å®šä¹‰ CV ä»»åŠ¡ç›®æ ‡ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 1200 ä¸‡ä¸ªâ€œå›¾åƒè¾“å…¥åˆ°è§£é‡Šæ€§æŒ‡ä»¤åˆ°è¾“å‡ºâ€ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆAR-based VLMï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥å›¾åƒå’Œè§£é‡Šæ€§æŒ‡ä»¤ä½œä¸ºè¾“å…¥ã€‚é€šè¿‡å­¦ä¹ éµå¾ªè¿™äº›æŒ‡ä»¤ï¼ŒAR-based VLM å®ç°äº†æŒ‡ä»¤çº§åˆ«çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ CV ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨ GitHub å­˜å‚¨åº“ä¸­å…¬å¼€ã€‚
[01.01.2025 09:11] Mistral response. {"id": "9023cd2bd127400d85b4c83ac06ccfdc", "object": "chat.completion", "created": 1735722677, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u8ba1\u7b97\u673a\u89c6\u89c9\", \"pinyin\": \"j\u00ec su\u00e0n j\u012b sh\u00ec ju\u00e9\", \"trans\": \"computer vision\"},\n    {\"word\": \"\u81ea\u7136\u8bed\u8a00\u5904\u7406\", \"pinyin\": \"z\u00ec r\u00e1n y\u01d4 y\u00e1n ch\u01d4 l\u01d0\", \"trans\": \"natural language processing\"},\n    {\"word\": \"\u89c2\u5bdf\", \"pinyin\": \"gu\u0101n ch\u00e1\", \"trans\": \"observe\"},\n    {\"word\": \"\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\", \"pinyin\": \"l\u00edng y\u00e0ng b\u011bn r\u00e8n w\u00f9 f\u00e0n hu\u00e0\", \"trans\": \"zero-shot task generalization\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u79bb\u6563\", \"pinyin\": \"l\u00ed s\u00e0n\", \"trans\": \"discrete\"},\n    {\"word\": \"\u672f\u8bed\u5316\", \"pinyin\": \"sh\u00f9 y\u01d4 hu\u00e0\", \"trans\": \"terminological\"},\n    {\"word\": \"\u4efb\u52a1\u5b9a\u4e49\", \"pinyin\": \"r\u00e8n w\u00f9 d\u00ecng y\u00ec\", \"trans\": \"task definition\"},\n    {\"word\": \"\u56fe\u50cf\u5206\u5272\", \"pinyin\": \"t\u00fa xi\u00e0ng f\u0113n g\u0113\", \"trans\": \"image segmentation\"},\n    {\"word\": \"\u969c\u788d\", \"pinyin\": \"zh\u00e0ng \u00e0i\", \"trans\": \"obstacle\"},\n    {\"word\": \"\u9a8c\u8bc1\", \"pinyin\": \"y\u00e0n zh\u00e8ng\", \"trans\": \"verify\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u89e3\u91ca\u6027\u6307\u4ee4\", \"pinyin\": \"ji\u011b sh\u00ec x\u00ecng zh\u01d0 l\u01d0ng\", \"trans\": \"explanatory instructions\"},\n    {\"word\": \"\u8bed\u8a00\u8f6c\u6362\", \"pinyin\": \"y\u01d4 y\u00e1n zhu\u01cen hu\u00e0n\", \"trans\": \"language transformation\"},\n    {\"word\": \"\u8f93\u5165\u56fe\u50cf\", \"pinyin\": \"sh\u016b r\u00f9 t\u00fa xi\u00e0ng\", \"trans\": \"input image\"},\n    {\"word\": \"\u8f93\u51fa\", \"pinyin\": \"sh\u016b ch\u016b\", \"trans\": \"output\"},\n    {\"word\": \"\u4e09\u5143\u7ec4\", \"pinyin\": \"s\u0101n yu\u00e1n z\u01d4\", \"trans\": \"triplet\"},\n    {\"word\": \"\u5927\u89c4\u6a21\u6570\u636e\u96c6\", \"pinyin\": \"d\u00e0 gu\u012b m\u00f3 sh\u00f9 j\u00f9\", \"trans\": \"large-scale dataset\"},\n    {\"word\": \"\u81ea\u56de\u5f52\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"z\u00ec hu\u00ed gu\u012b sh\u00ec ju\u00e9 y\u01d4 y\u00e1n m\u00f3 x\u00edng\", \"trans\": \"autoregressive vision-language model\"},\n    {\"word\": \"\u9075\u5faa\", \"pinyin\": \"z\u016bn zh\u00f2ng\", \"trans\": \"follow\"},\n    {\"word\": \"\u6307\u4ee4\u7ea7\u522b\", \"pinyin\": \"zh\u01d0 l\u01d0ng j\u00ed bi\u00e9\", \"trans\": \"instruction level\"},\n    {\"word\": \"\u96f6\u6837\u672c\u80fd\u529b\", \"pinyin\": \"l\u00edng y\u00e0ng b\u011bn n\u00e9ng l\u00ec\", \"trans\": \"zero-shot capability\"},\n    {\"word\": \"\u5c55\u793a\", \"pinyin\": \"zh\u01cen sh\u00ec\", \"trans\": \"demonstrate\"},\n    {\"word\": \"\u672a\u89c1\u8fc7\u7684\", \"pinyin\": \"w\u00e8i ji\u00e0n gu\u00f2 de\", \"trans\": \"unseen\"},\n    {\"word\": \"\u5b58\u50a8\u5e93\", \"pinyin\": \"c\u00fan ch\u01d4 k\u00f9\", \"trans\": \"repository\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 353, "total_tokens": 1262, "completion_tokens": 909}}
[01.01.2025 09:11] Response: [
    {"word": "è®¨è®º", "pinyin": "tÇo lÃ¹n", "trans": "discuss"},
    {"word": "è®¡ç®—æœºè§†è§‰", "pinyin": "jÃ¬ suÃ n jÄ« shÃ¬ juÃ©", "trans": "computer vision"},
    {"word": "è‡ªç„¶è¯­è¨€å¤„ç†", "pinyin": "zÃ¬ rÃ¡n yÇ” yÃ¡n chÇ” lÇ", "trans": "natural language processing"},
    {"word": "è§‚å¯Ÿ", "pinyin": "guÄn chÃ¡", "trans": "observe"},
    {"word": "é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–", "pinyin": "lÃ­ng yÃ ng bÄ›n rÃ¨n wÃ¹ fÃ n huÃ ", "trans": "zero-shot task generalization"},
    {"word": "æå‡º", "pinyin": "tÃ­ chÅ«", "trans": "propose"},
    {"word": "ç¦»æ•£", "pinyin": "lÃ­ sÃ n", "trans": "discrete"},
    {"word": "æœ¯è¯­åŒ–", "pinyin": "shÃ¹ yÇ” huÃ ", "trans": "terminological"},
    {"word": "ä»»åŠ¡å®šä¹‰", "pinyin": "rÃ¨n wÃ¹ dÃ¬ng yÃ¬", "trans": "task definition"},
    {"word": "å›¾åƒåˆ†å‰²", "pinyin": "tÃº xiÃ ng fÄ“n gÄ“", "trans": "image segmentation"},
    {"word": "éšœç¢", "pinyin": "zhÃ ng Ã i", "trans": "obstacle"},
    {"word": "éªŒè¯", "pinyin": "yÃ n zhÃ¨ng", "trans": "verify"},
    {"word": "å¼•å…¥", "pinyin": "yÇn rÃ¹", "trans": "introduce"},
    {"word": "è§£é‡Šæ€§æŒ‡ä»¤", "pinyin": "jiÄ› shÃ¬ xÃ¬ng zhÇ lÇng", "trans": "explanatory instructions"},
    {"word": "è¯­è¨€è½¬æ¢", "pinyin": "yÇ” yÃ¡n zhuÇn huÃ n", "trans": "language transformation"},
    {"word": "è¾“å…¥å›¾åƒ", "pinyin": "shÅ« rÃ¹ tÃº xiÃ ng", "trans": "input image"},
    {"word": "è¾“å‡º", "pinyin": "shÅ« chÅ«", "trans": "output"},
    {"word": "ä¸‰å…ƒç»„", "pinyin": "sÄn yuÃ¡n zÇ”", "trans": "triplet"},
    {"word": "å¤§è§„æ¨¡æ•°æ®é›†", "pinyin": "dÃ  guÄ« mÃ³ shÃ¹ jÃ¹", "trans": "large-scale dataset"},
    {"word": "è‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹", "pinyin": "zÃ¬ huÃ­ guÄ« shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng", "trans": "autoregressive vision-language model"},
    {"word": "éµå¾ª", "pinyin": "zÅ«n zhÃ²ng", "trans": "follow"},
    {"word": "æŒ‡ä»¤çº§åˆ«", "pinyin": "zhÇ lÇng jÃ­ biÃ©", "trans": "instruction level"},
    {"word": "é›¶æ ·æœ¬èƒ½åŠ›", "pinyin": "lÃ­ng yÃ ng bÄ›n nÃ©ng lÃ¬", "trans": "zero-shot capability"},
    {"word": "å±•ç¤º", "pinyin": "zhÇn shÃ¬", "trans": "demonstrate"},
    {"word": "æœªè§è¿‡çš„", "pinyin": "wÃ¨i jiÃ n guÃ² de", "trans": "unseen"},
    {"word": "å­˜å‚¨åº“", "pinyin": "cÃºn chÇ” kÃ¹", "trans": "repository"}
]
[01.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å°šæœªå®Œå…¨å®ç°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­è§‚å¯Ÿåˆ°çš„é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–ã€‚ä½œè€…æå‡ºï¼ŒCV ä½¿ç”¨ç¦»æ•£å’Œæœ¯è¯­åŒ–çš„ä»»åŠ¡å®šä¹‰ï¼ˆå¦‚â€œå›¾åƒåˆ†å‰²â€ï¼‰ï¼Œè¿™å¯èƒ½æ˜¯é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä¸»è¦éšœç¢ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…å¼•å…¥äº†è§£é‡Šæ€§æŒ‡ä»¤ï¼Œé€šè¿‡è¯¦ç»†çš„è¯­è¨€è½¬æ¢ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºæ¥å®šä¹‰ CV ä»»åŠ¡ç›®æ ‡ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 1200 ä¸‡ä¸ªâ€œå›¾åƒè¾“å…¥åˆ°è§£é‡Šæ€§æŒ‡ä»¤åˆ°è¾“å‡ºâ€ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªè‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆAR-based VLMï¼‰ï¼Œè¯¥æ¨¡å‹ä»¥å›¾åƒå’Œè§£é‡Šæ€§æŒ‡ä»¤ä½œä¸ºè¾“å…¥ã€‚é€šè¿‡å­¦ä¹ éµå¾ªè¿™äº›æŒ‡ä»¤ï¼ŒAR-based VLM å®ç°äº†æŒ‡ä»¤çº§åˆ«çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ CV ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨ GitHub å­˜å‚¨åº“ä¸­å…¬å¼€ã€‚
[01.01.2025 09:11] Mistral response. {"id": "da9adac17d034086b7729a30b4deb214", "object": "chat.completion", "created": 1735722699, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the fact that computer vision (CV) has not yet fully achieved the zero-shot task generalization observed in natural language processing (NLP). The authors suggest that CV's use of discrete and terminological task definitions (such as \"image segmentation\") may be a major obstacle to zero-shot task generalization. To validate this, the authors introduce explanatory instructions, defining CV task objectives through detailed language transformations from input images to outputs. They created a large-scale dataset containing 12 million \"image input to explanatory instruction to output\" triplets and trained an autoregressive vision-language model (AR-based VLM) that takes images and explanatory instructions as inputs. By learning to follow these instructions, the AR-based VLM achieved instruction-level zero-shot capabilities and demonstrated strong zero-shot generalization on unseen CV tasks. The code and dataset will be made publicly available on a GitHub repository."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 319, "total_tokens": 517, "completion_tokens": 198}}
[01.01.2025 09:11] Response: This article discusses the fact that computer vision (CV) has not yet fully achieved the zero-shot task generalization observed in natural language processing (NLP). The authors suggest that CV's use of discrete and terminological task definitions (such as "image segmentation") may be a major obstacle to zero-shot task generalization. To validate this, the authors introduce explanatory instructions, defining CV task objectives through detailed language transformations from input images to outputs. They created a large-scale dataset containing 12 million "image input to explanatory instruction to output" triplets and trained an autoregressive vision-language model (AR-based VLM) that takes images and explanatory instructions as inputs. By learning to follow these instructions, the AR-based VLM achieved instruction-level zero-shot capabilities and demonstrated strong zero-shot generalization on unseen CV tasks. The code and dataset will be made publicly available on a GitHub repository.
[01.01.2025 09:11] Renaming data file.
[01.01.2025 09:11] Renaming previous data. hf_papers.json to ./d/2025-01-01.json
[01.01.2025 09:11] Saving new data file.
[01.01.2025 09:11] Generating page.
[01.01.2025 09:11] Renaming previous page.
[01.01.2025 09:11] Renaming previous data. index.html to ./d/2025-01-01.html
[01.01.2025 09:11] [Experimental] Generating Chinese page for reading.
[01.01.2025 09:11] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'è®¡ç®—æœºè§†è§‰', 'pinyin': 'jÃ¬ suÃ n jÄ« shÃ¬ juÃ©', 'trans': 'computer vision'}, {'word': 'è‡ªç„¶è¯­è¨€å¤„ç†', 'pinyin': 'zÃ¬ rÃ¡n yÇ” yÃ¡n chÇ” lÇ', 'trans': 'natural language processing'}, {'word': 'è§‚å¯Ÿ', 'pinyin': 'guÄn chÃ¡', 'trans': 'observe'}, {'word': 'é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–', 'pinyin': 'lÃ­ng yÃ ng bÄ›n rÃ¨n wÃ¹ fÃ n huÃ ', 'trans': 'zero-shot task generalization'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'ç¦»æ•£', 'pinyin': 'lÃ­ sÃ n', 'trans': 'discrete'}, {'word': 'æœ¯è¯­åŒ–', 'pinyin': 'shÃ¹ yÇ” huÃ ', 'trans': 'terminological'}, {'word': 'ä»»åŠ¡å®šä¹‰', 'pinyin': 'rÃ¨n wÃ¹ dÃ¬ng yÃ¬', 'trans': 'task definition'}, {'word': 'å›¾åƒåˆ†å‰²', 'pinyin': 'tÃº xiÃ ng fÄ“n gÄ“', 'trans': 'image segmentation'}, {'word': 'éšœç¢', 'pinyin': 'zhÃ ng Ã i', 'trans': 'obstacle'}, {'word': 'éªŒè¯', 'pinyin': 'yÃ n zhÃ¨ng', 'trans': 'verify'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇn rÃ¹', 'trans': 'introduce'}, {'word': 'è§£é‡Šæ€§æŒ‡ä»¤', 'pinyin': 'jiÄ› shÃ¬ xÃ¬ng zhÇ lÇng', 'trans': 'explanatory instructions'}, {'word': 'è¯­è¨€è½¬æ¢', 'pinyin': 'yÇ” yÃ¡n zhuÇn huÃ n', 'trans': 'language transformation'}, {'word': 'è¾“å…¥å›¾åƒ', 'pinyin': 'shÅ« rÃ¹ tÃº xiÃ ng', 'trans': 'input image'}, {'word': 'è¾“å‡º', 'pinyin': 'shÅ« chÅ«', 'trans': 'output'}, {'word': 'ä¸‰å…ƒç»„', 'pinyin': 'sÄn yuÃ¡n zÇ”', 'trans': 'triplet'}, {'word': 'å¤§è§„æ¨¡æ•°æ®é›†', 'pinyin': 'dÃ  guÄ« mÃ³ shÃ¹ jÃ¹', 'trans': 'large-scale dataset'}, {'word': 'è‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹', 'pinyin': 'zÃ¬ huÃ­ guÄ« shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'autoregressive vision-language model'}, {'word': 'éµå¾ª', 'pinyin': 'zÅ«n zhÃ²ng', 'trans': 'follow'}, {'word': 'æŒ‡ä»¤çº§åˆ«', 'pinyin': 'zhÇ lÇng jÃ­ biÃ©', 'trans': 'instruction level'}, {'word': 'é›¶æ ·æœ¬èƒ½åŠ›', 'pinyin': 'lÃ­ng yÃ ng bÄ›n nÃ©ng lÃ¬', 'trans': 'zero-shot capability'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'}, {'word': 'æœªè§è¿‡çš„', 'pinyin': 'wÃ¨i jiÃ n guÃ² de', 'trans': 'unseen'}, {'word': 'å­˜å‚¨åº“', 'pinyin': 'cÃºn chÇ” kÃ¹', 'trans': 'repository'}]
[01.01.2025 09:11] Renaming previous Chinese page.
[01.01.2025 09:11] Renaming previous data. zh.html to ./d/2024-12-31_zh_reading_task.html
[01.01.2025 09:11] Writing Chinese reading task.
[01.01.2025 09:11] Writing result.
[01.01.2025 09:11] Renaming log file.
[01.01.2025 09:11] Renaming previous data. log.txt to ./logs/2025-01-01_last_log.txt
