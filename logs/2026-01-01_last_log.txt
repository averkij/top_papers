[01.01.2026 03:58] Read previous papers.
[01.01.2026 03:58] Generating top page (month).
[01.01.2026 03:58] Writing top page (month).
[01.01.2026 04:51] Read previous papers.
[01.01.2026 04:51] Get feed.
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24618
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24210
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.25070
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24551
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.25075
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24297
[01.01.2026 04:51] Extract page data from URL. URL: https://huggingface.co/papers/2512.24097
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.23988
[01.01.2026 04:51] Extract page data from URL. URL: https://huggingface.co/papers/2512.22905
[01.01.2026 04:51] Extract page data from URL. URL: https://huggingface.co/papers/2512.24385
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.23851
[01.01.2026 04:51] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24873
[01.01.2026 04:51] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.01.2026 04:51] No deleted papers detected.
[01.01.2026 04:51] Downloading and parsing papers (pdf, html). Total: 12.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.24618.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.24618.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.24618.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.24210.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.24210.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.24210.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.25070.
[01.01.2026 04:51] Downloading paper 2512.25070 from https://arxiv.org/pdf/2512.25070v1...
[01.01.2026 04:51] Failed to download and parse paper https://huggingface.co/papers/2512.25070: 'LTChar' object is not iterable
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.24551.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.24551.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.24551.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.25075.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.25075.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.25075.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.24297.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.24297.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.24297.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.24097.
[01.01.2026 04:51] Downloading paper 2512.24097 from https://arxiv.org/pdf/2512.24097v1...
[01.01.2026 04:51] Extracting affiliations from text.
[01.01.2026 04:51] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Factorized Learning for Temporally Grounded Video-Language Models Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng National University of Singapore wenzhengzeng@u.nus.edu, {daniel.difei.gao, mike.zheng.shou}@gmail.com, nght@nus.edu.sg 5 2 0 2 0 3 ] . [ 1 7 9 0 4 2 . 2 1 5 2 : r Figure 1. (a) Performance: Our method outperforms SOTA methods across various tasks (here we draw the maximum performance across methods, detailed in Sec. 6). (b) Model: We propose new framework D2VLM, where we decompose the generation objective into grounding then answering with evidence referencing paradigm and introduce evidence tokens to emphasize explicit event-level visual semantic capture. (c) Training Algorithm: We introduce Factorized Preference Optimization (FPO) that explicitly addresses both temporal grounding and textual response. factorized data synthesis approach is also designed to support FPO. "
[01.01.2026 04:51] Response: ```python
["National University of Singapore"]
```
[01.01.2026 04:51] Deleting PDF ./assets/pdf/2512.24097.pdf.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.23988.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.23988.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.23988.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.22905.
[01.01.2026 04:51] Downloading paper 2512.22905 from https://arxiv.org/pdf/2512.22905v1...
[01.01.2026 04:51] Extracting affiliations from text.
[01.01.2026 04:51] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 5 0 9 2 2 . 2 1 5 2 : r JavisGPT: Unified Multi-modal LLM for Sounding-Video Comprehension and Generation Kai Liu1,2, Jungang Li3, Yuchong Sun4, Shengqiong Wu2, Jianzhang Gao4, Daoan Zhang5, Wei Zhang6, Sheng Jin7, Sicheng Yu8, Geng Zhan9, Jiayi Ji2, Fan Zhou1, Liang Zheng10, Shuicheng Yan2, Hao Fei2, Tat-Seng Chua2 1ZJU, 2NUS, 3HKUST(GZ), 4RUC, 5UR, 6HZCU, 7NTU, 8SMU, 9USYD, 10ANU Project: https://JavisVerse.github.io/JavisGPT-page Figure 1: JavisGPT supports multi-level synchronized audio-video (SyncVA) content (i.e., sounding video) comprehension (left), and simultaneously complex instruction-based SyncVA generation, interleaved in-context generation (middle), and multi-turn proactive conversations (right). "
[01.01.2026 04:51] Response: ```python
['ZJU', 'NUS', 'HKUST(GZ)', 'RUC', 'UR', 'HZCU', 'NTU', 'SMU', 'USYD', 'ANU']
```
[01.01.2026 04:51] Deleting PDF ./assets/pdf/2512.22905.pdf.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.24385.
[01.01.2026 04:51] Downloading paper 2512.24385 from https://arxiv.org/pdf/2512.24385v1...
[01.01.2026 04:51] Extracting affiliations from text.
[01.01.2026 04:51] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Forging Spatial Intelligence: Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems 1, , Lingdong Kong 2, , Xiaolu Liu 1, , Hao Shi1, Wentong Li3 , Jianke Zhu 1, , Song Wang Steven C. H. Hoi4,5 5 2 0 2 0 ] . [ 1 5 8 3 4 2 . 2 1 5 2 : r 1Zhejiang University Astronautics 2National University of Singapore 4Alibaba Group 5Singapore Management University 3Nanjing University of Aeronautics and Equal Contributions Corresponding Author The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create unified understanding remains formidable challenge. This paper presents comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment. GitHub Repo: https://github.com/worldbench/awesome-spatial-intelligence With the rapid proliferation of autonomous platforms, ranging from self-driving vehicles [72, 122, 166] and "
[01.01.2026 04:51] Response: ```python
[
    "Zhejiang University",
    "National University of Singapore",
    "Alibaba Group",
    "Singapore Management University",
    "Nanjing University of Aeronautics"
]
```
[01.01.2026 04:51] Deleting PDF ./assets/pdf/2512.24385.pdf.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.23851.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.23851.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.23851.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Downloading and parsing paper https://huggingface.co/papers/2512.24873.
[01.01.2026 04:51] Extra JSON file exists (./assets/json/2512.24873.json), skip PDF parsing.
[01.01.2026 04:51] Paper image links file exists (./assets/img_data/2512.24873.json), skip HTML parsing.
[01.01.2026 04:51] Success.
[01.01.2026 04:51] Enriching papers with extra data.
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 0. Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.  					AI-generated summary 				 We introduce Youtu-LLM, a...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 1. GR-Dexter introduces a hardware-model-data framework for bimanual dexterous-hand robot manipulation using vision-language-action models, combining teleoperation data and multimodal datasets to achieve robust generalization.  					AI-generated summary 				 Vision-language-action (VLA) models have ena...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 2. High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a f...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 3. Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or le...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 4. SpaceTimePilot is a video diffusion model enabling independent control of spatial viewpoint and temporal motion through a time-embedding mechanism, temporal-warping training, and a synthetic dataset for precise space-time disentanglement.  					AI-generated summary 				 We present SpaceTimePilot, a ...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 5. Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constra...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 6. Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate ...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 7. An unsupervised framework using sparse auto-encoders identifies and controls interpretable reasoning behaviors in large language models through disentangled latent vectors.  					AI-generated summary 				 Despite the growing reasoning capabilities of recent large language models (LLMs), their intern...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 8. This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learna...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 9. The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors lik...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 10. We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length,...
[01.01.2026 04:51] ********************************************************************************
[01.01.2026 04:51] Abstract 11. The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.  					AI-generated summary 				 Agentic ...
[01.01.2026 04:51] Read previous papers.
[01.01.2026 04:51] Generating reviews via LLM API.
[01.01.2026 04:51] Using data from previous issue: {"categories": ["#synthetic", "#long_context", "#agents", "#training", "#small_models", "#architecture", "#reasoning", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ›Ñ‘Ğ³ĞºĞ°Ñ, Ğ½Ğ¾ ÑƒĞ¼Ğ½Ğ°Ñ: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼", "desc": "Youtu-LLM â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.96B Ğ¿Ğ°Ñ€
[01.01.2026 04:51] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#multimodal", "#training"], "emoji": "ğŸ¤–", "ru": {"title": "ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ»Ğ¾Ğ²ĞºĞ¾ÑÑ‚ÑŒ: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "GR-Dexter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
[01.01.2026 04:51] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#dataset", "#training", "#leakage", "#small_models", "#reasoning", "#rl", "#synthetic", "#data", "#benchmark"], "emoji": "ğŸ”®", "ru": {"title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğµ
[01.01.2026 04:51] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#training", "#rlhf", "#video", "#data"], "emoji": "ğŸ¬", "ru": {"title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹", "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
[01.01.2026 04:51] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#architecture", "#video", "#synthetic"], "emoji": "ğŸ¬", "ru": {"title": "ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸", "desc": "SpaceTimePilot â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€
[01.01.2026 04:51] Using data from previous issue: {"categories": ["#rl", "#math", "#multimodal", "#benchmark"], "emoji": "ğŸ“Š", "ru": {"title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ FIGR â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡
[01.01.2026 04:51] Querying the API.
[01.01.2026 04:51] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.
[01.01.2026 04:51] Response: ```json
{
  "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ DÂ²VLM â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ²Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (FPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹.",
  "emoji": "ğŸ¬",
  "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…"
}
```
[01.01.2026 04:51] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm."

[01.01.2026 04:51] Response: ```python
["VIDEO", "MULTIMODAL", "DATASET", "TRAINING", "RLHF"]
```
[01.01.2026 04:51] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm."

[01.01.2026 04:51] Response: ```python
['REASONING', 'SYNTHETIC', 'OPEN_SOURCE']
```

**Justification:**

1. **REASONING**: The paper explicitly discusses "logical hierarchy" and "logical structure" in video understanding, emphasizing how temporal grounding forms the foundation for reliable textual responses. The factorized learning perspective addresses logical dependencies between tasks.

2. **SYNTHETIC**: The paper states "We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding."

3. **OPEN_SOURCE**: The paper mentions "Our source code is available at https://github.com/nusnlp/d2vlm," indicating the authors are releasing their code publicly.
[01.01.2026 04:51] Error. Failed to parse JSON from LLM. ["REASONING", "SYNTHETIC", "OPEN_SOURCE"]


**Justification:**

1. **REASONING**: The paper explicitly discusses "logical hierarchy" and "logical structure" in video understanding, emphasizing how temporal grounding forms the foundation for reliable textual responses. The factorized learning perspective addresses logical dependencies between tasks.

2. **SYNTHETIC**: The paper states "We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding."

3. **OPEN_SOURCE**: The paper mentions "Our source code is available at https://github.com/nusnlp/d2vlm," indicating the authors are releasing their code publicly.
[01.01.2026 04:51] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces D^2VLM, a new framework designed to improve video understanding by separating the tasks of temporal grounding and textual response. The authors argue that accurate temporal grounding is essential for generating reliable textual responses, and they propose a method that emphasizes this relationship. By using a \'grounding then answering with evidence referencing\' approach, the framework incorporates evidence tokens to enhance event-level visual semantic understanding. Additionally, the novel factorized preference optimization (FPO) algorithm is introduced to better model the dependencies between these tasks, leading to improved performance in video-language tasks.","title":"Decoupling Temporal Grounding and Textual Response for Enhanced Video Understanding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces D^2VLM, a new framework designed to improve video understanding by separating the tasks of temporal grounding and textual response. The authors argue that accurate temporal grounding is essential for generating reliable textual responses, and they propose a method that emphasizes this relationship. By using a 'grounding then answering with evidence referencing' approach, the framework incorporates evidence tokens to enhance event-level visual semantic understanding. Additionally, the novel factorized preference optimization (FPO) algorithm is introduced to better model the dependencies between these tasks, leading to improved performance in video-language tasks.", title='Decoupling Temporal Grounding and Textual Response for Enhanced Video Understanding'))
[01.01.2026 04:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ€è¿‘çš„è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°äº†å¾ˆå¤§çš„æ½œåŠ›ï¼Œä½†åœ¨äº‹ä»¶çº§æ„ŸçŸ¥çš„æ—¶é—´å®šä½ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè§†é¢‘ç†è§£ä¸­çš„ä¸¤ä¸ªä¸»è¦å› ç´ ï¼ˆæ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”ï¼‰å½¢æˆäº†ä¸€ä¸ªé€»è¾‘å±‚æ¬¡ï¼šå‡†ç¡®çš„æ—¶é—´è¯æ®å®šä½ä¸ºå¯é çš„æ–‡æœ¬å“åº”å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬æå‡ºäº†D^2VLMæ¡†æ¶ï¼Œè§£è€¦è¿™ä¸¤ä¸ªä»»åŠ¡çš„å­¦ä¹ ï¼ŒåŒæ—¶å¼ºè°ƒå®ƒä»¬ä¹‹é—´çš„å†…åœ¨ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¼•å…¥æ–°çš„å› å­åŒ–åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆFPOï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–æ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”çš„å­¦ä¹ ã€‚","title":"è§£è€¦è§†é¢‘ç†è§£ä¸­çš„æ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ€è¿‘çš„è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°äº†å¾ˆå¤§çš„æ½œåŠ›ï¼Œä½†åœ¨äº‹ä»¶çº§æ„ŸçŸ¥çš„æ—¶é—´å®šä½ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè§†é¢‘ç†è§£ä¸­çš„ä¸¤ä¸ªä¸»è¦å› ç´ ï¼ˆæ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”ï¼‰å½¢æˆäº†ä¸€ä¸ªé€»è¾‘å±‚æ¬¡ï¼šå‡†ç¡®çš„æ—¶é—´è¯æ®å®šä½ä¸ºå¯é çš„æ–‡æœ¬å“åº”å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬æå‡ºäº†D^2VLMæ¡†æ¶ï¼Œè§£è€¦è¿™ä¸¤ä¸ªä»»åŠ¡çš„å­¦ä¹ ï¼ŒåŒæ—¶å¼ºè°ƒå®ƒä»¬ä¹‹é—´çš„å†…åœ¨ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¼•å…¥æ–°çš„å› å­åŒ–åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆFPOï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–æ—¶é—´å®šä½å’Œæ–‡æœ¬å“åº”çš„å­¦ä¹ ã€‚', title='è§£è€¦è§†é¢‘ç†è§£ä¸­çš„æ—¶é—´å®šä½ä¸æ–‡æœ¬å“åº”'))
[01.01.2026 04:52] Using data from previous issue: {"categories": ["#reasoning", "#interpretability"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RISE Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾
[01.01.2026 04:52] Querying the API.
[01.01.2026 04:52] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.
[01.01.2026 04:52] Response: ```json
{
  "desc": "JavisGPT â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ encoder-LLM-decoder Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ SyncFusion Ğ´Ğ»Ñ ÑĞ¿atio-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ pipeline Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³, Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… JavisInst-Omni Ñ 200K Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… GPT-4o, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.",
  "emoji": "ğŸ¬",
  "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM"
}
```
[01.01.2026 04:52] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings."

[01.01.2026 04:52] Response: ```python
['MULTIMODAL', 'AUDIO', 'VIDEO', 'DATASET', 'TRAINING', 'ARCHITECTURE']
```
[01.01.2026 04:52] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings."

[01.01.2026 04:52] Response: ```python
['OPEN_SOURCE']
```
[01.01.2026 04:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"JavisGPT is a groundbreaking multimodal large language model designed for understanding and generating audio and video together. It uses a unique architecture that combines an encoder, a large language model (LLM), and a decoder, along with a SyncFusion module to effectively merge audio and video data. The model is trained through a three-stage process that includes pretraining, fine-tuning, and instruction-tuning, enhancing its ability to handle complex multimodal tasks. With the support of a comprehensive dataset of audio-video-text dialogues, JavisGPT demonstrates superior performance in tasks requiring synchronized audio and video comprehension and generation.","title":"JavisGPT: Unifying Audio and Video Understanding with Multimodal Intelligence"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='JavisGPT is a groundbreaking multimodal large language model designed for understanding and generating audio and video together. It uses a unique architecture that combines an encoder, a large language model (LLM), and a decoder, along with a SyncFusion module to effectively merge audio and video data. The model is trained through a three-stage process that includes pretraining, fine-tuning, and instruction-tuning, enhancing its ability to handle complex multimodal tasks. With the support of a comprehensive dataset of audio-video-text dialogues, JavisGPT demonstrates superior performance in tasks requiring synchronized audio and video comprehension and generation.', title='JavisGPT: Unifying Audio and Video Understanding with Multimodal Intelligence'))
[01.01.2026 04:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†JavisGPTï¼Œè¿™æ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç”¨äºè”åˆéŸ³é¢‘-è§†é¢‘ï¼ˆJAVï¼‰ç†è§£å’Œç”Ÿæˆã€‚JavisGPTé‡‡ç”¨ç®€æ´çš„ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ï¼Œé…å¤‡SyncFusionæ¨¡å—ï¼Œå®ç°éŸ³é¢‘å’Œè§†é¢‘çš„æ—¶ç©ºèåˆï¼Œå¹¶ä½¿ç”¨åŒæ­¥æ„ŸçŸ¥çš„å¯å­¦ä¹ æŸ¥è¯¢æ¥è¿æ¥é¢„è®­ç»ƒçš„JAV-DiTç”Ÿæˆå™¨ã€‚è¯¥è®¾è®¡ä½¿å¾—ä»å¤šæ¨¡æ€æŒ‡ä»¤ä¸­è¿›è¡Œæ—¶é—´ä¸€è‡´çš„éŸ³é¢‘è§†é¢‘ç†è§£å’Œç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é¢„è®­ç»ƒã€éŸ³é¢‘è§†é¢‘å¾®è°ƒå’Œå¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒï¼Œä»¥é€æ­¥æ„å»ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚","title":"JavisGPTï¼šéŸ³è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†JavisGPTï¼Œè¿™æ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç”¨äºè”åˆéŸ³é¢‘-è§†é¢‘ï¼ˆJAVï¼‰ç†è§£å’Œç”Ÿæˆã€‚JavisGPTé‡‡ç”¨ç®€æ´çš„ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ï¼Œé…å¤‡SyncFusionæ¨¡å—ï¼Œå®ç°éŸ³é¢‘å’Œè§†é¢‘çš„æ—¶ç©ºèåˆï¼Œå¹¶ä½¿ç”¨åŒæ­¥æ„ŸçŸ¥çš„å¯å­¦ä¹ æŸ¥è¯¢æ¥è¿æ¥é¢„è®­ç»ƒçš„JAV-DiTç”Ÿæˆå™¨ã€‚è¯¥è®¾è®¡ä½¿å¾—ä»å¤šæ¨¡æ€æŒ‡ä»¤ä¸­è¿›è¡Œæ—¶é—´ä¸€è‡´çš„éŸ³é¢‘è§†é¢‘ç†è§£å’Œç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é¢„è®­ç»ƒã€éŸ³é¢‘è§†é¢‘å¾®è°ƒå’Œå¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒï¼Œä»¥é€æ­¥æ„å»ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚', title='JavisGPTï¼šéŸ³è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹'))
[01.01.2026 04:52] Querying the API.
[01.01.2026 04:52] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.
[01.01.2026 04:52] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² (ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ LiDAR) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞµĞ´Ğ¸Ğ½ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D-Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.",
  "emoji": "ğŸš—",
  "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°"
}
```
[01.01.2026 04:52] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment."

[01.01.2026 04:52] Response: ```python
['MULTIMODAL', '3D', 'ROBOTICS', 'DATASET', 'BENCHMARK']
```
[01.01.2026 04:52] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment."

[01.01.2026 04:52] Response: ```python
["SURVEY"]
```
[01.01.2026 04:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of creating Spatial Intelligence in autonomous systems by integrating data from various sensors like cameras and LiDAR. It introduces a framework for multi-modal pre-training that enhances the ability of models to understand and process diverse sensor inputs. The authors propose a unified taxonomy for pre-training methods, which helps in developing advanced tasks such as 3D object detection. Additionally, the paper highlights key challenges like computational efficiency and scalability, providing a roadmap for building versatile multi-modal foundation models for real-world applications.","title":"Building Spatial Intelligence through Multi-Modal Integration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of creating Spatial Intelligence in autonomous systems by integrating data from various sensors like cameras and LiDAR. It introduces a framework for multi-modal pre-training that enhances the ability of models to understand and process diverse sensor inputs. The authors propose a unified taxonomy for pre-training methods, which helps in developing advanced tasks such as 3D object detection. Additionally, the paper highlights key challenges like computational efficiency and scalability, providing a roadmap for building versatile multi-modal foundation models for real-world applications.', title='Building Spatial Intelligence through Multi-Modal Integration'))
[01.01.2026 04:52] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ„å»ºçœŸæ­£çš„ç©ºé—´æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ— äººæœºç­‰è‡ªä¸»ç³»ç»Ÿä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨é¢çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œåˆ†æäº†åŸºç¡€ä¼ æ„Ÿå™¨ç‰¹æ€§ä¸å­¦ä¹ ç­–ç•¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡å»ºç«‹ç»Ÿä¸€çš„é¢„è®­ç»ƒèŒƒç•´ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»å•æ¨¡æ€åŸºçº¿åˆ°å¤æ‚çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå­¦ä¹ ç”¨äºé«˜çº§ä»»åŠ¡çš„æ•´ä½“è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬è¯†åˆ«äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ç­‰å…³é”®ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†å®ç°é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è·¯çº¿å›¾ã€‚","title":"æ„å»ºå¤šæ¨¡æ€ç©ºé—´æ™ºèƒ½çš„æœªæ¥ä¹‹è·¯"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ„å»ºçœŸæ­£çš„ç©ºé—´æ™ºèƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ— äººæœºç­‰è‡ªä¸»ç³»ç»Ÿä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨é¢çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œåˆ†æäº†åŸºç¡€ä¼ æ„Ÿå™¨ç‰¹æ€§ä¸å­¦ä¹ ç­–ç•¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡å»ºç«‹ç»Ÿä¸€çš„é¢„è®­ç»ƒèŒƒç•´ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»å•æ¨¡æ€åŸºçº¿åˆ°å¤æ‚çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå­¦ä¹ ç”¨äºé«˜çº§ä»»åŠ¡çš„æ•´ä½“è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬è¯†åˆ«äº†è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ç­‰å…³é”®ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†å®ç°é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è·¯çº¿å›¾ã€‚', title='æ„å»ºå¤šæ¨¡æ€ç©ºé—´æ™ºèƒ½çš„æœªæ¥ä¹‹è·¯'))
[01.01.2026 04:52] Using data from previous issue: {"categories": ["#video", "#architecture", "#training"], "emoji": "ğŸ¬", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²", "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ PFP â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ Ñ†
[01.01.2026 04:52] Using data from previous issue: {"categories": ["#alignment", "#open_source", "#synthetic", "#dataset", "#agents", "#training", "#rlhf", "#optimization", "#benchmark"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ­ĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agentic Learni
[01.01.2026 04:52] Renaming data file.
[01.01.2026 04:52] Renaming previous data. hf_papers.json to ./d/2026-01-01.json
[01.01.2026 04:52] Saving new data file.
[01.01.2026 04:52] Generating page.
[01.01.2026 04:52] Renaming previous page.
[01.01.2026 04:52] Renaming previous data. index.html to ./d/2026-01-01.html
[01.01.2026 04:52] Writing result.
[01.01.2026 04:52] Renaming log file.
[01.01.2026 04:52] Renaming previous data. log.txt to ./logs/2026-01-01_last_log.txt
