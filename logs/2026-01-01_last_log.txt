[31.12.2025 23:21] Read previous papers.
[31.12.2025 23:21] Generating top page (month).
[31.12.2025 23:21] Writing top page (month).
[01.01.2026 01:57] Read previous papers.
[01.01.2026 01:57] Get feed.
[01.01.2026 01:57] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21185
[01.01.2026 01:57] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22525
[01.01.2026 01:57] Get page data from previous paper. URL: https://huggingface.co/papers/2512.23675
[01.01.2026 01:57] Get page data from previous paper. URL: https://huggingface.co/papers/2512.23165
[01.01.2026 01:57] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22469
[01.01.2026 01:57] Extract page data from URL. URL: https://huggingface.co/papers/2512.22206
[01.01.2026 01:57] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21008
[01.01.2026 01:57] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.01.2026 01:57] No deleted papers detected.
[01.01.2026 01:57] Downloading and parsing papers (pdf, html). Total: 7.
[01.01.2026 01:57] Downloading and parsing paper https://huggingface.co/papers/2512.21185.
[01.01.2026 01:57] Extra JSON file exists (./assets/json/2512.21185.json), skip PDF parsing.
[01.01.2026 01:57] Paper image links file exists (./assets/img_data/2512.21185.json), skip HTML parsing.
[01.01.2026 01:57] Success.
[01.01.2026 01:57] Downloading and parsing paper https://huggingface.co/papers/2512.22525.
[01.01.2026 01:57] Extra JSON file exists (./assets/json/2512.22525.json), skip PDF parsing.
[01.01.2026 01:57] Paper image links file exists (./assets/img_data/2512.22525.json), skip HTML parsing.
[01.01.2026 01:57] Success.
[01.01.2026 01:57] Downloading and parsing paper https://huggingface.co/papers/2512.23675.
[01.01.2026 01:57] Extra JSON file exists (./assets/json/2512.23675.json), skip PDF parsing.
[01.01.2026 01:57] Paper image links file exists (./assets/img_data/2512.23675.json), skip HTML parsing.
[01.01.2026 01:57] Success.
[01.01.2026 01:57] Downloading and parsing paper https://huggingface.co/papers/2512.23165.
[01.01.2026 01:57] Extra JSON file exists (./assets/json/2512.23165.json), skip PDF parsing.
[01.01.2026 01:57] Paper image links file exists (./assets/img_data/2512.23165.json), skip HTML parsing.
[01.01.2026 01:57] Success.
[01.01.2026 01:57] Downloading and parsing paper https://huggingface.co/papers/2512.22469.
[01.01.2026 01:57] Extra JSON file exists (./assets/json/2512.22469.json), skip PDF parsing.
[01.01.2026 01:57] Paper image links file exists (./assets/img_data/2512.22469.json), skip HTML parsing.
[01.01.2026 01:57] Success.
[01.01.2026 01:57] Downloading and parsing paper https://huggingface.co/papers/2512.22206.
[01.01.2026 01:57] Downloading paper 2512.22206 from https://arxiv.org/pdf/2512.22206v1...
[01.01.2026 01:57] Extracting affiliations from text.
[01.01.2026 01:57] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 6 0 2 2 2 . 2 1 5 2 : r CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks Yogeswar Reddy Thota yogeswarreddy.thota@utdallas.edu Abstract Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility between identity and residual feature representations as self-supervised skip signal. CosineGate measures semantic redundancy through the Cosine Incompatibility Ratio (CIR), defined as 1 cos(x, (x)), and uses Gumbel-Softmax relaxation to enable persample, per-block gating during training. progressive FLOPs regularization term controls average compute usage without destabilizing optimization. On CIFAR-10, CosineGate systematically spans the accuracyefficiency Pareto frontier: an aggressive configuration achieves 89.9% accuracy with 24.1% FLOPs savings, balanced configuration achieves 91.3% accuracy with 28.5% savings at epoch 160, and conservative configuration reaches peak of 93.2% accuracy with minimal compute reduction. These results match or exceed ResNet-20 (91.3%) while reducing computation, without auxiliary supervision, distillation, or task-specific heuristics. Our results demonstrate that simple geometric measures of feature incompatibility provide principled and effective signal for dynamic residual routing. Residual networks (ResNets) [1] form the backbone of modern deep learning systems due to their ability to train deep architectures using identity shortcut connections. Each residual block computes = + (x; ), (1) where denotes the block input and (x) represents learned residual transformation. This formulation stabilizes optimization by enabling gradient flow through identity paths and guarantees that the trivial solution (x) = 0 always exists. However, this architectur"
[01.01.2026 01:57] Response: ```python
["University of Texas at Dallas"]
```
[01.01.2026 01:57] Deleting PDF ./assets/pdf/2512.22206.pdf.
[01.01.2026 01:57] Success.
[01.01.2026 01:57] Downloading and parsing paper https://huggingface.co/papers/2512.21008.
[01.01.2026 01:57] Extra JSON file exists (./assets/json/2512.21008.json), skip PDF parsing.
[01.01.2026 01:57] Paper image links file exists (./assets/img_data/2512.21008.json), skip HTML parsing.
[01.01.2026 01:57] Success.
[01.01.2026 01:57] Enriching papers with extra data.
[01.01.2026 01:57] ********************************************************************************
[01.01.2026 01:57] Abstract 0. In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To suppor...
[01.01.2026 01:57] ********************************************************************************
[01.01.2026 01:57] Abstract 1. Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details....
[01.01.2026 01:57] ********************************************************************************
[01.01.2026 01:57] Abstract 2. We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on ...
[01.01.2026 01:57] ********************************************************************************
[01.01.2026 01:57] Abstract 3. We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly ...
[01.01.2026 01:57] ********************************************************************************
[01.01.2026 01:57] Abstract 4. The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code impl...
[01.01.2026 01:57] ********************************************************************************
[01.01.2026 01:57] Abstract 5. Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility...
[01.01.2026 01:57] ********************************************************************************
[01.01.2026 01:57] Abstract 6. Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understan...
[01.01.2026 01:57] Read previous papers.
[01.01.2026 01:57] Generating reviews via LLM API.
[01.01.2026 01:57] Using data from previous issue: {"categories": ["#3d", "#data", "#diffusion", "#dataset", "#open_source", "#architecture"], "emoji": "ğŸ²", "ru": {"title": "Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸", "desc": "UltraShape 1.0 â€” ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ 3D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾
[01.01.2026 01:57] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#dataset", "#benchmark"], "emoji": "ğŸ¨", "ru": {"title": "Ğ¡ĞºĞµÑ‚Ñ‡-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DreamOmni3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°
[01.01.2026 01:57] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#open_source", "#training", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²", "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ continual 
[01.01.2026 01:57] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization", "#rl", "#reasoning", "#math"], "emoji": "ğŸ¯", "ru": {"title": "Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Parameter-Efficient
[01.01.2026 01:57] Using data from previous issue: {"categories": ["#benchmark", "#plp"], "emoji": "ğŸ”", "ru": {"title": "Ğ“Ñ€Ğ°Ñ„ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ĞºĞ¾Ğ´Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GraphLocator â€” Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾
[01.01.2026 01:57] Querying the API.
[01.01.2026 01:57] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility between identity and residual feature representations as a self-supervised skip signal. CosineGate measures semantic redundancy through the Cosine Incompatibility Ratio (CIR), defined as 1 - cos(x, F(x)), and uses Gumbel-Softmax relaxation to enable per-sample, per-block gating during training. A progressive FLOPs regularization term controls average compute usage without destabilizing optimization. On CIFAR-10, CosineGate spans the accuracy-efficiency Pareto frontier: an aggressive configuration achieves 89.9 percent accuracy with 24.1 percent FLOPs savings, a balanced configuration achieves 91.3 percent accuracy with 28.5 percent savings at epoch 160, and a conservative configuration reaches a peak of 93.2 percent accuracy with minimal compute reduction. These results match or exceed ResNet-20 (91.3 percent) while reducing computation, without auxiliary supervision, distillation, or task-specific heuristics. Our results demonstrate that simple geometric measures of feature incompatibility provide a principled and effective signal for dynamic residual routing.
[01.01.2026 01:57] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ CosineGate â€” Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ ĞºĞ¾Ğ³Ğ´Ğ° Ñ‚Ğ¾Ğ¶Ğ´ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Cosine Incompatibility Ratio (CIR), Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ÑƒÑ ĞºĞ°Ğº 1 - cos(x, F(x)), Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ»Ğ°ĞºÑĞ°Ñ†Ğ¸Ñ Gumbel-Softmax Ğ´Ğ»Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ FLOPs ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€ĞµÑ‚Ğ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ: ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 28.5% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ResNet-20.",
  "emoji": "âš¡",
  "title": "ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ğ¹ Ğ»Ğ¸ÑˆĞ½ĞµĞµ: ÑƒĞ¼Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞµÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ"
}
```
[01.01.2026 01:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility between identity and residual feature representations as a self-supervised skip signal. CosineGate measures semantic redundancy through the Cosine Incompatibility Ratio (CIR), defined as 1 - cos(x, F(x)), and uses Gumbel-Softmax relaxation to enable per-sample, per-block gating during training. A progressive FLOPs regularization term controls average compute usage without destabilizing optimization. On CIFAR-10, CosineGate spans the accuracy-efficiency Pareto frontier: an aggressive configuration achieves 89.9 percent accuracy with 24.1 percent FLOPs savings, a balanced configuration achieves 91.3 percent accuracy with 28.5 percent savings at epoch 160, and a conservative configuration reaches a peak of 93.2 percent accuracy with minimal compute reduction. These results match or exceed ResNet-20 (91.3 percent) while reducing computation, without auxiliary supervision, distillation, or task-specific heuristics. Our results demonstrate that simple geometric measures of feature incompatibility provide a principled and effective signal for dynamic residual routing."

[01.01.2026 01:57] Response: ```python
["ARCHITECTURE", "INFERENCE", "TRAINING"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes CosineGate, a novel neural architecture component for dynamic routing in residual networks with a new gating mechanism based on cosine incompatibility.

- **INFERENCE**: The work focuses on reducing computational cost (FLOPs) during model deployment, which is a core inference optimization concern.

- **TRAINING**: The paper discusses training methodologies including Gumbel-Softmax relaxation, progressive FLOPs regularization, and end-to-end differentiable training procedures.
[01.01.2026 01:57] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "INFERENCE", "TRAINING"]


**Justification:**

- **ARCHITECTURE**: The paper proposes CosineGate, a novel neural architecture component for dynamic routing in residual networks with a new gating mechanism based on cosine incompatibility.

- **INFERENCE**: The work focuses on reducing computational cost (FLOPs) during model deployment, which is a core inference optimization concern.

- **TRAINING**: The paper discusses training methodologies including Gumbel-Softmax relaxation, progressive FLOPs regularization, and end-to-end differentiable training procedures.
[01.01.2026 01:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility between identity and residual feature representations as a self-supervised skip signal. CosineGate measures semantic redundancy through the Cosine Incompatibility Ratio (CIR), defined as 1 - cos(x, F(x)), and uses Gumbel-Softmax relaxation to enable per-sample, per-block gating during training. A progressive FLOPs regularization term controls average compute usage without destabilizing optimization. On CIFAR-10, CosineGate spans the accuracy-efficiency Pareto frontier: an aggressive configuration achieves 89.9 percent accuracy with 24.1 percent FLOPs savings, a balanced configuration achieves 91.3 percent accuracy with 28.5 percent savings at epoch 160, and a conservative configuration reaches a peak of 93.2 percent accuracy with minimal compute reduction. These results match or exceed ResNet-20 (91.3 percent) while reducing computation, without auxiliary supervision, distillation, or task-specific heuristics. Our results demonstrate that simple geometric measures of feature incompatibility provide a principled and effective signal for dynamic residual routing."

[01.01.2026 01:57] Response: ```python
["OPTIMIZATION"]
```

The paper focuses on improving training efficiency and computational optimization in deep residual networks through dynamic routing and reduced FLOPs (floating point operations), which directly relates to optimization methods for neural networks.
[01.01.2026 01:57] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on improving training efficiency and computational optimization in deep residual networks through dynamic routing and reduced FLOPs (floating point operations), which directly relates to optimization methods for neural networks.
[01.01.2026 01:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents CosineGate, a novel approach for optimizing deep residual networks by reducing unnecessary computations. It introduces a self-supervised mechanism that uses cosine incompatibility to determine when to skip certain residual blocks, thereby improving efficiency. The method employs a Cosine Incompatibility Ratio (CIR) to assess redundancy and utilizes Gumbel-Softmax for dynamic gating during training. The results show that CosineGate can achieve high accuracy while significantly lowering computational costs, outperforming traditional models without requiring additional supervision or complex heuristics.","title":"Optimizing Residual Networks with CosineGate for Efficiency and Accuracy"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents CosineGate, a novel approach for optimizing deep residual networks by reducing unnecessary computations. It introduces a self-supervised mechanism that uses cosine incompatibility to determine when to skip certain residual blocks, thereby improving efficiency. The method employs a Cosine Incompatibility Ratio (CIR) to assess redundancy and utilizes Gumbel-Softmax for dynamic gating during training. The results show that CosineGate can achieve high accuracy while significantly lowering computational costs, outperforming traditional models without requiring additional supervision or complex heuristics.', title='Optimizing Residual Networks with CosineGate for Efficiency and Accuracy'))
[01.01.2026 01:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ç°ä»£æ·±åº¦æ®‹å·®ç½‘ç»œåœ¨å¤„ç†æ¯ä¸ªè¾“å…¥æ—¶ä¼šå¯¹æ‰€æœ‰æ®‹å·®å—è¿›è¡Œå†—ä½™è®¡ç®—ï¼Œå³ä½¿èº«ä»½æ˜ å°„å·²ç»è¶³å¤Ÿã€‚æˆ‘ä»¬æå‡ºäº†CosineGateï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ®‹å·®ç½‘ç»œåŠ¨æ€è·¯ç”±çš„ç«¯åˆ°ç«¯å¯å¾®æ¶æ„ï¼Œåˆ©ç”¨èº«ä»½å’Œæ®‹å·®ç‰¹å¾è¡¨ç¤ºä¹‹é—´çš„ä½™å¼¦ä¸å…¼å®¹æ€§ä½œä¸ºè‡ªç›‘ç£è·³è¿‡ä¿¡å·ã€‚CosineGateé€šè¿‡ä½™å¼¦ä¸å…¼å®¹æ¯”ç‡ï¼ˆCIRï¼‰æ¥è¡¡é‡è¯­ä¹‰å†—ä½™ï¼Œå¹¶ä½¿ç”¨Gumbel-Softmaxæ¾å¼›æŠ€æœ¯å®ç°è®­ç»ƒæœŸé—´çš„é€æ ·æœ¬ã€é€å—é—¨æ§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCosineGateåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®ç‡ã€‚","title":"åŠ¨æ€è·¯ç”±ï¼Œä¼˜åŒ–è®¡ç®—æ•ˆç‡çš„åˆ›æ–°"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ç°ä»£æ·±åº¦æ®‹å·®ç½‘ç»œåœ¨å¤„ç†æ¯ä¸ªè¾“å…¥æ—¶ä¼šå¯¹æ‰€æœ‰æ®‹å·®å—è¿›è¡Œå†—ä½™è®¡ç®—ï¼Œå³ä½¿èº«ä»½æ˜ å°„å·²ç»è¶³å¤Ÿã€‚æˆ‘ä»¬æå‡ºäº†CosineGateï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ®‹å·®ç½‘ç»œåŠ¨æ€è·¯ç”±çš„ç«¯åˆ°ç«¯å¯å¾®æ¶æ„ï¼Œåˆ©ç”¨èº«ä»½å’Œæ®‹å·®ç‰¹å¾è¡¨ç¤ºä¹‹é—´çš„ä½™å¼¦ä¸å…¼å®¹æ€§ä½œä¸ºè‡ªç›‘ç£è·³è¿‡ä¿¡å·ã€‚CosineGateé€šè¿‡ä½™å¼¦ä¸å…¼å®¹æ¯”ç‡ï¼ˆCIRï¼‰æ¥è¡¡é‡è¯­ä¹‰å†—ä½™ï¼Œå¹¶ä½¿ç”¨Gumbel-Softmaxæ¾å¼›æŠ€æœ¯å®ç°è®­ç»ƒæœŸé—´çš„é€æ ·æœ¬ã€é€å—é—¨æ§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCosineGateåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®ç‡ã€‚', title='åŠ¨æ€è·¯ç”±ï¼Œä¼˜åŒ–è®¡ç®—æ•ˆç‡çš„åˆ›æ–°'))
[01.01.2026 01:57] Using data from previous issue: {"categories": ["#interpretability", "#security", "#multimodal", "#architecture", "#alignment"], "emoji": "ğŸ”“", "ru": {"title": "ĞšĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ GateBreaker â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸
[01.01.2026 01:57] Renaming data file.
[01.01.2026 01:57] Renaming previous data. hf_papers.json to ./d/2026-01-01.json
[01.01.2026 01:57] Saving new data file.
[01.01.2026 01:57] Generating page.
[01.01.2026 01:57] Renaming previous page.
[01.01.2026 01:57] Renaming previous data. index.html to ./d/2026-01-01.html
[01.01.2026 01:57] Writing result.
[01.01.2026 01:57] Renaming log file.
[01.01.2026 01:57] Renaming previous data. log.txt to ./logs/2026-01-01_last_log.txt
