[16.01.2025 08:12] Read previous papers.
[16.01.2025 08:12] Generating top page (month).
[16.01.2025 08:12] Writing top page (month).
[16.01.2025 09:10] Read previous papers.
[16.01.2025 09:10] Get feed.
[16.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08828
[16.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08994
[16.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08983
[16.01.2025 09:10] Extract page data from URL. URL: https://huggingface.co/papers/2501.07783
[16.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.09012
[16.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.08809
[16.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.09019
[16.01.2025 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.01.2025 09:10] No deleted papers detected.
[16.01.2025 09:10] Downloading and parsing papers (pdf, html). Total: 7.
[16.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.08828.
[16.01.2025 09:10] Extra JSON file exists (./assets/json/2501.08828.json), skip PDF parsing.
[16.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.08828.json), skip HTML parsing.
[16.01.2025 09:10] Success.
[16.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.08994.
[16.01.2025 09:10] Extra JSON file exists (./assets/json/2501.08994.json), skip PDF parsing.
[16.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.08994.json), skip HTML parsing.
[16.01.2025 09:10] Success.
[16.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.08983.
[16.01.2025 09:10] Extra JSON file exists (./assets/json/2501.08983.json), skip PDF parsing.
[16.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.08983.json), skip HTML parsing.
[16.01.2025 09:10] Success.
[16.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.07783.
[16.01.2025 09:10] Downloading paper 2501.07783 from http://arxiv.org/pdf/2501.07783v1...
[16.01.2025 09:10] Extracting affiliations from text.
[16.01.2025 09:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai 1 5 2 0 2 4 ] . [ 1 3 8 7 7 0 . 1 0 5 2 : r AbstractImage pyramids are widely adopted in topperforming methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose novel crossbranch feature interaction mechanism. To validate PIIP, we apply it to various perception models and representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIPLLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP. Index TermsVision Foundation Models, Visual Perception, Object Detection, Multimodal Large Language Models, Multimodal Understanding I. "
[16.01.2025 09:10] Response: ```python
[]
```
[16.01.2025 09:10] Extracting affiliations from text.
[16.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai 1 5 2 0 2 4 ] . [ 1 3 8 7 7 0 . 1 0 5 2 : r AbstractImage pyramids are widely adopted in topperforming methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose novel crossbranch feature interaction mechanism. To validate PIIP, we apply it to various perception models and representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIPLLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP. Index TermsVision Foundation Models, Visual Perception, Object Detection, Multimodal Large Language Models, Multimodal Understanding I. INTRODUCTION In modern computer vision, advanced image perception and understanding systems heavily depend on large-scale pretrained models, which often require tens of thousands to millions of GPU hours for pretraining [2][5]. To adapt these costly pretrained models for vision perception (e.g., object detection [6][9] and segmentation [10][12]) tasks, the common practice is to combine them with image pyramids [13], [14], i.e. upsample and downsample the image to multiple scales and process them independently, then fuse their outputs. This Zhaokai Wang and Xue Yang are with Shanghai Jiao Tong University. Xizhou Zhu, Wenhan Dou, Junqi Ge, and Jifeng Dai are with Tsinghua University. Gen Luo and Yu Qiao are with Shanghai Artificial Intelligence Laboratory. Hao Li and Changyao Tian are with The Chinese University of Hong Kong. Lewei Lu is with Sensetime. Zhaokai Wang, Xue Yang, Hao Li, Changyao Tian and Jifeng Dai are also with Shanghai Artificial Intelligence Laboratory. Zhaokai Wang, Xizhou Zhu and Xue Yang contribute equally to this work. Corresponding author: Jifeng Dai (daijifeng@tsinghua.edu.cn). preliminary version of this research is published in NeurIPS 2024 [1] as Spotlight. combination is crucial in constructing multi-scale features for visual perception and understanding on high-resolution images. Nonetheless, employing image pyramids with pretrained models incurs substantial computational overhead. Current image pyramids use the same large-scale model to process multiple scales of the same image (Figure 1 (b)), leading to quadratic growth in computational demand as image resolution scales increase. When certain computation budget is imposed, the maximum image resolution is limited, which negatively impacts model performance in visual perception tasks. Although feature pyramids [15][17] aim to reduce this overhead in dense prediction tasks, most top-ranking models [18][21] in the MS COCO challenge [22] still rely on image pyramids due to their performance advantages. Therefore, it is necessary to reduce the computing resources needed to build image pyramids while maintaining high performance for visual perception. High computational costs are also challenge in multimodal understanding. In the field of multimodal large language models (MLLMs) [23][29], previous works have validate that scaling up the resolution of the input image helps to improve the visual understanding ability of MLLMs. Some multi-resolution approaches also adopt image pyramids [30][32] (Figure 1 (f)). Others adopt dynamic high resolution strategy, i.e. partition the image into multiple slices and pass through the vision encoder [33][35] (Figure 1 (g)). However, these methods use visual models with same parameter scale to process large scale and small scale inputs. This inevitably imposes great computational burden, limiting the performance of multimodal understanding under computation budget. Despite that the vision encoder only take up small fraction of the total FLOPs compared with the language model, its depth and input size significantly affect the inference speed, as examined in [25], [36]. Consequently, it is also essential to increase the input resolution while maintaining an acceptable computation cost for multimodal understanding. To address this issue, our primary insight is that it is unnecessary to utilize vision models of identical size across all resolutions (Figure 1(b-c)) or adopt parameter-direct design (Figure 1(d)). Instead, we can adopt parameterinverted design and allow features at different resolutions to complement each other through effective feature fusion. This approach improves computational efficiency and avoids redundant modeling of similar information. In lower-resolution pyramid levels, larger models can efficiently extract rich semantic and contextual features because of the smaller input image size. Conversely, high-resolution branches should 2 Fig. 1. Different multi-resolution designs in visual perception and multimodal understanding. (a)(e) Plain network without multi-scale features. (b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions. (d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost. (g) Multi-resolution approaches on multimodal tasks based on grid partition. (h) Our efficient and effective parameter-inverted "
[16.01.2025 09:10] Mistral response. {"id": "6527908c20f549e5b190a91fbf8d7cad", "object": "chat.completion", "created": 1737018650, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Shanghai Jiao Tong University\",\n    \"Tsinghua University\",\n    \"Shanghai Artificial Intelligence Laboratory\",\n    \"The Chinese University of Hong Kong\",\n    \"Sensetime\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1603, "total_tokens": 1662, "completion_tokens": 59}}
[16.01.2025 09:10] Response: ```python
[
    "Shanghai Jiao Tong University",
    "Tsinghua University",
    "Shanghai Artificial Intelligence Laboratory",
    "The Chinese University of Hong Kong",
    "Sensetime"
]
```
[16.01.2025 09:10] Deleting PDF ./assets/pdf/2501.07783.pdf.
[16.01.2025 09:10] Success.
[16.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.09012.
[16.01.2025 09:10] Extra JSON file exists (./assets/json/2501.09012.json), skip PDF parsing.
[16.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.09012.json), skip HTML parsing.
[16.01.2025 09:10] Success.
[16.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.08809.
[16.01.2025 09:10] Downloading paper 2501.08809 from http://arxiv.org/pdf/2501.08809v1...
[16.01.2025 09:10] Failed to download and parse paper https://huggingface.co/papers/2501.08809: 'LTChar' object is not iterable
[16.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.09019.
[16.01.2025 09:10] Extra JSON file exists (./assets/json/2501.09019.json), skip PDF parsing.
[16.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.09019.json), skip HTML parsing.
[16.01.2025 09:10] Success.
[16.01.2025 09:10] Enriching papers with extra data.
[16.01.2025 09:10] ********************************************************************************
[16.01.2025 09:10] Abstract 0. Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance ...
[16.01.2025 09:10] ********************************************************************************
[16.01.2025 09:10] Abstract 1. Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of repre...
[16.01.2025 09:10] ********************************************************************************
[16.01.2025 09:10] Abstract 2. 3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distort...
[16.01.2025 09:10] ********************************************************************************
[16.01.2025 09:10] Abstract 3. Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address ...
[16.01.2025 09:10] ********************************************************************************
[16.01.2025 09:10] Abstract 4. We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method...
[16.01.2025 09:10] ********************************************************************************
[16.01.2025 09:10] Abstract 5. In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard...
[16.01.2025 09:10] ********************************************************************************
[16.01.2025 09:10] Abstract 6. The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at...
[16.01.2025 09:10] Read previous papers.
[16.01.2025 09:10] Generating reviews via LLM API.
[16.01.2025 09:10] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "🔍", "ru": {"title": "MMDocIR: Новый стандарт для мультимодального поиска документов", "desc": "Статья представляет новый бенчмарк MMDocIR для оценки систем мультимодального поиска документов. Бенчмарк включает две задачи: поиск на у
[16.01.2025 09:10] Using data from previous issue: {"categories": ["#video", "#diffusion", "#architecture"], "emoji": "🎬", "ru": {"title": "RepVideo: стабильные представления для качественной генерации видео", "desc": "Статья представляет RepVideo - улучшенную систему представлений для диффузионных моделей генерации видео на основе текста. Авторы об
[16.01.2025 09:10] Using data from previous issue: {"categories": ["#3d", "#dataset"], "emoji": "🏙️", "ru": {"title": "Композиционная генерация 4D-городов с разделением динамики и статики", "desc": "CityDreamer4D - это генеративная модель для создания неограниченных 4D-городов. Она разделяет генерацию динамических объектов (например, транспорта) и с
[16.01.2025 09:10] Querying the API.
[16.01.2025 09:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP.
[16.01.2025 09:10] Response: {
  "desc": "Статья представляет новую архитектуру нейронных сетей под названием Parameter-Inverted Image Pyramid Networks (PIIP). PIIP использует предобученные модели (ViT или CNN) в качестве ветвей для обработки многомасштабных изображений, где изображения с более высоким разрешением обрабатываются меньшими сетевыми ветвями для баланса вычислительных затрат и производительности. Авторы также предлагают новый механизм взаимодействия признаков между ветвями. PIIP демонстрирует превосходную производительность по сравнению с одноветвенными и существующими многоразрешающими подходами при меньших вычислительных затратах в задачах обнаружения объектов, сегментации, классификации изображений и мультимодального понимания.",

  "emoji": "🔍",

  "title": "Эффективные многомасштабные сети для точного визуального восприятия"
}
[16.01.2025 09:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP."

[16.01.2025 09:10] Response: ```python
["CV", "ARCHITECTURE", "MULTIMODAL"]
```
[16.01.2025 09:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at https://github.com/OpenGVLab/PIIP."

[16.01.2025 09:11] Response: ```python
[]
```
[16.01.2025 09:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Parameter-Inverted Image Pyramid Networks (PIIP), a new architecture designed to efficiently process multi-scale images for visual tasks. Unlike traditional methods that use a single large model for all resolutions, PIIP employs smaller branches for higher resolution images, reducing computational costs while maintaining performance. The architecture also features a unique cross-branch interaction mechanism to enhance feature integration across different scales. Experimental results demonstrate that PIIP outperforms existing methods in various tasks, achieving significant accuracy improvements with lower resource usage.","title":"Efficient Multi-Scale Processing with PIIP Networks"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces Parameter-Inverted Image Pyramid Networks (PIIP), a new architecture designed to efficiently process multi-scale images for visual tasks. Unlike traditional methods that use a single large model for all resolutions, PIIP employs smaller branches for higher resolution images, reducing computational costs while maintaining performance. The architecture also features a unique cross-branch interaction mechanism to enhance feature integration across different scales. Experimental results demonstrate that PIIP outperforms existing methods in various tasks, achieving significant accuracy improvements with lower resource usage.', title='Efficient Multi-Scale Processing with PIIP Networks'))
[16.01.2025 09:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的网络架构，称为参数反转图像金字塔网络（PIIP），旨在提高多尺度图像处理的效率。PIIP利用预训练模型作为分支，处理不同分辨率的图像，从而在性能和计算成本之间取得平衡。通过引入跨分支特征交互机制，PIIP能够有效整合来自不同空间尺度的信息。实验结果表明，PIIP在目标检测、分割和多模态理解等任务上表现优于现有方法，同时显著降低了计算成本。","title":"高效多尺度图像处理的新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文提出了一种新的网络架构，称为参数反转图像金字塔网络（PIIP），旨在提高多尺度图像处理的效率。PIIP利用预训练模型作为分支，处理不同分辨率的图像，从而在性能和计算成本之间取得平衡。通过引入跨分支特征交互机制，PIIP能够有效整合来自不同空间尺度的信息。实验结果表明，PIIP在目标检测、分割和多模态理解等任务上表现优于现有方法，同时显著降低了计算成本。', title='高效多尺度图像处理的新方法'))
[16.01.2025 09:11] Using data from previous issue: {"categories": ["#artificial intelligence", "#reasoning", "#hallucinations", "#multimodal", "#benchmark", "#dataset"], "emoji": "🎨", "ru": {"title": "Искусственный интеллект учится оценивать искусство", "desc": "Исследование посвящено использованию мультимодальных языковых моделей (MLLM) для оценки 
[16.01.2025 09:11] Using data from previous issue: {"categories": ["#audio", "#story_generation", "#multimodal", "#dataset"], "emoji": "🎵", "ru": {"title": "XMusic: ИИ-композитор нового поколения с управляемыми эмоциями", "desc": "Статья представляет XMusic - генерализованный фреймворк для генерации символической музыки, поддерживающий различные тип
[16.01.2025 09:11] Using data from previous issue: {"categories": ["#benchmark", "#video", "#long_context", "#diffusion"], "emoji": "🐍", "ru": {"title": "Бесконечное видео: Ouroboros-Diffusion для непрерывной генерации согласованного контента", "desc": "Эта статья представляет новый метод генерации видео произвольной длины под названием Ouroboros-Di
[16.01.2025 09:11] Trying to get texts in Chinese.
[16.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.
[16.01.2025 09:11] Mistral response. {"id": "7b80f90a422d4c4bb0e9ef7160a45ae0", "object": "chat.completion", "created": 1737018663, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\uff0c\u65e8\u5728\u4ece\u5927\u91cf\u6587\u6863\u4e2d\u8bc6\u522b\u548c\u68c0\u7d22\u56fe\u8868\u3001\u8868\u683c\u3001\u56fe\u5f62\u548c\u5e03\u5c40\u4fe1\u606f\u3002\u5c3d\u7ba1\u5176\u91cd\u8981\u6027\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5MMDocIR\uff0c\u5305\u62ec\u9875\u9762\u7ea7\u548c\u5e03\u5c40\u7ea7\u68c0\u7d22\u4efb\u52a1\u3002MMDocIR\u5305\u542b1,685\u4e2a\u4e13\u5bb6\u6807\u6ce8\u548c173,843\u4e2a\u81ea\u52a8\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d44\u6e90\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u89c6\u89c9\u68c0\u7d22\u5668\u6bd4\u6587\u672c\u68c0\u7d22\u5668\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u4f7f\u7528VLM-text\u7684\u6587\u672c\u68c0\u7d22\u5668\u4f18\u4e8e\u4f7f\u7528OCR-text\u7684\u68c0\u7d22\u5668\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 354, "total_tokens": 552, "completion_tokens": 198}}
[16.01.2025 09:11] Response: 这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。
[16.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。
[16.01.2025 09:11] Mistral response. {"id": "bf657100600342c5801b7751b4e6021d", "object": "chat.completion", "created": 1737018668, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\uff0c\u65e8\u5728\u4ece\u5927\u91cf\u6587\u6863\u4e2d\u8bc6\u522b\u548c\u68c0\u7d22\u56fe\u8868\u3001\u8868\u683c\u3001\u56fe\u5f62\u548c\u5e03\u5c40\u4fe1\u606f\u3002\u5c3d\u7ba1\u5176\u91cd\u8981\u6027\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5MMDocIR\uff0c\u5305\u62ec\u9875\u9762\u7ea7\u548c\u5e03\u5c40\u7ea7\u68c0\u7d22\u4efb\u52a1\u3002MMDocIR\u5305\u542b1,685\u4e2a\u4e13\u5bb6\u6807\u6ce8\u548c173,843\u4e2a\u81ea\u52a8\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d44\u6e90\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u89c6\u89c9\u68c0\u7d22\u5668\u6bd4\u6587\u672c\u68c0\u7d22\u5668\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u4f7f\u7528VLM-text\u7684\u6587\u672c\u68c0\u7d22\u5668\u4f18\u4e8e\u4f7f\u7528OCR-text\u7684\u68c0\u7d22\u5668\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng ji\u00e8 sh\u00e0o le du\u014d m\u00f3 t\u00e0i w\u00e9n d\u00e0ng ji\u00e0n su\u01d2, zh\u01d0 z\u00e0i c\u00f3ng d\u00e0 li\u00e0ng w\u00e9n d\u00e0ng zh\u014dng sh\u00ed bi\u00e9 h\u00e9 ji\u00e0n su\u01d2 t\u00fa bi\u01ceo, bi\u01ceo g\u00e9, t\u00fa x\u00edng h\u00e9 b\u00f9 j\u00fa x\u00ecn x\u012b. J\u01d0n gu\u01cen q\u00ed zh\u00f2ng y\u00e0o x\u00ecng, qu\u0113 f\u00e1 y\u01d2u xi\u00e0o de j\u012b zh\u01d4n c\u00e8 sh\u00ec l\u00e1i p\u00edng gu\u0304 x\u00ec t\u01d2ng x\u00ecng n\u00e9ng. W\u00e8i c\u01d0, w\u00e9n zh\u0101ng t\u00ed ch\u016b le y\u012b g\u00e8 x\u012bn de j\u012b zh\u01d4n c\u00e8 sh\u00ec MMDocIR, b\u0101o ku\u00f2 y\u00e8 mi\u00e0n j\u00ed h\u00e9 b\u00f9 j\u00fa j\u00ed ji\u00e0n su\u01d2 r\u00e8n w\u00f9. MMDocIR b\u0101o h\u00e1n 1,685 g\u00e8 zhu\u0101n ji\u0101 bi\u0101o zh\u00f9 h\u00e9 173,843 g\u00e8 z\u00ec d\u00f2ng bi\u0101o zh\u00f9 de w\u00e8n t\u00ed, sh\u00ec y\u012b g\u00e8 zh\u00f2ng y\u00e0o de z\u012b yu\u00e1n. Sh\u00ed y\u00e0n xi\u01cen sh\u00ec, sh\u00ec ju\u00e9 ji\u00e0n su\u01d2 q\u00ec b\u01d0 w\u00e9n b\u011bn ji\u00e0n su\u01d2 q\u00ec bi\u01ceo xi\u00e0n g\u00e8ng h\u01ceo, b\u00ecng qi\u011b sh\u01d0 y\u00f2ng VLM-text de w\u00e9n b\u011bn ji\u00e0n su\u01d2 q\u00ec y\u014du y\u00fa sh\u01d0 y\u00f2ng OCR-text de ji\u00e0n su\u01d2 q\u00ec."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 213, "total_tokens": 822, "completion_tokens": 609}}
[16.01.2025 09:11] Response: 这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。

zhè piān wén zhāng jiè shào le duō mó tài wén dàng jiàn suǒ, zhǐ zài cóng dà liàng wén dàng zhōng shí bié hé jiàn suǒ tú biǎo, biǎo gé, tú xíng hé bù jú xìn xī. Jǐn guǎn qí zhòng yào xìng, quē fá yǒu xiào de jī zhǔn cè shì lái píng gū xì tǒng xìng néng. Wèi cǐ, wén zhāng tí chū le yī gè xīn de jī zhǔn cè shì MMDocIR, bāo kuò yè miàn jí hé bù jú jí jiàn suǒ rèn wù. MMDocIR bāo hán 1,685 gè zhuān jiā biāo zhù hé 173,843 gè zì dòng biāo zhù de wèn tí, shì yī gè zhòng yào de zī yuán. Shí yàn xiǎn shì, shì jué jiàn suǒ qì bǐ wén běn jiàn suǒ qì biǎo xiàn gèng hǎo, bìng qiě shǐ yòng VLM-text de wén běn jiàn suǒ qì yōu yú shǐ yòng OCR-text de jiàn suǒ qì.
[16.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。
[16.01.2025 09:11] Mistral response. {"id": "cc0e3fd301f0464eba7151ad38782ee8", "object": "chat.completion", "created": 1737018681, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u591a\u6a21\u6001', 'pinyin': 'du\u014d m\u00f3 t\u00e0i', 'trans': 'multimodal'},\n{'word': '\u68c0\u7d22', 'pinyin': 'ji\u01cen su\u01d2', 'trans': 'retrieval'},\n{'word': '\u65e8\u5728', 'pinyin': 'zh\u01d0 z\u00e0i', 'trans': 'aim to'},\n{'word': '\u8bc6\u522b', 'pinyin': 'sh\u00ed bi\u00e9', 'trans': 'recognize'},\n{'word': '\u5e03\u5c40', 'pinyin': 'b\u00f9 ji\u00fa', 'trans': 'layout'},\n{'word': '\u5c3d\u7ba1', 'pinyin': 'j\u00ecn gu\u01cen', 'trans': 'although'},\n{'word': '\u57fa\u51c6', 'pinyin': 'j\u012b zh\u01d4n', 'trans': 'benchmark'},\n{'word': '\u8bc4\u4f30', 'pinyin': 'p\u00edng g\u016b', 'trans': 'evaluate'},\n{'word': '\u7cfb\u7edf', 'pinyin': 'x\u00ec t\u01d2ng', 'trans': 'system'},\n{'word': '\u6027\u80fd', 'pinyin': 'x\u00ecng n\u00e9ng', 'trans': 'performance'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u9875\u9762\u7ea7', 'pinyin': 'y\u00e8 mi\u00e0n j\u00ed', 'trans': 'page-level'},\n{'word': '\u6807\u6ce8', 'pinyin': 'bi\u0101o zh\u00f9', 'trans': 'annotation'},\n{'word': '\u8d44\u6e90', 'pinyin': 'z\u012b yu\u00e1n', 'trans': 'resource'},\n{'word': '\u89c6\u89c9', 'pinyin': 'sh\u00ec ju\u00e9', 'trans': 'visual'},\n{'word': '\u6587\u672c', 'pinyin': 'w\u00e9n b\u011bn', 'trans': 'text'},\n{'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'perform'},\n{'word': '\u4f18\u4e8e', 'pinyin': 'y\u014du y\u00fa', 'trans': 'superior to'},\n{'word': '\u4f7f\u7528', 'pinyin': 'sh\u01d0 y\u00f2ng', 'trans': 'use'},\n{'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 245, "total_tokens": 792, "completion_tokens": 547}}
[16.01.2025 09:11] Response: [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'},
{'word': '检索', 'pinyin': 'jiǎn suǒ', 'trans': 'retrieval'},
{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},
{'word': '识别', 'pinyin': 'shí bié', 'trans': 'recognize'},
{'word': '布局', 'pinyin': 'bù jiú', 'trans': 'layout'},
{'word': '尽管', 'pinyin': 'jìn guǎn', 'trans': 'although'},
{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},
{'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'},
{'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'},
{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},
{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},
{'word': '页面级', 'pinyin': 'yè miàn jí', 'trans': 'page-level'},
{'word': '标注', 'pinyin': 'biāo zhù', 'trans': 'annotation'},
{'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resource'},
{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},
{'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'},
{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'},
{'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'},
{'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'},
{'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]
[16.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章介绍了多模态文档检索，旨在从大量文档中识别和检索图表、表格、图形和布局信息。尽管其重要性，缺乏有效的基准测试来评估系统性能。为此，文章提出了一个新的基准测试MMDocIR，包括页面级和布局级检索任务。MMDocIR包含1,685个专家标注和173,843个自动标注的问题，是一个重要的资源。实验显示，视觉检索器比文本检索器表现更好，并且使用VLM-text的文本检索器优于使用OCR-text的检索器。
[16.01.2025 09:11] Mistral response. {"id": "e0c2d8b6cd694bdd8acd8b1f956ccfa6", "object": "chat.completion", "created": 1737018692, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces multimodal document retrieval, which aims to identify and retrieve charts, tables, graphics, and layout information from a large number of documents. Despite its importance, there is a lack of effective benchmark tests to evaluate system performance. To address this, the article proposes a new benchmark test called MMDocIR, which includes page-level and layout-level retrieval tasks. MMDocIR contains 1,685 expert-annotated and 173,843 automatically annotated questions, making it a valuable resource. Experiments show that visual retrievers perform better than text retrievers, and text retrievers using VLM-text outperform those using OCR-text."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 211, "total_tokens": 366, "completion_tokens": 155}}
[16.01.2025 09:11] Response: This article introduces multimodal document retrieval, which aims to identify and retrieve charts, tables, graphics, and layout information from a large number of documents. Despite its importance, there is a lack of effective benchmark tests to evaluate system performance. To address this, the article proposes a new benchmark test called MMDocIR, which includes page-level and layout-level retrieval tasks. MMDocIR contains 1,685 expert-annotated and 173,843 automatically annotated questions, making it a valuable resource. Experiments show that visual retrievers perform better than text retrievers, and text retrievers using VLM-text outperform those using OCR-text.
[16.01.2025 09:11] Renaming data file.
[16.01.2025 09:11] Renaming previous data. hf_papers.json to ./d/2025-01-16.json
[16.01.2025 09:11] Saving new data file.
[16.01.2025 09:11] Generating page.
[16.01.2025 09:11] Renaming previous page.
[16.01.2025 09:11] Renaming previous data. index.html to ./d/2025-01-16.html
[16.01.2025 09:11] [Experimental] Generating Chinese page for reading.
[16.01.2025 09:11] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '检索', 'pinyin': 'jiǎn suǒ', 'trans': 'retrieval'}, {'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '识别', 'pinyin': 'shí bié', 'trans': 'recognize'}, {'word': '布局', 'pinyin': 'bù jiú', 'trans': 'layout'}, {'word': '尽管', 'pinyin': 'jìn guǎn', 'trans': 'although'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '页面级', 'pinyin': 'yè miàn jí', 'trans': 'page-level'}, {'word': '标注', 'pinyin': 'biāo zhù', 'trans': 'annotation'}, {'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resource'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': 'OCR', 'pinyin': '', 'trans': 'Optical Character Recognition'}]
[16.01.2025 09:11] Renaming previous Chinese page.
[16.01.2025 09:11] Renaming previous data. zh.html to ./d/2025-01-15_zh_reading_task.html
[16.01.2025 09:11] Writing Chinese reading task.
[16.01.2025 09:11] Writing result.
[16.01.2025 09:11] Renaming log file.
[16.01.2025 09:11] Renaming previous data. log.txt to ./logs/2025-01-16_last_log.txt
