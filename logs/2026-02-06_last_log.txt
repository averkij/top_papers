[06.02.2026 06:44] Read previous papers.
[06.02.2026 06:44] Generating top page (month).
[06.02.2026 06:44] Writing top page (month).
[06.02.2026 07:42] Read previous papers.
[06.02.2026 07:42] Get feed.
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05261
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06028
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05986
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05327
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04210
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05885
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05216
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21296
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06035
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21937
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06040
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05975
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05842
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03036
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06034
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04884
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01965
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05857
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05393
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05551
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05023
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04683
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23174
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05871
[06.02.2026 07:42] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00298
[06.02.2026 07:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.02.2026 07:42] No deleted papers detected.
[06.02.2026 07:42] Downloading and parsing papers (pdf, html). Total: 25.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05261.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05261.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05261.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.06028.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.06028.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.06028.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05986.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05986.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05986.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05327.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05327.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05327.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.04210.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.04210.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.04210.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05885.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05885.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05885.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05216.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05216.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05216.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2601.21296.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2601.21296.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2601.21296.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.06035.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.06035.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.06035.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2601.21937.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2601.21937.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2601.21937.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.06040.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.06040.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.06040.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05975.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05975.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05975.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05842.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05842.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05842.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.03036.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.03036.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.03036.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.06034.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.06034.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.06034.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.04884.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.04884.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.04884.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.01965.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.01965.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.01965.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05857.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05857.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05857.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05393.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05393.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05393.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05551.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05551.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05551.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05023.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05023.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05023.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.04683.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.04683.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.04683.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2601.23174.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2601.23174.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2601.23174.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.05871.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.05871.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.05871.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Downloading and parsing paper https://huggingface.co/papers/2602.00298.
[06.02.2026 07:42] Extra JSON file exists (./assets/json/2602.00298.json), skip PDF parsing.
[06.02.2026 07:42] Paper image links file exists (./assets/img_data/2602.00298.json), skip HTML parsing.
[06.02.2026 07:42] Success.
[06.02.2026 07:42] Enriching papers with extra data.
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 0. Research analyzes RLVR algorithms' impact on response length in LLMs and VLMs, proposing LUSPO to eliminate length bias and improve reasoning performance.  					AI-generated summary 				 Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and ...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 1. Context Forcing addresses student-teacher mismatch in long video generation by using a long-context teacher to guide long-rollout students through a Slow-Fast Memory architecture that extends context length beyond 20 seconds.  					AI-generated summary 				 Recent approaches to real-time long video ...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 2. RISE-Video presents a novel benchmark for evaluating text-image-to-video synthesis models based on cognitive reasoning rather than visual fidelity, using a multi-dimensional metric system and automated LMM-based evaluation.  					AI-generated summary 				 While generative video models have achieved ...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 3. ProAct enhances LLM agents' long-horizon planning by combining supervised fine-tuning with search-derived trajectories and a Monte-Carlo critic for improved policy optimization.  					AI-generated summary 				 Existing Large Language Model (LLM) agents struggle in interactive environments requiring ...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 4. Scalable Interactive Oversight framework decomposes complex tasks into manageable decision trees to enhance human supervision and alignment in AI systems.  					AI-generated summary 				 As Large Language Models increasingly automate complex, long-horizon tasks such as vibe coding, a supervision gap...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 5. Reinforcement learning approach for kernel generation addresses reward hacking and optimization issues through specialized environment and unbiased policy gradient methods, achieving competitive performance with state-of-the-art models.  					AI-generated summary 				 High-quality kernel is critical...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 6. Large-scale semantic theorem retrieval system demonstrates superior performance over existing baselines using a 9.2 million theorem corpus with systematic analysis of representation context, language model choice, and embedding strategies.  					AI-generated summary 				 Searching for mathematical r...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 7. Dataset distillation method that balances informativeness and utility through game-theoretic and gradient-based optimization techniques, achieving improved performance on ImageNet-1K.  					AI-generated summary 				 Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 8. A scalable framework called InterPrior learns a unified generative controller through imitation learning and reinforcement learning to enable humanoids to generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination.  					AI-generated sum...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 9. DeR2 presents a controlled evaluation framework for assessing language models' document-grounded reasoning capabilities by isolating reasoning from retrieval and toolchain decisions.  					AI-generated summary 				 Despite strong performance on existing benchmarks, it remains unclear whether large l...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 10. SwimBird is a reasoning-switchable multimodal large language model that dynamically selects between text-only, vision-only, and interleaved vision-text reasoning modes based on input queries, achieving superior performance on both textual and visual tasks.  					AI-generated summary 				 Multimodal ...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 11. LLM-based retrievers show limited effectiveness in deep research agent workflows, with traditional BM25 performing better, though corpus-level test-time scaling can improve retrieval performance.  					AI-generated summary 				 Deep research agents have emerged as powerful systems for addressing com...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 12. Reinforcement World Model Learning enables LLM-based agents to better anticipate action consequences and adapt to environment dynamics through self-supervised training that aligns simulated and real-world state transitions in embedding space.  					AI-generated summary 				 Large language models (LL...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 13. LatentMem is a learnable multi-agent memory framework that customizes agent-specific memories through latent representations, improving performance in multi-agent systems without modifying underlying frameworks.  					AI-generated summary 				 Large language model (LLM)-powered multi-agent systems (...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 14. V-Retrver introduces an evidence-driven retrieval framework that enables multimodal large language models to actively verify visual evidence through an agentic reasoning process, improving retrieval accuracy and reasoning reliability.  					AI-generated summary 				 Multimodal Large Language Models ...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 15. Reinforced Attention Learning optimizes internal attention distributions in multimodal language models, improving information allocation and cross-modal alignment through policy-gradient methods.  					AI-generated summary 				 Post-training with Reinforcement Learning (RL) has substantially improve...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 16. CatRAG addresses limitations in retrieval-augmented generation by introducing a query-adaptive framework that improves multi-hop reasoning through symbolic anchoring, dynamic edge weighting, and key-fact passage enhancement.  					AI-generated summary 				 Recent advances in Retrieval-Augmented Gene...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 17. BABE is a biology-focused benchmark designed to evaluate AI systems' ability to perform experimental reasoning and causal inference similar to practicing scientists.  					AI-generated summary 				 The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogu...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 18. Large language models can be trained more efficiently by transferring knowledge from later training phases to earlier layers during initial training, achieving faster convergence and improved performance.  					AI-generated summary 				 As Large Language Models (LLMs) achieve remarkable empirical su...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 19. FastVMT accelerates video motion transfer by addressing computational redundancies in Diffusion Transformer architecture through localized attention masking and gradient reuse optimization.  					AI-generated summary 				 Video motion transfer aims to synthesize videos by generating visual content a...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 20. Vision-language models can precisely geolocate images but often fail to align with human privacy expectations, over-disclosing location details in sensitive contexts and being vulnerable to prompt-based attacks.  					AI-generated summary 				 Vision-language models (VLMs) have demonstrated strong p...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 21. Researchers developed a discrete audio codec called ReasoningCodec that separates audio into reasoning and reconstruction tokens for improved understanding and generation, and created UniAudio 2.0, a unified autoregressive model trained on large-scale text and audio data that shows strong performanc...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 22. DyCAST is a dynamic speech tokenizer that uses soft character-level alignment and duration modeling to enable variable-frame-rate tokenization, improving speech resynthesis quality with fewer tokens than traditional fixed-frame-rate codecs.  					AI-generated summary 				 Neural audio codecs are at ...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 23. Test-Time Correction addresses error accumulation in distilled autoregressive diffusion models for long-video synthesis by using initial frames as reference anchors to calibrate stochastic states during sampling.  					AI-generated summary 				 Distilled autoregressive diffusion models facilitate re...
[06.02.2026 07:42] ********************************************************************************
[06.02.2026 07:42] Abstract 24. Large language models fine-tuned on insecure datasets exhibit increased misalignment rates across diverse domains, with varying vulnerability levels and potential for generalization of misalignment behaviors.  					AI-generated summary 				 Emergent misalignment poses risks to AI safety as language ...
[06.02.2026 07:42] Read previous papers.
[06.02.2026 07:42] Generating reviews via LLM API.
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#optimization", "#rlhf", "#training", "#multimodal"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏—è –ø–æ –¥–ª–∏–Ω–µ –æ—Ç–≤–µ—Ç–∞ –≤ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç, –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ 
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#training", "#long_context", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —É—á–∏—Ç–µ–ª—è –∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–†–∞–±–æ—Ç–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —É—á–∏—Ç–µ–ª–µ–º –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–º –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#dataset", "#interpretability", "#video", "#multimodal"], "emoji": "üß†", "ru": {"title": "–û—Ç –∫—Ä–∞—Å–æ—Ç—ã –∫–∞–¥—Ä–æ–≤ –∫ –ª–æ–≥–∏–∫–µ –º–∏—Ä–∞: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–∞—é—â–µ–≥–æ –≤–∏–¥–µ–æ—Å–∏–Ω—Ç–µ–∑–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω RISE-Video ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ 
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#open_source", "#rlhf", "#agents", "#training", "#games"], "emoji": "üéØ", "ru": {"title": "–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ —Ä–∞–∑—É–º: –æ–±—É—á–µ–Ω–∏–µ LLM –¥–æ–ª–≥–æ—Ä–µ—á–µ–≤–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –±–µ–∑ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "ProAct ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–æ–ª–≥–æ—Ä–µ—á–µ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#alignment", "#optimization"], "emoji": "üå≥", "ru": {"title": "–†–∞—Å–∫–ª–∞–¥—ã–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å —á–µ—Ä–µ–∑ –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Scalable Interactive Oversight –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#science", "#benchmark", "#dataset", "#open_source", "#rl", "#optimization", "#plp", "#training"], "emoji": "‚ö°", "ru": {"title": "–û—Ç –≤–∑–ª–æ–º–∞ –Ω–∞–≥—Ä–∞–¥ –∫ —á–µ—Å—Ç–Ω–æ–º—É RL: –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö GPU-—è–¥–µ—Ä", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª
[06.02.2026 07:42] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–µ–º –≤ –º–∞—Å—à—Ç–∞–±–µ –≤–µ–±-—Å–µ—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–µ–º –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –∏–∑ 9.2 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–µ–æ—Ä–µ–º, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏–∑ arXiv –∏ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#optimization", "#data", "#synthetic"], "emoji": "üéØ", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω–¥–µ–Ω—Å–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ç–µ–æ—Ä–∏—é –∏–≥—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç InfoUtil ‚Äî –º–µ—Ç–æ–¥ –∫–æ–Ω–¥–µ–Ω—Å–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –Ω–∞–±–æ—Ä 
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#robotics", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –≥—É–º–∞–Ω–æ–∏–¥–æ–≤", "desc": "InterPrior ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#leakage", "#reasoning", "#benchmark", "#dataset", "#science", "#rag", "#interpretability"], "emoji": "üî¨", "ru": {"title": "–û—Ç–¥–µ–ª—è–µ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ—Ç –ø–æ–∏—Å–∫–∞: –∫–∞–∫ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –¥—É–º–∞—é—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "DeR2 ‚Äî —ç—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#architecture", "#training", "#interpretability", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "SwimBird ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#science", "#agents", "#benchmark", "#dataset", "#reasoning", "#rag"], "emoji": "üîç", "ru": {"title": "–ö–æ–≥–¥–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ–±–µ–∂–¥–∞—é—Ç —Ä–∞–∑—É–º: –ø–æ—á–µ–º—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ LLM –≤ –Ω–∞—É—á–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å LLM-based –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –≤ —Ä–∞–º–∫–∞—Ö –∞
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#agents", "#rl", "#training"], "emoji": "üåç", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Reinforcement World Model Learning (RWML) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è —Å
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#agents", "#optimization", "#training", "#architecture"], "emoji": "üß†", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "LatentMem ‚Äî —ç—Ç–æ –æ–±—É—á–∞–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–π –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–∞–∂–¥
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#rl", "#interpretability", "#rag", "#benchmark", "#agents"], "emoji": "üîç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é", "desc": "V-Retrver ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#video", "#reasoning", "#rl", "#optimization", "#multimodal", "#training"], "emoji": "üëÅÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Reinforced Attention Learning (RAL), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#graphs", "#reasoning", "#benchmark", "#open_source", "#rag"], "emoji": "üß≠", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –≥—Ä–∞—Ñ–∞–º –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è CatRAG ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–π 
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#benchmark", "#science", "#reasoning", "#dataset"], "emoji": "üß¨", "ru": {"title": "–ù–∞—É—á–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∫ –æ—Ç–∫—Ä—ã—Ç–∏—è–º", "desc": "BABE ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI —Å–∏—Å—Ç–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –ø—Ä
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#optimization", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ü–µ—Ä–µ–¥–∞—á–∞ –∑–Ω–∞–Ω–∏–π –æ—Ç –ø–æ–∑–¥–Ω–∏—Ö —Ñ–∞–∑ –∫ —Ä–∞–Ω–Ω–∏–º: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Late-to-Early Training (LET), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#video", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–õ–æ–∫–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏—é —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ 
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#security", "#ethics", "#cv", "#multimodal"], "emoji": "üîí", "ru": {"title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å: –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏ –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#open_source", "#training", "#audio", "#multimodal"], "emoji": "üéµ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞—É–¥–∏–æ–º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ ReasoningCodec ‚Äî –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –∞—É–¥–∏–æ–∫–æ–¥–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∞—É–¥–∏–æ
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#audio", "#rag", "#training"], "emoji": "üéµ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä–µ—á–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "DyCAST ‚Äî —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—è–≥–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤ –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã
[06.02.2026 07:42] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–ö–æ—Ä—Ä–µ–∫—Ü–∏—è –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–ø–æ—Ä–Ω—ã–µ –∫–∞–¥—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Test-Time Correction –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –≤ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑
[06.02.2026 07:42] Using data from previous issue: {"categories": ["#alignment", "#security", "#open_source"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–°–∫—Ä—ã—Ç–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö LLM'–æ–≤: —Ä–∏—Å–∫–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –¥–æ–º–µ–Ω–∞–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ —Å–∫–æ–º–ø—Ä–æ–º–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω
[06.02.2026 07:42] Renaming data file.
[06.02.2026 07:42] Renaming previous data. hf_papers.json to ./d/2026-02-06.json
[06.02.2026 07:42] Saving new data file.
[06.02.2026 07:42] Generating page.
[06.02.2026 07:42] Renaming previous page.
[06.02.2026 07:42] Renaming previous data. index.html to ./d/2026-02-06.html
[06.02.2026 07:42] Writing result.
[06.02.2026 07:42] Renaming log file.
[06.02.2026 07:42] Renaming previous data. log.txt to ./logs/2026-02-06_last_log.txt
