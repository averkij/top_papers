[12.01.2026 05:32] Read previous papers.
[12.01.2026 05:32] Generating top page (month).
[12.01.2026 05:32] Writing top page (month).
[12.01.2026 06:39] Read previous papers.
[12.01.2026 06:39] Get feed.
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05432
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06021
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06002
[12.01.2026 06:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.03017
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05808
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05930
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05905
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05573
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04720
[12.01.2026 06:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.03319
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05966
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04786
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05848
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04888
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05637
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05503
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04823
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05870
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05851
[12.01.2026 06:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02760
[12.01.2026 06:39] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.01.2026 06:39] No deleted papers detected.
[12.01.2026 06:39] Downloading and parsing papers (pdf, html). Total: 20.
[12.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.05432.
[12.01.2026 06:39] Extra JSON file exists (./assets/json/2601.05432.json), skip PDF parsing.
[12.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.05432.json), skip HTML parsing.
[12.01.2026 06:39] Success.
[12.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.06021.
[12.01.2026 06:39] Extra JSON file exists (./assets/json/2601.06021.json), skip PDF parsing.
[12.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.06021.json), skip HTML parsing.
[12.01.2026 06:39] Success.
[12.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.06002.
[12.01.2026 06:39] Extra JSON file exists (./assets/json/2601.06002.json), skip PDF parsing.
[12.01.2026 06:39] Paper image links file exists (./assets/img_data/2601.06002.json), skip HTML parsing.
[12.01.2026 06:39] Success.
[12.01.2026 06:39] Downloading and parsing paper https://huggingface.co/papers/2601.03017.
[12.01.2026 06:39] Downloading paper 2601.03017 from https://arxiv.org/pdf/2601.03017v1...
[12.01.2026 06:40] Extracting affiliations from text.
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MMFormalizer: Multimodal Autoformalization in the Wild Jing Xiong1, Qi Han1, Yunta Hsieh2, Hui Shen2, Huajian Xin3, Chaofan Tao1, Chenyang Zhao6, Hengyuan Zhang1, Taiqiang Wu1, Zhen Zhang4, Haochen Wang1, Zhongwei Wan5, Lingpeng Kong1, Ngai Wong1 1The University of Hong Kong 4University of California, Santa Barbara 2University of Michigan, Ann Arbor 3University of Edinburgh 5Ohio State University 6University of California, Los Angeles 6 2 0 2 6 ] . [ 1 7 1 0 3 0 . 1 0 6 2 : r https://MMFormalizer.github.io/ https://huggingface.co/datasets/menik1126/PhyX-AF junexiong@connect.hku.hk "
[12.01.2026 06:40] Response: ```python
[
    "The University of Hong Kong",
    "University of Michigan, Ann Arbor",
    "University of Edinburgh",
    "University of California, Santa Barbara",
    "Ohio State University",
    "University of California, Los Angeles"
]
```
[12.01.2026 06:40] Deleting PDF ./assets/pdf/2601.03017.pdf.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05808.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05808.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05808.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05930.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05930.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05930.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05905.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05905.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05905.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05573.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05573.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05573.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.04720.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.04720.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.04720.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.03319.
[12.01.2026 06:40] Downloading paper 2601.03319 from https://arxiv.org/pdf/2601.03319v1...
[12.01.2026 06:40] Extracting affiliations from text.
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CaricatureGS: Exaggerating 3D Gaussian Splatting Faces with Gaussian Curvature Technion Israel Institute of Technology, Haifa, Israel 6 2 0 2 6 ] . [ 1 9 1 3 3 0 . 1 0 6 2 : r Figure 1. Photorealistic 3D caricature avatars produced by our method. "
[12.01.2026 06:40] Response: ```python
["Technion Israel Institute of Technology, Haifa, Israel"]
```
[12.01.2026 06:40] Deleting PDF ./assets/pdf/2601.03319.pdf.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05966.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05966.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05966.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.04786.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.04786.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.04786.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05848.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05848.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05848.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.04888.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.04888.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.04888.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05637.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05637.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05637.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05503.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05503.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05503.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.04823.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.04823.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.04823.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05870.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05870.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05870.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.05851.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.05851.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.05851.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Downloading and parsing paper https://huggingface.co/papers/2601.02760.
[12.01.2026 06:40] Extra JSON file exists (./assets/json/2601.02760.json), skip PDF parsing.
[12.01.2026 06:40] Paper image links file exists (./assets/img_data/2601.02760.json), skip HTML parsing.
[12.01.2026 06:40] Success.
[12.01.2026 06:40] Enriching papers with extra data.
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 0. Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.  					AI-generated summary 				 The image geolocalization task aims to predict the location where ...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 1. A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as a critical technique for e...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 2. Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.  					AI-generated summary 				 Large langu...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 3. MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  					AI-generated summary 				 Autoformalization, which translates natural language mathematics i...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 4. EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 Large language models (LLMs) are expected to be t...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 5. Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.  					AI-generated summary 				 Autonomous machine learning agents have revolutionized sci...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 6. Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.  					AI-generated summary 				 As Large Language Models (LLMs) are increasingly deployed in real-world se...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 7. Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.  					AI-generated summary 				 This work prese...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 8. The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.  					AI-generated summary 				 In this report, we introduce the Qwen3-VL-E...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 9. A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  					AI-generated summary 				 A photorealistic and controllable ...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 10. VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.  					AI-generated summary 				 Recent advances in v...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 11. AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.  					AI-generated summary 				 Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement l...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 12. Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.  					AI-generated summary 				 Recent advancements in video generation have enabled the development of ``world m...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 13. SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.  					AI-generated summary 				 Large language model (LLM)-based search agents have proven promising for...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 14. Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.  					AI-generated summary 				 As generative models become ubiquitous, there is a critica...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 15. Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.  					AI-generated summary 				 Search-augmented large language models (LLM...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 16. DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.  					AI-generated summary 				 Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 17. Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.  					AI-generated summary 				 Recent advances in Reinforcement Learning with Verifi...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 18. Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.  					AI-generated summary 				 Real-time multimodal auto-completion is essential...
[12.01.2026 06:40] ********************************************************************************
[12.01.2026 06:40] Abstract 19. A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.  					AI-generated summary 				 Monocular depth estimation aims to recover the depth informatio...
[12.01.2026 06:40] Read previous papers.
[12.01.2026 06:40] Generating reviews via LLM API.
[12.01.2026 06:40] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#agents", "#cv", "#dataset", "#multimodal", "#rl", "#open_source", "#reasoning"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ö–∞—Ä—Ç—ã –∫–∞–∫ –∫–æ–º–ø–∞—Å: –Ω–∞—É—á–∏–º AI –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–µ–±—è –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª
[12.01.2026 06:40] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#optimization", "#hallucinations", "#rl", "#training", "#open_source", "#reasoning"], "emoji": "üîó", "ru": {"title": "–ù–∞–≥—Ä–∞–¥—ã —Å –æ—Å–æ–∑–Ω–∞–Ω–∏–µ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥ Citation-aware Rubric Rewards (CaR
[12.01.2026 06:40] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#optimization", "#training"], "emoji": "üß¨", "ru": {"title": "–ú–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–µ–∫ –¥–ª–∏–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª–∏–Ω–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Ö–∞—Ä–∞
[12.01.2026 06:40] Querying the API.
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  					AI-generated summary 				 Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io
[12.01.2026 06:40] Response: ```json
{
  "desc": "MMFormalizer ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Å —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç—ë–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–µ–∫—É—Ä—Å–∏–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–≤—è–∑–∏ –∫–∞–∂–¥–æ–π –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ PhyX-AF —Å 115 –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤, –≤–∫–ª—é—á–∞—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –º–µ—Ö–∞–Ω–∏–∫—É, –∫–≤–∞–Ω—Ç–æ–≤—É—é –º–µ—Ö–∞–Ω–∏–∫—É –∏ —Ç–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏–∫—É. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM-–º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–µ, —Ö–æ—Ç—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π –æ–±–ª–∞—Å—Ç—å—é.",
  "emoji": "üî¨",
  "title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–∑–∏–∫–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏"
}
```
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  					AI-generated summary 				 Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io"

[12.01.2026 06:40] Response: ```python
["MULTIMODAL", "DATASET", "BENCHMARK", "MATH"]
```
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  					AI-generated summary 				 Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io"

[12.01.2026 06:40] Response: ```python
['REASONING', 'SCIENCE']
```
[12.01.2026 06:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MMFormalizer is a novel framework that combines visual perception with formal mathematical reasoning to enhance autoformalization across various physical domains. It addresses the challenges of translating natural language mathematics into formal statements by integrating adaptive grounding with real-world entities. The system constructs formal propositions from visual elements through recursive grounding and axiom composition, ensuring that each abstraction is supported by visual evidence. Evaluated on the PhyX-AF benchmark, MMFormalizer demonstrates superior performance in physical reasoning tasks, marking a significant advancement in multimodal autoformalization capabilities.","title":"Bridging Visual Perception and Mathematical Reasoning with MMFormalizer"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MMFormalizer is a novel framework that combines visual perception with formal mathematical reasoning to enhance autoformalization across various physical domains. It addresses the challenges of translating natural language mathematics into formal statements by integrating adaptive grounding with real-world entities. The system constructs formal propositions from visual elements through recursive grounding and axiom composition, ensuring that each abstraction is supported by visual evidence. Evaluated on the PhyX-AF benchmark, MMFormalizer demonstrates superior performance in physical reasoning tasks, marking a significant advancement in multimodal autoformalization capabilities.', title='Bridging Visual Perception and Mathematical Reasoning with MMFormalizer'))
[12.01.2026 06:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MMFormalizer ÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅËá™Âä®ÂΩ¢ÂºèÂåñÂ∑•ÂÖ∑ÔºåÂÆÉÂ∞ÜËßÜËßâÊÑüÁü•‰∏éÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜÁõ∏ÁªìÂêàÔºåÊîØÊåÅ‰ªéÁªèÂÖ∏ÂäõÂ≠¶Âà∞ÈáèÂ≠êÂäõÂ≠¶ÁöÑÂ§çÊùÇÁâ©ÁêÜÈ¢ÜÂüü„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈÄÇÂ∫îÊÄßÂü∫Á°Ä‰∏éÁé∞ÂÆû‰∏ñÁïåÁöÑÊï∞Â≠¶ÂíåÁâ©ÁêÜÂÆû‰ΩìÊï¥ÂêàÔºåË∂ÖË∂ä‰∫ÜÊñáÊú¨ÁöÑËá™Âä®ÂΩ¢ÂºèÂåñ„ÄÇMMFormalizer ÈÄöËøáÈÄíÂΩíÂü∫Á°ÄÂíåÂÖ¨ÁêÜÁªÑÂêàÔºå‰ªéÊÑüÁü•Âü∫Á°ÄÊûÑÂª∫ÂΩ¢ÂºèÂëΩÈ¢òÔºåÁ°Æ‰øùÊØè‰∏™ÊäΩË±°ÈÉΩÊúâËßÜËßâËØÅÊçÆÊîØÊåÅ„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåMMFormalizer Âú®Â§öÊ®°ÊÄÅËá™Âä®ÂΩ¢ÂºèÂåñ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®Áâ©ÁêÜÊé®ÁêÜÊñπÈù¢ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§ÑÁêÜÁªèÂÖ∏ÂäõÂ≠¶ÂíåÈáèÂ≠êÂäõÂ≠¶Á≠âÈ¢ÜÂüüÁöÑËÉΩÂäõ„ÄÇ","title":"Â§öÊ®°ÊÄÅËá™Âä®ÂΩ¢ÂºèÂåñÁöÑÊ°•Ê¢Å"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MMFormalizer ÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅËá™Âä®ÂΩ¢ÂºèÂåñÂ∑•ÂÖ∑ÔºåÂÆÉÂ∞ÜËßÜËßâÊÑüÁü•‰∏éÂΩ¢ÂºèÊï∞Â≠¶Êé®ÁêÜÁõ∏ÁªìÂêàÔºåÊîØÊåÅ‰ªéÁªèÂÖ∏ÂäõÂ≠¶Âà∞ÈáèÂ≠êÂäõÂ≠¶ÁöÑÂ§çÊùÇÁâ©ÁêÜÈ¢ÜÂüü„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈÄÇÂ∫îÊÄßÂü∫Á°Ä‰∏éÁé∞ÂÆû‰∏ñÁïåÁöÑÊï∞Â≠¶ÂíåÁâ©ÁêÜÂÆû‰ΩìÊï¥ÂêàÔºåË∂ÖË∂ä‰∫ÜÊñáÊú¨ÁöÑËá™Âä®ÂΩ¢ÂºèÂåñ„ÄÇMMFormalizer ÈÄöËøáÈÄíÂΩíÂü∫Á°ÄÂíåÂÖ¨ÁêÜÁªÑÂêàÔºå‰ªéÊÑüÁü•Âü∫Á°ÄÊûÑÂª∫ÂΩ¢ÂºèÂëΩÈ¢òÔºåÁ°Æ‰øùÊØè‰∏™ÊäΩË±°ÈÉΩÊúâËßÜËßâËØÅÊçÆÊîØÊåÅ„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåMMFormalizer Âú®Â§öÊ®°ÊÄÅËá™Âä®ÂΩ¢ÂºèÂåñ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®Áâ©ÁêÜÊé®ÁêÜÊñπÈù¢ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§ÑÁêÜÁªèÂÖ∏ÂäõÂ≠¶ÂíåÈáèÂ≠êÂäõÂ≠¶Á≠âÈ¢ÜÂüüÁöÑËÉΩÂäõ„ÄÇ', title='Â§öÊ®°ÊÄÅËá™Âä®ÂΩ¢ÂºèÂåñÁöÑÊ°•Ê¢Å'))
[12.01.2026 06:40] Using data from previous issue: {"categories": ["#rl", "#dataset", "#agents", "#training"], "emoji": "üèóÔ∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "EnvScaler ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ–∑–∞ –ø—Ä–æ–≥—Ä–∞–º–º. –°–∏—Å—Ç–µ–º–∞
[12.01.2026 06:40] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset", "#open_source", "#science"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø—Ä–µ–∂–¥–µ, —á–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø
[12.01.2026 06:40] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ —É–±–µ–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö—Ä—É–ø–∫–∏–µ —É–±–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–ª–æ—Ö–æ –∏–∑–º–µ—Ä—è—é—Ç—Å—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –≤—Ä–æ–¥–µ Self-Consistency. 
[12.01.2026 06:40] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset", "#3d", "#multimodal", "#synthetic", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º —Å–∏–º–º–µ—Ç—Ä–∏–∏", "desc": "Orient Anything V2 ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ
[12.01.2026 06:40] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#rag", "#open_source", "#training", "#architecture", "#multimodal"], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏ Qwen3-VL-Embedding –∏ Qwen3-VL-Reranker, –∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –ø–æ
[12.01.2026 06:40] Querying the API.
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  					AI-generated summary 				 A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.
[12.01.2026 06:40] Response: ```json
{
  "desc": "–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —à–∞—Ä–∂–µ–π –ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ 3D-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä–∏–≤–∏–∑–Ω—É –ì–∞—É—Å—Å–∞ –¥–ª—è –ø—Ä–µ—É–≤–µ–ª–∏—á–µ–Ω–∏—è —á–µ—Ä—Ç –ª–∏—Ü–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ç–µ—Ö–Ω–∏–∫—É 3D Gaussian Splatting –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é, –∏–∑–±–µ–≥–∞—è –ø—Ä–æ–±–ª–µ–º—ã —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π —Å–≥–ª–∞–∂–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–¥–Ω–æ–º—É –Ω–∞–±–æ—Ä—É –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –∫–∞–∫ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, —Ç–∞–∫ –∏ –∫–∞—Ä–∏–∫–∞—Ç—É—Ä–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –∞–≤–∞—Ç–∞—Ä–∞. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é –∫–∞—Ä–∏–∫–∞—Ç—É—Ä—ã –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.",
  "emoji": "ü§™",
  "title": "–§–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–∫–∞—Ä–∏–∫–∞—Ç—É—Ä—ã —á–µ—Ä–µ–∑ –≥–∞—É—Å—Å–æ–≤—ã —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—Ä–∏–≤–∏–∑–Ω–æ–π"
}
```
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  					AI-generated summary 				 A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars."

[12.01.2026 06:40] Response: ```python
["3D", "MULTIMODAL"]
```
[12.01.2026 06:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  					AI-generated summary 				 A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars."

[12.01.2026 06:40] Response: ```python
[]
```

This paper is about 3D computer graphics and caricaturization techniques using Gaussian Splatting. While it mentions "AI-generated summary" in the text, the paper itself focuses on geometric surface manipulation, 3D rendering, and avatar creation rather than machine learning model development or any of the specified topics. It does not address AGI, games, interpretability, reasoning, transfer learning, graphs, ethics, security, optimization, surveys, diffusion models, alignment, story generation, hallucinations, long context, synthetic data for training, translation, data leakage, open-source contributions, scientific applications of language models, or low-resource languages.
[12.01.2026 06:40] Error. Failed to parse JSON from LLM. []


This paper is about 3D computer graphics and caricaturization techniques using Gaussian Splatting. While it mentions "AI-generated summary" in the text, the paper itself focuses on geometric surface manipulation, 3D rendering, and avatar creation rather than machine learning model development or any of the specified topics. It does not address AGI, games, interpretability, reasoning, transfer learning, graphs, ethics, security, optimization, surveys, diffusion models, alignment, story generation, hallucinations, long context, synthetic data for training, translation, data leakage, open-source contributions, scientific applications of language models, or low-resource languages.
[12.01.2026 06:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework for creating photorealistic 3D caricatures of faces that allows for real-time control and deformation. It combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to enhance the realism and fidelity of the avatars. The method involves extracting a FLAME mesh from multiview sequences and applying a curvature-weighted Poisson equation to achieve exaggerated forms. A novel training approach that alternates between real and synthesized images enables the framework to effectively represent both realistic and exaggerated avatars, resulting in superior performance compared to previous methods.","title":"Real-Time Control of Photorealistic 3D Caricatures"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework for creating photorealistic 3D caricatures of faces that allows for real-time control and deformation. It combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to enhance the realism and fidelity of the avatars. The method involves extracting a FLAME mesh from multiview sequences and applying a curvature-weighted Poisson equation to achieve exaggerated forms. A novel training approach that alternates between real and synthesized images enables the framework to effectively represent both realistic and exaggerated avatars, resulting in superior performance compared to previous methods.', title='Real-Time Control of Photorealistic 3D Caricatures'))
[12.01.2026 06:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÈÄºÁúüÁöÑ3DÊº´ÁîªÂåñÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÂü∫‰∫éÈ´òÊñØÊõ≤ÁéáÁöÑË°®Èù¢Â§∏Âº†Âíå3DÈ´òÊñØÁÇπ‰∫ëÊäÄÊúØÔºåËÉΩÂ§üÂàõÂª∫ÂèØÊéßÁöÑÁúüÂÆûÂ§¥ÂÉè„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ΩøÁî®È´òÊñØÊõ≤ÁéáËøõË°åË°®Èù¢Â§∏Âº†Ôºå‰ΩÜÂú®‰∏éÁ∫πÁêÜÁªìÂêàÊó∂ÂèØËÉΩÂØºËá¥Ëøá‰∫éÂπ≥ÊªëÁöÑÊ∏≤ÊüìÊïàÊûú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖÈááÁî®‰∫Ü3DÈ´òÊñØÁÇπ‰∫ëÊäÄÊúØÔºåËÉΩÂ§üÁîüÊàêÁúüÂÆûÁöÑÂ§öËßÜËßíÂ§¥ÂÉè„ÄÇÈÄöËøá‰∫§Êõø‰ΩøÁî®ÁúüÂÆûÂíåÂêàÊàêÁöÑÁõëÁù£ËÆ≠ÁªÉÔºåËØ•Ê°ÜÊû∂ÊèêÈ´ò‰∫ÜÂ§¥ÂÉèÁöÑÁúüÂÆûÊÑüÔºåÊîØÊåÅÂ±ÄÈÉ®ÁºñËæëÔºåÂπ∂ÂÖÅËÆ∏ÂØπÊº´ÁîªÊïàÊûúÁöÑÂº∫Â∫¶ËøõË°åËøûÁª≠ÊéßÂà∂„ÄÇ","title":"ÈÄºÁúüÁöÑ3DÊº´ÁîªÂåñÂ§¥ÂÉèÁîüÊàê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÈÄºÁúüÁöÑ3DÊº´ÁîªÂåñÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÂü∫‰∫éÈ´òÊñØÊõ≤ÁéáÁöÑË°®Èù¢Â§∏Âº†Âíå3DÈ´òÊñØÁÇπ‰∫ëÊäÄÊúØÔºåËÉΩÂ§üÂàõÂª∫ÂèØÊéßÁöÑÁúüÂÆûÂ§¥ÂÉè„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ΩøÁî®È´òÊñØÊõ≤ÁéáËøõË°åË°®Èù¢Â§∏Âº†Ôºå‰ΩÜÂú®‰∏éÁ∫πÁêÜÁªìÂêàÊó∂ÂèØËÉΩÂØºËá¥Ëøá‰∫éÂπ≥ÊªëÁöÑÊ∏≤ÊüìÊïàÊûú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖÈááÁî®‰∫Ü3DÈ´òÊñØÁÇπ‰∫ëÊäÄÊúØÔºåËÉΩÂ§üÁîüÊàêÁúüÂÆûÁöÑÂ§öËßÜËßíÂ§¥ÂÉè„ÄÇÈÄöËøá‰∫§Êõø‰ΩøÁî®ÁúüÂÆûÂíåÂêàÊàêÁöÑÁõëÁù£ËÆ≠ÁªÉÔºåËØ•Ê°ÜÊû∂ÊèêÈ´ò‰∫ÜÂ§¥ÂÉèÁöÑÁúüÂÆûÊÑüÔºåÊîØÊåÅÂ±ÄÈÉ®ÁºñËæëÔºåÂπ∂ÂÖÅËÆ∏ÂØπÊº´ÁîªÊïàÊûúÁöÑÂº∫Â∫¶ËøõË°åËøûÁª≠ÊéßÂà∂„ÄÇ', title='ÈÄºÁúüÁöÑ3DÊº´ÁîªÂåñÂ§¥ÂÉèÁîüÊàê'))
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –≤–∏–¥–µ–æ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "VideoAR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—É—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –º–Ω–æ–≥
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#cv", "#agents", "#inference", "#rl"], "emoji": "üñºÔ∏è", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "AgentOCR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –∏—Å—Ç–æ—Ä–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#synthetic", "#video", "#training", "#open_source"], "emoji": "üé¨", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–∫–æ–Ω–æ–≤ —Ñ–∏–∑–∏–∫–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ª–æ–∂–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ Goal Force ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ —è–≤–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#open_source"], "emoji": "üîç", "ru": {"title": "–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "SmartSearch ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è LLM-based –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
[12.01.2026 06:41] Using data from previous issue: {"categories": [], "emoji": "üéÆ", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –∏–ª–ª—é–∑–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –µ–≥–æ –≥—Ä–∞–Ω–∏—Ü", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç–∏–∂–∏–º—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#hallucinations", "#optimization"], "emoji": "üîç", "ru": {"title": "–ö–æ–≥–¥–∞ –ø–æ–∏—Å–∫ –≤—Ä–µ–¥–∏—Ç: –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å —á—Ä–µ–∑–º–µ—Ä–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π –≤ –ø–æ–∏—Å–∫-–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –ø–æ–∏—Å–∫–æ–º, –∫–æ–≥–¥–∞ 
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Ä–∞–Ω–≥–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ DR-LoRA –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–Ω–≥–æ–≤ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ LoRA –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts –≤–æ –≤—Ä–µ–º—è f
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#optimization", "#math", "#rl", "#training", "#reasoning"], "emoji": "üåø", "ru": {"title": "–í–µ—Ç–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–∑–∫–æ–µ –º–µ—Å—Ç–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ IIB-LPO –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–ª–ª–∞–ø—Å–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal"], "emoji": "üí¨", "ru": {"title": "–£–º–Ω–æ–µ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∑–∞–¥–∞—á–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è (MAC), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ —á–∞—Ç
[12.01.2026 06:41] Using data from previous issue: {"categories": ["#small_models", "#3d", "#training", "#data", "#architecture", "#cv"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã —á–µ—Ä–µ–∑ –±–∞–ª–∞–Ω—Å –¥–∏–∑–∞–π–Ω–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π DINOv3 –≤ –∫–∞—á–µ
[12.01.2026 06:41] Renaming data file.
[12.01.2026 06:41] Renaming previous data. hf_papers.json to ./d/2026-01-12.json
[12.01.2026 06:41] Saving new data file.
[12.01.2026 06:41] Generating page.
[12.01.2026 06:41] Renaming previous page.
[12.01.2026 06:41] Renaming previous data. index.html to ./d/2026-01-12.html
[12.01.2026 06:41] Writing result.
[12.01.2026 06:41] Renaming log file.
[12.01.2026 06:41] Renaming previous data. log.txt to ./logs/2026-01-12_last_log.txt
