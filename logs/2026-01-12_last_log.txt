[12.01.2026 10:28] Read previous papers.
[12.01.2026 10:28] Generating top page (month).
[12.01.2026 10:28] Writing top page (month).
[12.01.2026 11:22] Read previous papers.
[12.01.2026 11:22] Get feed.
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05432
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03017
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06002
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03319
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06021
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05808
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05930
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05966
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04786
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05905
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04720
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05573
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04888
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05848
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05637
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05403
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04823
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04726
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02760
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05960
[12.01.2026 11:22] Extract page data from URL. URL: https://huggingface.co/papers/2601.05741
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05503
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04544
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05870
[12.01.2026 11:22] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05851
[12.01.2026 11:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.01.2026 11:22] No deleted papers detected.
[12.01.2026 11:22] Downloading and parsing papers (pdf, html). Total: 25.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05432.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05432.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05432.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.03017.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.03017.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.03017.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.06002.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.06002.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.06002.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.03319.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.03319.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.03319.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.06021.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.06021.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.06021.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05808.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05808.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05808.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05930.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05930.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05930.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05966.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05966.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05966.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.04786.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.04786.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.04786.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05905.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05905.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05905.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.04720.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.04720.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.04720.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05573.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05573.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05573.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.04888.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.04888.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.04888.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05848.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05848.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05848.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05637.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05637.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05637.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05403.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05403.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05403.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.04823.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.04823.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.04823.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.04726.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.04726.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.04726.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.02760.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.02760.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.02760.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05960.
[12.01.2026 11:22] Extra JSON file exists (./assets/json/2601.05960.json), skip PDF parsing.
[12.01.2026 11:22] Paper image links file exists (./assets/img_data/2601.05960.json), skip HTML parsing.
[12.01.2026 11:22] Success.
[12.01.2026 11:22] Downloading and parsing paper https://huggingface.co/papers/2601.05741.
[12.01.2026 11:22] Downloading paper 2601.05741 from https://arxiv.org/pdf/2601.05741v1...
[12.01.2026 11:27] Extracting affiliations from text.
[12.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers Guray Ozgur1,2, Eduarda Caldeira1,2, Tahar Chettaoui1,2, Jan Niklas Kolf1,2, Marco Huber1,2, Naser Damer1,2, Fadi Boutros1 1Fraunhofer IGD, Germany, 2TU Darmstadt, Germany 6 2 0 2 9 ] . [ 1 1 4 7 5 0 . 1 0 6 2 : r a "
[12.01.2026 11:27] Response: ```python
['Fraunhofer IGD', 'TU Darmstadt']
```
[12.01.2026 11:27] Deleting PDF ./assets/pdf/2601.05741.pdf.
[12.01.2026 11:27] Failed to download and parse paper https://huggingface.co/papers/2601.05741: Function execution timed out after 300 seconds.
[12.01.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2601.05503.
[12.01.2026 11:27] Extra JSON file exists (./assets/json/2601.05503.json), skip PDF parsing.
[12.01.2026 11:27] Paper image links file exists (./assets/img_data/2601.05503.json), skip HTML parsing.
[12.01.2026 11:27] Success.
[12.01.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2601.04544.
[12.01.2026 11:27] Extra JSON file exists (./assets/json/2601.04544.json), skip PDF parsing.
[12.01.2026 11:27] Paper image links file exists (./assets/img_data/2601.04544.json), skip HTML parsing.
[12.01.2026 11:27] Success.
[12.01.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2601.05870.
[12.01.2026 11:27] Extra JSON file exists (./assets/json/2601.05870.json), skip PDF parsing.
[12.01.2026 11:27] Paper image links file exists (./assets/img_data/2601.05870.json), skip HTML parsing.
[12.01.2026 11:27] Success.
[12.01.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2601.05851.
[12.01.2026 11:27] Extra JSON file exists (./assets/json/2601.05851.json), skip PDF parsing.
[12.01.2026 11:27] Paper image links file exists (./assets/img_data/2601.05851.json), skip HTML parsing.
[12.01.2026 11:27] Success.
[12.01.2026 11:27] Enriching papers with extra data.
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 0. Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.  					AI-generated summary 				 The image geolocalization task aims to predict the location where ...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 1. MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  					AI-generated summary 				 Autoformalization, which translates natural language mathematics i...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 2. Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.  					AI-generated summary 				 Large langu...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 3. A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  					AI-generated summary 				 A photorealistic and controllable ...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 4. A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as a critical technique for e...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 5. EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 Large language models (LLMs) are expected to be t...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 6. Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.  					AI-generated summary 				 Autonomous machine learning agents have revolutionized sci...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 7. VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.  					AI-generated summary 				 Recent advances in v...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 8. AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.  					AI-generated summary 				 Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement l...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 9. Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.  					AI-generated summary 				 As Large Language Models (LLMs) are increasingly deployed in real-world se...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 10. The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.  					AI-generated summary 				 In this report, we introduce the Qwen3-VL-E...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 11. Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.  					AI-generated summary 				 This work prese...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 12. SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.  					AI-generated summary 				 Large language model (LLM)-based search agents have proven promising for...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 13. Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.  					AI-generated summary 				 Recent advancements in video generation have enabled the development of ``world m...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 14. Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.  					AI-generated summary 				 As generative models become ubiquitous, there is a critica...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 15. A comprehensive benchmark evaluates behavioral biases in large language models for multilingual financial misinformation detection across diverse economic scenarios.  					AI-generated summary 				 Large language models (LLMs) have been widely applied across various domains of finance. Since their t...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 16. DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.  					AI-generated summary 				 Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 17. CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.  					AI-generated summary 				 Large language models (LLMs) are increasingly deployed as intelligent...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 18. A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.  					AI-generated summary 				 Monocular depth estimation aims to recover the depth informatio...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 19. A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.  					AI-generated summary 				 We propose a framework that amortizes the cost ...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 20. ViTNT-FIQA measures face image quality by analyzing patch embedding stability across Vision Transformer blocks with a single forward pass.  					AI-generated summary 				 Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit onl...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 21. Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.  					AI-generated summary 				 Search-augmented large language models (LLM...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 22. A multi-agent system router that uses dynamic agent onboarding and natural language reasoning chains to improve routing accuracy and reduce conflicts in enterprise applications.  					AI-generated summary 				 Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance int...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 23. Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.  					AI-generated summary 				 Recent advances in Reinforcement Learning with Verifi...
[12.01.2026 11:27] ********************************************************************************
[12.01.2026 11:27] Abstract 24. Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.  					AI-generated summary 				 Real-time multimodal auto-completion is essential...
[12.01.2026 11:27] Read previous papers.
[12.01.2026 11:27] Generating reviews via LLM API.
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#agents", "#cv", "#dataset", "#multimodal", "#rl", "#open_source", "#reasoning"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ö–∞—Ä—Ç—ã –∫–∞–∫ –∫–æ–º–ø–∞—Å: –Ω–∞—É—á–∏–º AI –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–µ–±—è –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#math", "#dataset", "#benchmark", "#reasoning", "#science", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–∑–∏–∫–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "MMFormalizer ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#optimization", "#training"], "emoji": "üß¨", "ru": {"title": "–ú–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–µ–∫ –¥–ª–∏–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª–∏–Ω–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Ö–∞—Ä–∞
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#multimodal", "#3d"], "emoji": "ü§™", "ru": {"title": "–§–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–∫–∞—Ä–∏–∫–∞—Ç—É—Ä—ã —á–µ—Ä–µ–∑ –≥–∞—É—Å—Å–æ–≤—ã —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—Ä–∏–≤–∏–∑–Ω–æ–π", "desc": "–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —à–∞—Ä–∂–µ–π –ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ 3D-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä–∏–≤–∏–∑–Ω—É –ì–∞—É—Å—Å–∞ –¥–ª—è –ø—Ä–µ—É–≤–µ–ª–∏—á–µ–Ω–∏—è —á–µ—Ä
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#optimization", "#hallucinations", "#rl", "#training", "#open_source", "#reasoning"], "emoji": "üîó", "ru": {"title": "–ù–∞–≥—Ä–∞–¥—ã —Å –æ—Å–æ–∑–Ω–∞–Ω–∏–µ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥ Citation-aware Rubric Rewards (CaR
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#rl", "#dataset", "#agents", "#training"], "emoji": "üèóÔ∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "EnvScaler ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ–∑–∞ –ø—Ä–æ–≥—Ä–∞–º–º. –°–∏—Å—Ç–µ–º–∞
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset", "#open_source", "#science"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø—Ä–µ–∂–¥–µ, —á–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –≤–∏–¥–µ–æ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "VideoAR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—É—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –º–Ω–æ–≥
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#cv", "#agents", "#inference", "#rl"], "emoji": "üñºÔ∏è", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "AgentOCR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –∏—Å—Ç–æ—Ä–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[12.01.2026 11:27] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ —É–±–µ–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö—Ä—É–ø–∫–∏–µ —É–±–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–ª–æ—Ö–æ –∏–∑–º–µ—Ä—è—é—Ç—Å—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –≤—Ä–æ–¥–µ Self-Consistency. 
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#rag", "#open_source", "#training", "#architecture", "#multimodal"], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏ Qwen3-VL-Embedding –∏ Qwen3-VL-Reranker, –∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –ø–æ
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset", "#3d", "#multimodal", "#synthetic", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º —Å–∏–º–º–µ—Ç—Ä–∏–∏", "desc": "Orient Anything V2 ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#open_source"], "emoji": "üîç", "ru": {"title": "–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "SmartSearch ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è LLM-based –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#synthetic", "#video", "#training", "#open_source"], "emoji": "üé¨", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–∫–æ–Ω–æ–≤ —Ñ–∏–∑–∏–∫–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ª–æ–∂–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ Goal Force ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ —è–≤–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä
[12.01.2026 11:27] Using data from previous issue: {"categories": [], "emoji": "üéÆ", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –∏–ª–ª—é–∑–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –µ–≥–æ –≥—Ä–∞–Ω–∏—Ü", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç–∏–∂–∏–º—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#multilingual", "#ethics", "#low_resource", "#benchmark", "#survey", "#dataset", "#open_source"], "emoji": "üí∞", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π LLM –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Ä–∞–Ω–≥–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ DR-LoRA –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–Ω–≥–æ–≤ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ LoRA –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts –≤–æ –≤—Ä–µ–º—è f
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#graphs"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞—Ñ —Å–æ–±—ã—Ç–∏–π –∫–∞–∫ –ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "CompassMem ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–∞–º—è—Ç–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–æ–±—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –æ–ø—ã—Ç –∞–≥–µ–Ω—Ç–∞ –≤ –≤–∏–¥–µ –≥—Ä–∞—Ñ–∞ —Å–æ–±—ã—Ç–∏–π —Å —è–≤–Ω—ã–º–∏ –ª–æ–≥
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#small_models", "#3d", "#training", "#data", "#architecture", "#cv"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã —á–µ—Ä–µ–∑ –±–∞–ª–∞–Ω—Å –¥–∏–∑–∞–π–Ω–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π DINOv3 –≤ –∫–∞—á–µ
[12.01.2026 11:27] Using data from previous issue: {"categories": [], "emoji": "üíæ", "ru": {"title": "–ò–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫—Ä–∏—Ç–∏–∫–∏ –≤ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è: —ç–∫–æ–Ω–æ–º–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–∏–∫–∏ –≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç
[12.01.2026 11:27] Querying the API.
[12.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ViTNT-FIQA measures face image quality by analyzing patch embedding stability across Vision Transformer blocks with a single forward pass.  					AI-generated summary 				 Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.
[12.01.2026 11:27] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ViTNT-FIQA ‚Äî –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª–∏—Ü –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–∞—Ç—á–µ–π —á–µ—Ä–µ–∑ —Å–ª–æ–∏ Vision Transformer. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–µ–π —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ –¥–µ–≥—Ä–∞–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–∞–æ—Ç–∏—á–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤—ã—á–∏—Å–ª—è–µ—Ç –µ–≤–∫–ª–∏–¥–æ–≤—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏—Ö –≤ –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–≥–æ –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –±–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤–æ—Å—å–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.",
  "emoji": "üé≠",
  "title": "–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü –≤ Vision Transformer"
}
```
[12.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ViTNT-FIQA measures face image quality by analyzing patch embedding stability across Vision Transformer blocks with a single forward pass.  					AI-generated summary 				 Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model."

[12.01.2026 11:27] Response: ```python
["CV", "BENCHMARK"]
```
[12.01.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ViTNT-FIQA measures face image quality by analyzing patch embedding stability across Vision Transformer blocks with a single forward pass.  					AI-generated summary 				 Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model."

[12.01.2026 11:27] Response: ```python
["INTERPRETABILITY", "SYNTHETIC"]
```

**Justification:**

1. **INTERPRETABILITY**: The paper analyzes model behavior by examining "patch embedding stability across Vision Transformer blocks" and studying "feature refinement trajectories." It investigates how intermediate representations evolve through transformer blocks, which is a form of model behavior analysis and interpretation.

2. **SYNTHETIC**: The paper explicitly mentions "empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels," indicating the use of synthetic data for training and validation purposes.
[12.01.2026 11:27] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "SYNTHETIC"]


**Justification:**

1. **INTERPRETABILITY**: The paper analyzes model behavior by examining "patch embedding stability across Vision Transformer blocks" and studying "feature refinement trajectories." It investigates how intermediate representations evolve through transformer blocks, which is a form of model behavior analysis and interpretation.

2. **SYNTHETIC**: The paper explicitly mentions "empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels," indicating the use of synthetic data for training and validation purposes.
[12.01.2026 11:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ViTNT-FIQA is a novel method for assessing the quality of face images by analyzing the stability of patch embeddings through Vision Transformer (ViT) blocks. Unlike traditional methods that rely on final-layer outputs or require multiple passes, this approach is training-free and only needs a single forward pass. It measures the consistency of feature refinement by calculating Euclidean distances between normalized patch embeddings from consecutive transformer blocks. The results show that high-quality images maintain stable trajectories, while lower-quality images exhibit erratic changes, making ViTNT-FIQA efficient and effective for face image quality assessment.","title":"Assessing Face Image Quality with Stability in Vision Transformers"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ViTNT-FIQA is a novel method for assessing the quality of face images by analyzing the stability of patch embeddings through Vision Transformer (ViT) blocks. Unlike traditional methods that rely on final-layer outputs or require multiple passes, this approach is training-free and only needs a single forward pass. It measures the consistency of feature refinement by calculating Euclidean distances between normalized patch embeddings from consecutive transformer blocks. The results show that high-quality images maintain stable trajectories, while lower-quality images exhibit erratic changes, making ViTNT-FIQA efficient and effective for face image quality assessment.', title='Assessing Face Image Quality with Stability in Vision Transformers'))
[12.01.2026 11:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ViTNT-FIQAÊòØ‰∏ÄÁßçÈù¢ÈÉ®ÂõæÂÉèË¥®ÈáèËØÑ‰º∞ÊñπÊ≥ïÔºåÈÄöËøáÂàÜÊûêËßÜËßâÂèòÊç¢Âô®ÔºàViTÔºâÂùó‰∏≠Ë°•‰∏ÅÂµåÂÖ•ÁöÑÁ®≥ÂÆöÊÄßÊù•Ë°°ÈáèÂõæÂÉèË¥®Èáè„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåViTNT-FIQA‰∏çÈúÄË¶ÅËÆ≠ÁªÉÔºå‰∏îÂè™ÈúÄ‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠Âç≥ÂèØÂÆåÊàêËØÑ‰º∞„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËÆ°ÁÆóÁõ∏ÈÇªÂèòÊç¢Âô®Âùó‰πãÈó¥L2ÂΩí‰∏ÄÂåñË°•‰∏ÅÂµåÂÖ•ÁöÑÊ¨ßÂá†ÈáåÂæóË∑ùÁ¶ªÔºåÁîüÊàêÂõæÂÉèÁ∫ßË¥®ÈáèËØÑÂàÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈ´òË¥®ÈáèÁöÑÈù¢ÈÉ®ÂõæÂÉèÂú®ÁâπÂæÅÊèêÁÇºËøáÁ®ã‰∏≠Ë°®Áé∞Âá∫Á®≥ÂÆöÁöÑËΩ®ËøπÔºåËÄåÂä£Ë¥®ÂõæÂÉèÂàôÊòæÁ§∫Âá∫‰∏çËßÑÂàôÁöÑÂèòÂåñ„ÄÇ","title":"ViTNT-FIQAÔºöÈ´òÊïàÁöÑÈù¢ÈÉ®ÂõæÂÉèË¥®ÈáèËØÑ‰º∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ViTNT-FIQAÊòØ‰∏ÄÁßçÈù¢ÈÉ®ÂõæÂÉèË¥®ÈáèËØÑ‰º∞ÊñπÊ≥ïÔºåÈÄöËøáÂàÜÊûêËßÜËßâÂèòÊç¢Âô®ÔºàViTÔºâÂùó‰∏≠Ë°•‰∏ÅÂµåÂÖ•ÁöÑÁ®≥ÂÆöÊÄßÊù•Ë°°ÈáèÂõæÂÉèË¥®Èáè„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåViTNT-FIQA‰∏çÈúÄË¶ÅËÆ≠ÁªÉÔºå‰∏îÂè™ÈúÄ‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠Âç≥ÂèØÂÆåÊàêËØÑ‰º∞„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËÆ°ÁÆóÁõ∏ÈÇªÂèòÊç¢Âô®Âùó‰πãÈó¥L2ÂΩí‰∏ÄÂåñË°•‰∏ÅÂµåÂÖ•ÁöÑÊ¨ßÂá†ÈáåÂæóË∑ùÁ¶ªÔºåÁîüÊàêÂõæÂÉèÁ∫ßË¥®ÈáèËØÑÂàÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈ´òË¥®ÈáèÁöÑÈù¢ÈÉ®ÂõæÂÉèÂú®ÁâπÂæÅÊèêÁÇºËøáÁ®ã‰∏≠Ë°®Áé∞Âá∫Á®≥ÂÆöÁöÑËΩ®ËøπÔºåËÄåÂä£Ë¥®ÂõæÂÉèÂàôÊòæÁ§∫Âá∫‰∏çËßÑÂàôÁöÑÂèòÂåñ„ÄÇ', title='ViTNT-FIQAÔºöÈ´òÊïàÁöÑÈù¢ÈÉ®ÂõæÂÉèË¥®ÈáèËØÑ‰º∞ÊñπÊ≥ï'))
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#hallucinations", "#optimization"], "emoji": "üîç", "ru": {"title": "–ö–æ–≥–¥–∞ –ø–æ–∏—Å–∫ –≤—Ä–µ–¥–∏—Ç: –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å —á—Ä–µ–∑–º–µ—Ä–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π –≤ –ø–æ–∏—Å–∫-–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –ø–æ–∏—Å–∫–æ–º, –∫–æ–≥–¥–∞ 
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#agents"], "emoji": "üö¶", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å –ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –¥–ª—è –≥–∏–±–∫–∏—Ö –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TCAR, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –º
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#optimization", "#math", "#rl", "#training", "#reasoning"], "emoji": "üåø", "ru": {"title": "–í–µ—Ç–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–∑–∫–æ–µ –º–µ—Å—Ç–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ IIB-LPO –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–ª–ª–∞–ø—Å–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à
[12.01.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal"], "emoji": "üí¨", "ru": {"title": "–£–º–Ω–æ–µ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∑–∞–¥–∞—á–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è (MAC), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ —á–∞—Ç
[12.01.2026 11:27] Renaming data file.
[12.01.2026 11:27] Renaming previous data. hf_papers.json to ./d/2026-01-12.json
[12.01.2026 11:27] Saving new data file.
[12.01.2026 11:27] Generating page.
[12.01.2026 11:27] Renaming previous page.
[12.01.2026 11:27] Renaming previous data. index.html to ./d/2026-01-12.html
[12.01.2026 11:27] Writing result.
[12.01.2026 11:27] Renaming log file.
[12.01.2026 11:27] Renaming previous data. log.txt to ./logs/2026-01-12_last_log.txt
