[12.01.2026 16:29] Read previous papers.
[12.01.2026 16:29] Generating top page (month).
[12.01.2026 16:29] Writing top page (month).
[12.01.2026 17:25] Read previous papers.
[12.01.2026 17:25] Get feed.
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05432
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03017
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03319
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06002
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06021
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05808
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05930
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04720
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04786
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05882
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05966
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05905
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05848
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05573
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05403
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04888
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02760
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05637
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04823
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04726
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04544
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05960
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05851
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05741
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05503
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05870
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05699
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05376
[12.01.2026 17:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04175
[12.01.2026 17:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.01.2026 17:25] No deleted papers detected.
[12.01.2026 17:25] Downloading and parsing papers (pdf, html). Total: 29.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05432.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05432.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05432.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.03017.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.03017.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.03017.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.03319.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.03319.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.03319.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.06002.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.06002.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.06002.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.06021.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.06021.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.06021.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05808.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05808.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05808.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05930.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05930.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05930.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.04720.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.04720.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.04720.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.04786.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.04786.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.04786.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05882.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05882.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05882.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05966.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05966.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05966.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05905.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05905.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05905.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05848.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05848.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05848.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05573.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05573.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05573.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05403.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05403.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05403.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.04888.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.04888.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.04888.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.02760.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.02760.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.02760.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05637.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05637.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05637.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.04823.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.04823.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.04823.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.04726.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.04726.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.04726.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.04544.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.04544.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.04544.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05960.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05960.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05960.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05851.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05851.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05851.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05741.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05741.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05741.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05503.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05503.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05503.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05870.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05870.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05870.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05699.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05699.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05699.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.05376.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.05376.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.05376.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Downloading and parsing paper https://huggingface.co/papers/2601.04175.
[12.01.2026 17:25] Extra JSON file exists (./assets/json/2601.04175.json), skip PDF parsing.
[12.01.2026 17:25] Paper image links file exists (./assets/img_data/2601.04175.json), skip HTML parsing.
[12.01.2026 17:25] Success.
[12.01.2026 17:25] Enriching papers with extra data.
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 0. Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.  					AI-generated summary 				 The image geolocalization task aims to predict the location where ...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 1. MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.  					AI-generated summary 				 Autoformalization, which translates natural language mathematics i...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 2. A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.  					AI-generated summary 				 A photorealistic and controllable ...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 3. Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.  					AI-generated summary 				 Large langu...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 4. A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as a critical technique for e...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 5. EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.  					AI-generated summary 				 Large language models (LLMs) are expected to be t...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 6. Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.  					AI-generated summary 				 Autonomous machine learning agents have revolutionized sci...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 7. The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.  					AI-generated summary 				 In this report, we introduce the Qwen3-VL-E...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 8. AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.  					AI-generated summary 				 Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement l...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 9. Preference tuning of language models shows varying generalization capabilities under domain shift, with pseudo-labeling adaptation strategies effectively reducing performance degradation in summarization and question-answering tasks.  					AI-generated summary 				 Preference tuning aligns pretraine...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 10. VideoAR presents a large-scale visual autoregressive framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling, achieving state-of-the-art results with improved efficiency and temporal consistency.  					AI-generated summary 				 Recent advances in v...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 11. Large language models exhibit brittle beliefs under contextual perturbations, which are better measured by structural consistency metrics and addressed through structure-aware training methods.  					AI-generated summary 				 As Large Language Models (LLMs) are increasingly deployed in real-world se...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 12. Video generation models trained on synthetic physics primitives demonstrate zero-shot generalization to complex real-world scenarios by modeling force propagation through time and space.  					AI-generated summary 				 Recent advancements in video generation have enabled the development of ``world m...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 13. Orient Anything V2 enhances 3D orientation understanding through scalable 3D asset synthesis, symmetry-aware periodic distribution fitting, and multi-frame relative rotation prediction, achieving state-of-the-art performance across multiple benchmarks.  					AI-generated summary 				 This work prese...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 14. A comprehensive benchmark evaluates behavioral biases in large language models for multilingual financial misinformation detection across diverse economic scenarios.  					AI-generated summary 				 Large language models (LLMs) have been widely applied across various domains of finance. Since their t...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 15. SmartSearch enhances LLM-based search agents through process rewards and query refinement mechanisms that improve intermediate search query quality via a three-stage curriculum learning approach.  					AI-generated summary 				 Large language model (LLM)-based search agents have proven promising for...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 16. A lightweight monocular depth estimation framework uses DINOv3 as visual encoder and a compact transformer decoder to achieve higher accuracy with reduced computational overhead and improved data quality.  					AI-generated summary 				 Monocular depth estimation aims to recover the depth informatio...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 17. Generative models' controllability is theoretically analyzed through a framework that estimates controllable sets with distribution-free bounds, revealing that controllability is fragile and context-dependent.  					AI-generated summary 				 As generative models become ubiquitous, there is a critica...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 18. DR-LoRA dynamically adjusts LoRA ranks for experts in Mixture-of-Experts models based on task-specific demands, improving parameter efficiency and performance.  					AI-generated summary 				 Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 19. CompassMem is an event-centric memory framework that organizes experiences into an Event Graph to enable structured memory navigation and long-horizon reasoning beyond traditional retrieval methods.  					AI-generated summary 				 Large language models (LLMs) are increasingly deployed as intelligent...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 20. A multi-agent system router that uses dynamic agent onboarding and natural language reasoning chains to improve routing accuracy and reduce conflicts in enterprise applications.  					AI-generated summary 				 Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance int...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 21. A framework converts transient critiques into retrievable guidelines using a file-based memory system and agent-controlled tool calls, enabling LLMs to match test-time refinement performance with reduced inference costs.  					AI-generated summary 				 We propose a framework that amortizes the cost ...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 22. Multimodal auto-completion leverages visual and textual context to improve real-time prediction accuracy in conversational interfaces, with a router framework enabling efficient model selection based on dialog context.  					AI-generated summary 				 Real-time multimodal auto-completion is essential...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 23. ViTNT-FIQA measures face image quality by analyzing patch embedding stability across Vision Transformer blocks with a single forward pass.  					AI-generated summary 				 Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit onl...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 24. Search-augmented large language models suffer from over-searching behavior that wastes computational resources and introduces hallucinations, with findings showing varied impacts across model types and conversation contexts.  					AI-generated summary 				 Search-augmented large language models (LLM...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 25. Latent Policy Optimization via Iterative Information Bottleneck addresses exploration collapse in LLM reasoning by enabling topological branching of reasoning trajectories through information bottleneck principles.  					AI-generated summary 				 Recent advances in Reinforcement Learning with Verifi...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 26. Afri-MCQA benchmark demonstrates poor performance of open-weight LLMs in African languages, highlighting the need for culturally grounded pretraining and speech-first approaches in AI development.  					AI-generated summary 				 Africa is home to over one-third of the world's languages, yet remains ...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 27. Persona conditioning in clinical language models produces context-dependent effects on performance and safety that vary systematically with professional role and interaction style, challenging assumptions of monotonic improvement in expert behavior.  					AI-generated summary 				 Persona conditioni...
[12.01.2026 17:25] ********************************************************************************
[12.01.2026 17:25] Abstract 28. Legal alignment explores how legal principles and methods can guide AI system design to ensure safety, ethics, and compliance through three research directions involving rule adherence, legal reasoning methods, and structural blueprints for AI reliability and trust.  					AI-generated summary 				 A...
[12.01.2026 17:25] Read previous papers.
[12.01.2026 17:25] Generating reviews via LLM API.
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#agents", "#cv", "#dataset", "#multimodal", "#rl", "#open_source", "#reasoning"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ö–∞—Ä—Ç—ã –∫–∞–∫ –∫–æ–º–ø–∞—Å: –Ω–∞—É—á–∏–º AI –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–µ–±—è –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#math", "#dataset", "#benchmark", "#reasoning", "#science", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–∑–∏–∫–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "MMFormalizer ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#multimodal", "#3d"], "emoji": "ü§™", "ru": {"title": "–§–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–∫–∞—Ä–∏–∫–∞—Ç—É—Ä—ã —á–µ—Ä–µ–∑ –≥–∞—É—Å—Å–æ–≤—ã —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—Ä–∏–≤–∏–∑–Ω–æ–π", "desc": "–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —à–∞—Ä–∂–µ–π –ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ 3D-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä–∏–≤–∏–∑–Ω—É –ì–∞—É—Å—Å–∞ –¥–ª—è –ø—Ä–µ—É–≤–µ–ª–∏—á–µ–Ω–∏—è —á–µ—Ä
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#optimization", "#training"], "emoji": "üß¨", "ru": {"title": "–ú–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–µ–∫ –¥–ª–∏–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª–∏–Ω–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Ö–∞—Ä–∞
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#optimization", "#hallucinations", "#rl", "#training", "#open_source", "#reasoning"], "emoji": "üîó", "ru": {"title": "–ù–∞–≥—Ä–∞–¥—ã —Å –æ—Å–æ–∑–Ω–∞–Ω–∏–µ–º —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥ Citation-aware Rubric Rewards (CaR
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#rl", "#dataset", "#agents", "#training"], "emoji": "üèóÔ∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "EnvScaler ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ–∑–∞ –ø—Ä–æ–≥—Ä–∞–º–º. –°–∏—Å—Ç–µ–º–∞
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#dataset", "#open_source", "#science"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø—Ä–µ–∂–¥–µ, —á–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#rag", "#open_source", "#training", "#architecture", "#multimodal"], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏ Qwen3-VL-Embedding –∏ Qwen3-VL-Reranker, –∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –ø–æ
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#cv", "#agents", "#inference", "#rl"], "emoji": "üñºÔ∏è", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "AgentOCR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –∏—Å—Ç–æ—Ä–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –≤–∏–∑—É–∞–ª—å
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#alignment", "#training", "#rlhf", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å–º–µ–Ω–µ –æ–±–ª–∞—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Å–µ–≤–¥–æ—Ä–∞–∑–º–µ—Ç–∫—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ–≥–¥–∞ –ø–µ—Ä–µ—Ö
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –≤–∏–¥–µ–æ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "VideoAR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—É—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –º–Ω–æ–≥
[12.01.2026 17:25] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ —É–±–µ–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö—Ä—É–ø–∫–∏–µ —É–±–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–ª–æ—Ö–æ –∏–∑–º–µ—Ä—è—é—Ç—Å—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –≤—Ä–æ–¥–µ Self-Consistency. 
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#synthetic", "#video", "#training", "#open_source"], "emoji": "üé¨", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–∫–æ–Ω–æ–≤ —Ñ–∏–∑–∏–∫–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ª–æ–∂–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ Goal Force ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ —è–≤–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#dataset", "#3d", "#multimodal", "#synthetic", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º —Å–∏–º–º–µ—Ç—Ä–∏–∏", "desc": "Orient Anything V2 ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#multilingual", "#ethics", "#low_resource", "#benchmark", "#survey", "#dataset", "#open_source"], "emoji": "üí∞", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π LLM –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#open_source"], "emoji": "üîç", "ru": {"title": "–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "SmartSearch ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è LLM-based –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#small_models", "#3d", "#training", "#data", "#architecture", "#cv"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã —á–µ—Ä–µ–∑ –±–∞–ª–∞–Ω—Å –¥–∏–∑–∞–π–Ω–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π DINOv3 –≤ –∫–∞—á–µ
[12.01.2026 17:25] Using data from previous issue: {"categories": [], "emoji": "üéÆ", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –∏–ª–ª—é–∑–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –µ–≥–æ –≥—Ä–∞–Ω–∏—Ü", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç–∏–∂–∏–º—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Ä–∞–Ω–≥–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ DR-LoRA –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–Ω–≥–æ–≤ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ LoRA –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts –≤–æ –≤—Ä–µ–º—è f
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#graphs"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞—Ñ —Å–æ–±—ã—Ç–∏–π –∫–∞–∫ –ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "CompassMem ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–∞–º—è—Ç–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–æ–±—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –æ–ø—ã—Ç –∞–≥–µ–Ω—Ç–∞ –≤ –≤–∏–¥–µ –≥—Ä–∞—Ñ–∞ —Å–æ–±—ã—Ç–∏–π —Å —è–≤–Ω—ã–º–∏ –ª–æ–≥
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#agents"], "emoji": "üö¶", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å –ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –¥–ª—è –≥–∏–±–∫–∏—Ö –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TCAR, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –º
[12.01.2026 17:25] Using data from previous issue: {"categories": [], "emoji": "üíæ", "ru": {"title": "–ò–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫—Ä–∏—Ç–∏–∫–∏ –≤ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è: —ç–∫–æ–Ω–æ–º–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–∏–∫–∏ –≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal"], "emoji": "üí¨", "ru": {"title": "–£–º–Ω–æ–µ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∑–∞–¥–∞—á–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è (MAC), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ —á–∞—Ç
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#cv", "#benchmark"], "emoji": "üé≠", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü –≤ Vision Transformer", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ViTNT-FIQA ‚Äî –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª–∏—Ü –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–∞—Ç—á–µ–π —á–µ—Ä–µ–∑ —Å
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#hallucinations", "#optimization"], "emoji": "üîç", "ru": {"title": "–ö–æ–≥–¥–∞ –ø–æ–∏—Å–∫ –≤—Ä–µ–¥–∏—Ç: –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å —á—Ä–µ–∑–º–µ—Ä–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π –≤ –ø–æ–∏—Å–∫-–¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –ø–æ–∏—Å–∫–æ–º, –∫–æ–≥–¥–∞ 
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#optimization", "#math", "#rl", "#training", "#reasoning"], "emoji": "üåø", "ru": {"title": "–í–µ—Ç–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–∑–∫–æ–µ –º–µ—Å—Ç–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ IIB-LPO –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–ª–ª–∞–ø—Å–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#open_source", "#audio", "#low_resource", "#multimodal", "#benchmark", "#multilingual", "#dataset"], "emoji": "üåç", "ru": {"title": "–ê—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ —è–∑—ã–∫–∏ —Ç—Ä–µ–±—É—é—Ç –∫—É–ª—å—Ç—É—Ä–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Afri-MCQA ‚Äî –ø–µ—Ä–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–ø
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#interpretability", "#ethics", "#alignment"], "emoji": "‚öïÔ∏è", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω—ã –≤—Ä–∞—á–µ–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: —É–ª—É—á—à–µ–Ω–∏–µ –∏–ª–∏ –∏–ª–ª—é–∑–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏?", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –ø–µ—Ä—Å–æ–Ω—ã (—Ä–æ–ª–∏ –≤—Ä–∞—á–∞, –º–µ–¥—Å–µ—Å—Ç—Ä—ã) –Ω
[12.01.2026 17:25] Using data from previous issue: {"categories": ["#ethics", "#alignment"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ü—Ä–∞–≤–æ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ: –æ—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –∫ –Ω–∞–¥–µ–∂–Ω—ã–º AI —Å–∏—Å—Ç–µ–º–∞–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∞–≤–æ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ (legal alignment) ‚Äî –Ω–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ AI, –∫–æ—Ç–æ—Ä–æ–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏ –º–µ—Ç–æ–¥—ã –¥–ª—è 
[12.01.2026 17:25] Renaming data file.
[12.01.2026 17:25] Renaming previous data. hf_papers.json to ./d/2026-01-12.json
[12.01.2026 17:25] Saving new data file.
[12.01.2026 17:25] Generating page.
[12.01.2026 17:25] Renaming previous page.
[12.01.2026 17:25] Renaming previous data. index.html to ./d/2026-01-12.html
[12.01.2026 17:25] Writing result.
[12.01.2026 17:25] Renaming log file.
[12.01.2026 17:25] Renaming previous data. log.txt to ./logs/2026-01-12_last_log.txt
