[26.10.2024 08:14] [Experimental] Generating an image for paper Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss.
[26.10.2024 08:14] [Experimental] Image for paper Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss already exists.
[26.10.2024 08:14] [Experimental] Generating an image for paper Can Knowledge Editing Really Correct Hallucinations?.
[26.10.2024 08:14] [Experimental] Image for paper Can Knowledge Editing Really Correct Hallucinations? already exists.
[26.10.2024 08:14] [Experimental] Generating an image for paper LOGO -- Long cOntext aliGnment via efficient preference Optimization.
[26.10.2024 08:14] [Experimental] Image for paper LOGO -- Long cOntext aliGnment via efficient preference Optimization already exists.
[26.10.2024 08:14] [Experimental] Generating an image for paper Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch.
[26.10.2024 08:14] [Experimental] Image for paper Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch already exists.
[26.10.2024 08:14] [Experimental] Generating an image for paper Framer: Interactive Frame Interpolation.
[26.10.2024 08:14] [Experimental] Image for paper Framer: Interactive Frame Interpolation already exists.
[26.10.2024 08:14] [Experimental] Generating an image for paper Unbounded: A Generative Infinite Game of Character Life Simulation.
[26.10.2024 08:14] [Experimental] Image for paper Unbounded: A Generative Infinite Game of Character Life Simulation already exists.
[26.10.2024 09:59] Read previous papers.
[26.10.2024 09:59] Get feed.
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17243
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16251
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18533
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18693
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18978
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18975
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18798
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18572
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18745
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18362
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18977
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18775
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18451
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18538
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18976
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15999
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17897
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17779
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15580
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18505
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18785
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18958
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18252
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16429
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18194
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18441
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18860
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18647
[26.10.2024 09:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18234
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 0. Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumptio...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 1. Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 2. Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the ge...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 3. The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 4. We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory ...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 5. We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 6. Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approa...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 7. Efficient long-context language modeling remains a significant challenge in Natural Language Processing (NLP). While Transformers dominate language tasks, they struggle with long sequences due to quadratic computational complexity in training and linearly scaling memory costs during inference. Recen...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 8. Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their tr...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 9. Web development involves turning UI designs into functional webpages, which can be difficult for both beginners and experienced developers due to the complexity of HTML's hierarchical structures and styles. While Large Language Models (LLMs) have shown promise in generating source code, two major ch...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 10. This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propo...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 11. Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first compre...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 12. In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data c...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 13. Segmenting an object in a video presents significant challenges. Each pixel must be accurately labelled, and these labels must remain consistent across frames. The difficulty increases when the segmentation is with arbitrary granularity, meaning the number of segments can vary arbitrarily, and masks...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 14. Recent years have witnessed a significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluati...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 15. Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context -- this phenomenon, known as context-memory knowledge conflicts, can lead to undesirable model behaviour...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 16. Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earl...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 17. Recent advancements in multimodal fusion have witnessed the remarkable success of vision-language (VL) models, which excel in various multimodal applications such as image captioning and visual question answering. However, building VL models requires substantial hardware resources, where efficiency ...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 18. Large Language Models (LLMs) are thought to struggle with arithmetic learning due to the inherent differences between language modeling and numerical computation, but concrete evidence has been lacking. This work responds to this claim through a two-side experiment. We first investigate whether LLMs...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 19. We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a high-quality 500GB subset of the Chinese Corpora Internet 3.0 (CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a novel two-stage hybrid filtering pipeline that significantly enhances data quality. To eval...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 20. Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editi...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 21. Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through c...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 22. The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical d...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 23. Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 24. Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution.   Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the n...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 25. In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of ...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 26. Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval hea...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 27. Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate ...
[26.10.2024 09:59] ********************************************************************************
[26.10.2024 09:59] Abstract 28. We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model....
[26.10.2024 09:59] Read previous papers.
[26.10.2024 09:59] Generating reviews via LLM API.
[26.10.2024 09:59] Using data from previous issue: {"desc": "В статье представлен новый подход к вычислению контрастивной функции потерь, позволяющий значительно увеличить размер батча без роста потребления памяти GPU. Авторы предлагают стратегию вычислений на основе тайлинга, которая разбивает расчет контрастивной функции потерь на небольшие блоки.
[26.10.2024 09:59] Using data from previous issue: {"desc": "Данная статья посвящена проблеме галлюцинаций в больших языковых моделях (LLM) и методам редактирования знаний для их исправления. Авторы предлагают новый бенчмарк HalluEditBench для оценки эффективности методов редактирования знаний в исправлении реальных галлюцинаций. Бенчмарк включает м
[26.10.2024 09:59] Using data from previous issue: {"desc": "Эта статья представляет новый метод обучения под названием LOGO для улучшения генеративных способностей моделей с длинным контекстом (LCM). LOGO использует оптимизацию предпочтений без эталона и метод синтеза позиций для создания обучающих данных. Авторы обучили модель Llama-3-8B-Instruct-
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет ScaleQuest - новый метод синтеза данных для улучшения способностей LLM к рассуждениям. Используя небольшие модели с открытым исходным кодом, ScaleQuest генерирует вопросы с нуля без необходимости в исходных данных. С помощью этого метода был создан набор данных из 1 мил
[26.10.2024 09:59] Using data from previous issue: {"desc": "Предложен метод Framer для интерактивной интерполяции кадров, позволяющий создавать плавные переходы между двумя изображениями с учетом пользовательских предпочтений. Система поддерживает настройку процесса перехода путем задания траектории выбранных ключевых точек, что обеспечивает более 
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет концепцию генеративной бесконечной игры, которая использует генеративные модели для создания видеоигры, выходящей за рамки традиционных конечных систем. Авторы разработали игру Unbounded - симулятор жизни персонажа, полностью основанный на генеративных моделях. В игре и
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет новый метод Code-as-Intermediary Translation (CIT) для улучшения визуальных рассуждений в мультимодальных больших языковых моделях (MLLM). Метод использует код как посредник для перевода визуальных представлений графиков в текстовые, что позволяет LLM понимать межмодаль
[26.10.2024 09:59] Using data from previous issue: {"desc": "Исследователи представили Taipan - гибридную архитектуру, сочетающую Mamba-2 с селективными слоями внимания для эффективной обработки длинных последовательностей в языковом моделировании. Taipan идентифицирует важные токены, требующие взаимодействия на большом расстоянии, и усиливает их пр
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет новый метод под названием STRING (ShifTed Rotray position embeddING) для улучшения эффективной длины контекста больших языковых моделей (LLM). Авторы обнаружили, что существующие LLM часто не могут эффективно использовать весь заявленный контекст из-за особенностей расп
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет новую стратегию тонкой настройки моделей для генерации HTML-кода из UI-дизайнов. Метод Waffle использует механизм внимания, учитывающий структуру HTML, и контрастивное обучение для улучшения понимания моделями связи между изображениями UI и HTML-кодом. Эксперименты пока
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет MotionCLR - модель диффузии движения на основе механизмов внимания для интерактивного редактирования генерации движений человека. Модель использует самовнимание для измерения последовательного сходства между кадрами и кросс-внимание для нахождения детальных соответствий
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет новый бенчмарк W-Bench для оценки устойчивости методов водяных знаков к современным техникам редактирования изображений. Авторы предлагают метод VINE, который значительно повышает робастность водяных знаков, сохраняя высокое качество изображений. VINE использует анализ 
[26.10.2024 09:59] Using data from previous issue: {"desc": "В этой работе представлен набор методов для улучшения моделирования вознаграждений для больших языковых моделей (LLM), с акцентом на техники, ориентированные на данные. Авторы предлагают эффективные стратегии отбора и фильтрации данных для курирования высококачественных открытых наборов да
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья посвящена проблеме сегментации объектов в видео с произвольной гранулярностью. Авторы предлагают использовать предобученную диффузионную модель для перехода от текста к изображению, дополненную механизмом отслеживания. Этот подход позволяет эффективно справляться с различными сценар
[26.10.2024 09:59] Using data from previous issue: {"desc": "В статье представлен новый бенчмарк CAMEL-Bench для оценки мультимодальных языковых моделей (LMM) на арабском языке. Бенчмарк охватывает 8 разнообразных областей и 38 подобластей, включая понимание нескольких изображений, сложное визуальное восприятие и анализ видео. CAMEL-Bench содержит о
[26.10.2024 09:59] Using data from previous issue: {"desc": "Данная статья посвящена проблеме конфликтов знаний в больших языковых моделях (LLM). Авторы предлагают метод SpARE, использующий предобученные разреженные автоэнкодеры для управления выбором знаний в LLM. SpARE идентифицирует функциональные особенности, контролирующие выбор знаний, и приме
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет новый подход к улучшению работы трансформеров под названием ResFormer. Он решает проблему концентрации внимания в глубоких слоях путем добавления остаточного соединения от значений первого слоя ко всем последующим. Предложенная модификация SVFormer использует одинаковые
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет ADEM-VL - эффективный метод для моделей зрения-языка, основанный на предобученных больших языковых моделях (LLM). Он использует механизм кросс-внимания без параметров для измерения сходства в мультимодальном слиянии, что значительно сокращает количество обучаемых параме
[26.10.2024 09:59] Using data from previous issue: {"desc": "Это исследование посвящено изучению способностей больших языковых моделей (LLM) к обучению арифметике. Авторы обнаружили, что хотя LLM могут идентифицировать частичные произведения после обучения, они не способны использовать их для арифметических задач. Исследование также показало, что LL
[26.10.2024 09:59] Using data from previous issue: {"desc": "Представлен CCI3.0-HQ - высококачественный набор данных объемом 500 ГБ, полученный из Chinese Corpora Internet 3.0. Для его создания использовался новый двухэтапный гибридный конвейер фильтрации, значительно улучшающий качество данных. Модель с 0.5 миллиардами параметров, обученная на этом
[26.10.2024 09:59] Using data from previous issue: {"desc": "В статье рассматривается влияние методов редактирования на общие способности языковых моделей. Исследование показывает, что существующие методы редактирования приводят к неизбежному ухудшению производительности на общих бенчмарках, особенно при большом количестве правок. Модели, настроенны
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет новый подход к пониманию моделей согласованности, рассматривая процесс шумоподавления как марковский процесс принятия решений. Авторы предлагают метод Stable Consistency Tuning (SCT), который использует обучение с временной разницей и снижение дисперсии для улучшения пр
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья предлагает новый подход к обучению с подкреплением по обратной связи человека (RLHF) для больших языковых моделей. Авторы предлагают разделить процессы генерации и обучения, что позволяет асинхронно генерировать новые образцы и одновременно обучаться на старых. Исследование показыва
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет инструмент Pantograph для автоматического доказательства теорем с помощью машинного обучения. Pantograph обеспечивает интерфейс к ассистенту доказательств Lean 4 и позволяет эффективно искать доказательства с помощью алгоритмов, таких как поиск методом Монте-Карло. Инст
[26.10.2024 09:59] Using data from previous issue: {"desc": "ZIP-FIT - это новый метод отбора данных для обучения языковых моделей, использующий сжатие gzip для измерения соответствия между потенциальными обучающими данными и целевым распределением задачи. В сравнении с существующими подходами, ZIP-FIT значительно превосходит базовые методы в задача
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья представляет глубокий анализ математических формулировок и вероятностных оптимизаций ключевых компонентов модели Transformer в области генеративного ИИ. Авторы предлагают оптимальное решение для кодирования подслов (SWE), основанное на алгоритме byte-pair encoding (BPE) и подходе Wo
[26.10.2024 09:59] Using data from previous issue: {"desc": "Исследователи предложили новый метод декодирования под названием DeCoRe для уменьшения галлюцинаций в больших языковых моделях (LLM). Метод основан на маскировании определенных голов внимания в архитектуре Transformer, отвечающих за извлечение контекстной информации. DeCoRe сравнивает выхо
[26.10.2024 09:59] Using data from previous issue: {"desc": "Исследователи изучают возможность применения принципов масштабирования данных в робототехнике, в частности, в манипуляциях роботов. Они провели масштабное эмпирическое исследование, собрав более 40 000 демонстраций и выполнив более 15 000 реальных запусков роботов. Результаты показали, что
[26.10.2024 09:59] Using data from previous issue: {"desc": "Статья рассматривает мультимодельное спекулятивное сэмплирование в контексте языковых моделей. Авторы предлагают оптимальную двухэтапную схему выбора токенов, использующую важностную выборку. Для случая двух идентичных черновых моделей установлены условия для 100% вероятности принятия и по
[26.10.2024 09:59] Trying to get texts in Chinese.
[26.10.2024 09:59] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.
[26.10.2024 09:59] Mistral response. {"id": "1daea2c5de1e48188b08a23f7524a89f", "object": "chat.completion", "created": 1729936788, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5bf9\u6bd4\u635f\u5931\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002\u5927\u6279\u91cf\u6570\u636e\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u53d7\u5230GPU\u5185\u5b58\u9650\u5236\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u7684\u8ba1\u7b97\u7b56\u7565\uff0c\u907f\u514d\u5168\u90e8\u5b9e\u4f8b\u5316\u76f8\u4f3c\u5ea6\u77e9\u9635\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u5f15\u5165\u4e86\u591a\u7ea7\u5757\u7b56\u7565\uff0c\u5229\u7528\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6269\u5c55\u5230\u524d\u6240\u672a\u6709\u7684\u6279\u91cf\u5927\u5c0f\uff0c\u4f8b\u5982\u57288\u621632\u4e2aA800 80GB\u4e0a\u4f7f\u75284M\u621612M\u7684\u6279\u91cf\u5927\u5c0f\u8bad\u7ec3CLIP-ViT-L/14\u6a21\u578b\uff0c\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u5185\u5b58\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\uff0c\u5b83\u5728\u4fdd\u6301\u76f8\u5f53\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u5185\u5b58\u51cf\u5c11\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 298, "total_tokens": 532, "completion_tokens": 234}}
[26.10.2024 09:59] Response: 这篇文章讨论了对比损失在表示学习中的应用。大批量数据可以提高性能，但受到GPU内存限制。作者提出了一种基于块的计算策略，避免全部实例化相似度矩阵。此外，他们引入了多级块策略，利用分布式系统的层次结构。实验结果显示，该方法可以扩展到前所未有的批量大小，例如在8或32个A800 80GB上使用4M或12M的批量大小训练CLIP-ViT-L/14模型，而不牺牲准确性。与最先进的内存高效解决方案相比，它在保持相当速度的同时，实现了两个数量级的内存减少。代码将公开发布。
[26.10.2024 09:59] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章讨论了对比损失在表示学习中的应用。大批量数据可以提高性能，但受到GPU内存限制。作者提出了一种基于块的计算策略，避免全部实例化相似度矩阵。此外，他们引入了多级块策略，利用分布式系统的层次结构。实验结果显示，该方法可以扩展到前所未有的批量大小，例如在8或32个A800 80GB上使用4M或12M的批量大小训练CLIP-ViT-L/14模型，而不牺牲准确性。与最先进的内存高效解决方案相比，它在保持相当速度的同时，实现了两个数量级的内存减少。代码将公开发布。
[26.10.2024 10:00] Mistral response. {"id": "c2838343fda746a293323ea85b859d92", "object": "chat.completion", "created": 1729936799, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5bf9\u6bd4\u635f\u5931\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002\u5927\u6279\u91cf\u6570\u636e\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u53d7\u5230GPU\u5185\u5b58\u9650\u5236\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u7684\u8ba1\u7b97\u7b56\u7565\uff0c\u907f\u514d\u5168\u90e8\u5b9e\u4f8b\u5316\u76f8\u4f3c\u5ea6\u77e9\u9635\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u5f15\u5165\u4e86\u591a\u7ea7\u5757\u7b56\u7565\uff0c\u5229\u7528\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6269\u5c55\u5230\u524d\u6240\u672a\u6709\u7684\u6279\u91cf\u5927\u5c0f\uff0c\u4f8b\u5982\u57288\u621632\u4e2aA800 80GB\u4e0a\u4f7f\u75284M\u621612M\u7684\u6279\u91cf\u5927\u5c0f\u8bad\u7ec3CLIP-ViT-L/14\u6a21\u578b\uff0c\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u5185\u5b58\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\uff0c\u5b83\u5728\u4fdd\u6301\u76f8\u5f53\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u5185\u5b58\u51cf\u5c11\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng t\u01ceo l\u00f9n le du\u00ec b\u01d0 s\u01d4n sh\u012b z\u00e0i bi\u01ceo sh\u00ec xu\u00e9 x\u00ed zh\u014dng de y\u00ecng y\u00f2ng. d\u00e0 p\u012b li\u00e0ng sh\u00f9 j\u00f9 k\u011b y\u01d0 t\u00ed g\u0101o x\u00ecng n\u00e9ng, d\u00e0n sh\u00f2u d\u00e0o GPU n\u00e8i c\u00fan xi\u00e0n zh\u00ec. zu\u00f2 zh\u011b t\u00ed ch\u016b le y\u012b zh\u01d2ng j\u012b z\u00e0i ku\u00e0i de j\u00ec su\u00e0n c\u00e8 l\u00fc\u00e8, b\u00ec mi\u01cen qu\u00e1n b\u00f9 sh\u00ed l\u00ec hu\u00ec xi\u0101ng s\u00ec d\u00f9 ju\u01cen zh\u00e8n. c\u01d0 w\u00e0i, t\u0101 men y\u01d0n r\u00f9 le du\u014d j\u00ed ku\u00e0i c\u00e8 l\u00fc\u00e8, l\u00ec y\u00f2ng f\u0113n b\u00f9 sh\u00ec x\u00ec t\u01d2ng de c\u00e9ng c\u00ec ji\u00e8 g\u00f2u. sh\u00ed y\u00e0n ji\u00e9 gu\u01d2 xi\u01cen sh\u00ec, g\u0101i f\u01ceng f\u01ce k\u011b y\u01d0 ku\u00f2 zh\u01cen d\u00e0o qi\u00e1n su\u01d2 w\u00e8i y\u01d2u de p\u012b li\u00e0ng d\u00e0 xi\u01ceo, l\u00ec r\u00fa z\u00e0i 8 hu\u00f2 32 g\u00e8 A800 80GB sh\u00e0ng sh\u01d0 y\u00f2ng 4M hu\u00f2 12M de p\u012b li\u00e0ng d\u00e0 xi\u01ceo x\u00f9n li\u00e0n CLIP-ViT-L/14 m\u00f3 x\u00edng, \u00e9r b\u00f9 x\u012b sh\u0113ng zh\u01d4n qu\u00e8 x\u00ecng. y\u01d4 zu\u00ec xi\u0101n j\u00ecn de n\u00e8i c\u00fan g\u0101o xi\u00e0o j\u00ecn j\u00ece f\u01ceng \u0101n xi\u0101ng b\u01d0, t\u0101 z\u00e0i b\u01ceo ch\u00ed xi\u0101ng d\u0101ng s\u00f9 d\u00f9 de sh\u00ed hu\u00e0, sh\u00ed xi\u00e0n le li\u01ceng g\u00e8 sh\u00f9 li\u00e0ng j\u00ed de n\u00e8i c\u00fan ji\u01cen sh\u01ceo. d\u00e0i m\u01ce ji\u0101ng g\u014dng k\u0101i f\u0101 b\u00f9.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 249, "total_tokens": 993, "completion_tokens": 744}}
[26.10.2024 10:00] Response: 这篇文章讨论了对比损失在表示学习中的应用。大批量数据可以提高性能，但受到GPU内存限制。作者提出了一种基于块的计算策略，避免全部实例化相似度矩阵。此外，他们引入了多级块策略，利用分布式系统的层次结构。实验结果显示，该方法可以扩展到前所未有的批量大小，例如在8或32个A800 80GB上使用4M或12M的批量大小训练CLIP-ViT-L/14模型，而不牺牲准确性。与最先进的内存高效解决方案相比，它在保持相当速度的同时，实现了两个数量级的内存减少。代码将公开发布。

zhè piān wén zhāng tǎo lùn le duì bǐ sǔn shī zài biǎo shì xué xí zhōng de yìng yòng. dà pī liàng shù jù kě yǐ tí gāo xìng néng, dàn shòu dào GPU nèi cún xiàn zhì. zuò zhě tí chū le yī zhǒng jī zài kuài de jì suàn cè lüè, bì miǎn quán bù shí lì huì xiāng sì dù juǎn zhèn. cǐ wài, tā men yǐn rù le duō jí kuài cè lüè, lì yòng fēn bù shì xì tǒng de céng cì jiè gòu. shí yàn jié guǒ xiǎn shì, gāi fǎng fǎ kě yǐ kuò zhǎn dào qián suǒ wèi yǒu de pī liàng dà xiǎo, lì rú zài 8 huò 32 gè A800 80GB shàng shǐ yòng 4M huò 12M de pī liàng dà xiǎo xùn liàn CLIP-ViT-L/14 mó xíng, ér bù xī shēng zhǔn què xìng. yǔ zuì xiān jìn de nèi cún gāo xiào jìn jìe fǎng ān xiāng bǐ, tā zài bǎo chí xiāng dāng sù dù de shí huà, shí xiàn le liǎng gè shù liàng jí de nèi cún jiǎn shǎo. dài mǎ jiāng gōng kāi fā bù.
[26.10.2024 10:00] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章讨论了对比损失在表示学习中的应用。大批量数据可以提高性能，但受到GPU内存限制。作者提出了一种基于块的计算策略，避免全部实例化相似度矩阵。此外，他们引入了多级块策略，利用分布式系统的层次结构。实验结果显示，该方法可以扩展到前所未有的批量大小，例如在8或32个A800 80GB上使用4M或12M的批量大小训练CLIP-ViT-L/14模型，而不牺牲准确性。与最先进的内存高效解决方案相比，它在保持相当速度的同时，实现了两个数量级的内存减少。代码将公开发布。
[26.10.2024 10:01] Mistral response. {"id": "c24b5901d3334f7995f5e4f5729c9bee", "object": "chat.completion", "created": 1729936823, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u5bf9\u6bd4\", \"pinyin\": \"du\u00ec b\u01d0\", \"trans\": \"contrast\"},\n    {\"word\": \"\u635f\u5931\", \"pinyin\": \"s\u01d4n sh\u012b\", \"trans\": \"loss\"},\n    {\"word\": \"\u8868\u793a\", \"pinyin\": \"bi\u01ceo sh\u00ec\", \"trans\": \"representation\"},\n    {\"word\": \"\u5b66\u4e60\", \"pinyin\": \"xu\u00e9 x\u00ed\", \"trans\": \"learning\"},\n    {\"word\": \"\u5927\u6279\u91cf\", \"pinyin\": \"d\u00e0 p\u012b li\u00e0ng\", \"trans\": \"large batch\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecng n\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"GPU\", \"pinyin\": \"GPU\", \"trans\": \"GPU\"},\n    {\"word\": \"\u5185\u5b58\", \"pinyin\": \"n\u00e8i c\u00fan\", \"trans\": \"memory\"},\n    {\"word\": \"\u9650\u5236\", \"pinyin\": \"xi\u00e0n zh\u00ec\", \"trans\": \"limit\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u57fa\u4e8e\", \"pinyin\": \"j\u012b y\u00fa\", \"trans\": \"based on\"},\n    {\"word\": \"\u5757\", \"pinyin\": \"ku\u00e0i\", \"trans\": \"block\"},\n    {\"word\": \"\u8ba1\u7b97\", \"pinyin\": \"j\u00ec su\u00e0n\", \"trans\": \"compute\"},\n    {\"word\": \"\u7b56\u7565\", \"pinyin\": \"c\u00e8 l\u00fc\u00e8\", \"trans\": \"strategy\"},\n    {\"word\": \"\u907f\u514d\", \"pinyin\": \"b\u00ec mi\u01cen\", \"trans\": \"avoid\"},\n    {\"word\": \"\u5168\u90e8\", \"pinyin\": \"qu\u00e1n b\u00f9\", \"trans\": \"entire\"},\n    {\"word\": \"\u5b9e\u4f8b\u5316\", \"pinyin\": \"sh\u00ed l\u00ec hu\u00e0\", \"trans\": \"instantiate\"},\n    {\"word\": \"\u76f8\u4f3c\u5ea6\", \"pinyin\": \"xi\u0101ng s\u00ec d\u00f9\", \"trans\": \"similarity\"},\n    {\"word\": \"\u77e9\u9635\", \"pinyin\": \"j\u01d4 zh\u00e8n\", \"trans\": \"matrix\"},\n    {\"word\": \"\u6b64\u5916\", \"pinyin\": \"c\u01d0 w\u00e0i\", \"trans\": \"moreover\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u591a\u7ea7\", \"pinyin\": \"du\u014d j\u00ed\", \"trans\": \"multi-level\"},\n    {\"word\": \"\u5206\u5e03\u5f0f\", \"pinyin\": \"f\u0113n b\u00f9 sh\u00ec\", \"trans\": \"distributed\"},\n    {\"word\": \"\u7cfb\u7edf\", \"pinyin\": \"x\u00ec t\u01d2ng\", \"trans\": \"system\"},\n    {\"word\": \"\u5c42\u6b21\", \"pinyin\": \"c\u00e9ng c\u00ec\", \"trans\": \"hierarchy\"},\n    {\"word\": \"\u7ed3\u6784\", \"pinyin\": \"ji\u00e9 g\u00f2u\", \"trans\": \"structure\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00ed y\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9 gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u663e\u793a\", \"pinyin\": \"xi\u01cen sh\u00ec\", \"trans\": \"show\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ng f\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u6269\u5c55\", \"pinyin\": \"ku\u00f2 zh\u01cen\", \"trans\": \"scale\"},\n    {\"word\": \"\u524d\u6240\u672a\u6709\", \"pinyin\": \"qi\u00e1n su\u01d2 w\u00e8i y\u01d2u\", \"trans\": \"unprecedented\"},\n    {\"word\": \"\u6279\u91cf\", \"pinyin\": \"p\u012b li\u00e0ng\", \"trans\": \"batch\"},\n    {\"word\": \"\u5927\u5c0f\", \"pinyin\": \"d\u00e0 xi\u01ceo\", \"trans\": \"size\"},\n    {\"word\": \"\u4f8b\u5982\", \"pinyin\": \"l\u00ec r\u00fa\", \"trans\": \"for example\"},\n    {\"word\": \"\u4f7f\u7528\", \"pinyin\": \"sh\u01d0 y\u00f2ng\", \"trans\": \"use\"},\n    {\"word\": \"\u8bad\u7ec3\", \"pinyin\": \"x\u00f9n li\u00e0n\", \"trans\": \"train\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u727a\u7272\", \"pinyin\": \"x\u012b sh\u0113ng\", \"trans\": \"sacrifice\"},\n    {\"word\": \"\u51c6\u786e\u6027\", \"pinyin\": \"zh\u01d4n qu\u00e8 x\u00ecng\", \"trans\": \"accuracy\"},\n    {\"word\": \"\u6700\u5148\u8fdb\", \"pinyin\": \"zu\u00ec xi\u0101n j\u00ecn\", \"trans\": \"most advanced\"},\n    {\"word\": \"\u5185\u5b58\u9ad8\u6548\", \"pinyin\": \"n\u00e8i c\u00fan g\u0101o xi\u00e0o\", \"trans\": \"memory-efficient\"},\n    {\"word\": \"\u89e3\u51b3\u65b9\u6848\", \"pinyin\": \"ji\u011b ju\u00e9 f\u0101ng \u00e0n\", \"trans\": \"solution\"},\n    {\"word\": \"\u76f8\u6bd4\", \"pinyin\": \"xi\u0101ng b\u01d0\", \"trans\": \"compared to\"},\n    {\"word\": \"\u4fdd\u6301\", \"pinyin\": \"b\u01ceo ch\u00ed\", \"trans\": \"maintain\"},\n    {\"word\": \"\u76f8\u5f53\", \"pinyin\": \"xi\u0101ng d\u0101ng\", \"trans\": \"considerable\"},\n    {\"word\": \"\u901f\u5ea6\", \"pinyin\": \"s\u00f9 d\u00f9\", \"trans\": \"speed\"},\n    {\"word\": \"\u540c\u65f6\", \"pinyin\": \"t\u00f3ng sh\u00ed\", \"trans\": \"simultaneously\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u6570\u91cf\u7ea7\", \"pinyin\": \"sh\u00f9 li\u00e0ng j\u00ed\", \"trans\": \"order of magnitude\"},\n    {\"word\": \"\u51cf\u5c11\", \"pinyin\": \"ji\u01cen sh\u01ceo\", \"trans\": \"reduce\"},\n    {\"word\": \"\u4ee3\u7801\", \"pinyin\": \"d\u00e0i m\u01ce\", \"trans\": \"code\"},\n    {\"word\": \"\u516c\u5f00\", \"pinyin\": \"g\u014dng k\u0101i\", \"trans\": \"public\"},\n    {\"word\": \"\u53d1\u5e03\", \"pinyin\": \"f\u0101 b\u00f9\", \"trans\": \"release\"}\n]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 281, "total_tokens": 1858, "completion_tokens": 1577}}
[26.10.2024 10:01] Response: [
    {"word": "讨论", "pinyin": "tǎo lùn", "trans": "discuss"},
    {"word": "对比", "pinyin": "duì bǐ", "trans": "contrast"},
    {"word": "损失", "pinyin": "sǔn shī", "trans": "loss"},
    {"word": "表示", "pinyin": "biǎo shì", "trans": "representation"},
    {"word": "学习", "pinyin": "xué xí", "trans": "learning"},
    {"word": "大批量", "pinyin": "dà pī liàng", "trans": "large batch"},
    {"word": "性能", "pinyin": "xìng néng", "trans": "performance"},
    {"word": "GPU", "pinyin": "GPU", "trans": "GPU"},
    {"word": "内存", "pinyin": "nèi cún", "trans": "memory"},
    {"word": "限制", "pinyin": "xiàn zhì", "trans": "limit"},
    {"word": "提出", "pinyin": "tí chū", "trans": "propose"},
    {"word": "基于", "pinyin": "jī yú", "trans": "based on"},
    {"word": "块", "pinyin": "kuài", "trans": "block"},
    {"word": "计算", "pinyin": "jì suàn", "trans": "compute"},
    {"word": "策略", "pinyin": "cè lüè", "trans": "strategy"},
    {"word": "避免", "pinyin": "bì miǎn", "trans": "avoid"},
    {"word": "全部", "pinyin": "quán bù", "trans": "entire"},
    {"word": "实例化", "pinyin": "shí lì huà", "trans": "instantiate"},
    {"word": "相似度", "pinyin": "xiāng sì dù", "trans": "similarity"},
    {"word": "矩阵", "pinyin": "jǔ zhèn", "trans": "matrix"},
    {"word": "此外", "pinyin": "cǐ wài", "trans": "moreover"},
    {"word": "引入", "pinyin": "yǐn rù", "trans": "introduce"},
    {"word": "多级", "pinyin": "duō jí", "trans": "multi-level"},
    {"word": "分布式", "pinyin": "fēn bù shì", "trans": "distributed"},
    {"word": "系统", "pinyin": "xì tǒng", "trans": "system"},
    {"word": "层次", "pinyin": "céng cì", "trans": "hierarchy"},
    {"word": "结构", "pinyin": "jié gòu", "trans": "structure"},
    {"word": "实验", "pinyin": "shí yàn", "trans": "experiment"},
    {"word": "结果", "pinyin": "jié guǒ", "trans": "result"},
    {"word": "显示", "pinyin": "xiǎn shì", "trans": "show"},
    {"word": "方法", "pinyin": "fāng fǎ", "trans": "method"},
    {"word": "扩展", "pinyin": "kuò zhǎn", "trans": "scale"},
    {"word": "前所未有", "pinyin": "qián suǒ wèi yǒu", "trans": "unprecedented"},
    {"word": "批量", "pinyin": "pī liàng", "trans": "batch"},
    {"word": "大小", "pinyin": "dà xiǎo", "trans": "size"},
    {"word": "例如", "pinyin": "lì rú", "trans": "for example"},
    {"word": "使用", "pinyin": "shǐ yòng", "trans": "use"},
    {"word": "训练", "pinyin": "xùn liàn", "trans": "train"},
    {"word": "模型", "pinyin": "mó xíng", "trans": "model"},
    {"word": "牺牲", "pinyin": "xī shēng", "trans": "sacrifice"},
    {"word": "准确性", "pinyin": "zhǔn què xìng", "trans": "accuracy"},
    {"word": "最先进", "pinyin": "zuì xiān jìn", "trans": "most advanced"},
    {"word": "内存高效", "pinyin": "nèi cún gāo xiào", "trans": "memory-efficient"},
    {"word": "解决方案", "pinyin": "jiě jué fāng àn", "trans": "solution"},
    {"word": "相比", "pinyin": "xiāng bǐ", "trans": "compared to"},
    {"word": "保持", "pinyin": "bǎo chí", "trans": "maintain"},
    {"word": "相当", "pinyin": "xiāng dāng", "trans": "considerable"},
    {"word": "速度", "pinyin": "sù dù", "trans": "speed"},
    {"word": "同时", "pinyin": "tóng shí", "trans": "simultaneously"},
    {"word": "实现", "pinyin": "shí xiàn", "trans": "achieve"},
    {"word": "数量级", "pinyin": "shù liàng jí", "trans": "order of magnitude"},
    {"word": "减少", "pinyin": "jiǎn shǎo", "trans": "reduce"},
    {"word": "代码", "pinyin": "dài mǎ", "trans": "code"},
    {"word": "公开", "pinyin": "gōng kāi", "trans": "public"},
    {"word": "发布", "pinyin": "fā bù", "trans": "release"}
]
[26.10.2024 10:01] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章讨论了对比损失在表示学习中的应用。大批量数据可以提高性能，但受到GPU内存限制。作者提出了一种基于块的计算策略，避免全部实例化相似度矩阵。此外，他们引入了多级块策略，利用分布式系统的层次结构。实验结果显示，该方法可以扩展到前所未有的批量大小，例如在8或32个A800 80GB上使用4M或12M的批量大小训练CLIP-ViT-L/14模型，而不牺牲准确性。与最先进的内存高效解决方案相比，它在保持相当速度的同时，实现了两个数量级的内存减少。代码将公开发布。
[26.10.2024 10:01] Mistral response. {"id": "0c321f034dc347ac991a7ccb987044d2", "object": "chat.completion", "created": 1729936875, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "This article discusses the application of contrastive loss in representation learning. Large-scale data can improve performance but is constrained by GPU memory. The authors propose a block-based computation strategy to avoid fully instantiating the similarity matrix. Additionally, they introduce a multi-level block strategy that leverages the hierarchical structure of distributed systems. Experimental results show that this method can scale to unprecedented batch sizes, such as training the CLIP-ViT-L/14 model with batch sizes of 4M or 12M on 8 or 32 A800 80GB GPUs without sacrificing accuracy. Compared to state-of-the-art memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory usage while maintaining comparable speed. The code will be made publicly available.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 247, "total_tokens": 428, "completion_tokens": 181}}
[26.10.2024 10:01] Response: This article discusses the application of contrastive loss in representation learning. Large-scale data can improve performance but is constrained by GPU memory. The authors propose a block-based computation strategy to avoid fully instantiating the similarity matrix. Additionally, they introduce a multi-level block strategy that leverages the hierarchical structure of distributed systems. Experimental results show that this method can scale to unprecedented batch sizes, such as training the CLIP-ViT-L/14 model with batch sizes of 4M or 12M on 8 or 32 A800 80GB GPUs without sacrificing accuracy. Compared to state-of-the-art memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory usage while maintaining comparable speed. The code will be made publicly available.
[26.10.2024 10:01] Renaming data file.
[26.10.2024 10:01] Renaming previous data. hf_papers.json to ./d/2024-10-25.json
[26.10.2024 10:01] Saving new data file.
[26.10.2024 10:01] Generating page.
[26.10.2024 10:01] Renaming previous page.
[26.10.2024 10:01] Renaming previous data. index.html to ./d/2024-10-25.html
[26.10.2024 10:01] [Experimental] Generating Chinese page for reading.
[26.10.2024 10:01] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '对比', 'pinyin': 'duì bǐ', 'trans': 'contrast'}, {'word': '损失', 'pinyin': 'sǔn shī', 'trans': 'loss'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'representation'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learning'}, {'word': '大批量', 'pinyin': 'dà pī liàng', 'trans': 'large batch'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': 'GPU', 'pinyin': 'GPU', 'trans': 'GPU'}, {'word': '内存', 'pinyin': 'nèi cún', 'trans': 'memory'}, {'word': '限制', 'pinyin': 'xiàn zhì', 'trans': 'limit'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': '块', 'pinyin': 'kuài', 'trans': 'block'}, {'word': '计算', 'pinyin': 'jì suàn', 'trans': 'compute'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '避免', 'pinyin': 'bì miǎn', 'trans': 'avoid'}, {'word': '全部', 'pinyin': 'quán bù', 'trans': 'entire'}, {'word': '实例化', 'pinyin': 'shí lì huà', 'trans': 'instantiate'}, {'word': '相似度', 'pinyin': 'xiāng sì dù', 'trans': 'similarity'}, {'word': '矩阵', 'pinyin': 'jǔ zhèn', 'trans': 'matrix'}, {'word': '此外', 'pinyin': 'cǐ wài', 'trans': 'moreover'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': '多级', 'pinyin': 'duō jí', 'trans': 'multi-level'}, {'word': '分布式', 'pinyin': 'fēn bù shì', 'trans': 'distributed'}, {'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'}, {'word': '层次', 'pinyin': 'céng cì', 'trans': 'hierarchy'}, {'word': '结构', 'pinyin': 'jié gòu', 'trans': 'structure'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'show'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'scale'}, {'word': '前所未有', 'pinyin': 'qián suǒ wèi yǒu', 'trans': 'unprecedented'}, {'word': '批量', 'pinyin': 'pī liàng', 'trans': 'batch'}, {'word': '大小', 'pinyin': 'dà xiǎo', 'trans': 'size'}, {'word': '例如', 'pinyin': 'lì rú', 'trans': 'for example'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '牺牲', 'pinyin': 'xī shēng', 'trans': 'sacrifice'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '最先进', 'pinyin': 'zuì xiān jìn', 'trans': 'most advanced'}, {'word': '内存高效', 'pinyin': 'nèi cún gāo xiào', 'trans': 'memory-efficient'}, {'word': '解决方案', 'pinyin': 'jiě jué fāng àn', 'trans': 'solution'}, {'word': '相比', 'pinyin': 'xiāng bǐ', 'trans': 'compared to'}, {'word': '保持', 'pinyin': 'bǎo chí', 'trans': 'maintain'}, {'word': '相当', 'pinyin': 'xiāng dāng', 'trans': 'considerable'}, {'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'}, {'word': '同时', 'pinyin': 'tóng shí', 'trans': 'simultaneously'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '数量级', 'pinyin': 'shù liàng jí', 'trans': 'order of magnitude'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'}, {'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}]
[26.10.2024 10:01] Renaming previous Chinese page.
[26.10.2024 10:01] Renaming previous data. zh.html to ./d/2024-10-25_zh_reading_task.html
[26.10.2024 10:01] Writing result.
[26.10.2024 10:01] Writing Chinese reading task.
[26.10.2024 10:01] Renaming log file.
[26.10.2024 10:01] Renaming previous data. log.txt to ./logs/2024-10-26_last_log.txt
