[25.10.2024 22:11] [Experimental] Generating an image for paper Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss.
[25.10.2024 22:11] [Experimental] Image for paper Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss already exists.
[25.10.2024 22:11] [Experimental] Generating an image for paper LOGO -- Long cOntext aliGnment via efficient preference Optimization.
[25.10.2024 22:11] [Experimental] Image for paper LOGO -- Long cOntext aliGnment via efficient preference Optimization already exists.
[25.10.2024 22:11] [Experimental] Generating an image for paper Can Knowledge Editing Really Correct Hallucinations?.
[25.10.2024 22:11] [Experimental] Image for paper Can Knowledge Editing Really Correct Hallucinations? already exists.
[25.10.2024 22:11] [Experimental] Generating an image for paper Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch.
[25.10.2024 22:11] [Experimental] Image for paper Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch already exists.
[25.10.2024 22:11] [Experimental] Generating an image for paper Framer: Interactive Frame Interpolation.
[25.10.2024 22:11] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'Framer: Interactive Frame Interpolation' Text: 'We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory of some selected keypoints. Such a design enjoys two clear benefits. First, incorporating human interaction mitigates the issue arising from numerous possibilities of transforming one image to another, and in turn enables finer control of local motions. Second, as the most basic form of interaction, keypoints help establish the correspondence across frames, enhancing the model to handle challenging cases (e.g., objects on the start and end frames are of different shapes and styles). It is noteworthy that our system also offers an "autopilot" mode, where we introduce a module to estimate the keypoints and refine the trajectory automatically, to simplify the usage in practice. Extensive experimental results demonstrate the appealing performance of Framer on various applications, such as image morphing, time-lapse video generation, cartoon interpolation, etc. The code, the model, and the interface will be released to facilitate further research.'
[25.10.2024 22:11] Response: **Image Prompt:** A surreal depiction of a large, abstract frame hanging in a dreamlike landscape, with fluid, morphing shapes transitioning between two distinct images within the frame. Keypoints are represented as colorful dots connected by delicate, wavy lines that create a sense of motion and interaction. The background features a gradient of soft colors, blending seamlessly to symbolize the smooth transition process. Floating around the frame are stylized representations of various objects in different shapes and styles, emphasizing the theme of transformation. 

**Label Text:** 'Framer: Interactive Frame Interpolation'
[25.10.2024 22:11] Generating image by prompt: **Image Prompt:** A surreal depiction of a large, abstract frame hanging in a dreamlike landscape, with fluid, morphing shapes transitioning between two distinct images within the frame. Keypoints are represented as colorful dots connected by delicate, wavy lines that create a sense of motion and interaction. The background features a gradient of soft colors, blending seamlessly to symbolize the smooth transition process. Floating around the frame are stylized representations of various objects in different shapes and styles, emphasizing the theme of transformation. 

**Label Text:** 'Framer: Interactive Frame Interpolation'.
[25.10.2024 22:11] Saving generated image from https://fal.media/files/elephant/mWXkFUNA-8rwbzVtz6-XU.png to ccd15b8785ec7920.jpg.
[26.10.2024 00:56] Read previous papers.
[26.10.2024 00:56] Get feed.
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17243
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16251
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18533
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18693
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18978
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18975
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18798
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18745
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18362
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18775
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18451
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18538
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18977
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18572
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18976
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18958
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15999
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17779
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15580
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18505
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18785
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18252
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17897
[26.10.2024 00:56] Extract page data from URL. URL: https://huggingface.co/papers/2410.16429
[26.10.2024 00:56] Extract page data from URL. URL: https://huggingface.co/papers/2410.18194
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18441
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18860
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18647
[26.10.2024 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18234
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 0. Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumptio...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 1. Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 2. Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the ge...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 3. The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 4. We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory ...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 5. We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 6. Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approa...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 7. Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their tr...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 8. Web development involves turning UI designs into functional webpages, which can be difficult for both beginners and experienced developers due to the complexity of HTML's hierarchical structures and styles. While Large Language Models (LLMs) have shown promise in generating source code, two major ch...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 9. Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first compre...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 10. In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data c...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 11. Segmenting an object in a video presents significant challenges. Each pixel must be accurately labelled, and these labels must remain consistent across frames. The difficulty increases when the segmentation is with arbitrary granularity, meaning the number of segments can vary arbitrarily, and masks...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 12. This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propo...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 13. Efficient long-context language modeling remains a significant challenge in Natural Language Processing (NLP). While Transformers dominate language tasks, they struggle with long sequences due to quadratic computational complexity in training and linearly scaling memory costs during inference. Recen...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 14. Recent years have witnessed a significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluati...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 15. Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through c...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 16. Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context -- this phenomenon, known as context-memory knowledge conflicts, can lead to undesirable model behaviour...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 17. Recent advancements in multimodal fusion have witnessed the remarkable success of vision-language (VL) models, which excel in various multimodal applications such as image captioning and visual question answering. However, building VL models requires substantial hardware resources, where efficiency ...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 18. Large Language Models (LLMs) are thought to struggle with arithmetic learning due to the inherent differences between language modeling and numerical computation, but concrete evidence has been lacking. This work responds to this claim through a two-side experiment. We first investigate whether LLMs...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 19. We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a high-quality 500GB subset of the Chinese Corpora Internet 3.0 (CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a novel two-stage hybrid filtering pipeline that significantly enhances data quality. To eval...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 20. Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editi...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 21. The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical d...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 22. Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earl...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 23. Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 24. Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution.   Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the n...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 25. In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of ...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 26. Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval hea...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 27. Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate ...
[26.10.2024 00:56] ********************************************************************************
[26.10.2024 00:56] Abstract 28. We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model....
[26.10.2024 00:56] Read previous papers.
[26.10.2024 00:56] Generating reviews via LLM API.
[26.10.2024 00:56] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—á–∏—Å–ª–µ–Ω–∏—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –±–µ–∑ —Ä–æ—Å—Ç–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∞–π–ª–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ä–∞—Å—á–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ –±–ª–æ–∫–∏.
[26.10.2024 00:56] Using data from previous issue: {"desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –∏ –º–µ—Ç–æ–¥–∞–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ HalluEditBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –º
[26.10.2024 00:56] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º LOGO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (LCM). LOGO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–∞ –∏ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å Llama-3-8B-Instruct-
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ScaleQuest - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –Ø–ë–ú –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ò—Å–ø–æ–ª—å–∑—É—è –Ω–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, ScaleQuest –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã —Å –Ω—É–ª—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 1 –º–∏–ª
[26.10.2024 00:56] Using data from previous issue: {"desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Framer –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–ª–∞–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É –¥–≤—É–º—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ –ø—É—Ç–µ–º –∑–∞–¥–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ 
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –∏–≥—Ä—ã, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä—ã, –≤—ã—Ö–æ–¥—è—â–µ–π –∑–∞ —Ä–∞–º–∫–∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–Ω–µ—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∏–≥—Ä—É Unbounded - —Å–∏–º—É–ª—è—Ç–æ—Ä –∂–∏–∑–Ω–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞, –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –í –∏–≥—Ä–µ –∏
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Code-as-Intermediary Translation (CIT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–¥ –∫–∞–∫ –ø–æ—Å—Ä–µ–¥–Ω–∏–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –ø–æ–Ω–∏–º–∞—Ç—å –º–µ–∂–º–æ–¥–∞–ª—å
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º STRING (ShifTed Rotray position embeddING) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ LLM —á–∞—Å—Ç–æ –Ω–µ –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å—å –∑–∞—è–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑-–∑–∞ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ä–∞—Å–ø
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ HTML-–∫–æ–¥–∞ –∏–∑ UI-–¥–∏–∑–∞–π–Ω–æ–≤. –ú–µ—Ç–æ–¥ Waffle –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É HTML, –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ UI –∏ HTML-–∫–æ–¥–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ W-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ç–µ—Ö–Ω–∏–∫–∞–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ VINE, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. VINE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–Ω–∞–ª–∏–∑ 
[26.10.2024 00:56] Using data from previous issue: {"desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—Ç–±–æ—Ä–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—É—é –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞—Ä
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MotionCLR - –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –∏ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
[26.10.2024 00:56] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Taipan - –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —Å–æ—á–µ—Ç–∞—é—â—É—é Mamba-2 —Å —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—ã–º–∏ —Å–ª–æ—è–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏. Taipan –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤–∞–∂–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –±–æ–ª—å—à–æ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏, –∏ —É—Å–∏–ª–∏–≤–∞–µ—Ç –∏—Ö –ø—Ä
[26.10.2024 00:56] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CAMEL-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º —è–∑—ã–∫–µ. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 8 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ 38 –ø–æ–¥–æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–ª–æ–∂–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ. CAMEL-Bench —Å–æ–¥–µ—Ä–∂–∏—Ç –æ
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –º–æ–¥–µ–ª–µ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –ø—Ä–æ—Ü–µ—Å—Å —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Stable Consistency Tuning (SCT), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü–µ–π –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä
[26.10.2024 00:56] Using data from previous issue: {"desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ SpARE, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—ã–±–æ—Ä–æ–º –∑–Ω–∞–Ω–∏–π –≤ LLM. SpARE –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—â–∏–µ –≤—ã–±–æ—Ä –∑–Ω–∞–Ω–∏–π, –∏ –ø—Ä–∏–º–µ
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ADEM-VL - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Å–ª–∏—è–Ω–∏–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ
[26.10.2024 00:56] Using data from previous issue: {"desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –æ–±—É—á–µ–Ω–∏—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–µ. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ö–æ—Ç—è LLM –º–æ–≥—É—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –æ–Ω–∏ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ LL
[26.10.2024 00:56] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω CCI3.0-HQ - –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–º–æ–º 500 –ì–ë, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ Chinese Corpora Internet 3.0. –î–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—â–∏–π –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å —Å 0.5 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —ç—Ç–æ–º
[26.10.2024 00:56] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ–±—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–∏–∑–±–µ–∂–Ω–æ–º—É —É—Ö—É–¥—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–±—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∞–≤–æ–∫. –ú–æ–¥–µ–ª–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ —á–µ–ª–æ–≤–µ–∫–∞ (RLHF) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –æ–±—Ä–∞–∑—Ü—ã –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ —Å—Ç–∞—Ä—ã—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ResFormer. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ—è—Ö –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –æ—Ç –∑–Ω–∞—á–µ–Ω–∏–π –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –∫–æ –≤—Å–µ–º –ø–æ—Å–ª–µ–¥—É—é—â–∏–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è SVFormer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
[26.10.2024 00:56] Querying the API.
[26.10.2024 00:56] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper, we introduce Pantograph, a tool that provides a versatile interface to the Lean 4 proof assistant and enables efficient proof search via powerful search algorithms such as Monte Carlo Tree Search. In addition, Pantograph enables high-level reasoning by enabling a more robust handling of Lean 4's inference steps. We provide an overview of Pantograph's architecture and features. We also report on an illustrative use case: using machine learning models and proof sketches to prove Lean 4 theorems. Pantograph's innovative features pave the way for more advanced machine learning models to perform complex proof searches and high-level reasoning, equipping future researchers to design more versatile and powerful theorem provers.
[26.10.2024 00:56] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç Pantograph –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º —Å –ø–æ–º–æ—â—å—é –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. Pantograph –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ Lean 4 –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–∫–∞—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ–∏—Å–∫ –º–µ—Ç–æ–¥–æ–º –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–ª–∞–≥–æ–¥–∞—Ä—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ —à–∞–≥–æ–≤ –≤—ã–≤–æ–¥–∞ –≤ Lean 4. –ê–≤—Ç–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Pantograph –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –≤ Lean 4 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
  "emoji": "üß†",
  "title": "Pantograph: –º–æ—Å—Ç –º–µ–∂–¥—É –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º —Ç–µ–æ—Ä–µ–º"
}
[26.10.2024 00:56] OpenAI request. Model: gpt-4o-mini. Prompt: You are an expert classifier of machine learning research papers. Analyze the following research paper text and classify it into one or more relevant categories from the list below. Consider the paper's main contributions, methodologies, and applications.

Categories:
1. DATASET: Papers that introduce new datasets or make significant modifications to existing ones
2. DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
3. BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
4. AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
5. CV: Papers developing computer vision methods or visual processing systems
6. RL: Papers investigating reinforcement learning theory or applications
7. RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
8. RAG: Papers advancing retrieval-augmented generation techniques
9. PLP: Papers about Programming Language Processing models or programming benchmarks
10. INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
11. 3D: Papers on 3D content generation, processing, or understanding
12. AUDIO: Papers advancing speech/audio processing or generation
13. VIDEO: Papers on video analysis, generation, or understanding
14. MULTIMODAL: Papers combining multiple input/output modalities
15. MATH: Papers focused on mathematical theory and algorithms
16. MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities
17. ARCHITECTURE: Papers proposing novel neural architectures or components
18. MEDICINE: Papers applying ML to medical/healthcare domains
19. TRAINING: Papers improving model training or fine-tuning methods
20. ROBOTICS: Papers on robotic systems and embodied AI
21. AGI: Papers discussing artificial general intelligence concepts
22. GAMES: Papers applying ML to games or game development
23. INTERPRETABILITY: Papers analyzing model behavior and explanations
24. REASONING: Papers enhancing logical reasoning capabilities
25. TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
26. GRAPHS: Papers advancing graph neural networks and applications
27. ETHICS: Papers addressing AI ethics, fairness, and bias
28. SECURITY: Papers on model security and adversarial robustness
29. QUANTUM: Papers combining quantum computing and ML
30. EDGE_COMPUTING: Papers on ML deployment for resource-constrained devices
31. OPTIMIZATION: Papers advancing training optimization methods
32. SURVEY: Papers comprehensively reviewing research areas
33. DIFFUSION: Papers on diffusion-based generative models
34. ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
35. STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
36. HALLUCINATION: Papers about the hallucinations in language models, hallucinations analysis and mitigation

Return only JSON with flat array of categories that match the given text.

Paper text to classify:

"Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper, we introduce Pantograph, a tool that provides a versatile interface to the Lean 4 proof assistant and enables efficient proof search via powerful search algorithms such as Monte Carlo Tree Search. In addition, Pantograph enables high-level reasoning by enabling a more robust handling of Lean 4's inference steps. We provide an overview of Pantograph's architecture and features. We also report on an illustrative use case: using machine learning models and proof sketches to prove Lean 4 theorems. Pantograph's innovative features pave the way for more advanced machine learning models to perform complex proof searches and high-level reasoning, equipping future researchers to design more versatile and powerful theorem provers."

[26.10.2024 00:56] Response: ```json
["MATH", "ARCHITECTURE", "REASONING"]
```
[26.10.2024 00:56] Get embedding for a paper via LLM API.
[26.10.2024 00:56] Querying the API.
[26.10.2024 00:56] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution.   Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the nuanced patterns needed for tasks like Autoformalization or code generation.   Methods that do consider the target distribution often rely on simplistic, sometimes noisy, representations, like hashed n-gram features, which can lead to collisions and introduce noise.   We introduce ZIP-FIT, a data selection framework that uses gzip compression to directly measure alignment between potential training data and the target task distribution.   In extensive evaluations on Autoformalization and Python code generation, ZIP-FIT significantly outperforms leading baselines like DSIR and D4.   Models trained on ZIP-FIT-selected data achieve their lowest cross-entropy loss up to 85.1\% faster than baselines, demonstrating that better task alignment leads to more efficient learning.   In addition, ZIP-FIT performs selection up to 65.8\% faster than DSIR and two orders of magnitude faster than D4.   Notably, ZIP-FIT shows that smaller, well-aligned datasets often outperform larger but less targeted ones, demonstrating that a small amount of higher quality data is superior to a large amount of lower quality data.   Our results imply that task-aware data selection is crucial for efficient domain adaptation, and that compression offers a principled way to measure task alignment.   By showing that targeted data selection can dramatically improve task-specific performance, our work provides new insights into the relationship between data quality, task alignment, and model learning efficiency.
[26.10.2024 00:56] Response: {
  "desc": "ZIP-FIT - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–∂–∞—Ç–∏–µ gzip –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º–∏ –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ —Ü–µ–ª–µ–≤—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∑–∞–¥–∞—á–∏. –í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, ZIP-FIT –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤ –∑–∞–¥–∞—á–∞—Ö –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –Ω–∞ Python. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö ZIP-FIT, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ–π –æ—à–∏–±–∫–∏ –¥–æ 85.1% –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ, –Ω–æ —Ö–æ—Ä–æ—à–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –±–æ–ª—å—à–∏–µ, –Ω–æ –º–µ–Ω–µ–µ —Ü–µ–ª–µ–≤—ã–µ.",
  "emoji": "üéØ",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[26.10.2024 00:56] OpenAI request. Model: gpt-4o-mini. Prompt: You are an expert classifier of machine learning research papers. Analyze the following research paper text and classify it into one or more relevant categories from the list below. Consider the paper's main contributions, methodologies, and applications.

Categories:
1. DATASET: Papers that introduce new datasets or make significant modifications to existing ones
2. DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
3. BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
4. AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
5. CV: Papers developing computer vision methods or visual processing systems
6. RL: Papers investigating reinforcement learning theory or applications
7. RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
8. RAG: Papers advancing retrieval-augmented generation techniques
9. PLP: Papers about Programming Language Processing models or programming benchmarks
10. INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
11. 3D: Papers on 3D content generation, processing, or understanding
12. AUDIO: Papers advancing speech/audio processing or generation
13. VIDEO: Papers on video analysis, generation, or understanding
14. MULTIMODAL: Papers combining multiple input/output modalities
15. MATH: Papers focused on mathematical theory and algorithms
16. MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities
17. ARCHITECTURE: Papers proposing novel neural architectures or components
18. MEDICINE: Papers applying ML to medical/healthcare domains
19. TRAINING: Papers improving model training or fine-tuning methods
20. ROBOTICS: Papers on robotic systems and embodied AI
21. AGI: Papers discussing artificial general intelligence concepts
22. GAMES: Papers applying ML to games or game development
23. INTERPRETABILITY: Papers analyzing model behavior and explanations
24. REASONING: Papers enhancing logical reasoning capabilities
25. TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
26. GRAPHS: Papers advancing graph neural networks and applications
27. ETHICS: Papers addressing AI ethics, fairness, and bias
28. SECURITY: Papers on model security and adversarial robustness
29. QUANTUM: Papers combining quantum computing and ML
30. EDGE_COMPUTING: Papers on ML deployment for resource-constrained devices
31. OPTIMIZATION: Papers advancing training optimization methods
32. SURVEY: Papers comprehensively reviewing research areas
33. DIFFUSION: Papers on diffusion-based generative models
34. ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
35. STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
36. HALLUCINATION: Papers about the hallucinations in language models, hallucinations analysis and mitigation

Return only JSON with flat array of categories that match the given text.

Paper text to classify:

"Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution.   Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the nuanced patterns needed for tasks like Autoformalization or code generation.   Methods that do consider the target distribution often rely on simplistic, sometimes noisy, representations, like hashed n-gram features, which can lead to collisions and introduce noise.   We introduce ZIP-FIT, a data selection framework that uses gzip compression to directly measure alignment between potential training data and the target task distribution.   In extensive evaluations on Autoformalization and Python code generation, ZIP-FIT significantly outperforms leading baselines like DSIR and D4.   Models trained on ZIP-FIT-selected data achieve their lowest cross-entropy loss up to 85.1\% faster than baselines, demonstrating that better task alignment leads to more efficient learning.   In addition, ZIP-FIT performs selection up to 65.8\% faster than DSIR and two orders of magnitude faster than D4.   Notably, ZIP-FIT shows that smaller, well-aligned datasets often outperform larger but less targeted ones, demonstrating that a small amount of higher quality data is superior to a large amount of lower quality data.   Our results imply that task-aware data selection is crucial for efficient domain adaptation, and that compression offers a principled way to measure task alignment.   By showing that targeted data selection can dramatically improve task-specific performance, our work provides new insights into the relationship between data quality, task alignment, and model learning efficiency."

[26.10.2024 00:56] Response: ```json
["DATA", "TRAINING", "TRANSFER_LEARNING"]
```
[26.10.2024 00:56] Get embedding for a paper via LLM API.
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏ Transformer –≤ –æ–±–ª–∞—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥—Å–ª–æ–≤ (SWE), –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ byte-pair encoding (BPE) –∏ –ø–æ–¥—Ö–æ–¥–µ Wo
[26.10.2024 00:56] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DeCoRe –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Transformer, –æ—Ç–≤–µ—á–∞—é—â–∏—Ö –∑–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. DeCoRe —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –≤—ã—Ö–æ
[26.10.2024 00:56] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è—Ö —Ä–æ–±–æ—Ç–æ–≤. –û–Ω–∏ –ø—Ä–æ–≤–µ–ª–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, —Å–æ–±—Ä–∞–≤ –±–æ–ª–µ–µ 40 000 –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –∏ –≤—ã–ø–æ–ª–Ω–∏–≤ –±–æ–ª–µ–µ 15 000 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤ —Ä–æ–±–æ—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ
[26.10.2024 00:56] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–µ–ª—å–Ω–æ–µ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ö–µ–º—É –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –≤–∞–∂–Ω–æ—Å—Ç–Ω—É—é –≤—ã–±–æ—Ä–∫—É. –î–ª—è —Å–ª—É—á–∞—è –¥–≤—É—Ö –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —á–µ—Ä–Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —É—Å–ª–æ–≤–∏—è –¥–ª—è 100% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω—è—Ç–∏—è –∏ –ø–æ
[26.10.2024 00:56] Loading Chinese text from previous data.
[26.10.2024 00:56] Renaming data file.
[26.10.2024 00:56] Renaming previous data. hf_papers.json to ./d/2024-10-25.json
[26.10.2024 00:56] Saving new data file.
[26.10.2024 00:56] Generating page.
[26.10.2024 00:56] Renaming previous page.
[26.10.2024 00:56] Renaming previous data. index.html to ./d/2024-10-25.html
[26.10.2024 00:56] [Experimental] Generating Chinese page for reading.
[26.10.2024 00:56] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÂØπÊØî', 'pinyin': 'du√¨ b«ê', 'trans': 'contrast'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'}, {'word': 'Ë°®Á§∫', 'pinyin': 'bi«éo sh√¨', 'trans': 'representation'}, {'word': 'Â≠¶‰π†', 'pinyin': 'xu√© x√≠', 'trans': 'learning'}, {'word': 'Â§ßÊâπÈáè', 'pinyin': 'd√† pƒ´ li√†ng', 'trans': 'large-scale'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÂèóÈôê‰∫é', 'pinyin': 'sh√≤u xi√†n y√∫', 'trans': 'limited by'}, {'word': 'GPU', 'pinyin': 'GPU', 'trans': 'GPU'}, {'word': 'ÂÜÖÂ≠ò', 'pinyin': 'n√®i c√∫n', 'trans': 'memory'}, {'word': 'Ê∂àËÄó', 'pinyin': 'xiƒÅo h√†o', 'trans': 'consumption'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´ y√∫', 'trans': 'based on'}, {'word': 'Âùó', 'pinyin': 'ku√†i', 'trans': 'block'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨ su√†n', 'trans': 'computation'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'ÈÅøÂÖç', 'pinyin': 'b√¨ mi«én', 'trans': 'avoid'}, {'word': 'ÂÖ®ÈÉ®', 'pinyin': 'qu√°n b√π', 'trans': 'entire'}, {'word': 'ÂÆû‰æãÂåñ', 'pinyin': 'sh√≠ l√¨ hu√†', 'trans': 'instantiate'}, {'word': 'Áõ∏‰ººÂ∫¶', 'pinyin': 'xiƒÅng s√¨ d√π', 'trans': 'similarity'}, {'word': 'Áü©Èòµ', 'pinyin': 'j«î zh√®n', 'trans': 'matrix'}, {'word': 'Ê≠§Â§ñ', 'pinyin': 'c«ê w√†i', 'trans': 'moreover'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'}, {'word': 'Â§öÁ∫ß', 'pinyin': 'du≈ç j√≠', 'trans': 'multi-level'}, {'word': 'ÂàÜÂ∏ÉÂºè', 'pinyin': 'fƒìn b√π sh√¨', 'trans': 'distributed'}, {'word': 'Á≥ªÁªü', 'pinyin': 'x√¨ t«íng', 'trans': 'system'}, {'word': 'Â±ÇÊ¨°', 'pinyin': 'c√©ng c√¨', 'trans': 'hierarchy'}, {'word': 'ÁªìÊûÑ', 'pinyin': 'ji√© g√≤u', 'trans': 'structure'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«én sh√¨', 'trans': 'show'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤ zh«én', 'trans': 'scale'}, {'word': 'ÂâçÊâÄÊú™Êúâ', 'pinyin': 'qi√°n su«í w√®i y«íu', 'trans': 'unprecedented'}, {'word': 'ÊâπÈáè', 'pinyin': 'pƒ´ li√†ng', 'trans': 'batch'}, {'word': 'Â§ßÂ∞è', 'pinyin': 'd√† xi«éo', 'trans': 'size'}, {'word': '‰øùÊåÅ', 'pinyin': 'b«éo ch√≠', 'trans': 'maintain'}, {'word': 'Á≤æÂ∫¶', 'pinyin': 'jƒ´ng d√π', 'trans': 'accuracy'}, {'word': '‰æãÂ¶Ç', 'pinyin': 'l√¨ r√∫', 'trans': 'for example'}, {'word': '‰ΩøÁî®', 'pinyin': 'sh«ê y√≤ng', 'trans': 'use'}, {'word': 'A800', 'pinyin': 'A800', 'trans': 'A800'}, {'word': '80GB', 'pinyin': '80GB', 'trans': '80GB'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'train'}, {'word': 'CLIP-ViT-L/14', 'pinyin': 'CLIP-ViT-L/14', 'trans': 'CLIP-ViT-L/14'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': '‰∏é', 'pinyin': 'y«î', 'trans': 'with'}, {'word': 'ÊúÄÂÖàËøõ', 'pinyin': 'zu√¨ xiƒÅn j√¨n', 'trans': 'most advanced'}, {'word': 'ÂÜÖÂ≠òÈ´òÊïà', 'pinyin': 'n√®i c√∫n gƒÅo xi√†o', 'trans': 'memory-efficient'}, {'word': 'Ëß£ÂÜ≥ÊñπÊ°à', 'pinyin': 'jiƒõ ju√© fƒÅng √†n', 'trans': 'solution'}, {'word': 'Áõ∏ÊØî', 'pinyin': 'xiƒÅng b«ê', 'trans': 'compared to'}, {'word': 'Áõ∏ÂΩì', 'pinyin': 'xiƒÅng dƒÅng', 'trans': 'comparable'}, {'word': 'ÈÄüÂ∫¶', 'pinyin': 's√π d√π', 'trans': 'speed'}, {'word': 'ÂêåÊó∂', 'pinyin': 't√≥ng sh√≠', 'trans': 'simultaneously'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Êï∞ÈáèÁ∫ß', 'pinyin': 'sh√π li√†ng j√≠', 'trans': 'order of magnitude'}, {'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«én sh«éo', 'trans': 'reduce'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†i m«é', 'trans': 'code'}, {'word': 'Â∞Ü', 'pinyin': 'jiƒÅng', 'trans': 'will'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'ÂèëÂ∏É', 'pinyin': 'fƒÅ b√π', 'trans': 'release'}]
[26.10.2024 00:56] Renaming previous Chinese page.
[26.10.2024 00:56] Renaming previous data. zh.html to ./d/2024-10-25_zh_reading_task.html
[26.10.2024 00:56] Writing result.
[26.10.2024 00:56] Writing Chinese reading task.
[26.10.2024 00:56] Renaming log file.
[26.10.2024 00:56] Renaming previous data. log.txt to ./logs/2024-10-26_last_log.txt
