[11.06.2025 08:17] Read previous papers.
[11.06.2025 08:17] Generating top page (month).
[11.06.2025 08:17] Writing top page (month).
[11.06.2025 09:13] Read previous papers.
[11.06.2025 09:13] Get feed.
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06751
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08009
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08002
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07927
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07177
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04614
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09040
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05167
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08887
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07932
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05928
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07047
[11.06.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05700
[11.06.2025 09:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.06.2025 09:13] No deleted papers detected.
[11.06.2025 09:13] Downloading and parsing papers (pdf, html). Total: 13.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.06751.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.06751.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.06751.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.08009.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.08009.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.08009.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.08002.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.08002.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.08002.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.07927.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.07927.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.07927.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.07177.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.07177.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.07177.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.04614.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.04614.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.04614.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.09040.
[11.06.2025 09:13] Downloading paper 2506.09040 from http://arxiv.org/pdf/2506.09040v1...
[11.06.2025 09:13] Failed to download and parse paper https://huggingface.co/papers/2506.09040: No /Root object! - Is this really a PDF?
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.05167.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.05167.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.05167.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.08887.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.08887.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.08887.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.07932.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.07932.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.07932.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.05928.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.05928.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.05928.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.07047.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.07047.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.07047.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2506.05700.
[11.06.2025 09:13] Extra JSON file exists (./assets/json/2506.05700.json), skip PDF parsing.
[11.06.2025 09:13] Paper image links file exists (./assets/img_data/2506.05700.json), skip HTML parsing.
[11.06.2025 09:13] Success.
[11.06.2025 09:13] Enriching papers with extra data.
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 0. LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  					AI-generated summary 				 This paper evaluates geopolitical biases in LLMs with respect to vario...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 1. Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  					AI-generated summary 				 We introduce Self Forcing, a novel training paradigm for a...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 2. A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  					AI-generated summary 				 Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D ...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 3. The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  					AI-generated summary 				 Inequality proving, crucial across diverse scientifi...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 4. Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  					AI-generated summary 				 Advancements in diffusion models have significantly improved video quality, directing attention...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 5. A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  					AI-generated summary 				 In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimod...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 6. Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  					AI-generated summary 				 Typical large vision-language models (LVLMs) apply autoreg...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 7. ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  					AI-generated summary 				 Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging externa...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 8. The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  					AI-generated summary 				 The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is ...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 9. A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  					AI-generated summary 				 We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-train...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 10. A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  					AI-generated summary 				 Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to ...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 11. Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tack...
[11.06.2025 09:13] ********************************************************************************
[11.06.2025 09:13] Abstract 12. RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  					AI-generated summary 				 Recent advances in larg...
[11.06.2025 09:13] Read previous papers.
[11.06.2025 09:13] Generating reviews via LLM API.
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#ethics", "#alignment", "#data", "#dataset"], "emoji": "üåç", "ru": {"title": "–ì–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –≤ LLM: –≤—ã–∑–æ–≤ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏—á–µ
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#video", "#diffusion", "#training"], "emoji": "üé¨", "ru": {"title": "Self Forcing: —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Self Forcing. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É 
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#synthetic", "#3d", "#multimodal"], "emoji": "üåê", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D-–º–∏—Ä–∞", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ 3D-—Å—Ü–µ–Ω, –¥–æ—Å—Ç–∏–≥–∞—é—â–∞—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö 
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#survey", "#data", "#benchmark", "#dataset", "#reasoning", "#math"], "emoji": "üìä", "ru": {"title": "LLM –º–æ–≥—É—Ç –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç, –Ω–æ –Ω–µ –º–æ–≥—É—Ç –¥–æ–∫–∞–∑–∞—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ —Å—Ç—Ä–æ–≥
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Frame Guidance –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–∫–∞–¥—Ä–æ–≤—ã–µ —Å–∏–≥
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#multimodal", "#optimization", "#rl", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ GUI —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π –∫—Ä–∏—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–µ–¥–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π –∫—Ä–∏—Ç–∏–∫–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#benchmark", "#games", "#cv", "#multimodal"], "emoji": "üß†", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (ASVR) —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#rag", "#alignment", "#long_context"], "emoji": "üîç", "ru": {"title": "ECoRAG: –£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤", "desc": "ECoRAG - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (ODQA). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#video", "#multimodal"], "emoji": "üé•", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–∏–¥–µ–æ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DiscoVLA - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#synthetic", "#architecture", "#3d"], "emoji": "üóúÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ 3D-–¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "Squeeze3D - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∂–∞—Ç–∏—è 3D-–¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω–µ–π —Å–∂–∞—Ç–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∏–∑—É
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–ì–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–∞—è —Å–º–µ—Å—å –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –≥–µ—Ç–µ—Ä–æ–≥
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#benchmark", "#dataset", "#math"], "emoji": "üß†", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –æ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ª–æ–≥–∏–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Mathesis - –ø–µ—Ä–≤—ã–π —Å–∫–≤–æ–∑–Ω–æ–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å
[11.06.2025 09:13] Using data from previous issue: {"categories": ["#data", "#multimodal", "#reasoning", "#training", "#healthcare", "#open_source"], "emoji": "üìä", "ru": {"title": "–£–º–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏", "desc": "RKEFino1 - —ç—Ç–æ –º–æ–¥–µ–ª—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —É–ª—É—á—à–µ–Ω–Ω–∞—è –∑–Ω–∞–Ω–∏—è–º–∏ –æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ
[11.06.2025 09:13] Trying to get texts in Chinese.
[11.06.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  					AI-generated summary 				 This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.
[11.06.2025 09:13] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "invalid_request_error", "param": null, "code": null}
[11.06.2025 09:13] Failed to get Chinese text: 'choices'
[11.06.2025 09:13] Renaming data file.
[11.06.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-06-11.json
[11.06.2025 09:13] Saving new data file.
[11.06.2025 09:13] Generating page.
[11.06.2025 09:13] Renaming previous page.
[11.06.2025 09:13] Renaming previous data. index.html to ./d/2025-06-11.html
[11.06.2025 09:13] [Experimental] Generating Chinese page for reading.
[11.06.2025 09:13] Writing result.
[11.06.2025 09:13] Renaming log file.
[11.06.2025 09:13] Renaming previous data. log.txt to ./logs/2025-06-11_last_log.txt
