[11.06.2025 21:11] Read previous papers.
[11.06.2025 21:11] Generating top page (month).
[11.06.2025 21:11] Writing top page (month).
[11.06.2025 22:11] Read previous papers.
[11.06.2025 22:11] Get feed.
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06751
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09040
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08672
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07927
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08009
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08002
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04614
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07177
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05167
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08279
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07932
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07045
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08887
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08500
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08300
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07976
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05928
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05700
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07047
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04688
[11.06.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04020
[11.06.2025 22:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.06.2025 22:11] No deleted papers detected.
[11.06.2025 22:11] Downloading and parsing papers (pdf, html). Total: 21.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.06751.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.06751.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.06751.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.09040.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.09040.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.09040.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.08672.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.08672.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.08672.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.07927.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.07927.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.07927.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.08009.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.08009.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.08009.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.08002.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.08002.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.08002.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.04614.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.04614.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.04614.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.07177.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.07177.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.07177.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.05167.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.05167.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.05167.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.08279.
[11.06.2025 22:11] Downloading paper 2506.08279 from http://arxiv.org/pdf/2506.08279v1...
[11.06.2025 22:11] Failed to download and parse paper https://huggingface.co/papers/2506.08279: No /Root object! - Is this really a PDF?
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.07932.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.07932.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.07932.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.07045.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.07045.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.07045.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.08887.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.08887.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.08887.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.08500.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.08500.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.08500.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.08300.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.08300.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.08300.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.07976.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.07976.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.07976.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.05928.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.05928.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.05928.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.05700.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.05700.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.05700.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.07047.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.07047.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.07047.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.04688.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.04688.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.04688.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2506.04020.
[11.06.2025 22:11] Extra JSON file exists (./assets/json/2506.04020.json), skip PDF parsing.
[11.06.2025 22:11] Paper image links file exists (./assets/img_data/2506.04020.json), skip HTML parsing.
[11.06.2025 22:11] Success.
[11.06.2025 22:11] Enriching papers with extra data.
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 0. LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  					AI-generated summary 				 This paper evaluates geopolitical biases in LLMs with respect to vario...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 1. Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  					AI-generated summary 				 Typical large vision-language models (LVLMs) apply autoreg...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 2. RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.  					AI-generated summary 				 Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviat...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 3. The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  					AI-generated summary 				 Inequality proving, crucial across diverse scientifi...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 4. Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  					AI-generated summary 				 We introduce Self Forcing, a novel training paradigm for a...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 5. A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  					AI-generated summary 				 Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D ...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 6. A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  					AI-generated summary 				 In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimod...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 7. Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  					AI-generated summary 				 Advancements in diffusion models have significantly improved video quality, directing attention...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 8. ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  					AI-generated summary 				 Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging externa...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 9. Mirage generates realistic video from audio inputs, integrating with speech synthesis to create compelling multimodal content through a unified, self-attention-based training approach.  					AI-generated summary 				 From professional filmmaking to user-generated content, creators and consumers have...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 10. A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  					AI-generated summary 				 We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-train...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 11. A dataset with annotations aids in fine-tuning MLLMs for accurate detection and localization of AI-generated images with meaningful explanations.  					AI-generated summary 				 The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection metho...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 12. The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  					AI-generated summary 				 The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is ...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 13. CONFLICTS, a benchmark for evaluating how LLMs handle knowledge conflicts in RAG, reveals significant challenges in conflict resolution but shows improvement with explicit prompting.  					AI-generated summary 				 Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large ...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 14. Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.  					AI-generated summary 				 Large language models (LLMs) use data to learn about the world in order t...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 15. Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.  					AI-generated summary 				 The current paradigm of test-time scaling relies on generating long reasoning trac...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 16. A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  					AI-generated summary 				 Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to ...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 17. RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  					AI-generated summary 				 Recent advances in larg...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 18. Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tack...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 19. MMRefine evaluates the error refinement capabilities of Multimodal Large Language Models through a benchmark that categorizes errors and identifies performance bottlenecks.  					AI-generated summary 				 This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the erro...
[11.06.2025 22:11] ********************************************************************************
[11.06.2025 22:11] Abstract 20. QQSUM-RAG extends Retrieval-Augmented Generation to provide diverse, representative Key Point summaries with quantified opinions for product question answering, outperforming existing methods.  					AI-generated summary 				 Review-based Product Question Answering (PQA) allows e-commerce platforms t...
[11.06.2025 22:11] Read previous papers.
[11.06.2025 22:11] Generating reviews via LLM API.
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#ethics", "#alignment", "#data", "#dataset"], "emoji": "üåç", "ru": {"title": "–ì–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –≤ LLM: –≤—ã–∑–æ–≤ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏—á–µ
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#benchmark", "#games", "#cv", "#multimodal"], "emoji": "üß†", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (ASVR) —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#rl", "#optimization", "#small_models", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RuleReasoner - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –≤ –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#survey", "#data", "#benchmark", "#dataset", "#reasoning", "#math"], "emoji": "üìä", "ru": {"title": "LLM –º–æ–≥—É—Ç –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç, –Ω–æ –Ω–µ –º–æ–≥—É—Ç –¥–æ–∫–∞–∑–∞—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ —Å—Ç—Ä–æ–≥
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#video", "#diffusion", "#training"], "emoji": "üé¨", "ru": {"title": "Self Forcing: —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Self Forcing. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É 
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#synthetic", "#3d", "#multimodal"], "emoji": "üåê", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D-–º–∏—Ä–∞", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ 3D-—Å—Ü–µ–Ω, –¥–æ—Å—Ç–∏–≥–∞—é—â–∞—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö 
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#multimodal", "#optimization", "#rl", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ GUI —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π –∫—Ä–∏—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–µ–¥–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π –∫—Ä–∏—Ç–∏–∫–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Frame Guidance –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–∫–∞–¥—Ä–æ–≤—ã–µ —Å–∏–≥
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#rag", "#alignment", "#long_context"], "emoji": "üîç", "ru": {"title": "ECoRAG: –£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤", "desc": "ECoRAG - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (ODQA). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#multimodal", "#audio", "#games"], "emoji": "üé¨", "ru": {"title": "–ó–≤—É–∫ –æ–∂–∏–≤–∞–µ—Ç: Mirage –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –≤–∏–¥–µ–æ", "desc": "Mirage - —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#synthetic", "#architecture", "#3d"], "emoji": "üóúÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ 3D-–¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "Squeeze3D - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∂–∞—Ç–∏—è 3D-–¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω–µ–π —Å–∂–∞—Ç–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∏–∑—É
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#data", "#training", "#reasoning", "#optimization", "#hallucinations", "#dataset"], "emoji": "üîç", "ru": {"title": "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ò–ò-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#video", "#multimodal"], "emoji": "üé•", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–∏–¥–µ–æ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DiscoVLA - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#rag", "#alignment"], "emoji": "üß†", "ru": {"title": "–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –¥–ª—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CONFLICTS - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–∑—Ä–µ—à–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –∑–Ω–∞–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ R
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#synthetic", "#data"], "emoji": "üìö", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π: –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ", "desc": "–î–∞—Ç–∞—Å–µ—Ç Institutional Books 1.0 –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—à–∏—Ä–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –∫–Ω–∏–≥ –∏–∑ –ø—É–±–ª–∏—á–Ω–æ–≥–æ –¥–æ—Å—Ç–æ—è–Ω–∏—è –ì–∞—Ä–≤–∞—Ä–¥—Å–∫–æ–π –±–∏–±–ª–∏
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#agents", "#open_source", "#training"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ–±-–∞–≥–µ–Ω—Ç—ã: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–ì–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–∞—è —Å–º–µ—Å—å –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –≥–µ—Ç–µ—Ä–æ–≥
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#data", "#multimodal", "#reasoning", "#training", "#healthcare", "#open_source"], "emoji": "üìä", "ru": {"title": "–£–º–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏", "desc": "RKEFino1 - —ç—Ç–æ –º–æ–¥–µ–ª—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —É–ª—É—á—à–µ–Ω–Ω–∞—è –∑–Ω–∞–Ω–∏—è–º–∏ –æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#benchmark", "#dataset", "#math"], "emoji": "üß†", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –æ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ª–æ–≥–∏–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Mathesis - –ø–µ—Ä–≤—ã–π —Å–∫–≤–æ–∑–Ω–æ–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#multimodal", "#open_source"], "emoji": "üîç", "ru": {"title": "MMRefine: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MMRefine - –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å 
[11.06.2025 22:11] Using data from previous issue: {"categories": ["#rag", "#optimization", "#multimodal", "#open_source", "#games"], "emoji": "üõçÔ∏è", "ru": {"title": "–£–º–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ —Ç–æ–≤–∞—Ä–∞—Ö —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –º–Ω–µ–Ω–∏–π –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ QQSUM-RAG –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∑—ã–≤–æ–≤. –≠—Ç–æ
[11.06.2025 22:11] Loading Chinese text from previous data.
[11.06.2025 22:11] Renaming data file.
[11.06.2025 22:11] Renaming previous data. hf_papers.json to ./d/2025-06-11.json
[11.06.2025 22:11] Saving new data file.
[11.06.2025 22:11] Generating page.
[11.06.2025 22:11] Renaming previous page.
[11.06.2025 22:11] Renaming previous data. index.html to ./d/2025-06-11.html
[11.06.2025 22:11] [Experimental] Generating Chinese page for reading.
[11.06.2025 22:11] Chinese vocab [{'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': 'Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√†x√≠ng y«îy√°n m√≥x√≠ng', 'trans': 'large language models'}, {'word': 'Âú∞ÁºòÊîøÊ≤ª', 'pinyin': 'd√¨yu√°n zh√®ngzh√¨', 'trans': 'geopolitical'}, {'word': 'ÂÅèËßÅ', 'pinyin': 'piƒÅnji√†n', 'trans': 'bias'}, {'word': 'Ê∂âÂèä', 'pinyin': 'sh√®j√≠', 'trans': 'involve'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ênr√π', 'trans': 'introduce'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': '‰∏≠Á´ã', 'pinyin': 'zh≈çngl√¨', 'trans': 'neutral'}, {'word': 'ÂØπÁ´ã', 'pinyin': 'du√¨l√¨', 'trans': 'opposing'}, {'word': 'ËßÇÁÇπ', 'pinyin': 'guƒÅndi«én', 'trans': 'viewpoint'}, {'word': 'ÂÄæÂêë‰∫é', 'pinyin': 'qƒ´ngxi√†ngy√∫', 'trans': 'tend towards'}, {'word': 'Âèô‰∫ã', 'pinyin': 'x√πsh√¨', 'trans': 'narrative'}, {'word': 'ÁÆÄÂçï', 'pinyin': 'ji«éndƒÅn', 'trans': 'simple'}, {'word': 'ÂéªÂÅèËßÅ', 'pinyin': 'q√π piƒÅnji√†n', 'trans': 'debias'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'ÊïàÊûú', 'pinyin': 'xi√†ogu«í', 'trans': 'effect'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íuxi√†n', 'trans': 'limited'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'}, {'word': 'ÂèÇ‰∏éËÄÖ', 'pinyin': 'cƒÅny√πzhƒõ', 'trans': 'participant'}, {'word': 'Ê†áÁ≠æ', 'pinyin': 'biƒÅoqiƒÅn', 'trans': 'label'}, {'word': 'Êõ¥Êîπ', 'pinyin': 'gƒìngg«éi', 'trans': 'change'}, {'word': 'ÊïèÊÑü', 'pinyin': 'm«êng«én', 'trans': 'sensitive'}, {'word': 'ÊîæÂ§ß', 'pinyin': 'f√†ngd√†', 'trans': 'amplify'}, {'word': 'ËØÜÂà´', 'pinyin': 'sh√≠bi√©', 'trans': 'identify'}, {'word': '‰∏ç‰∏ÄËá¥', 'pinyin': 'b√π yƒ´zh√¨', 'trans': 'inconsistent'}]
[11.06.2025 22:11] Renaming previous Chinese page.
[11.06.2025 22:11] Renaming previous data. zh.html to ./d/2025-06-10_zh_reading_task.html
[11.06.2025 22:11] Writing Chinese reading task.
[11.06.2025 22:11] Writing result.
[11.06.2025 22:11] Renaming log file.
[11.06.2025 22:11] Renaming previous data. log.txt to ./logs/2025-06-11_last_log.txt
