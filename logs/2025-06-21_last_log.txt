[20.06.2025 23:11] Read previous papers.
[20.06.2025 23:11] Generating top page (month).
[20.06.2025 23:11] Writing top page (month).
[21.06.2025 01:58] Read previous papers.
[21.06.2025 01:58] Get feed.
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14965
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09827
[21.06.2025 01:58] Extract page data from URL. URL: https://huggingface.co/papers/2506.15564
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15154
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14837
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15455
[21.06.2025 01:58] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.06.2025 01:58] No deleted papers detected.
[21.06.2025 01:58] Downloading and parsing papers (pdf, html). Total: 6.
[21.06.2025 01:58] Downloading and parsing paper https://huggingface.co/papers/2506.14965.
[21.06.2025 01:58] Extra JSON file exists (./assets/json/2506.14965.json), skip PDF parsing.
[21.06.2025 01:58] Paper image links file exists (./assets/img_data/2506.14965.json), skip HTML parsing.
[21.06.2025 01:58] Success.
[21.06.2025 01:58] Downloading and parsing paper https://huggingface.co/papers/2506.09827.
[21.06.2025 01:58] Extra JSON file exists (./assets/json/2506.09827.json), skip PDF parsing.
[21.06.2025 01:58] Paper image links file exists (./assets/img_data/2506.09827.json), skip HTML parsing.
[21.06.2025 01:58] Success.
[21.06.2025 01:58] Downloading and parsing paper https://huggingface.co/papers/2506.15564.
[21.06.2025 01:58] Downloading paper 2506.15564 from http://arxiv.org/pdf/2506.15564v1...
[21.06.2025 01:59] Extracting affiliations from text.
[21.06.2025 01:59] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 4 6 5 5 1 . 6 0 5 2 : r Show-o2: Improved Native Unified Multimodal Models Jinheng Xie1 Zhenheng Yang2 Mike Zheng Shou1 1 Show Lab, National University of Singapore 2 ByteDance "
[21.06.2025 01:59] Response: ```python
["Show Lab, National University of Singapore", "ByteDance"]
```
[21.06.2025 01:59] Deleting PDF ./assets/pdf/2506.15564.pdf.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Downloading and parsing paper https://huggingface.co/papers/2506.15154.
[21.06.2025 01:59] Extra JSON file exists (./assets/json/2506.15154.json), skip PDF parsing.
[21.06.2025 01:59] Paper image links file exists (./assets/img_data/2506.15154.json), skip HTML parsing.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Downloading and parsing paper https://huggingface.co/papers/2506.14837.
[21.06.2025 01:59] Extra JSON file exists (./assets/json/2506.14837.json), skip PDF parsing.
[21.06.2025 01:59] Paper image links file exists (./assets/img_data/2506.14837.json), skip HTML parsing.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Downloading and parsing paper https://huggingface.co/papers/2506.15455.
[21.06.2025 01:59] Extra JSON file exists (./assets/json/2506.15455.json), skip PDF parsing.
[21.06.2025 01:59] Paper image links file exists (./assets/img_data/2506.15455.json), skip HTML parsing.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Enriching papers with extra data.
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 0. Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning,...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 1. EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.  					AI-generated summary 				 The advancement of text-to-speech and audio generation models necessi...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 2. Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 3. SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  					AI-generated summary 				 Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases a...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 4. ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  					AI-generated summary 				 Recently, multimodal large language models (MLLMs) have attracted increasing research at...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 5. RE-IMAGINE evaluates the reasoning abilities of Large Language Models by generating variations of problems that cannot be solved by memorization, indicating reliance on statistical recall.  					AI-generated summary 				 Recent Large Language Models (LLMs) have reported high accuracy on reasoning be...
[21.06.2025 01:59] Read previous papers.
[21.06.2025 01:59] Generating reviews via LLM API.
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#training", "#rl", "#open_source", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "Guru: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ—Ä–ø—É—Å Guru –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#science", "#synthetic", "#data", "#audio"], "emoji": "üé≠", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –≤ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "EmoNet-Voice - —ç—Ç–æ –Ω–æ–≤—ã–π —Ä–µ—Å—É—Ä—Å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –≤ —Ä–µ—á–∏, –≤–∫–ª—é—á–∞—é—â–∏–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è
[21.06.2025 01:59] Querying the API.
[21.06.2025 01:59] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o.
[21.06.2025 01:59] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Show-o2 - —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤ –≤ 3D –ø—Ä–∏—á–∏–Ω–Ω–æ–º –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–µ. –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–µ—Ç –µ–¥–∏–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ–±—ä–µ–¥–∏–Ω—è—è —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ. Show-o2 –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.",
  "emoji": "üß†",
  "title": "Show-o2: –ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
}
[21.06.2025 01:59] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o."

[21.06.2025 01:59] Response: ```python
['MULTIMODAL', '3D', 'VIDEO', 'CV', 'TRAINING']
```
[21.06.2025 01:59] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o."

[21.06.2025 01:59] Response: ```python
["GAMES", "OPTIMIZATION", "OPEN_SOURCE"]
```
[21.06.2025 01:59] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Show-o2, a novel multimodal model that combines autoregressive modeling and flow matching within a 3D causal variational autoencoder framework. This approach allows for the creation of unified visual representations that can effectively handle both image and video data. By employing a dual-path strategy for spatial and temporal fusion, Show-o2 enhances scalability and performance in multimodal tasks. The model is trained using a two-stage recipe, resulting in a versatile system capable of generating and understanding text, images, and videos.","title":"Unified Multimodal Mastery with Show-o2"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Show-o2, a novel multimodal model that combines autoregressive modeling and flow matching within a 3D causal variational autoencoder framework. This approach allows for the creation of unified visual representations that can effectively handle both image and video data. By employing a dual-path strategy for spatial and temporal fusion, Show-o2 enhances scalability and performance in multimodal tasks. The model is trained using a two-stage recipe, resulting in a versatile system capable of generating and understanding text, images, and videos.', title='Unified Multimodal Mastery with Show-o2'))
[21.06.2025 01:59] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑÁªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÁß∞‰∏∫Show-o2„ÄÇÂÆÉÂà©Áî®Ëá™ÂõûÂΩíÂª∫Ê®°ÂíåÊµÅÂåπÈÖçÊäÄÊúØÔºåÂú®3DÂõ†ÊûúÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÁöÑÂü∫Á°Ä‰∏äÊûÑÂª∫Áªü‰∏ÄÁöÑËßÜËßâË°®Á§∫„ÄÇÈÄöËøáÁ©∫Èó¥ÂíåÊó∂Èó¥ÁöÑÂèåË∑ØÂæÑËûçÂêàÔºåShow-o2ËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÂõæÂÉèÂíåËßÜÈ¢ëÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàê‰ªªÂä°„ÄÇËÆ∫ÊñáËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉÊñπÊ°àÔºå‰ª•‰æøÊúâÊïàÂ≠¶‰π†Âπ∂Êâ©Â±ïÂà∞Êõ¥Â§ßÁöÑÊ®°Âûã„ÄÇ","title":"Show-o2ÔºöÁªü‰∏ÄÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàêÁöÑÂàõÊñ∞Ê®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑÁªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÁß∞‰∏∫Show-o2„ÄÇÂÆÉÂà©Áî®Ëá™ÂõûÂΩíÂª∫Ê®°ÂíåÊµÅÂåπÈÖçÊäÄÊúØÔºåÂú®3DÂõ†ÊûúÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÁöÑÂü∫Á°Ä‰∏äÊûÑÂª∫Áªü‰∏ÄÁöÑËßÜËßâË°®Á§∫„ÄÇÈÄöËøáÁ©∫Èó¥ÂíåÊó∂Èó¥ÁöÑÂèåË∑ØÂæÑËûçÂêàÔºåShow-o2ËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÂõæÂÉèÂíåËßÜÈ¢ëÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàê‰ªªÂä°„ÄÇËÆ∫ÊñáËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉÊñπÊ°àÔºå‰ª•‰æøÊúâÊïàÂ≠¶‰π†Âπ∂Êâ©Â±ïÂà∞Êõ¥Â§ßÁöÑÊ®°Âûã„ÄÇ', title='Show-o2ÔºöÁªü‰∏ÄÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÁîüÊàêÁöÑÂàõÊñ∞Ê®°Âûã'))
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#audio", "#games", "#multimodal", "#architecture", "#science", "#dataset", "#training"], "emoji": "üéµ", "ru": {"title": "SonicVerse: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –º—É–∑—ã–∫–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "SonicVerse - —ç—Ç–æ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –º—É–∑—ã–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#optimization", "#training", "#multimodal"], "emoji": "üìä", "ru": {"title": "–¢–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É—Ç–æ—á–Ω–µ–Ω–∏—è", "desc": "ChartIR - —ç—Ç–æ –º–µ—Ç–æ–¥ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#math"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∞—Ä–∏–∞—Ü–∏–π –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ RE-IMAGINE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≥–µ–Ω–µ
[21.06.2025 01:59] Renaming data file.
[21.06.2025 01:59] Renaming previous data. hf_papers.json to ./d/2025-06-20.json
[21.06.2025 01:59] Saving new data file.
[21.06.2025 01:59] Generating page.
[21.06.2025 01:59] Renaming previous page.
[21.06.2025 01:59] Renaming previous data. index.html to ./d/2025-06-20.html
[21.06.2025 01:59] Writing result.
[21.06.2025 01:59] Renaming log file.
[21.06.2025 01:59] Renaming previous data. log.txt to ./logs/2025-06-21_last_log.txt
