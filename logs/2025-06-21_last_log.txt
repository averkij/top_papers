[20.06.2025 23:11] Read previous papers.
[20.06.2025 23:11] Generating top page (month).
[20.06.2025 23:11] Writing top page (month).
[21.06.2025 01:58] Read previous papers.
[21.06.2025 01:58] Get feed.
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14965
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09827
[21.06.2025 01:58] Extract page data from URL. URL: https://huggingface.co/papers/2506.15564
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15154
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14837
[21.06.2025 01:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15455
[21.06.2025 01:58] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.06.2025 01:58] No deleted papers detected.
[21.06.2025 01:58] Downloading and parsing papers (pdf, html). Total: 6.
[21.06.2025 01:58] Downloading and parsing paper https://huggingface.co/papers/2506.14965.
[21.06.2025 01:58] Extra JSON file exists (./assets/json/2506.14965.json), skip PDF parsing.
[21.06.2025 01:58] Paper image links file exists (./assets/img_data/2506.14965.json), skip HTML parsing.
[21.06.2025 01:58] Success.
[21.06.2025 01:58] Downloading and parsing paper https://huggingface.co/papers/2506.09827.
[21.06.2025 01:58] Extra JSON file exists (./assets/json/2506.09827.json), skip PDF parsing.
[21.06.2025 01:58] Paper image links file exists (./assets/img_data/2506.09827.json), skip HTML parsing.
[21.06.2025 01:58] Success.
[21.06.2025 01:58] Downloading and parsing paper https://huggingface.co/papers/2506.15564.
[21.06.2025 01:58] Downloading paper 2506.15564 from http://arxiv.org/pdf/2506.15564v1...
[21.06.2025 01:59] Extracting affiliations from text.
[21.06.2025 01:59] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 4 6 5 5 1 . 6 0 5 2 : r Show-o2: Improved Native Unified Multimodal Models Jinheng Xie1 Zhenheng Yang2 Mike Zheng Shou1 1 Show Lab, National University of Singapore 2 ByteDance "
[21.06.2025 01:59] Response: ```python
["Show Lab, National University of Singapore", "ByteDance"]
```
[21.06.2025 01:59] Deleting PDF ./assets/pdf/2506.15564.pdf.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Downloading and parsing paper https://huggingface.co/papers/2506.15154.
[21.06.2025 01:59] Extra JSON file exists (./assets/json/2506.15154.json), skip PDF parsing.
[21.06.2025 01:59] Paper image links file exists (./assets/img_data/2506.15154.json), skip HTML parsing.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Downloading and parsing paper https://huggingface.co/papers/2506.14837.
[21.06.2025 01:59] Extra JSON file exists (./assets/json/2506.14837.json), skip PDF parsing.
[21.06.2025 01:59] Paper image links file exists (./assets/img_data/2506.14837.json), skip HTML parsing.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Downloading and parsing paper https://huggingface.co/papers/2506.15455.
[21.06.2025 01:59] Extra JSON file exists (./assets/json/2506.15455.json), skip PDF parsing.
[21.06.2025 01:59] Paper image links file exists (./assets/img_data/2506.15455.json), skip HTML parsing.
[21.06.2025 01:59] Success.
[21.06.2025 01:59] Enriching papers with extra data.
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 0. Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning,...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 1. EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.  					AI-generated summary 				 The advancement of text-to-speech and audio generation models necessi...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 2. Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 3. SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  					AI-generated summary 				 Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases a...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 4. ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  					AI-generated summary 				 Recently, multimodal large language models (MLLMs) have attracted increasing research at...
[21.06.2025 01:59] ********************************************************************************
[21.06.2025 01:59] Abstract 5. RE-IMAGINE evaluates the reasoning abilities of Large Language Models by generating variations of problems that cannot be solved by memorization, indicating reliance on statistical recall.  					AI-generated summary 				 Recent Large Language Models (LLMs) have reported high accuracy on reasoning be...
[21.06.2025 01:59] Read previous papers.
[21.06.2025 01:59] Generating reviews via LLM API.
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#training", "#rl", "#open_source", "#reasoning", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "Guru: Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Guru Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆ
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#science", "#synthetic", "#data", "#audio"], "emoji": "ğŸ­", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "EmoNet-Voice - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ñ€ĞµÑ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ
[21.06.2025 01:59] Querying the API.
[21.06.2025 01:59] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o.
[21.06.2025 01:59] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Show-o2 - ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² 3D Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Show-o2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.",
  "emoji": "ğŸ§ ",
  "title": "Show-o2: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°"
}
[21.06.2025 01:59] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o."

[21.06.2025 01:59] Response: ```python
['MULTIMODAL', '3D', 'VIDEO', 'CV', 'TRAINING']
```
[21.06.2025 01:59] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  					AI-generated summary 				 This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o."

[21.06.2025 01:59] Response: ```python
["GAMES", "OPTIMIZATION", "OPEN_SOURCE"]
```
[21.06.2025 01:59] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Show-o2, a novel multimodal model that combines autoregressive modeling and flow matching within a 3D causal variational autoencoder framework. This approach allows for the creation of unified visual representations that can effectively handle both image and video data. By employing a dual-path strategy for spatial and temporal fusion, Show-o2 enhances scalability and performance in multimodal tasks. The model is trained using a two-stage recipe, resulting in a versatile system capable of generating and understanding text, images, and videos.","title":"Unified Multimodal Mastery with Show-o2"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Show-o2, a novel multimodal model that combines autoregressive modeling and flow matching within a 3D causal variational autoencoder framework. This approach allows for the creation of unified visual representations that can effectively handle both image and video data. By employing a dual-path strategy for spatial and temporal fusion, Show-o2 enhances scalability and performance in multimodal tasks. The model is trained using a two-stage recipe, resulting in a versatile system capable of generating and understanding text, images, and videos.', title='Unified Multimodal Mastery with Show-o2'))
[21.06.2025 01:59] Response: ParsedChatCompletionMessage[Article](content='{"desc":"è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œç§°ä¸ºShow-o2ã€‚å®ƒåˆ©ç”¨è‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…æŠ€æœ¯ï¼Œåœ¨3Då› æœå˜åˆ†è‡ªç¼–ç å™¨çš„åŸºç¡€ä¸Šæ„å»ºç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºã€‚é€šè¿‡ç©ºé—´å’Œæ—¶é—´çš„åŒè·¯å¾„èåˆï¼ŒShow-o2èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ã€‚è®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥ä¾¿æœ‰æ•ˆå­¦ä¹ å¹¶æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹ã€‚","title":"Show-o2ï¼šç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œç§°ä¸ºShow-o2ã€‚å®ƒåˆ©ç”¨è‡ªå›å½’å»ºæ¨¡å’ŒæµåŒ¹é…æŠ€æœ¯ï¼Œåœ¨3Då› æœå˜åˆ†è‡ªç¼–ç å™¨çš„åŸºç¡€ä¸Šæ„å»ºç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºã€‚é€šè¿‡ç©ºé—´å’Œæ—¶é—´çš„åŒè·¯å¾„èåˆï¼ŒShow-o2èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å›¾åƒå’Œè§†é¢‘çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ã€‚è®ºæ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥ä¾¿æœ‰æ•ˆå­¦ä¹ å¹¶æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹ã€‚', title='Show-o2ï¼šç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„åˆ›æ–°æ¨¡å‹'))
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#audio", "#games", "#multimodal", "#architecture", "#science", "#dataset", "#training"], "emoji": "ğŸµ", "ru": {"title": "SonicVerse: ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ", "desc": "SonicVerse - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#optimization", "#training", "#multimodal"], "emoji": "ğŸ“Š", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ", "desc": "ChartIR - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ñ
[21.06.2025 01:59] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#math"], "emoji": "ğŸ§ ", "ru": {"title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RE-IMAGINE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ³ĞµĞ½Ğµ
[21.06.2025 01:59] Renaming data file.
[21.06.2025 01:59] Renaming previous data. hf_papers.json to ./d/2025-06-20.json
[21.06.2025 01:59] Saving new data file.
[21.06.2025 01:59] Generating page.
[21.06.2025 01:59] Renaming previous page.
[21.06.2025 01:59] Renaming previous data. index.html to ./d/2025-06-20.html
[21.06.2025 01:59] Writing result.
[21.06.2025 01:59] Renaming log file.
[21.06.2025 01:59] Renaming previous data. log.txt to ./logs/2025-06-21_last_log.txt
