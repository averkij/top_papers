[25.03.2025 20:12] Read previous papers.
[25.03.2025 20:12] Generating top page (month).
[25.03.2025 20:12] Writing top page (month).
[25.03.2025 21:09] Read previous papers.
[25.03.2025 21:09] Get feed.
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18878
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17359
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18942
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18945
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18892
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18033
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17489
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18813
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18102
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17439
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18013
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18948
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18940
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18908
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18886
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18923
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17811
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14428
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18866
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15879
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18769
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17422
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18559
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18018
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17500
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18352
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18470
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17760
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17735
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16924
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18071
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14774
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13074
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18674
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18494
[25.03.2025 21:09] Extract page data from URL. URL: https://huggingface.co/papers/2503.18406
[25.03.2025 21:09] Extract page data from URL. URL: https://huggingface.co/papers/2503.16709
[25.03.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16426
[25.03.2025 21:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.03.2025 21:09] No deleted papers detected.
[25.03.2025 21:09] Downloading and parsing papers (pdf, html). Total: 38.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18878.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18878.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18878.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17359.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17359.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17359.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18942.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18942.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18942.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18945.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18945.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18945.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18892.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18892.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18892.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18033.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18033.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18033.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17489.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17489.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17489.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18813.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18813.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18813.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18102.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18102.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18102.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17439.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17439.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17439.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18013.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18013.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18013.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18948.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18948.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18948.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18940.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18940.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18940.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18908.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18908.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18908.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18886.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18886.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18886.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18923.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18923.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18923.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17811.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17811.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17811.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.14428.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.14428.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.14428.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18866.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18866.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18866.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.15879.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.15879.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.15879.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18769.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18769.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18769.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17422.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17422.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17422.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18559.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18559.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18559.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18018.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18018.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18018.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17500.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17500.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17500.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18352.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18352.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18352.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18470.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18470.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18470.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17760.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17760.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17760.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.17735.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.17735.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.17735.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.16924.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.16924.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.16924.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18071.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18071.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18071.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.14774.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.14774.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.14774.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.13074.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.13074.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.13074.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18674.
[25.03.2025 21:09] Downloading paper 2503.18674 from http://arxiv.org/pdf/2503.18674v1...
[25.03.2025 21:09] Failed to download and parse paper https://huggingface.co/papers/2503.18674: max() arg is an empty sequence
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18494.
[25.03.2025 21:09] Extra JSON file exists (./assets/json/2503.18494.json), skip PDF parsing.
[25.03.2025 21:09] Paper image links file exists (./assets/img_data/2503.18494.json), skip HTML parsing.
[25.03.2025 21:09] Success.
[25.03.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2503.18406.
[25.03.2025 21:09] Downloading paper 2503.18406 from http://arxiv.org/pdf/2503.18406v1...
[25.03.2025 21:10] Extracting affiliations from text.
[25.03.2025 21:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 6 0 4 8 1 . 3 0 5 2 : r Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning Sherry X. Chen University of California, Santa Barbara xchen774,sra,psen@ucsb.edu "
[25.03.2025 21:10] Response: ```python
["University of California, Santa Barbara"]
```
[25.03.2025 21:10] Deleting PDF ./assets/pdf/2503.18406.pdf.
[25.03.2025 21:10] Success.
[25.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.16709.
[25.03.2025 21:10] Downloading paper 2503.16709 from http://arxiv.org/pdf/2503.16709v1...
[25.03.2025 21:10] Extracting affiliations from text.
[25.03.2025 21:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 9 0 7 6 1 . 3 0 5 2 : r QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge Xuan Shen1, Weize Ma2, Jing Liu3, Changdi Yang1, Rui Ding2, Quanyi Wang4, Henghui Ding5, Wei Niu6, Yanzhi Wang1, Pu Zhao1*, Jun Lin2*, Jiuxiang Gu7* 1Northeastern University, 2Nanjing University, 3Monash University, 4Nanjing University of Information Science and Technology, 5Fudan University, 6University of Georgia, 7Adobe Research {shen.xu,yanz.wang,p.zhao}@northeastern.edu, weizema@smail.nju.edu.cn, jlin@nju.edu.cn, jigu@adobe.com "
[25.03.2025 21:10] Response: ```python
[
    "Northeastern University",
    "Nanjing University",
    "Monash University",
    "Nanjing University of Information Science and Technology",
    "Fudan University",
    "University of Georgia",
    "Adobe Research"
]
```
[25.03.2025 21:10] Deleting PDF ./assets/pdf/2503.16709.pdf.
[25.03.2025 21:10] Success.
[25.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.16426.
[25.03.2025 21:10] Extra JSON file exists (./assets/json/2503.16426.json), skip PDF parsing.
[25.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.16426.json), skip HTML parsing.
[25.03.2025 21:10] Success.
[25.03.2025 21:10] Enriching papers with extra data.
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 0. Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 1. Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 2. With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scalin...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 3. The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core ca...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 4. DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduc...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 5. Omnimatte aims to decompose a given video into semantically meaningful layers, including the background and individual objects along with their associated effects, such as shadows and reflections. Existing methods often require extensive training or costly self-supervised optimization. In this paper...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 6. Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (M...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 7. Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer a...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 8. Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 9. Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 10. Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However,...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 11. Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into a series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflict...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 12. Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video re...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 13. We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 14. Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 15. Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchma...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 16. Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle wit...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 17. Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we pro...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 18. Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent th...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 19. Non-factoid question-answering (NFQA) poses a significant challenge due to its open-ended nature, diverse intents, and the need for multi-aspect reasoning, which renders conventional factoid QA approaches, including retrieval-augmented generation (RAG), inadequate. Unlike factoid questions, non-fact...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 20. This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, a...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 21. The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as a flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 22. Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g.,iGPUs and mobile ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 23. Large Language Models (LLMs) have significantly advanced various fields, particularly coding, mathematical reasoning, and logical problem solving. However, a critical question remains: Do these mathematical reasoning abilities persist when LLMs are presented with culturally adapted math problems? Sp...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 24. The outcome of Large Language Model (LLM) pre-training strongly depends on weight initialization and variance control strategies. Although the importance of initial variance control has been well documented in neural networks in general, the literature on initialization and management of its growth ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 25. In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aestheti...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 26. We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of in...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 27. Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 28. Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 29. 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for real-time, high-performance rendering, enabling a wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have show...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 30. Language models have recently advanced into the realm of reasoning, yet it is through multimodal reasoning that we can fully unlock the potential to achieve more comprehensive, human-like cognitive capabilities. This survey provides a systematic overview of the recent multimodal reasoning approaches...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 31. White balance (WB) correction in scenes with multiple illuminants remains a persistent challenge in computer vision. Recent methods explored fusion-based approaches, where a neural network linearly blends multiple sRGB versions of an input image, each processed with predefined WB presets. However, w...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 32. While recent advancing image super-resolution (SR) techniques are continually improving the perceptual quality of their outputs, they can usually fail in quantitative evaluations. This inconsistency leads to a growing distrust in existing image metrics for SR evaluations. Though image evaluation dep...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 33. We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe ...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 34. The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex softwa...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 35. Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) g...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 36. Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer vision, supporting numerous real-world applications. However, deploying accurate depth estimation models on resource-limited edge devices, especially Application-Specific Integrated Circuits (ASICs), is challenging due to the...
[25.03.2025 21:10] ********************************************************************************
[25.03.2025 21:10] Abstract 37. The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities across varied applications. While some contempor...
[25.03.2025 21:10] Read previous papers.
[25.03.2025 21:10] Generating reviews via LLM API.
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#training", "#interpretability", "#open_source", "#architecture", "#reasoning"], "emoji": "🧠", "ru": {"title": "Раскрывая тайны рассуждений искусственного интеллекта", "desc": "Исследователи изучили внутренние механизмы рассуждений в языковых моделях, используя разреженные автоэнкод
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#video", "#architecture", "#games", "#multimodal"], "emoji": "🎮", "ru": {"title": "Революция в разработке игр: ИИ-генерируемые миры будущего", "desc": "Статья предлагает концепцию Генеративных Игровых Движков (GGE), основанных на Интерактивной Генеративной Видео технологии (IGV). GG
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#video", "#inference", "#games", "#optimization", "#benchmark"], "emoji": "🎥", "ru": {"title": "Масштабирование времени тестирования: новый подход к улучшению генерации видео", "desc": "Статья исследует применение метода масштабирования времени тестирования (TTS) для улучшения качес
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#video", "#3d", "#reasoning", "#agents", "#synthetic"], "emoji": "🧠", "ru": {"title": "Единая система для геометрического моделирования мира и планирования действий", "desc": "Статья представляет Aether - унифицированную систему для геометрически-осознанного рассуждения в моделях ми
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#rl", "#training", "#small_models"], "emoji": "🧠", "ru": {"title": "Развитие рассуждений в языковых моделях через RL с нуля", "desc": "Это исследование посвящено применению обучения с подкреплением (RL) без предварительной подготовки для развития способ
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#optimization", "#video"], "emoji": "🎬", "ru": {"title": "OmnimatteZero: Быстрая и эффективная декомпозиция видео без обучения", "desc": "OmnimatteZero - это новый подход к декомпозиции видео на семантически значимые слои без необходимости обучения. Метод исполь
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#multimodal", "#hallucinations", "#benchmark", "#alignment", "#open_source"], "emoji": "🤖", "ru": {"title": "Универсальная оценка мультимодальных ИИ-моделей: от понимания к генерации", "desc": "Статья представляет новые бенчмарки TaskAnything и JudgeAnything для оценки мультимодальн
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#inference", "#security", "#agents", "#benchmark"], "emoji": "🛡️", "ru": {"title": "CaMeL: Надежная защита LLM-агентов от атак внедрения промптов", "desc": "Статья представляет CaMeL - новый метод защиты агентов на основе больших языковых моделей (LLM) от атак внедрения промптов. Ca
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#agents", "#math", "#reasoning", "#science", "#benchmark"], "emoji": "🤖", "ru": {"title": "Коллективный разум ИИ: AgentRxiv объединяет LLM-агентов для ускорения научных открытий", "desc": "Статья представляет AgentRxiv - фреймворк, позволяющий агентам на основе больших языковых моде
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#math", "#training", "#reasoning", "#dataset", "#data"], "emoji": "🧮", "ru": {"title": "Учимся на ошибках: новый подход к улучшению математических способностей ИИ", "desc": "Эта статья предлагает метод LEMMA для улучшения способности больших языковых моделей (LLM) решать математичес
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#benchmark", "#rlhf"], "emoji": "👁️", "ru": {"title": "Vision-R1: Революция в обучении визуально-языковых моделей без ручной разметки", "desc": "В статье представлен новый алгоритм обучения с подкреплением для крупных визуально-языковых моделей п
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#diffusion", "#training", "#open_source", "#cv"], "emoji": "🖼️", "ru": {"title": "Эквивариантное моделирование для эффективной генерации изображений", "desc": "Статья представляет новый подход к генеративному моделированию изображений, основанный на
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#inference", "#video"], "emoji": "⏱️", "ru": {"title": "Ускорение диффузионных моделей без потери качества", "desc": "Статья представляет новый метод ускорения работы диффузионных моделей под названием Bottleneck Sampling. Этот подход использует
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#inference"], "emoji": "🚀", "ru": {"title": "Ускорение больших языковых моделей через параллелизацию FFN слоев", "desc": "Статья представляет технику оптимизации архитектуры под названием FFN Fusion, которая сокращает последовательные в
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#diffusion", "#video", "#training", "#cv"], "emoji": "🔬", "ru": {"title": "CFG-Zero*: Оптимизированное руководство для улучшения генеративных моделей", "desc": "Статья посвящена улучшению техники Classifier-Free Guidance (CFG) для моделей диффузии и
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#video", "#interpretability", "#reasoning", "#long_context", "#multimodal", "#rag", "#benchmark"], "emoji": "🎥", "ru": {"title": "Новый стандарт оценки фактической точности видео-языковых моделей", "desc": "Статья представляет Video SimpleQA - первый комплексный бенчмарк для оценки 
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#small_models", "#reasoning", "#training"], "emoji": "🪶", "ru": {"title": "Легковесный NL2SQL: Feather-SQL поднимает планку для малых языковых моделей", "desc": "Статья представляет Feather-SQL - новый легковесный фреймворк для малых языковых моделей (SL
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#training", "#multimodal", "#architecture", "#games", "#diffusion", "#video"], "emoji": "🎬", "ru": {"title": "MagicComp: Усовершенствованная генерация видео по тексту без дополнительного обучения", "desc": "MagicComp - это метод генерации видео по тексту, не требующий дополнительног
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#transfer_learning", "#synthetic", "#data", "#math"], "emoji": "🧠", "ru": {"title": "Раскрытие скрытых мыслей для эффективного обучения языковых моделей", "desc": "Статья предлагает метод повышения эффективности предобучения языковых моделей в условиях 
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#rag", "#open_source", "#benchmark", "#optimization", "#dataset", "#reasoning"], "emoji": "🧠", "ru": {"title": "Typed-RAG: умное разложение вопросов для лучших ответов", "desc": "Статья представляет новый подход к ответам на нефактоидные вопросы (NFQA) под названием Typed-RAG. Этот 
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#architecture", "#synthetic", "#reasoning", "#3d"], "emoji": "🧠", "ru": {"title": "AlphaSpace: Прорыв в пространственном мышлении ИИ", "desc": "AlphaSpace - это новая методология, разработанная для улучшения пространственного мышления больших языковых моделей (LLM) в навигации по 3D
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#reasoning", "#inference"], "emoji": "🚀", "ru": {"title": "Ускорение языковых моделей на RISC-V процессорах", "desc": "Данная статья посвящена оптимизации инференса больших языковых моделей (LLM) на процессорах RISC-V, в частности на Sophon SG2042. 
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#long_context", "#video", "#optimization", "#open_source", "#training", "#small_models", "#data"], "emoji": "🐦", "ru": {"title": "Эффективная генерация видео по тексту для мобильных устройств", "desc": "Статья представляет легковесную модель генерации видео по тексту под названием H
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#synthetic", "#dataset", "#math", "#data", "#reasoning", "#benchmark"], "emoji": "🧮", "ru": {"title": "Культурный контекст влияет на математические способности ИИ", "desc": "Исследование показывает, что большие языковые модели (LLM) испытывают трудности с решением математических зад
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#optimization", "#training", "#architecture"], "emoji": "⚖️", "ru": {"title": "Точная настройка весов - ключ к эффективным языковым моделям", "desc": "Статья представляет новые методы инициализации весов и контроля дисперсии для предобучения больших языко
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#training", "#cv", "#benchmark"], "emoji": "🖼️", "ru": {"title": "Революция в генерации 4K изображений с помощью диффузионных моделей", "desc": "Статья представляет Diffusion-4K - новый фреймворк для синтеза ультра-высокого разрешения изображений с помощью 
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#3d", "#training", "#rl", "#optimization", "#games", "#open_source", "#reasoning"], "emoji": "🧠", "ru": {"title": "MetaSpatial: Улучшение 3D пространственного мышления ИИ с помощью обучения с подкреплением", "desc": "MetaSpatial - это первая система на основе обучения с подкрепление
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#cv", "#architecture", "#training"], "emoji": "🖼️", "ru": {"title": "CODA: эффективная дискретизация изображений для генеративных моделей", "desc": "Статья представляет CODA - новый подход к дискретизации изображений для задач генерации. CO
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#video", "#dataset", "#optimization", "#training", "#transfer_learning"], "emoji": "🎬", "ru": {"title": "Эффективная генерация видео с нуля вместо тонкой настройки больших моделей", "desc": "Статья представляет новый подход к генерации видео в условиях ограниченных ресурсов. Авторы 
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#3d", "#inference", "#open_source"], "emoji": "🎨", "ru": {"title": "OMG: Оптимизация 3D Gaussian Splatting для эффективного рендеринга", "desc": "В этой статье представлен метод Optimized Minimal Gaussians (OMG) для оптимизации 3D Gaussian Splatting. OMG значительно
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#reasoning", "#multimodal", "#survey"], "emoji": "🤖", "ru": {"title": "Мультимодальное рассуждение: путь к человекоподобному ИИ", "desc": "Эта статья представляет собой обзор современных подходов к мультимодальному рассуждению в области машинного обучения. А
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#dataset", "#optimization"], "emoji": "📸", "ru": {"title": "Трансформеры и большие данные для идеального баланса белого", "desc": "Статья представляет новый подход к коррекции баланса белого в сценах с несколькими источниками освещения. Авторы предлагают эффективную модель на
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#cv", "#ethics"], "emoji": "🔍", "ru": {"title": "Переосмысление эталонов в оценке алгоритмов сверхразрешения", "desc": "Статья исследует проблему оценки качества алгоритмов сверхразрешения изображений. Авторы обнаружили, что эталонные изобр
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#ethics", "#diffusion", "#training"], "emoji": "🕺", "ru": {"title": "Безопасная генерация движений: разобучение токсичной анимации", "desc": "Статья представляет задачу разобучения генерации токсичных движений человека в системах преобразования текста в ани
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#training", "#benchmark", "#agents", "#agi", "#architecture", "#reasoning"], "emoji": "🤖", "ru": {"title": "CURA: Агент с вербальным контролем для решения сложных задач программирования", "desc": "Статья представляет CURA - систему агента для понимания и рассуждения о коде, улучшенн
[25.03.2025 21:10] Querying the API.
[25.03.2025 21:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP, a self-supervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) [19] and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel Instruct-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at https://github.com/SherryXTChen/Instruct-CLIP.git.
[25.03.2025 21:10] Response: {
  "desc": "Статья представляет Instruct-CLIP - самообучаемый метод для улучшения наборов данных для обучения моделей редактирования изображений по текстовым инструкциям. Метод анализирует семантические изменения между оригинальными и отредактированными изображениями, чтобы уточнить инструкции и лучше согласовать их с фактическими изменениями. Instruct-CLIP адаптирован для работы с латентными изображениями и временными шагами диффузии, что позволяет использовать его для обучения латентных диффузионных моделей. С помощью Instruct-CLIP авторы улучшили набор данных InstructPix2Pix и обучили модель, способную создавать редактирования, более точно соответствующие заданным инструкциям.",
  "emoji": "🖼️",
  "title": "Улучшение редактирования изображений по инструкциям с помощью самообучения"
}
[25.03.2025 21:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP, a self-supervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) [19] and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel Instruct-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at https://github.com/SherryXTChen/Instruct-CLIP.git."

[25.03.2025 21:10] Response: ```python
["DATASET", "DATA", "CV", "TRAINING"]
```
[25.03.2025 21:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP, a self-supervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) [19] and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel Instruct-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at https://github.com/SherryXTChen/Instruct-CLIP.git."

[25.03.2025 21:10] Response: ```python
['DIFFUSION', 'OPTIMIZATION']
```
[25.03.2025 21:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Instruct-CLIP, a self-supervised method designed to improve the alignment between image edits and natural language instructions. It addresses the limitations of traditional text-to-image models that often produce poorly aligned image pairs for training. By learning the semantic changes between original and edited images, Instruct-CLIP refines existing datasets and enhances the training of latent diffusion models. The method results in a more accurate model that generates image edits that closely match the specified instructions, significantly improving the quality of automated image editing.","title":"Aligning Image Edits with Natural Language Instructions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Instruct-CLIP, a self-supervised method designed to improve the alignment between image edits and natural language instructions. It addresses the limitations of traditional text-to-image models that often produce poorly aligned image pairs for training. By learning the semantic changes between original and edited images, Instruct-CLIP refines existing datasets and enhances the training of latent diffusion models. The method results in a more accurate model that generates image edits that closely match the specified instructions, significantly improving the quality of automated image editing.', title='Aligning Image Edits with Natural Language Instructions'))
[25.03.2025 21:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为Instruct-CLIP的自监督方法，旨在改善基于自然语言指令的图像编辑模型的性能。通过学习原始图像与编辑图像之间的语义变化，Instruct-CLIP能够更好地对齐现有数据集中的指令。该方法还适应了处理噪声潜在图像和扩散时间步，以便在潜在扩散模型中有效地执行指令与图像变化之间的对齐。最终，使用Instruct-CLIP修正后的数据集生成了超过12万条样本，从而提高了模型对指令的响应能力。","title":"提升图像编辑指令对齐的自监督方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为Instruct-CLIP的自监督方法，旨在改善基于自然语言指令的图像编辑模型的性能。通过学习原始图像与编辑图像之间的语义变化，Instruct-CLIP能够更好地对齐现有数据集中的指令。该方法还适应了处理噪声潜在图像和扩散时间步，以便在潜在扩散模型中有效地执行指令与图像变化之间的对齐。最终，使用Instruct-CLIP修正后的数据集生成了超过12万条样本，从而提高了模型对指令的响应能力。', title='提升图像编辑指令对齐的自监督方法'))
[25.03.2025 21:10] Querying the API.
[25.03.2025 21:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer vision, supporting numerous real-world applications. However, deploying accurate depth estimation models on resource-limited edge devices, especially Application-Specific Integrated Circuits (ASICs), is challenging due to the high computational and memory demands. Recent advancements in foundational depth estimation deliver impressive results but further amplify the difficulty of deployment on ASICs. To address this, we propose QuartDepth which adopts post-training quantization to quantize MDE models with hardware accelerations for ASICs. Our approach involves quantizing both weights and activations to 4-bit precision, reducing the model size and computation cost. To mitigate the performance degradation, we introduce activation polishing and compensation algorithm applied before and after activation quantization, as well as a weight reconstruction method for minimizing errors in weight quantization. Furthermore, we design a flexible and programmable hardware accelerator by supporting kernel fusion and customized instruction programmability, enhancing throughput and efficiency. Experimental results demonstrate that our framework achieves competitive accuracy while enabling fast inference and higher energy efficiency on ASICs, bridging the gap between high-performance depth estimation and practical edge-device applicability. Code: https://github.com/shawnricecake/quart-depth
[25.03.2025 21:10] Response: {
  "desc": "Статья представляет QuartDepth - подход к квантизации моделей монокулярной оценки глубины для применения на ASIC. Авторы предлагают квантизацию весов и активаций до 4 бит, а также методы полировки активаций и реконструкции весов для минимизации потери точности. Разработан гибкий аппаратный ускоритель с поддержкой слияния ядер и программируемости инструкций. Эксперименты показывают, что подход обеспечивает конкурентную точность при быстром выводе и высокой энергоэффективности на ASIC.",
  "emoji": "🔬",
  "title": "Эффективная оценка глубины на краевых устройствах"
}
[25.03.2025 21:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer vision, supporting numerous real-world applications. However, deploying accurate depth estimation models on resource-limited edge devices, especially Application-Specific Integrated Circuits (ASICs), is challenging due to the high computational and memory demands. Recent advancements in foundational depth estimation deliver impressive results but further amplify the difficulty of deployment on ASICs. To address this, we propose QuartDepth which adopts post-training quantization to quantize MDE models with hardware accelerations for ASICs. Our approach involves quantizing both weights and activations to 4-bit precision, reducing the model size and computation cost. To mitigate the performance degradation, we introduce activation polishing and compensation algorithm applied before and after activation quantization, as well as a weight reconstruction method for minimizing errors in weight quantization. Furthermore, we design a flexible and programmable hardware accelerator by supporting kernel fusion and customized instruction programmability, enhancing throughput and efficiency. Experimental results demonstrate that our framework achieves competitive accuracy while enabling fast inference and higher energy efficiency on ASICs, bridging the gap between high-performance depth estimation and practical edge-device applicability. Code: https://github.com/shawnricecake/quart-depth"

[25.03.2025 21:10] Response: ```python
['CV', 'INFERENCE']
```
[25.03.2025 21:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer vision, supporting numerous real-world applications. However, deploying accurate depth estimation models on resource-limited edge devices, especially Application-Specific Integrated Circuits (ASICs), is challenging due to the high computational and memory demands. Recent advancements in foundational depth estimation deliver impressive results but further amplify the difficulty of deployment on ASICs. To address this, we propose QuartDepth which adopts post-training quantization to quantize MDE models with hardware accelerations for ASICs. Our approach involves quantizing both weights and activations to 4-bit precision, reducing the model size and computation cost. To mitigate the performance degradation, we introduce activation polishing and compensation algorithm applied before and after activation quantization, as well as a weight reconstruction method for minimizing errors in weight quantization. Furthermore, we design a flexible and programmable hardware accelerator by supporting kernel fusion and customized instruction programmability, enhancing throughput and efficiency. Experimental results demonstrate that our framework achieves competitive accuracy while enabling fast inference and higher energy efficiency on ASICs, bridging the gap between high-performance depth estimation and practical edge-device applicability. Code: https://github.com/shawnricecake/quart-depth"

[25.03.2025 21:10] Response: ```python
["OPTIMIZATION"]
```
[25.03.2025 21:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents QuartDepth, a novel approach for Monocular Depth Estimation (MDE) that focuses on optimizing models for resource-limited edge devices like ASICs. It utilizes post-training quantization to reduce the model size and computational requirements by quantizing weights and activations to 4-bit precision. To counteract potential performance loss from quantization, the authors introduce techniques such as activation polishing and a weight reconstruction method. The proposed framework not only maintains competitive accuracy but also enhances inference speed and energy efficiency, making high-performance depth estimation feasible on edge devices.","title":"Optimizing Depth Estimation for Edge Devices with QuartDepth"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents QuartDepth, a novel approach for Monocular Depth Estimation (MDE) that focuses on optimizing models for resource-limited edge devices like ASICs. It utilizes post-training quantization to reduce the model size and computational requirements by quantizing weights and activations to 4-bit precision. To counteract potential performance loss from quantization, the authors introduce techniques such as activation polishing and a weight reconstruction method. The proposed framework not only maintains competitive accuracy but also enhances inference speed and energy efficiency, making high-performance depth estimation feasible on edge devices.', title='Optimizing Depth Estimation for Edge Devices with QuartDepth'))
[25.03.2025 21:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"单目深度估计（MDE）在计算机视觉中扮演着重要角色，广泛应用于现实世界中。然而，在资源有限的边缘设备上，尤其是应用特定集成电路（ASICs）上，准确的深度估计模型的部署面临挑战，因为其计算和内存需求较高。为了解决这个问题，我们提出了QuartDepth，通过后训练量化技术对MDE模型进行量化，以适应ASICs的硬件加速。我们的方案通过将权重和激活量化到4位精度，减少模型大小和计算成本，同时引入激活抛光和补偿算法，以减轻性能下降。","title":"高效的单目深度估计解决方案"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='单目深度估计（MDE）在计算机视觉中扮演着重要角色，广泛应用于现实世界中。然而，在资源有限的边缘设备上，尤其是应用特定集成电路（ASICs）上，准确的深度估计模型的部署面临挑战，因为其计算和内存需求较高。为了解决这个问题，我们提出了QuartDepth，通过后训练量化技术对MDE模型进行量化，以适应ASICs的硬件加速。我们的方案通过将权重和激活量化到4位精度，减少模型大小和计算成本，同时引入激活抛光和补偿算法，以减轻性能下降。', title='高效的单目深度估计解决方案'))
[25.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#optimization", "#transfer_learning", "#training", "#architecture"], "emoji": "🛰️", "ru": {"title": "DynamicVis: эффективная модель машинного зрения для анализа спутниковых снимков", "desc": "Статья представляет DynamicVis - модель машинного зрения для анализа спутниковых сни
[25.03.2025 21:10] Loading Chinese text from previous data.
[25.03.2025 21:10] Renaming data file.
[25.03.2025 21:10] Renaming previous data. hf_papers.json to ./d/2025-03-25.json
[25.03.2025 21:10] Saving new data file.
[25.03.2025 21:10] Generating page.
[25.03.2025 21:10] Renaming previous page.
[25.03.2025 21:10] Renaming previous data. index.html to ./d/2025-03-25.html
[25.03.2025 21:10] [Experimental] Generating Chinese page for reading.
[25.03.2025 21:10] Chinese vocab [{'word': '现代', 'pinyin': 'xiàndài', 'trans': 'modern'}, {'word': '面临', 'pinyin': 'miànlín', 'trans': 'face'}, {'word': '创意', 'pinyin': 'chuàngyì', 'trans': 'creativity'}, {'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'}, {'word': '挑战', 'pinyin': 'tiǎozhàn', 'trans': 'challenge'}, {'word': '视频', 'pinyin': 'shìpín', 'trans': 'video'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '逼真', 'pinyin': 'bīzhēn', 'trans': 'realistic'}, {'word': '互动', 'pinyin': 'hùdòng', 'trans': 'interactive'}, {'word': '虚拟', 'pinyin': 'xūnǐ', 'trans': 'virtual'}, {'word': '环境', 'pinyin': 'huánjìng', 'trans': 'environment'}, {'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'}, {'word': '引擎', 'pinyin': 'yǐnqíng', 'trans': 'engine'}, {'word': '基础', 'pinyin': 'jīchǔ', 'trans': 'foundation'}, {'word': '利用', 'pinyin': 'lìyòng', 'trans': 'utilize'}, {'word': '优势', 'pinyin': 'yōushì', 'trans': 'advantage'}, {'word': '无限制', 'pinyin': 'wúxiànzhì', 'trans': 'unlimited'}, {'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high quality'}, {'word': '内容', 'pinyin': 'nèiróng', 'trans': 'content'}, {'word': '物理', 'pinyin': 'wùlǐ', 'trans': 'physical'}, {'word': '意识', 'pinyin': 'yìshí', 'trans': 'awareness'}, {'word': '建模', 'pinyin': 'jiànmó', 'trans': 'modeling'}, {'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'}, {'word': '核心', 'pinyin': 'héxīn', 'trans': 'core'}, {'word': '模块', 'pinyin': 'mókuài', 'trans': 'module'}, {'word': '分层', 'pinyin': 'fēncéng', 'trans': 'layered'}, {'word': '成熟度', 'pinyin': 'chéngshúdù', 'trans': 'maturity'}, {'word': '路线图', 'pinyin': 'lùxiàntú', 'trans': 'roadmap'}, {'word': '开辟', 'pinyin': 'kāipì', 'trans': 'open up'}, {'word': '途径', 'pinyin': 'tújìng', 'trans': 'path'}, {'word': '时代', 'pinyin': 'shídài', 'trans': 'era'}]
[25.03.2025 21:10] Renaming previous Chinese page.
[25.03.2025 21:10] Renaming previous data. zh.html to ./d/2025-03-24_zh_reading_task.html
[25.03.2025 21:10] Writing Chinese reading task.
[25.03.2025 21:10] Writing result.
[25.03.2025 21:10] Renaming log file.
[25.03.2025 21:10] Renaming previous data. log.txt to ./logs/2025-03-25_last_log.txt
