[06.01.2025 16:12] Read previous papers.
[06.01.2025 16:12] Generating top page (month).
[06.01.2025 16:12] Writing top page (month).
[06.01.2025 17:09] Read previous papers.
[06.01.2025 17:09] Get feed.
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01895
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01957
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01904
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01821
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21059
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01073
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.00874
[06.01.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01540
[06.01.2025 17:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.01.2025 17:09] No deleted papers detected.
[06.01.2025 17:09] Downloading and parsing papers (pdf, html). Total: 8.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2501.01895.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2501.01895.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2501.01895.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2501.01957.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2501.01957.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2501.01957.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2501.01904.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2501.01904.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2501.01904.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2501.01821.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2501.01821.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2501.01821.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2412.21059.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2412.21059.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2412.21059.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2501.01073.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2501.01073.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2501.01073.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2501.00874.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2501.00874.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2501.00874.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2501.01540.
[06.01.2025 17:09] Extra JSON file exists (./assets/json/2501.01540.json), skip PDF parsing.
[06.01.2025 17:09] Paper image links file exists (./assets/img_data/2501.01540.json), skip HTML parsing.
[06.01.2025 17:09] Success.
[06.01.2025 17:09] Enriching papers with extra data.
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 0. We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continu...
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 1. Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in bot...
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 2. Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more c...
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 3. Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Exis...
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 4. We present a general strategy to aligning visual generation models -- both image and video generation -- with human preference. To start with, we build VisionReward -- a fine-grained and multi-dimensional reward model. We decompose human preferences in images and videos into multiple dimensions, eac...
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 5. Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternativ...
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 6. Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largel...
[06.01.2025 17:09] ********************************************************************************
[06.01.2025 17:09] Abstract 7. Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM...
[06.01.2025 17:09] Read previous papers.
[06.01.2025 17:09] Generating reviews via LLM API.
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#3d", "#data", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "EnerVerse: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤", "desc": "EnerVerse - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –±—É–¥—É—â–µ–≥–æ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–µ—Ä—Ç
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#training", "#cv", "#multimodal", "#benchmark", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏: —Ä–µ—á—å –∏ –∑—Ä–µ–Ω–∏–µ –≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#transfer_learning", "#training"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∏ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –º–µ–¥–ª–µ–Ω
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#rlhf", "#agents", "#alignment", "#training"], "emoji": "ü§ñ", "ru": {"title": "SDPO: –ù–æ–≤—ã–π —à–∞–≥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–æ—Ü–∏–∞–ª—å–Ω–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#rag", "#training", "#open_source", "#cv", "#video", "#optimization", "#alignment"], "emoji": "üé•", "ru": {"title": "VisionReward: –º–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#architecture", "#data", "#graphs"], "emoji": "üï∏Ô∏è", "ru": {"title": "G2PT: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–æ–≤ - Graph Generative Pre-trained Transformer (
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#transfer_learning", "#architecture", "#benchmark", "#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–µ–∑ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "LUSIFER - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã
[06.01.2025 17:09] Using data from previous issue: {"categories": ["#benchmark", "#data", "#science", "#agents"], "emoji": "üß™", "ru": {"title": "BoxingGym: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò –≤ –Ω–∞—É—á–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ BoxingGym –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –Ω–∞—É—á–Ω–æ–º—É –æ—Ç–∫—Ä—ã—Ç–∏—é. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 10 —Å—Ä–µ–¥, –º–æ
[06.01.2025 17:09] Loading Chinese text from previous data.
[06.01.2025 17:09] Renaming data file.
[06.01.2025 17:09] Renaming previous data. hf_papers.json to ./d/2025-01-06.json
[06.01.2025 17:09] Saving new data file.
[06.01.2025 17:09] Generating page.
[06.01.2025 17:09] Renaming previous page.
[06.01.2025 17:09] Renaming previous data. index.html to ./d/2025-01-06.html
[06.01.2025 17:09] [Experimental] Generating Chinese page for reading.
[06.01.2025 17:09] Chinese vocab [{'word': 'framework', 'pinyin': 'ku√†ngji√†', 'trans': 'Ê°ÜÊû∂'}, {'word': 'generating', 'pinyin': 'shƒìngch√©ng', 'trans': 'ÁîüÊàê'}, {'word': 'future', 'pinyin': 'w√®il√°i', 'trans': 'Êú™Êù•'}, {'word': 'space', 'pinyin': 'k≈çngjiƒÅn', 'trans': 'Á©∫Èó¥'}, {'word': 'robotic', 'pinyin': 'jƒ´qir√©n', 'trans': 'Êú∫Âô®‰∫∫'}, {'word': 'tasks', 'pinyin': 'r√®nw√π', 'trans': '‰ªªÂä°'}, {'word': 'attention', 'pinyin': 'zh√πy√¨', 'trans': 'Ê≥®ÊÑè'}, {'word': 'mechanisms', 'pinyin': 'jƒ´zh√¨', 'trans': 'Êú∫Âà∂'}, {'word': 'consistent', 'pinyin': 'w√∫gu«í', 'trans': '‰∏ÄËá¥'}, {'word': 'modeling', 'pinyin': 'm√≥x√≠ng', 'trans': 'Âª∫Ê®°'}, {'word': 'memory', 'pinyin': 'j√¨y√¨', 'trans': 'ËÆ∞ÂøÜ'}, {'word': 'context', 'pinyin': 'q«îw√©n', 'trans': '‰∏ä‰∏ãÊñá'}, {'word': 'sequence', 'pinyin': 'x√πli√®', 'trans': 'Â∫èÂàó'}, {'word': 'generation', 'pinyin': 'shƒìngch√©ng', 'trans': 'ÁîüÊàê'}, {'word': 'FAV', 'pinyin': 'Fƒìi-ƒíi-Wƒìi', 'trans': 'FAV'}, {'word': 'enhances', 'pinyin': 'zƒìngqi√°ng', 'trans': 'Â¢ûÂº∫'}, {'word': 'observation', 'pinyin': 'guƒÅnch√°', 'trans': 'ËßÇÂØü'}, {'word': 'adaptability', 'pinyin': 'sh√¨y√¨ngx√¨ng', 'trans': 'ÈÄÇÂ∫îÊÄß'}, {'word': 'engine', 'pinyin': 'y«ênq√≠ng', 'trans': 'ÂºïÊìé'}, {'word': '4DGS', 'pinyin': 'S√¨-Dƒ´-Jƒ´-ƒís', 'trans': '4DGS'}, {'word': 'improves', 'pinyin': 'g«éish√†n', 'trans': 'ÊîπÂñÑ'}, {'word': 'quality', 'pinyin': 'zh√¨li√†ng', 'trans': 'Ë¥®Èáè'}, {'word': 'diversity', 'pinyin': 'du≈çy√†ngx√¨ng', 'trans': 'Â§öÊ†∑ÊÄß'}, {'word': 'experiments', 'pinyin': 'sh√≠y√†n', 'trans': 'ÂÆûÈ™å'}, {'word': 'show', 'pinyin': 'xi«énsh√¨', 'trans': 'ÊòæÁ§∫'}, {'word': 'boosts', 'pinyin': 'zƒìngqi√°ng', 'trans': 'Â¢ûÂº∫'}, {'word': 'performance', 'pinyin': 'bi«éoxi√†n', 'trans': 'Ë°®Áé∞'}, {'word': 'long-range', 'pinyin': 'ch√°ngyu«én', 'trans': 'ÈïøËøú'}]
[06.01.2025 17:09] Renaming previous Chinese page.
[06.01.2025 17:09] Renaming previous data. zh.html to ./d/2025-01-05_zh_reading_task.html
[06.01.2025 17:09] Writing Chinese reading task.
[06.01.2025 17:09] Writing result.
[06.01.2025 17:09] Renaming log file.
[06.01.2025 17:09] Renaming previous data. log.txt to ./logs/2025-01-06_last_log.txt
