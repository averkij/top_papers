[06.01.2025 06:15] Read previous papers.
[06.01.2025 06:15] Generating top page (month).
[06.01.2025 06:15] Writing top page (month).
[06.01.2025 07:10] Read previous papers.
[06.01.2025 07:10] Get feed.
[06.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01895
[06.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01957
[06.01.2025 07:10] Extract page data from URL. URL: https://huggingface.co/papers/2501.01073
[06.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01904
[06.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.00874
[06.01.2025 07:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.01.2025 07:10] No deleted papers detected.
[06.01.2025 07:10] Downloading and parsing papers (pdf, html). Total: 5.
[06.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.01895.
[06.01.2025 07:10] Extra JSON file exists (./assets/json/2501.01895.json), skip PDF parsing.
[06.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.01895.json), skip HTML parsing.
[06.01.2025 07:10] Success.
[06.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.01957.
[06.01.2025 07:10] Extra JSON file exists (./assets/json/2501.01957.json), skip PDF parsing.
[06.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.01957.json), skip HTML parsing.
[06.01.2025 07:10] Success.
[06.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.01073.
[06.01.2025 07:10] Downloading paper 2501.01073 from http://arxiv.org/pdf/2501.01073v1...
[06.01.2025 07:11] Extracting affiliations from text.
[06.01.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 3 7 0 1 0 . 1 0 5 2 : r Graph Generative Pre-trained Transformer Xiaohui Chen 1 Yinkai Wang 1 Jiaxing He 2 Yuanqi Du 3 Soha Hassoun 1 Xiaolin Xu 2 Liping Liu "
[06.01.2025 07:11] Response: ```python
[]
```
[06.01.2025 07:11] Extracting affiliations from text.
[06.01.2025 07:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 3 7 0 1 0 . 1 0 5 2 : r Graph Generative Pre-trained Transformer Xiaohui Chen 1 Yinkai Wang 1 Jiaxing He 2 Yuanqi Du 3 Soha Hassoun 1 Xiaolin Xu 2 Liping LiuGraph generation is critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PTs capabilities as generalpurpose foundation model, we explore fine-tuning strategies for two downstream applications: goaloriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction. 1. Introduction Graph generation has become crucial task in various domains, from chemical discovery to social network analysis, owing to graphs ability to represent complex relationships and produce realistic, structured data (Du et al., 2021; Zhu et al., 2022). Recent advancements in graph generative models primarily focus on permutation-invariant methods leveraging diffusion-based approaches (Ho et al., 2020; Austin et al., 2021). For example, models like EDP-GNN (Niu et al., 2020) and GDSS (Jo et al., 2022a) represent graphs using continuous adjacency matrices. In contrast, DiGress (Vignac et al., 2022) and EDGE (Chen et al., 2023) employ discrete diffusion, treating node types and all node pairs 1Tufts University 2Northeastern University 3Cornell University. Correspondence to: Xiaohui Chen <xiaohui.chen@tufts.edu>, Liping Liu <liping.liu@tufts.edu>. 1 (edges) as categorical variables. Earlier neural graph generation methods introduced permutation-dependent models, such as GraphRNN (You et al., 2018b) and DeepGMG (Li et al., 2018). hese approaches employed auto-regressive frameworks (e.g., RNNs or LSTMs (Sherstinsky, 2020)) to sequentially generate graphs. For instance, GraphRNN generates adjacency matrix entries step-by-step. DeepGMG frames graph generation as sequence of actions (e.g., addnode, add-edge), and utilizes an agent-based model to learn the action trajectories. The discrete diffusion-based methods, in particular, gain more attention due to it perfectly aligns with the nature of discrete structure of graphs. However, sampling from discrete diffusion models relies on the conditional-independent assumption (Gu et al., 2017): the distribution of each entry at time is independent of others, given the observation at time t1. This may lead to poor approximation of the true distribution, as multiple trajectories can emerge from the current state. similar argument is made in Campbell et al. (2022), drawing connection to the Tau-Leaping approximation, which allows changing multiple entries at single time step. Ideally, the most accurate sampling is to change only one dimension at one timestep, or effectively, employ large number of denoising steps. In the graph generation problem, this is equivalent to only change the label of one node, or change one node pair in terms of addition, deletion or alteration at time. Such sampling schema naturally degenerates into an any-order auto-regressive model (Hoogeboom et al., 2021; Campbell et al., 2022). Due to the recent success of large language models (Achiam et al., 2023; Dubey et al., 2024), in this work, we revisit the family of auto-regressive graph generative models. We propose new method to represent graph as sequence. Specifically, our designed sequence consists of two parts: node definition and edge definition. The node definition is first established to provide information about the node index and node type. After that, the edge definition specifies how edges are connected by using the defined node indices, as well as the edge labels. Our propose representation is sparse as it only define edges that explicitly exits, contrasting to the adjacency representation. We then utilize transformer decoder to approximate the sequence distribution via the next-token prediction loss. We name it Graph Generative Pre-trained Transformer (G2PT). Model (Rep.)#Network Calls #Variables Decomposition Graph Generative Pre-trained Transformer Diffusion (A) p(AT ) (cid:89) t=1 p(At1At) Auto-regressive (A) (cid:89) i1 (cid:89) i= j=1 p(Ai,jA<i,<i1, Ai,<j) Auto-regressive (E) p(e1) (cid:89) i= p(eie<i) O(T n2) O(n2) O(n2) O(m) O(m)Table 1. Overview of graph generative model families combined with the used data representation (Rep.). n: number of nodes. m: number of edges. In the illustration, we use solid line fo"
[06.01.2025 07:11] Mistral response. {"id": "39b2339e79fa489ab3f2b242bef6aba5", "object": "chat.completion", "created": 1736147465, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Tufts University', 'Northeastern University', 'Cornell University']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1391, "total_tokens": 1417, "completion_tokens": 26}}
[06.01.2025 07:11] Response: ```python
['Tufts University', 'Northeastern University', 'Cornell University']
```
[06.01.2025 07:11] Deleting PDF ./assets/pdf/2501.01073.pdf.
[06.01.2025 07:11] Success.
[06.01.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2501.01904.
[06.01.2025 07:11] Extra JSON file exists (./assets/json/2501.01904.json), skip PDF parsing.
[06.01.2025 07:11] Paper image links file exists (./assets/img_data/2501.01904.json), skip HTML parsing.
[06.01.2025 07:11] Success.
[06.01.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2501.00874.
[06.01.2025 07:11] Extra JSON file exists (./assets/json/2501.00874.json), skip PDF parsing.
[06.01.2025 07:11] Paper image links file exists (./assets/img_data/2501.00874.json), skip HTML parsing.
[06.01.2025 07:11] Success.
[06.01.2025 07:11] Enriching papers with extra data.
[06.01.2025 07:11] ********************************************************************************
[06.01.2025 07:11] Abstract 0. We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continu...
[06.01.2025 07:11] ********************************************************************************
[06.01.2025 07:11] Abstract 1. Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in bot...
[06.01.2025 07:11] ********************************************************************************
[06.01.2025 07:11] Abstract 2. Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternativ...
[06.01.2025 07:11] ********************************************************************************
[06.01.2025 07:11] Abstract 3. Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more c...
[06.01.2025 07:11] ********************************************************************************
[06.01.2025 07:11] Abstract 4. Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largel...
[06.01.2025 07:11] Read previous papers.
[06.01.2025 07:11] Generating reviews via LLM API.
[06.01.2025 07:11] Using data from previous issue: {"categories": ["#3d", "#data", "#robotics"], "emoji": "🤖", "ru": {"title": "EnerVerse: Революция в пространственном моделировании для роботов-манипуляторов", "desc": "EnerVerse - это комплексная система для генерации пространства будущего в задачах роботизированной манипуляции. Она использует сверт
[06.01.2025 07:11] Using data from previous issue: {"categories": ["#training", "#cv", "#multimodal", "#benchmark", "#audio"], "emoji": "🗣️", "ru": {"title": "Революция в мультимодальном взаимодействии: речь и зрение в одной модели", "desc": "В статье представлена новая методология обучения мультимодальных языковых моделей, объединяющая визуальную и
[06.01.2025 07:11] Querying the API.
[06.01.2025 07:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction.
[06.01.2025 07:11] Response: {
  "desc": "В статье представлена новая модель генерации графов - Graph Generative Pre-trained Transformer (G2PT). G2PT использует альтернативный подход к представлению графов в виде последовательностей множеств узлов и рёбер вместо матриц смежности. Модель обучается предсказывать следующий токен автореgressивным способом. G2PT показывает превосходные результаты в генерации как общих графов, так и молекул, а также демонстрирует хорошую адаптивность к различным задачам.",
  "emoji": "🕸️",
  "title": "G2PT: Универсальный трансформер для эффективной генерации графов"
}
[06.01.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction."

[06.01.2025 07:11] Response: ```python
['DATASET', 'DATA', 'TRAINING', 'ARCHITECTURE']
```
[06.01.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction."

[06.01.2025 07:11] Response: ```python
['GRAPHS', 'OPTIMIZATION']
```
[06.01.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper focuses on improving graph generation, which is important for tasks like designing molecules and analyzing social networks. Instead of using the common adjacency matrix, it proposes a new way to represent graphs as sequences of node and edge sets, making the encoding more efficient. The authors introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns to generate graph structures by predicting the next token in a sequence. Through various experiments, they demonstrate that G2PT outperforms existing models in generating graphs and is effective in applications like molecular design and predicting graph properties.","title":"Revolutionizing Graph Generation with G2PT"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper focuses on improving graph generation, which is important for tasks like designing molecules and analyzing social networks. Instead of using the common adjacency matrix, it proposes a new way to represent graphs as sequences of node and edge sets, making the encoding more efficient. The authors introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns to generate graph structures by predicting the next token in a sequence. Through various experiments, they demonstrate that G2PT outperforms existing models in generating graphs and is effective in applications like molecular design and predicting graph properties.', title='Revolutionizing Graph Generation with G2PT'))
[06.01.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"图生成在许多领域中非常重要，比如分子设计和社交网络分析，因为它能够建模复杂的关系和结构化数据。本文提出了一种新的图表示方法，将图表示为节点集和边集的序列，而不是传统的邻接矩阵。基于这种表示，我们引入了图生成预训练变换器（G2PT），这是一种通过下一个标记预测学习图结构的自回归模型。实验结果表明，G2PT在通用图和分子数据集上表现出色，并且在分子设计和属性预测等下游任务中具有很强的适应性和多功能性。","title":"图生成的创新：G2PT模型"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='图生成在许多领域中非常重要，比如分子设计和社交网络分析，因为它能够建模复杂的关系和结构化数据。本文提出了一种新的图表示方法，将图表示为节点集和边集的序列，而不是传统的邻接矩阵。基于这种表示，我们引入了图生成预训练变换器（G2PT），这是一种通过下一个标记预测学习图结构的自回归模型。实验结果表明，G2PT在通用图和分子数据集上表现出色，并且在分子设计和属性预测等下游任务中具有很强的适应性和多功能性。', title='图生成的创新：G2PT模型'))
[06.01.2025 07:11] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#transfer_learning", "#training"], "emoji": "🧠", "ru": {"title": "Обучение мультимодальных ИИ длительным рассуждениям через текст", "desc": "Статья описывает исследование в области мультимодальных больших языковых моделей (MLLM) и их способности к медлен
[06.01.2025 07:11] Using data from previous issue: {"categories": ["#transfer_learning", "#architecture", "#benchmark", "#multilingual", "#low_resource"], "emoji": "🌍", "ru": {"title": "Универсальные многоязычные эмбеддинги без многоязычного обучения", "desc": "LUSIFER - это новый подход к созданию многоязычных эмбеддингов без использования многоязы
[06.01.2025 07:11] Loading Chinese text from previous data.
[06.01.2025 07:11] Renaming data file.
[06.01.2025 07:11] Renaming previous data. hf_papers.json to ./d/2025-01-06.json
[06.01.2025 07:11] Saving new data file.
[06.01.2025 07:11] Generating page.
[06.01.2025 07:11] Renaming previous page.
[06.01.2025 07:11] Renaming previous data. index.html to ./d/2025-01-06.html
[06.01.2025 07:11] [Experimental] Generating Chinese page for reading.
[06.01.2025 07:11] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '语料库', 'pinyin': 'yǔ liào kù', 'trans': 'corpus'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-training'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language models'}, {'word': '图像-文本对', 'pinyin': 'tú xiàng wén běn duì', 'trans': 'image-text pairs'}, {'word': '提取', 'pinyin': 'tí qū', 'trans': 'extract'}, {'word': '基础知识', 'pinyin': 'jī chǔ zhī shi', 'trans': 'foundational knowledge'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '分类法', 'pinyin': 'fēn lèi fǎ', 'trans': 'classification method'}, {'word': '系统地', 'pinyin': 'xì tǒng de', 'trans': 'systematically'}, {'word': '收集', 'pinyin': 'shōu jí', 'trans': 'collect'}, {'word': '教学视频', 'pinyin': 'jiào xué shì pín', 'trans': 'educational videos'}, {'word': '逐步', 'pinyin': 'zhú bù', 'trans': 'step-by-step'}, {'word': '精炼', 'pinyin': 'jīng liàn', 'trans': 'refine'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '知识', 'pinyin': 'zhī shi', 'trans': 'knowledge'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '密集型', 'pinyin': 'mì jí xíng', 'trans': 'intensive'}, {'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[06.01.2025 07:11] Renaming previous Chinese page.
[06.01.2025 07:11] Renaming previous data. zh.html to ./d/2025-01-05_zh_reading_task.html
[06.01.2025 07:11] Writing Chinese reading task.
[06.01.2025 07:11] Writing result.
[06.01.2025 07:11] Renaming log file.
[06.01.2025 07:11] Renaming previous data. log.txt to ./logs/2025-01-06_last_log.txt
