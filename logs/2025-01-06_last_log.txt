[06.01.2025 07:11] Read previous papers.
[06.01.2025 07:11] Generating top page (month).
[06.01.2025 07:11] Writing top page (month).
[06.01.2025 08:14] Read previous papers.
[06.01.2025 08:14] Get feed.
[06.01.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01895
[06.01.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01957
[06.01.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01073
[06.01.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2501.01904
[06.01.2025 08:14] Get page data from previous paper. URL: https://huggingface.co/papers/2501.00874
[06.01.2025 08:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.01.2025 08:14] No deleted papers detected.
[06.01.2025 08:14] Downloading and parsing papers (pdf, html). Total: 5.
[06.01.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2501.01895.
[06.01.2025 08:14] Extra JSON file exists (./assets/json/2501.01895.json), skip PDF parsing.
[06.01.2025 08:14] Paper image links file exists (./assets/img_data/2501.01895.json), skip HTML parsing.
[06.01.2025 08:14] Success.
[06.01.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2501.01957.
[06.01.2025 08:14] Extra JSON file exists (./assets/json/2501.01957.json), skip PDF parsing.
[06.01.2025 08:14] Paper image links file exists (./assets/img_data/2501.01957.json), skip HTML parsing.
[06.01.2025 08:14] Success.
[06.01.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2501.01073.
[06.01.2025 08:14] Extra JSON file exists (./assets/json/2501.01073.json), skip PDF parsing.
[06.01.2025 08:14] Paper image links file exists (./assets/img_data/2501.01073.json), skip HTML parsing.
[06.01.2025 08:14] Success.
[06.01.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2501.01904.
[06.01.2025 08:14] Extra JSON file exists (./assets/json/2501.01904.json), skip PDF parsing.
[06.01.2025 08:14] Paper image links file exists (./assets/img_data/2501.01904.json), skip HTML parsing.
[06.01.2025 08:14] Success.
[06.01.2025 08:14] Downloading and parsing paper https://huggingface.co/papers/2501.00874.
[06.01.2025 08:14] Extra JSON file exists (./assets/json/2501.00874.json), skip PDF parsing.
[06.01.2025 08:14] Paper image links file exists (./assets/img_data/2501.00874.json), skip HTML parsing.
[06.01.2025 08:14] Success.
[06.01.2025 08:14] Enriching papers with extra data.
[06.01.2025 08:14] ********************************************************************************
[06.01.2025 08:14] Abstract 0. We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continu...
[06.01.2025 08:14] ********************************************************************************
[06.01.2025 08:14] Abstract 1. Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in bot...
[06.01.2025 08:14] ********************************************************************************
[06.01.2025 08:14] Abstract 2. Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternativ...
[06.01.2025 08:14] ********************************************************************************
[06.01.2025 08:14] Abstract 3. Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more c...
[06.01.2025 08:14] ********************************************************************************
[06.01.2025 08:14] Abstract 4. Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largel...
[06.01.2025 08:14] Read previous papers.
[06.01.2025 08:14] Generating reviews via LLM API.
[06.01.2025 08:14] Using data from previous issue: {"categories": ["#3d", "#data", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "EnerVerse: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤", "desc": "EnerVerse - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –±—É–¥—É—â–µ–≥–æ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–µ—Ä—Ç
[06.01.2025 08:14] Using data from previous issue: {"categories": ["#training", "#cv", "#multimodal", "#benchmark", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏: —Ä–µ—á—å –∏ –∑—Ä–µ–Ω–∏–µ –≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏
[06.01.2025 08:14] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#architecture", "#data", "#graphs"], "emoji": "üï∏Ô∏è", "ru": {"title": "G2PT: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–æ–≤ - Graph Generative Pre-trained Transformer (
[06.01.2025 08:14] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#transfer_learning", "#training"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∏ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –º–µ–¥–ª–µ–Ω
[06.01.2025 08:14] Using data from previous issue: {"categories": ["#transfer_learning", "#architecture", "#benchmark", "#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–µ–∑ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "LUSIFER - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã
[06.01.2025 08:14] Loading Chinese text from previous data.
[06.01.2025 08:14] Renaming data file.
[06.01.2025 08:14] Renaming previous data. hf_papers.json to ./d/2025-01-06.json
[06.01.2025 08:14] Saving new data file.
[06.01.2025 08:14] Generating page.
[06.01.2025 08:14] Renaming previous page.
[06.01.2025 08:14] Renaming previous data. index.html to ./d/2025-01-06.html
[06.01.2025 08:14] [Experimental] Generating Chinese page for reading.
[06.01.2025 08:14] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'ËØ≠ÊñôÂ∫ì', 'pinyin': 'y«î li√†o k√π', 'trans': 'corpus'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-training'}, {'word': 'ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language models'}, {'word': 'ÂõæÂÉè-ÊñáÊú¨ÂØπ', 'pinyin': 't√∫ xi√†ng w√©n bƒõn du√¨', 'trans': 'image-text pairs'}, {'word': 'ÊèêÂèñ', 'pinyin': 't√≠ quÃÑ', 'trans': 'extract'}, {'word': 'Âü∫Á°ÄÁü•ËØÜ', 'pinyin': 'jƒ´ ch«î zhƒ´ shi', 'trans': 'foundational knowledge'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨ q√≠', 'trans': 'alignment'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'ÂàÜÁ±ªÊ≥ï', 'pinyin': 'fƒìn l√®i f«é', 'trans': 'classification method'}, {'word': 'Á≥ªÁªüÂú∞', 'pinyin': 'x√¨ t«íng de', 'trans': 'systematically'}, {'word': 'Êî∂ÈõÜ', 'pinyin': 'sh≈çu j√≠', 'trans': 'collect'}, {'word': 'ÊïôÂ≠¶ËßÜÈ¢ë', 'pinyin': 'ji√†o xu√© sh√¨ p√≠n', 'trans': 'educational videos'}, {'word': 'ÈÄêÊ≠•', 'pinyin': 'zh√∫ b√π', 'trans': 'step-by-step'}, {'word': 'Á≤æÁÇº', 'pinyin': 'jƒ´ng li√†n', 'trans': 'refine'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'Èü≥È¢ë', 'pinyin': 'yƒ´n p√≠n', 'trans': 'audio'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'Áü•ËØÜ', 'pinyin': 'zhƒ´ shi', 'trans': 'knowledge'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ÂØÜÈõÜÂûã', 'pinyin': 'm√¨ j√≠ x√≠ng', 'trans': 'intensive'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n wu', 'trans': 'task'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†i m«é', 'trans': 'code'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[06.01.2025 08:14] Renaming previous Chinese page.
[06.01.2025 08:14] Renaming previous data. zh.html to ./d/2025-01-05_zh_reading_task.html
[06.01.2025 08:14] Writing Chinese reading task.
[06.01.2025 08:14] Writing result.
[06.01.2025 08:14] Renaming log file.
[06.01.2025 08:14] Renaming previous data. log.txt to ./logs/2025-01-06_last_log.txt
