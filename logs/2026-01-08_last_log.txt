[08.01.2026 17:28] Read previous papers.
[08.01.2026 17:28] Generating top page (month).
[08.01.2026 17:28] Writing top page (month).
[08.01.2026 18:31] Read previous papers.
[08.01.2026 18:31] Get feed.
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02151
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03509
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03872
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03986
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04151
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04194
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04171
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02075
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00423
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03471
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03699
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03315
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03467
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03448
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02933
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03236
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04090
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03955
[08.01.2026 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00705
[08.01.2026 18:31] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.01.2026 18:31] No deleted papers detected.
[08.01.2026 18:31] Downloading and parsing papers (pdf, html). Total: 19.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.02151.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.02151.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.02151.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03509.
[08.01.2026 18:31] Downloading paper 2601.03509 from https://arxiv.org/pdf/2601.03509v1...
[08.01.2026 18:31] Failed to download and parse paper https://huggingface.co/papers/2601.03509: 'LTChar' object is not iterable
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03872.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03872.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03872.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03986.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03986.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03986.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.04151.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.04151.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.04151.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.04194.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.04194.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.04194.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.04171.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.04171.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.04171.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.02075.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.02075.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.02075.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.00423.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.00423.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.00423.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03471.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03471.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03471.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03699.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03699.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03699.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03315.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03315.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03315.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03467.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03467.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03467.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03448.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03448.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03448.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.02933.
[08.01.2026 18:31] Downloading paper 2601.02933 from https://arxiv.org/pdf/2601.02933v1...
[08.01.2026 18:31] Failed to download and parse paper https://huggingface.co/papers/2601.02933: 'LTChar' object is not iterable
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03236.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03236.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03236.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.04090.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.04090.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.04090.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.03955.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.03955.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.03955.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Downloading and parsing paper https://huggingface.co/papers/2601.00705.
[08.01.2026 18:31] Extra JSON file exists (./assets/json/2601.00705.json), skip PDF parsing.
[08.01.2026 18:31] Paper image links file exists (./assets/img_data/2601.00705.json), skip HTML parsing.
[08.01.2026 18:31] Success.
[08.01.2026 18:31] Enriching papers with extra data.
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 0. Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.  					AI-generated summary 				 Supervised Fine-Tuning (SFT) is the standard...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 1. Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.  					AI-generated summary 				 We study continual skill acquisition in open-ended embodied environments wh...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 2. ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.  					AI-generated summary 				 The integra...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 3. Researchers developed Benchmark^2, a framework with three metrics to evaluate benchmark quality for large language models, revealing significant variations in existing benchmarks and enabling more efficient evaluation through selective benchmark construction.  					AI-generated summary 				 The rapi...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 4. Klear addresses audio-video joint generation challenges through a unified model architecture, progressive multitask training, and large-scale dense-caption data construction, achieving superior alignment and generalization.  					AI-generated summary 				 Audio-video joint generation has progressed ...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 5. CHORD is a universal generative framework that extracts Lagrangian motion information from Eulerian video representations to synthesize diverse 4D dynamic scenes without requiring category-specific rules or large datasets.  					AI-generated summary 				 Dynamic objects in our physical 4D (3D + time...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 6. Agentic Rubrics enable efficient and scalable verification for software engineering agents by creating context-aware checklists that outperform traditional methods while maintaining interpretability.  					AI-generated summary 				 Verification is critical for improving agents: it provides the rewar...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 7. MDAgent2 enables automated molecular dynamics code generation and question answering through domain-adapted language models and a multi-agent runtime system.  					AI-generated summary 				 Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials scienc...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 8. Entropy-aware policy optimization method for reinforcement learning in flow matching models that improves exploration through SDE and ODE sampling strategies.  					AI-generated summary 				 Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stoc...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 9. EpiQAL presents a novel benchmark for evaluating epidemiological reasoning in language models through three distinct subsets measuring factual recall, multi-step inference, and conclusion reconstruction from scientific literature.  					AI-generated summary 				 Reliable epidemiological reasoning re...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 10. RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.  					AI-generated summary 				 As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against ad...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 11. A case study of four attempts to autonomously generate ML research papers using LLM agents reveals recurring failure modes and proposes design principles for robust AI-scientist systems.  					AI-generated summary 				 We report a case study of four end-to-end attempts to autonomously generate ML re...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 12. ThinkRL-Edit enhances reasoning-centric image editing through reinforcement learning by expanding visual reasoning exploration beyond denoising stochasticity and using unbiased reward strategies.  					AI-generated summary 				 Instruction-driven image editing with unified multimodal generative mode...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 13. Language models pre-trained with a framework combining standard next-token prediction and structured language learning tasks show enhanced linguistic competence without sacrificing general reasoning capabilities.  					AI-generated summary 				 Language models (LMs) are pre-trained on raw text datas...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 14. Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.  					AI-generated summary 				 Human evaluation is the gold standard for multilingual NLP, but is ofte...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 15. MAGMA is a multi-graph memory architecture that improves long-context reasoning in language models by separating memory representation from retrieval logic across semantic, temporal, causal, and entity dimensions.  					AI-generated summary 				 Memory-Augmented Generation (MAG) extends Large Langua...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 16. Gen3R combines foundational reconstruction models with video diffusion models to generate 3D scenes with RGB videos and geometric information through aligned latents.  					AI-generated summary 				 We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and v...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 17. A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.  					AI-generated summary 				 Existing 1D visual tokeniz...
[08.01.2026 18:31] ********************************************************************************
[08.01.2026 18:31] Abstract 18. RGS-SLAM presents a robust Gaussian-splatting SLAM framework that uses dense multi-view correspondences and DINOv3 descriptors for efficient, stable mapping with improved rendering fidelity.  					AI-generated summary 				 We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replac...
[08.01.2026 18:31] Read previous papers.
[08.01.2026 18:31] Generating reviews via LLM API.
[08.01.2026 18:31] Using data from previous issue: {"categories": [], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —ç–Ω—Ç—Ä–æ–ø–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (SFT) –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ
[08.01.2026 18:31] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è –Ω–∞–≤—ã–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Programmatic Skill Network (PSN) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ä–µ–¥–∞—Ö, –≥–¥–µ –Ω–∞–≤—ã–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–µ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≥
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rl", "#agents", "#benchmark", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é", "desc": "ATLAS ‚Äî —ç—Ç–æ –¥–≤—É—Ö–ø—É—Ç—ë–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#benchmark"], "emoji": "üìè", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π: —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Benchmark¬≤, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –º–µ—Ç
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#data", "#training", "#dataset", "#audio", "#video", "#multimodal", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Ä–æ–∂–¥–µ–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –æ–±—Ä–∞–∑–∞: –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Klear ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≥–µ–Ω–µ
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#video", "#synthetic", "#3d", "#multimodal", "#robotics"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 4D –¥–∏–Ω–∞–º–∏–∫–∏ –∏–∑ 2D –≤–∏–¥–µ–æ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª", "desc": "CHORD ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–≤–∏–∂–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ 2D –≤–∏–¥–µ–æ –≤ –≠
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#interpretability", "#plp", "#benchmark", "#rl"], "emoji": "‚úÖ", "ru": {"title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —á–µ–∫–ª–∏—Å—Ç—ã –≤–º–µ—Å—Ç–æ –∫–æ–¥–∞: –∞–≥–µ–Ω—Ç–∏–≤–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Agentic Rubrics –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏–π –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –±
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#data", "#training", "#agents", "#dataset", "#plp", "#benchmark", "#rl", "#optimization", "#open_source", "#science", "#small_models"], "emoji": "‚öõÔ∏è", "ru": {"title": "–Ø–∑—ã–∫ –¥–ª—è –Ω–∞—É–∫–∏: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "MDAgent2 ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#optimization"], "emoji": "üé≤", "ru": {"title": "–≠–Ω—Ç—Ä–æ–ø–∏—è –∫–∞–∫ –∫–ª—é—á –∫ –ª—É—á—à–µ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è E-GRPO ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π —ç–Ω—Ç—Ä–æ–ø–∏—é, –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ 
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#healthcare", "#science"], "emoji": "üî¨", "ru": {"title": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω EpiQAL ‚Äî –ø–µ—Ä–≤—ã–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è 
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#open_source", "#security", "#dataset", "#benchmark"], "emoji": "üîí", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RedBench ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π 37 —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#hallucinations", "#science"], "emoji": "ü§ñ", "ru": {"title": "–ù–∞ –ø—É—Ç–∏ –∫ –Ω–∞–¥–µ–∂–Ω—ã–º AI-—É—á–µ–Ω—ã–º: —É—Ä–æ–∫–∏ –∏–∑ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é —Å–∏—Å—Ç–µ–º—ã –∏–∑ —à–µ—Å
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#reasoning", "#training", "#cv", "#rl", "#optimization", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ reinforcement learning –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "ThinkRL-Edit ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ
[08.01.2026 18:31] Using data from previous issue: {"categories": [], "emoji": "üìö", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L2T ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ —Å —è–≤–Ω—ã–º–∏ –∑
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#machine_translation", "#open_source", "#multilingual", "#benchmark"], "emoji": "üë•", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ: —É–ø—Ä–æ—â–µ–Ω–∏–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ NLP —á–µ—Ä–µ–∑ Pearmut", "desc": "Pearmut ‚Äî —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä–∞—è —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ª—é–¥—å–º–∏ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#rag", "#graphs", "#long_context", "#interpretability", "#reasoning", "#agents", "#architecture"], "emoji": "üß†", "ru": {"title": "–ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "MAGMA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ–≥—Ä–∞—Ñ–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–æ–ª–≥–æ–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#video", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–µ–Ω–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "Gen3R –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω. 
[08.01.2026 18:31] Using data from previous issue: {"categories": ["#cv", "#training", "#architecture"], "emoji": "üèóÔ∏è", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Å—Ç–∞—Ç–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –≤–∏–¥–µ–Ω–∏–µ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Residual Tokenizer, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Å—Ç–∞—Ç–∫–∏
[08.01.2026 18:31] Using data from previous issue: {"categories": [], "emoji": "üó∫Ô∏è", "ru": {"title": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–ª–æ—Ç–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ì–∞—É—Å—Å–æ–≤–∞ SLAM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç RGS-SLAM, –º–æ—â–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ì–∞—É—Å—Å–æ–≤—ã—Ö —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω—ã. –í–º–µ—Å—Ç–æ –ø–æ—Å—Ç–µ–ø
[08.01.2026 18:31] Renaming data file.
[08.01.2026 18:31] Renaming previous data. hf_papers.json to ./d/2026-01-08.json
[08.01.2026 18:31] Saving new data file.
[08.01.2026 18:31] Generating page.
[08.01.2026 18:31] Renaming previous page.
[08.01.2026 18:31] Renaming previous data. index.html to ./d/2026-01-08.html
[08.01.2026 18:31] Writing result.
[08.01.2026 18:31] Renaming log file.
[08.01.2026 18:31] Renaming previous data. log.txt to ./logs/2026-01-08_last_log.txt
