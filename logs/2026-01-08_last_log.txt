[08.01.2026 14:28] Read previous papers.
[08.01.2026 14:28] Generating top page (month).
[08.01.2026 14:28] Writing top page (month).
[08.01.2026 15:28] Read previous papers.
[08.01.2026 15:28] Get feed.
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02151
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03509
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03872
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03986
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04151
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04194
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04171
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02075
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00423
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03699
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03471
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03448
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03315
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03467
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03236
[08.01.2026 15:28] Extract page data from URL. URL: https://huggingface.co/papers/2601.02933
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04090
[08.01.2026 15:28] Extract page data from URL. URL: https://huggingface.co/papers/2601.03955
[08.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00705
[08.01.2026 15:28] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.01.2026 15:28] No deleted papers detected.
[08.01.2026 15:28] Downloading and parsing papers (pdf, html). Total: 19.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.02151.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.02151.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.02151.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03509.
[08.01.2026 15:28] Downloading paper 2601.03509 from https://arxiv.org/pdf/2601.03509v1...
[08.01.2026 15:28] Failed to download and parse paper https://huggingface.co/papers/2601.03509: 'LTChar' object is not iterable
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03872.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03872.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03872.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03986.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03986.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03986.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.04151.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.04151.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.04151.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.04194.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.04194.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.04194.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.04171.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.04171.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.04171.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.02075.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.02075.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.02075.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.00423.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.00423.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.00423.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03699.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03699.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03699.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03471.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03471.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03471.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03448.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03448.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03448.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03315.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03315.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03315.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03467.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03467.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03467.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03236.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.03236.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.03236.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.02933.
[08.01.2026 15:28] Downloading paper 2601.02933 from https://arxiv.org/pdf/2601.02933v1...
[08.01.2026 15:28] Failed to download and parse paper https://huggingface.co/papers/2601.02933: 'LTChar' object is not iterable
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.04090.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.04090.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.04090.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.03955.
[08.01.2026 15:28] Downloading paper 2601.03955 from https://arxiv.org/pdf/2601.03955v1...
[08.01.2026 15:28] Extracting affiliations from text.
[08.01.2026 15:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation Xu Zhang1,2,* Cheng Da2 Huan Yang2, Kun Gai2 Ming Lu1, Zhan Ma1 1Vision Lab, Nanjing University 2Kolors Team, Kuaishou Technology 6 2 0 2 ] . [ 1 5 5 9 3 0 . 1 0 6 2 : r a "
[08.01.2026 15:28] Response: ```python
[
    "Vision Lab, Nanjing University",
    "Kolors Team, Kuaishou Technology"
]
```
[08.01.2026 15:28] Deleting PDF ./assets/pdf/2601.03955.pdf.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.00705.
[08.01.2026 15:28] Extra JSON file exists (./assets/json/2601.00705.json), skip PDF parsing.
[08.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.00705.json), skip HTML parsing.
[08.01.2026 15:28] Success.
[08.01.2026 15:28] Enriching papers with extra data.
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 0. Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.  					AI-generated summary 				 Supervised Fine-Tuning (SFT) is the standard...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 1. Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.  					AI-generated summary 				 We study continual skill acquisition in open-ended embodied environments wh...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 2. ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.  					AI-generated summary 				 The integra...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 3. Researchers developed Benchmark^2, a framework with three metrics to evaluate benchmark quality for large language models, revealing significant variations in existing benchmarks and enabling more efficient evaluation through selective benchmark construction.  					AI-generated summary 				 The rapi...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 4. Klear addresses audio-video joint generation challenges through a unified model architecture, progressive multitask training, and large-scale dense-caption data construction, achieving superior alignment and generalization.  					AI-generated summary 				 Audio-video joint generation has progressed ...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 5. CHORD is a universal generative framework that extracts Lagrangian motion information from Eulerian video representations to synthesize diverse 4D dynamic scenes without requiring category-specific rules or large datasets.  					AI-generated summary 				 Dynamic objects in our physical 4D (3D + time...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 6. Agentic Rubrics enable efficient and scalable verification for software engineering agents by creating context-aware checklists that outperform traditional methods while maintaining interpretability.  					AI-generated summary 				 Verification is critical for improving agents: it provides the rewar...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 7. MDAgent2 enables automated molecular dynamics code generation and question answering through domain-adapted language models and a multi-agent runtime system.  					AI-generated summary 				 Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials scienc...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 8. Entropy-aware policy optimization method for reinforcement learning in flow matching models that improves exploration through SDE and ODE sampling strategies.  					AI-generated summary 				 Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stoc...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 9. RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.  					AI-generated summary 				 As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against ad...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 10. EpiQAL presents a novel benchmark for evaluating epidemiological reasoning in language models through three distinct subsets measuring factual recall, multi-step inference, and conclusion reconstruction from scientific literature.  					AI-generated summary 				 Reliable epidemiological reasoning re...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 11. Language models pre-trained with a framework combining standard next-token prediction and structured language learning tasks show enhanced linguistic competence without sacrificing general reasoning capabilities.  					AI-generated summary 				 Language models (LMs) are pre-trained on raw text datas...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 12. A case study of four attempts to autonomously generate ML research papers using LLM agents reveals recurring failure modes and proposes design principles for robust AI-scientist systems.  					AI-generated summary 				 We report a case study of four end-to-end attempts to autonomously generate ML re...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 13. ThinkRL-Edit enhances reasoning-centric image editing through reinforcement learning by expanding visual reasoning exploration beyond denoising stochasticity and using unbiased reward strategies.  					AI-generated summary 				 Instruction-driven image editing with unified multimodal generative mode...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 14. MAGMA is a multi-graph memory architecture that improves long-context reasoning in language models by separating memory representation from retrieval logic across semantic, temporal, causal, and entity dimensions.  					AI-generated summary 				 Memory-Augmented Generation (MAG) extends Large Langua...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 15. Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.  					AI-generated summary 				 Human evaluation is the gold standard for multilingual NLP, but is ofte...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 16. Gen3R combines foundational reconstruction models with video diffusion models to generate 3D scenes with RGB videos and geometric information through aligned latents.  					AI-generated summary 				 We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and v...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 17. A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.  					AI-generated summary 				 Existing 1D visual tokeniz...
[08.01.2026 15:28] ********************************************************************************
[08.01.2026 15:28] Abstract 18. RGS-SLAM presents a robust Gaussian-splatting SLAM framework that uses dense multi-view correspondences and DINOv3 descriptors for efficient, stable mapping with improved rendering fidelity.  					AI-generated summary 				 We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replac...
[08.01.2026 15:28] Read previous papers.
[08.01.2026 15:28] Generating reviews via LLM API.
[08.01.2026 15:28] Using data from previous issue: {"categories": [], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —ç–Ω—Ç—Ä–æ–ø–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (SFT) –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ
[08.01.2026 15:28] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è –Ω–∞–≤—ã–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Programmatic Skill Network (PSN) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ä–µ–¥–∞—Ö, –≥–¥–µ –Ω–∞–≤—ã–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–µ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≥
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rl", "#agents", "#benchmark", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é", "desc": "ATLAS ‚Äî —ç—Ç–æ –¥–≤—É—Ö–ø—É—Ç—ë–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#benchmark"], "emoji": "üìè", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π: —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Benchmark¬≤, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –º–µ—Ç
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#data", "#training", "#dataset", "#audio", "#video", "#multimodal", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Ä–æ–∂–¥–µ–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –æ–±—Ä–∞–∑–∞: –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –∞—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Klear ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≥–µ–Ω–µ
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#video", "#synthetic", "#3d", "#multimodal", "#robotics"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 4D –¥–∏–Ω–∞–º–∏–∫–∏ –∏–∑ 2D –≤–∏–¥–µ–æ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª", "desc": "CHORD ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–≤–∏–∂–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ 2D –≤–∏–¥–µ–æ –≤ –≠
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#interpretability", "#plp", "#benchmark", "#rl"], "emoji": "‚úÖ", "ru": {"title": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —á–µ–∫–ª–∏—Å—Ç—ã –≤–º–µ—Å—Ç–æ –∫–æ–¥–∞: –∞–≥–µ–Ω—Ç–∏–≤–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Agentic Rubrics –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏–π –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –±
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#data", "#training", "#agents", "#dataset", "#plp", "#benchmark", "#rl", "#optimization", "#open_source", "#science", "#small_models"], "emoji": "‚öõÔ∏è", "ru": {"title": "–Ø–∑—ã–∫ –¥–ª—è –Ω–∞—É–∫–∏: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "MDAgent2 ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#training", "#rl", "#alignment", "#optimization"], "emoji": "üé≤", "ru": {"title": "–≠–Ω—Ç—Ä–æ–ø–∏—è –∫–∞–∫ –∫–ª—é—á –∫ –ª—É—á—à–µ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è E-GRPO ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π —ç–Ω—Ç—Ä–æ–ø–∏—é, –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ 
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#open_source", "#security", "#dataset", "#benchmark"], "emoji": "üîí", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RedBench ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π 37 —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#benchmark", "#healthcare", "#science"], "emoji": "üî¨", "ru": {"title": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω EpiQAL ‚Äî –ø–µ—Ä–≤—ã–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è 
[08.01.2026 15:28] Using data from previous issue: {"categories": [], "emoji": "üìö", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç L2T ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ —Å —è–≤–Ω—ã–º–∏ –∑
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#hallucinations", "#science"], "emoji": "ü§ñ", "ru": {"title": "–ù–∞ –ø—É—Ç–∏ –∫ –Ω–∞–¥–µ–∂–Ω—ã–º AI-—É—á–µ–Ω—ã–º: —É—Ä–æ–∫–∏ –∏–∑ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é —Å–∏—Å—Ç–µ–º—ã –∏–∑ —à–µ—Å
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#reasoning", "#training", "#cv", "#rl", "#optimization", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ reinforcement learning –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "ThinkRL-Edit ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#rag", "#graphs", "#long_context", "#interpretability", "#reasoning", "#agents", "#architecture"], "emoji": "üß†", "ru": {"title": "–ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "MAGMA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ–≥—Ä–∞—Ñ–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–æ–ª–≥–æ–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂
[08.01.2026 15:28] Querying the API.
[08.01.2026 15:28] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.  					AI-generated summary 				 Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.
[08.01.2026 15:28] Response: ```json
{
  "desc": "Pearmut ‚Äî —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä–∞—è —É–ø—Ä–æ—â–∞–µ—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ª—é–¥—å–º–∏ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ª—ë–≥–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –±–∞—Ä—å–µ—Ä—ã –ø—Ä–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö NLP —Å–∏—Å—Ç–µ–º, –Ω–æ —á–∞—Å—Ç–æ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏–∑-–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Å–æ–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. Pearmut –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ü–µ–Ω–∫–∏ (DA, ESA, MQM), —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–µ –ø–æ–¥ –Ω–æ–≤—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã, –∏ –≤–∫–ª—é—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∞–±—Å–æ–ª—é—Ç–Ω—É—é –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É, –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–µ–ª–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º –∏ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–º–µ—Å—Ç–æ —Ä–µ–¥–∫–∏—Ö —É—Å–∏–ª–∏–π.",
  "emoji": "üë•",
  "title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ: —É–ø—Ä–æ—â–µ–Ω–∏–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ NLP —á–µ—Ä–µ–∑ Pearmut"
}
```
[08.01.2026 15:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.  					AI-generated summary 				 Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort."

[08.01.2026 15:28] Response: ```python
['BENCHMARK', 'MULTILINGUAL']
```
[08.01.2026 15:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.  					AI-generated summary 				 Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort."

[08.01.2026 15:28] Response: ```python
['TRANSLATION', 'OPEN_SOURCE']
```
[08.01.2026 15:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Pearmut is a platform designed to streamline human evaluation in multilingual natural language processing (NLP). It addresses the challenges of traditional evaluation methods, which are often complex and time-consuming, by offering a lightweight solution that supports various evaluation protocols and learning strategies. The platform focuses on machine translation tasks and allows for both standard and customizable evaluation approaches, making it easier for researchers to incorporate human evaluation into their workflows. By simplifying the process, Pearmut aims to make reliable human evaluation a regular part of model development and assessment.","title":"Simplifying Human Evaluation in Multilingual NLP with Pearmut"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Pearmut is a platform designed to streamline human evaluation in multilingual natural language processing (NLP). It addresses the challenges of traditional evaluation methods, which are often complex and time-consuming, by offering a lightweight solution that supports various evaluation protocols and learning strategies. The platform focuses on machine translation tasks and allows for both standard and customizable evaluation approaches, making it easier for researchers to incorporate human evaluation into their workflows. By simplifying the process, Pearmut aims to make reliable human evaluation a regular part of model development and assessment.', title='Simplifying Human Evaluation in Multilingual NLP with Pearmut'))
[08.01.2026 15:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PearmutÊòØ‰∏Ä‰∏™ÁÆÄÂåñÂ§öËØ≠Ë®ÄËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∫∫Á±ªËØÑ‰º∞ÁöÑÂπ≥Âè∞ÔºåÊèê‰æõËΩªÈáèÁ∫ßÁöÑÁ´ØÂà∞Á´ØËØÑ‰º∞Ëß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆÉÊîØÊåÅÂ§öÁßçËØÑ‰º∞ÂçèËÆÆÂíåÂ≠¶‰π†Á≠ñÁï•ÔºåÁâπÂà´ÂÖ≥Ê≥®Êú∫Âô®ÁøªËØë‰ªªÂä°„ÄÇÈÄöËøáÂÆûÁé∞Ê†áÂáÜËØÑ‰º∞ÂçèËÆÆÔºåPearmutÊ∂àÈô§‰∫ÜÂ∏∏ËßÅÁöÑÂÖ•Èó®ÈöúÁ¢çÔºå‰Ωø‰∫∫Á±ªËØÑ‰º∞ÂèòÂæóÂÉèËá™Âä®ËØÑ‰º∞‰∏ÄÊ†∑ÁÆÄÂçï„ÄÇËØ•Âπ≥Âè∞‰ΩøÂæóÂèØÈù†ÁöÑ‰∫∫Á±ªËØÑ‰º∞Êàê‰∏∫Ê®°ÂûãÂºÄÂèëÂíåËØäÊñ≠ÁöÑÂ∏∏ËßÑÁªÑÊàêÈÉ®ÂàÜÔºåËÄå‰∏çÊòØÂÅ∂Â∞îÁöÑÂä™Âäõ„ÄÇ","title":"ÁÆÄÂåñÂ§öËØ≠Ë®ÄNLPÁöÑ‰∫∫Á±ªËØÑ‰º∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PearmutÊòØ‰∏Ä‰∏™ÁÆÄÂåñÂ§öËØ≠Ë®ÄËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∫∫Á±ªËØÑ‰º∞ÁöÑÂπ≥Âè∞ÔºåÊèê‰æõËΩªÈáèÁ∫ßÁöÑÁ´ØÂà∞Á´ØËØÑ‰º∞Ëß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆÉÊîØÊåÅÂ§öÁßçËØÑ‰º∞ÂçèËÆÆÂíåÂ≠¶‰π†Á≠ñÁï•ÔºåÁâπÂà´ÂÖ≥Ê≥®Êú∫Âô®ÁøªËØë‰ªªÂä°„ÄÇÈÄöËøáÂÆûÁé∞Ê†áÂáÜËØÑ‰º∞ÂçèËÆÆÔºåPearmutÊ∂àÈô§‰∫ÜÂ∏∏ËßÅÁöÑÂÖ•Èó®ÈöúÁ¢çÔºå‰Ωø‰∫∫Á±ªËØÑ‰º∞ÂèòÂæóÂÉèËá™Âä®ËØÑ‰º∞‰∏ÄÊ†∑ÁÆÄÂçï„ÄÇËØ•Âπ≥Âè∞‰ΩøÂæóÂèØÈù†ÁöÑ‰∫∫Á±ªËØÑ‰º∞Êàê‰∏∫Ê®°ÂûãÂºÄÂèëÂíåËØäÊñ≠ÁöÑÂ∏∏ËßÑÁªÑÊàêÈÉ®ÂàÜÔºåËÄå‰∏çÊòØÂÅ∂Â∞îÁöÑÂä™Âäõ„ÄÇ', title='ÁÆÄÂåñÂ§öËØ≠Ë®ÄNLPÁöÑ‰∫∫Á±ªËØÑ‰º∞'))
[08.01.2026 15:28] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#video", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–µ–Ω–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "Gen3R –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω. 
[08.01.2026 15:28] Querying the API.
[08.01.2026 15:28] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.  					AI-generated summary 				 Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.
[08.01.2026 15:28] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Residual Tokenizer, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Å—Ç–∞—Ç–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∑–∞–∏–º—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏–∑ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–∏–¥–µ–Ω–∏–µ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞, –≤–∫–ª—é—á–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–≤—è–∑–∏. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Å—Ç–∞—Ç–∫–∏ –∫–∞–∫ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∫—Ä–æ—Å—Å-—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Å–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –±–æ–ª–µ–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –í–≤–µ–¥–µ–Ω–Ω—ã–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è —Ü–µ–ª—ã–π —É—Ä–æ–≤–µ–Ω—å —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ —Ä–∞–∑ –≤–º–µ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –¥–æ—Å—Ç–∏–≥–∞—è gFID 2.34 –Ω–∞ ImageNet-256 –≤—Å–µ–≥–æ –∑–∞ 9 —à–∞–≥–æ–≤.",
  "emoji": "üèóÔ∏è",
  "title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –æ—Å—Ç–∞—Ç–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –≤–∏–¥–µ–Ω–∏–µ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ"
}
```
[08.01.2026 15:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.  					AI-generated summary 				 Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok."

[08.01.2026 15:28] Response: ```python
['CV', 'ARCHITECTURE', 'TRAINING']
```
[08.01.2026 15:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.  					AI-generated summary 				 Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok."

[08.01.2026 15:28] Response: ```python
['OPTIMIZATION']
```

**Reasoning:** The paper introduces a novel visual tokenizer (Residual Tokenizer) designed to improve autoregressive image generation efficiency and effectiveness. The core contribution focuses on optimizing the tokenization and generation process through hierarchical residual designs and reducing sampling steps, which are optimization-related improvements to model training and inference. While the paper involves generative models and vision, it primarily addresses optimization of the generation pipeline rather than fitting other listed topics like DIFFUSION (which focuses on diffusion-based models), SURVEY, or other categories.
[08.01.2026 15:28] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


**Reasoning:** The paper introduces a novel visual tokenizer (Residual Tokenizer) designed to improve autoregressive image generation efficiency and effectiveness. The core contribution focuses on optimizing the tokenization and generation process through hierarchical residual designs and reducing sampling steps, which are optimization-related improvements to model training and inference. While the paper involves generative models and vision, it primarily addresses optimization of the generation pipeline rather than fitting other listed topics like DIFFUSION (which focuses on diffusion-based models), SURVEY, or other categories.
[08.01.2026 15:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents a new visual tokenizer called Residual Tokenizer (ResTok) designed to enhance autoregressive image generation. Unlike traditional tokenizers that mimic language models, ResTok utilizes hierarchical residuals to better capture the structure of visual data. This approach allows for improved feature fusion and reduces information overlap, leading to more effective latent distributions for modeling. Additionally, a hierarchical autoregressive generator is introduced, which accelerates the generation process by predicting multiple latent tokens simultaneously, resulting in significant performance improvements.","title":"Revolutionizing Image Generation with Hierarchical Residuals"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents a new visual tokenizer called Residual Tokenizer (ResTok) designed to enhance autoregressive image generation. Unlike traditional tokenizers that mimic language models, ResTok utilizes hierarchical residuals to better capture the structure of visual data. This approach allows for improved feature fusion and reduces information overlap, leading to more effective latent distributions for modeling. Additionally, a hierarchical autoregressive generator is introduced, which accelerates the generation process by predicting multiple latent tokens simultaneously, resulting in significant performance improvements.', title='Revolutionizing Image Generation with Hierarchical Residuals'))
[08.01.2026 15:28] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑ1DËßÜËßâÊ†áËÆ∞Âô®ÔºåÁß∞‰∏∫ÊÆãÂ∑ÆÊ†áËÆ∞Âô®ÔºàResidual TokenizerÔºâÔºåÂÆÉÈÄöËøáÂºïÂÖ•Â±ÇÊ¨°ÊÆãÂ∑ÆÊù•ÊîπÂñÑËá™ÂõûÂΩíÂõæÂÉèÁîüÊàê„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éËØ≠Ë®ÄÂª∫Ê®°ÁöÑÊñπÊ≥ï‰∏çÂêåÔºåÊÆãÂ∑ÆÊ†áËÆ∞Âô®Âà©Áî®ËßÜËßâÁâπÂÆöÁöÑËÆæËÆ°ÂéüÂàôÔºåÊûÑÂª∫Â±ÇÊ¨°ÂåñÁöÑÊÆãÂ∑ÆÁªìÊûÑÔºå‰ª•Â¢ûÂº∫ÂõæÂÉèÂíåÊΩúÂú®Ê†áËÆ∞ÁöÑË°®Á§∫ËÉΩÂäõ„ÄÇÈÄöËøáÈÄêÊ≠•ÂêàÂπ∂Ëé∑ÂæóÁöÑÂ±ÇÊ¨°Ë°®Á§∫ÔºåËÉΩÂ§üÂú®ÊØè‰∏ÄÂ±ÇÂÆûÁé∞Ë∑®Â±ÇÁâπÂæÅËûçÂêàÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑË°®Áé∞Âäõ„ÄÇÂêåÊó∂ÔºåÂ±ÇÊ¨°‰πãÈó¥ÁöÑËØ≠‰πâÊÆãÂ∑ÆÈò≤Ê≠¢‰∫Ü‰ø°ÊÅØÈáçÂè†Ôºå‰ΩøÂæóÊΩúÂú®ÂàÜÂ∏ÉÊõ¥Âä†ÈõÜ‰∏≠Ôºå‰æø‰∫éËá™ÂõûÂΩíÂª∫Ê®°„ÄÇ","title":"ÂºïÂÖ•Â±ÇÊ¨°ÊÆãÂ∑ÆÔºåÊèêÂçáÂõæÂÉèÁîüÊàêËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑ1DËßÜËßâÊ†áËÆ∞Âô®ÔºåÁß∞‰∏∫ÊÆãÂ∑ÆÊ†áËÆ∞Âô®ÔºàResidual TokenizerÔºâÔºåÂÆÉÈÄöËøáÂºïÂÖ•Â±ÇÊ¨°ÊÆãÂ∑ÆÊù•ÊîπÂñÑËá™ÂõûÂΩíÂõæÂÉèÁîüÊàê„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éËØ≠Ë®ÄÂª∫Ê®°ÁöÑÊñπÊ≥ï‰∏çÂêåÔºåÊÆãÂ∑ÆÊ†áËÆ∞Âô®Âà©Áî®ËßÜËßâÁâπÂÆöÁöÑËÆæËÆ°ÂéüÂàôÔºåÊûÑÂª∫Â±ÇÊ¨°ÂåñÁöÑÊÆãÂ∑ÆÁªìÊûÑÔºå‰ª•Â¢ûÂº∫ÂõæÂÉèÂíåÊΩúÂú®Ê†áËÆ∞ÁöÑË°®Á§∫ËÉΩÂäõ„ÄÇÈÄöËøáÈÄêÊ≠•ÂêàÂπ∂Ëé∑ÂæóÁöÑÂ±ÇÊ¨°Ë°®Á§∫ÔºåËÉΩÂ§üÂú®ÊØè‰∏ÄÂ±ÇÂÆûÁé∞Ë∑®Â±ÇÁâπÂæÅËûçÂêàÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑË°®Áé∞Âäõ„ÄÇÂêåÊó∂ÔºåÂ±ÇÊ¨°‰πãÈó¥ÁöÑËØ≠‰πâÊÆãÂ∑ÆÈò≤Ê≠¢‰∫Ü‰ø°ÊÅØÈáçÂè†Ôºå‰ΩøÂæóÊΩúÂú®ÂàÜÂ∏ÉÊõ¥Âä†ÈõÜ‰∏≠Ôºå‰æø‰∫éËá™ÂõûÂΩíÂª∫Ê®°„ÄÇ', title='ÂºïÂÖ•Â±ÇÊ¨°ÊÆãÂ∑ÆÔºåÊèêÂçáÂõæÂÉèÁîüÊàêËÉΩÂäõ'))
[08.01.2026 15:28] Using data from previous issue: {"categories": [], "emoji": "üó∫Ô∏è", "ru": {"title": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–ª–æ—Ç–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ì–∞—É—Å—Å–æ–≤–∞ SLAM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç RGS-SLAM, –º–æ—â–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—Ä–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ì–∞—É—Å—Å–æ–≤—ã—Ö —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω—ã. –í–º–µ—Å—Ç–æ –ø–æ—Å—Ç–µ–ø
[08.01.2026 15:28] Renaming data file.
[08.01.2026 15:28] Renaming previous data. hf_papers.json to ./d/2026-01-08.json
[08.01.2026 15:28] Saving new data file.
[08.01.2026 15:28] Generating page.
[08.01.2026 15:28] Renaming previous page.
[08.01.2026 15:28] Renaming previous data. index.html to ./d/2026-01-08.html
[08.01.2026 15:28] Writing result.
[08.01.2026 15:28] Renaming log file.
[08.01.2026 15:28] Renaming previous data. log.txt to ./logs/2026-01-08_last_log.txt
