[01.04.2025 12:20] Read previous papers.
[01.04.2025 12:20] Generating top page (month).
[01.04.2025 12:20] Writing top page (month).
[01.04.2025 13:22] Read previous papers.
[01.04.2025 13:22] Get feed.
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23461
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23307
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24235
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24290
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24388
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24370
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23284
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24364
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24115
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23077
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18809
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23829
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19901
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.21694
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23730
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14941
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24391
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23022
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20286
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18225
[01.04.2025 13:22] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23913
[01.04.2025 13:22] Extract page data from URL. URL: https://huggingface.co/papers/2503.22655
[01.04.2025 13:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[01.04.2025 13:22] No deleted papers detected.
[01.04.2025 13:22] Downloading and parsing papers (pdf, html). Total: 22.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23461.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.23461.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.23461.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23307.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.23307.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.23307.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.24235.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.24235.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.24235.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.24290.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.24290.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.24290.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.24388.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.24388.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.24388.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.24370.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.24370.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.24370.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23284.
[01.04.2025 13:22] Downloading paper 2503.23284 from http://arxiv.org/pdf/2503.23284v1...
[01.04.2025 13:22] Failed to download and parse paper https://huggingface.co/papers/2503.23284: max() arg is an empty sequence
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.24364.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.24364.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.24364.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.24115.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.24115.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.24115.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23077.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.23077.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.23077.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.18809.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.18809.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.18809.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23829.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.23829.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.23829.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.19901.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.19901.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.19901.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.21694.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.21694.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.21694.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23730.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.23730.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.23730.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.14941.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.14941.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.14941.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.24391.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.24391.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.24391.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23022.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.23022.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.23022.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.20286.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.20286.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.20286.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.18225.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.18225.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.18225.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.23913.
[01.04.2025 13:22] Extra JSON file exists (./assets/json/2503.23913.json), skip PDF parsing.
[01.04.2025 13:22] Paper image links file exists (./assets/img_data/2503.23913.json), skip HTML parsing.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Downloading and parsing paper https://huggingface.co/papers/2503.22655.
[01.04.2025 13:22] Downloading paper 2503.22655 from http://arxiv.org/pdf/2503.22655v1...
[01.04.2025 13:22] Extracting affiliations from text.
[01.04.2025 13:22] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Unicorn: Text-Only Data Synthesis for Vision Language Model Training Xiaomin Yu 1 * Pengxiang Ding 2 3 * Wenjie Zhang 2 Siteng Huang 2 3 Songyang Gao 4 Chengwei Qin 5 Kejian Wu 1 Zhaoxin Fan 6 Ziyue Qiao 7 Donglin Wang "
[01.04.2025 13:22] Response: []
[01.04.2025 13:22] Extracting affiliations from text.
[01.04.2025 13:22] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Unicorn: Text-Only Data Synthesis for Vision Language Model Training Xiaomin Yu 1 * Pengxiang Ding 2 3 * Wenjie Zhang 2 Siteng Huang 2 3 Songyang Gao 4 Chengwei Qin 5 Kejian Wu 1 Zhaoxin Fan 6 Ziyue Qiao 7 Donglin WangTraining vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can highquality multimodal training data be synthesized purely from text? To tackle this, we propose cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds usIn Stage ing large language models (LLMs). 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instructiontuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers cost-effective and scalable solution for VLMs training. Code is available at https: //github.com/Yu-xm/Unicorn.git. 5 2 0 2 8 2 ] . [ 1 5 5 6 2 2 . 3 0 5 2 : r 1. Introduction The rapid advancement of VLMs (Liu et al., 2023; Zhu et al., 2023; Liu et al., 2024c) has underscored the critical importance of large-scale, high-quality image-text pair train1Xreal 2Westlake University 3Zhejiang University 4Shanghai AI Lab 5Nanyang Technological University 6Beihang University 7The Great Bay University. Correspondence to: Xiaomin Yu <xmyu02@gmail.com>. Figure 1. Unlike traditional image-text data synthesis frameworks, Unicorn removes the dependency on real image data, offering more efficient and scalable solution by cutting down API costs, synthesis time, and storage requirements. ing data (Chen et al., 2023; Zhao et al., 2024; Bai et al., 2024). However, scaling up such high-quality data remains an enduring challenge, constraining the potential for further breakthroughs in VLMs. Traditional methods for acquiring image-text pairs primarily rely on two strategies: (1) Manual annotation (Lin et al., 2015; Plummer et al., 2016). Manual annotation ensures quality but is constrained by cost, scale, and diversity. (2) Large-scale web crawling (Sharma et al., 2018) is scalable but introduces issues such as inconsistent data quality, compliance risks, and copyright concerns. Recent works, such as ShareGPT4V (Chen et al., 2023), have explored augmenting real images with fine-grained captions generated by advanced models like GPT-4v, resulting in highquality datasets that achieve notable improvements in visuallanguage tasks. However, such synthetic data methods still face significant cost constraints. In contrast, text data (Wu et al., 2024; Long et al., 2024) is abundant, inexpensive, and diversified, presenting an untapped opportunity. This raises fundamental question: Can we synthesize high1 Unicorn: Text-Only Data Synthesis for Vision Language Model Training quality multimodal training data for VLMs using only text, without relying on real/synthetic images? The answer is affirmative: recent studies have revealed that cross-modality representation transfer can be achieved in training-free way (Zhang et al., 2024) by exploiting the geometric structure of multimodal contrastive representation spaces (Liang et al., 2022; Zhang et al., 2023). While this phenomenon has been explored in small-scale tasks such as image captioning (Tewel et al., 2022; Li et al., 2023b) and visual question answering (Gu et al., 2023; Liu et al., 2024e), its potential for generating large-scale synthetic data has yet to be fully realized. Building on this insight, as shown in Fig. 1, we propose scalable multimodal synthetic data framework, termed Unicorn, that mitigates the modality gap (Liang et al., 2022; Zhang et al., 2023) to eliminate the dependency on real images. By utilizing the geometry of the shared representation space (Zhang et al., 2024), Unicorn generates high-quality synthetic pretraining and instruction-tuning data directly from text. Unicorn not only significantly lowers costs but also enables efficient construction of diverse, high-quality VLM training datasets, providing novel large-scale data synthesis method for VLM training. Unicorn adopts cross-integrated three-stage pipeline to construct diverse, high-quality multimodal datasets. Importantly, the first two stages synthesize data entirely within the textual modality, while the final stage performs the critical transfer from text representation space to visual representation space. This process produces two key datasets: Unicorn-1.2M for multimodal pretraining and Unicorn471K-Instruction for instruction-tuning. Specifically, in Stage 1: Diverse Caption Data Synthesis, Qwen2.5-72BInstruction (Yang et al., 2024) is used to add detailed information to 1.2M sparse caption seeds. The process ultimately yields 1.2M textual diverse captions. These diverse captions include both open-domain (Lin et al., 2015; Plummer et al., 2016; Sharma et al., 2018; Ordonez et al., 2011) and domain-specific content (Horn et al., 2021; Kaur et al., 2019), as the foundation for subsequent stages. In Stage 2: Instruction-Tuning Data Generation, 471K captions from Stage 1 are utilized with Qwen2.5-72B-Instruction (Yang et al., 2024) to generate instruction-tuning data encompassing three tasks: multiple-choice, question-answering, and complex reasoning. Finally, in Stage 3: Modality Representation Transfer, we transfer (Liang et al., 2022; Zhang et al., 2024) the diverse caption representations encoded by LLM2CLIP (Huang et al., 2024) into the visual representation space, obtaining synthetic image representations. Ultimately, we synthesize two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instructiontuning. Overall, Unicorn enables cost-efficient, scalable, and high-quality multimodal dataset generation without relying on real images. Using the synthetic datasets Unicorn-1.2M and Unicorn471K-Instruc"
[01.04.2025 13:22] Mistral response. {"id": "a5332e4316b34e4a9fda2f570d14e225", "object": "chat.completion", "created": 1743513777, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Westlake University', 'Zhejiang University', 'Shanghai AI Lab', 'Nanyang Technological University', 'Beihang University', 'The Great Bay University']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1862, "total_tokens": 1909, "completion_tokens": 47}}
[01.04.2025 13:22] Response: ```python
['Westlake University', 'Zhejiang University', 'Shanghai AI Lab', 'Nanyang Technological University', 'Beihang University', 'The Great Bay University']
```
[01.04.2025 13:22] Deleting PDF ./assets/pdf/2503.22655.pdf.
[01.04.2025 13:22] Success.
[01.04.2025 13:22] Enriching papers with extra data.
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 0. This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tac...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 1. Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly fr...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 2. As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabili...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 3. We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightfo...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 4. Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 5. Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grai...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 6. Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we ...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 7. We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally i...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 8. The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset speci...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 9. Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inf...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 10. In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-tho...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 11. Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR t...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 12. Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hin...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 13. It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 14. The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sac...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 15. Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design ...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 16. Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 17. In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impress...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 18. Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focuse...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 19. Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as ...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 20. The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring ...
[01.04.2025 13:22] ********************************************************************************
[01.04.2025 13:22] Abstract 21. Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from t...
[01.04.2025 13:22] Read previous papers.
[01.04.2025 13:22] Generating reviews via LLM API.
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#cv", "#dataset", "#benchmark"], "emoji": "üìù", "ru": {"title": "TextCrafter: –ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ TextCrafter –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é 
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#video", "#benchmark", "#story_generation"], "emoji": "üé≠", "ru": {"title": "MoCha: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ò–ò-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏—Å—Ç–æ—Ä–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ MoCha –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞—é—â–∏—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—á–∏ –∏ —Ç–µ–∫—Å—Ç–∞. 
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#survey", "#math", "#reasoning", "#training"], "emoji": "üîç", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (TTS) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#training", "#rl", "#open_source", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "Open-Reasoner-Zero - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agents", "#rl"], "emoji": "üß†", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π RIG (Reasoning and Imaginat
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#architecture", "#open_source", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏–µ–º –ò–ò: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Thinkin
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#games", "#video"], "emoji": "‚úèÔ∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Å–∫–µ—Ç—á–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–µ—Ç—á–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å –±–ª–æ–∫–∞–º–∏
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#dataset", "#inference", "#reasoning"], "emoji": "üîç", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—é—â–∏–π —Ç–æ—á–Ω–æ—Å—Ç
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#synthetic", "#open_source", "#benchmark", "#data"], "emoji": "üé≠", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –±–æ—Ä—å–±—ã —Å —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–º –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TeleAntiFraud-28k - –ø–µ—Ä–≤—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –∞—É–¥–∏–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–ª–µ–∫–æ
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#reasoning", "#survey", "#interpretability", "#optimization", "#inference", "#architecture"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –≤ –º–æ–¥–µ–ª—è—Ö –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∫—Ä—É
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "LLM –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫ –¥–ª—è –∑–∞–¥–∞—á –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–∞–Ω
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rlhf", "#training", "#rl", "#open_source"], "emoji": "üß†", "ru": {"title": "RLVR: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∑–Ω–∞–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#transfer_learning", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –≥–∏–±–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º", "desc": "TokenHSI - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization", "#3d", "#diffusion", "#open_source"], "emoji": "üßä", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ 3D-—Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#benchmark", "#multilingual"], "emoji": "üá∞üá∑", "ru": {"title": "KOFFVQA: –æ–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–æ–º —è–∑—ã–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ KOFFVQA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –Ω–∞ –∫–æ—Ä–µ–π—Å–∫–æ–º —è–∑—ã–∫–µ. –ë
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal", "#interpretability", "#alignment"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –∑–∞–¥–∞—á –≤
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#open_source", "#3d", "#video"], "emoji": "üåü", "ru": {"title": "Easi3R: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è 4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Easi3R –¥–ª—è 4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –¥–æ–æ–±—É—á–µ
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#3d", "#optimization", "#diffusion", "#games"], "emoji": "üßä", "ru": {"title": "MeshCraft: –±—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ 3D-—Å–µ—Ç–æ–∫ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ç–æ–ø–æ–ª–æ–≥–∏–µ–π", "desc": "MeshCraft - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å–µ—Ç–æ–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#robotics", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–¢–µ–Ω–∑–æ—Ä–∏–∑–∞—Ü–∏—è EMO: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–π –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (EMO) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–Ω–∑–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É—Å–∫
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "üéõÔ∏è", "ru": {"title": "DeLoRA: –£—Å—Ç–æ–π—á–∏–≤–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "DeLoRA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –æ–±—É—á–∞–µ–º—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –Ω–∏–∑–∫–æ–≥–æ —Ä–∞
[01.04.2025 13:22] Using data from previous issue: {"categories": ["#training", "#reasoning", "#math", "#benchmark", "#optimization"], "emoji": "üßÆ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ
[01.04.2025 13:22] Querying the API.
[01.04.2025 13:22] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.
[01.04.2025 13:23] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∏–Ω—Ç–µ–∑–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞—Ç—å –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö Unicorn-1.2M –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ Unicorn-471K-Instruction –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ü–æ–¥—Ö–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "emoji": "ü¶Ñ",
  "title": "–°–∏–Ω—Ç–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –í–Ø–ú"
}
[01.04.2025 13:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git."

[01.04.2025 13:23] Response: ```python
['DATASET', 'DATA', 'MULTIMODAL']
```
[01.04.2025 13:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git."

[01.04.2025 13:23] Response: ```python
['SYNTHETIC', 'REASONING']
```
[01.04.2025 13:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel framework for synthesizing multimodal training data for vision-language models (VLMs) using only text. The proposed three-stage process generates high-quality image-text pairs without the need for real images, significantly reducing costs. In the first stage, diverse captions are created from sparse seeds using large language models, followed by the generation of instruction-tuning tasks in the second stage. Finally, the textual representations are transformed into visual representations, resulting in two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, enhancing the training of VLMs.","title":"Synthesize High-Quality Multimodal Data from Text Alone!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel framework for synthesizing multimodal training data for vision-language models (VLMs) using only text. The proposed three-stage process generates high-quality image-text pairs without the need for real images, significantly reducing costs. In the first stage, diverse captions are created from sparse seeds using large language models, followed by the generation of instruction-tuning tasks in the second stage. Finally, the textual representations are transformed into visual representations, resulting in two datasets: Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, enhancing the training of VLMs.', title='Synthesize High-Quality Multimodal Data from Text Alone!'))
[01.04.2025 13:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçË∑®ÈõÜÊàêÁöÑ‰∏âÈò∂ÊÆµÂ§öÊ®°ÊÄÅÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂÉè-ÊñáÊú¨ÂØπ„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊâ©Â±ïÁ®ÄÁñèÁöÑÊñáÊú¨ÁßçÂ≠êÔºåÂêàÊàê‰∫Ü120‰∏áÊù°ËØ≠‰πâÂ§öÊ†∑ÁöÑÈ´òË¥®ÈáèÊñáÊú¨ÊèèËø∞„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÂ∞Ü47‰∏áÊù°ÊñáÊú¨ÊèèËø∞ËΩ¨Âåñ‰∏∫Â§öËΩÆÊåá‰ª§Ë∞É‰ºò‰ªªÂä°Ôºå‰ª•ÊîØÊåÅÂ§çÊùÇÊé®ÁêÜ„ÄÇÊúÄÂêéÔºåÁ¨¨‰∏âÈò∂ÊÆµÂ∞ÜÊñáÊú¨Ë°®Á§∫ËΩ¨Êç¢‰∏∫ËßÜËßâË°®Á§∫Ôºå‰ªéËÄåÁîüÊàêÂ§öÊ†∑ÁöÑÂêàÊàêÂõæÂÉèË°®Á§∫ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊó†ÈúÄÁúüÂÆûÂõæÂÉèÁöÑÈ´òÊïàËÆ≠ÁªÉÊñπÊ°à„ÄÇ","title":"Êó†ÂõæÂÉèÈ´òÊïàÂêàÊàêÂ§öÊ®°ÊÄÅÊï∞ÊçÆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçË∑®ÈõÜÊàêÁöÑ‰∏âÈò∂ÊÆµÂ§öÊ®°ÊÄÅÊï∞ÊçÆÂêàÊàêÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂÉè-ÊñáÊú¨ÂØπ„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊâ©Â±ïÁ®ÄÁñèÁöÑÊñáÊú¨ÁßçÂ≠êÔºåÂêàÊàê‰∫Ü120‰∏áÊù°ËØ≠‰πâÂ§öÊ†∑ÁöÑÈ´òË¥®ÈáèÊñáÊú¨ÊèèËø∞„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÂ∞Ü47‰∏áÊù°ÊñáÊú¨ÊèèËø∞ËΩ¨Âåñ‰∏∫Â§öËΩÆÊåá‰ª§Ë∞É‰ºò‰ªªÂä°Ôºå‰ª•ÊîØÊåÅÂ§çÊùÇÊé®ÁêÜ„ÄÇÊúÄÂêéÔºåÁ¨¨‰∏âÈò∂ÊÆµÂ∞ÜÊñáÊú¨Ë°®Á§∫ËΩ¨Êç¢‰∏∫ËßÜËßâË°®Á§∫Ôºå‰ªéËÄåÁîüÊàêÂ§öÊ†∑ÁöÑÂêàÊàêÂõæÂÉèË°®Á§∫ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊó†ÈúÄÁúüÂÆûÂõæÂÉèÁöÑÈ´òÊïàËÆ≠ÁªÉÊñπÊ°à„ÄÇ', title='Êó†ÂõæÂÉèÈ´òÊïàÂêàÊàêÂ§öÊ®°ÊÄÅÊï∞ÊçÆ'))
[01.04.2025 13:23] Loading Chinese text from previous data.
[01.04.2025 13:23] Renaming data file.
[01.04.2025 13:23] Renaming previous data. hf_papers.json to ./d/2025-04-01.json
[01.04.2025 13:23] Saving new data file.
[01.04.2025 13:23] Generating page.
[01.04.2025 13:23] Renaming previous page.
[01.04.2025 13:23] Renaming previous data. index.html to ./d/2025-04-01.html
[01.04.2025 13:23] [Experimental] Generating Chinese page for reading.
[01.04.2025 13:23] Chinese vocab [{'word': 'ËßÜÈ¢ëÁîüÊàê', 'pinyin': 'sh√¨p√≠n shƒìngch√©ng', 'trans': 'video generation'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìngw√©i', 'trans': 'called'}, {'word': 'Talking Characters', 'pinyin': 'Talking Characters', 'trans': 'Talking Characters'}, {'word': 'ËØ≠Èü≥', 'pinyin': 'y«îyƒ´n', 'trans': 'audio'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©nbƒõn', 'trans': 'text'}, {'word': 'Âä®ÁîªËßíËâ≤', 'pinyin': 'd√≤nghu√† ju√©s√®', 'trans': 'animated character'}, {'word': 'talking head', 'pinyin': 'talking head', 'trans': 'talking head'}, {'word': 'ÂÖ®Ë∫´ÂÉè', 'pinyin': 'qu√°nshƒìn xi√†ng', 'trans': 'full-body image'}, {'word': 'Èù¢ÈÉ®', 'pinyin': 'mi√†nb√π', 'trans': 'face'}, {'word': 'MoCha', 'pinyin': 'MoCha', 'trans': 'MoCha'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ËØ≠Èü≥-ËßÜÈ¢ëÁ™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂', 'pinyin': 'y«îyƒ´n sh√¨p√≠n chuƒÅngk«íu zh√πy√¨l√¨ jƒ´zh√¨', 'trans': 'audio-video window attention mechanism'}, {'word': 'Á≤æÁ°ÆÂêåÊ≠•', 'pinyin': 'jƒ´ngqu√® t√≥ngb√π', 'trans': 'precise synchronization'}, {'word': 'Â§ßËßÑÊ®°', 'pinyin': 'd√†guƒ´m√≥', 'trans': 'large-scale'}, {'word': 'ËØ≠Èü≥Ê†áÊ≥®', 'pinyin': 'y«îyƒ´n biƒÅozh√π', 'trans': 'audio annotation'}, {'word': 'ËßÜÈ¢ëÊï∞ÊçÆÈõÜ', 'pinyin': 'sh√¨p√≠n sh√πj√πj√≠', 'trans': 'video dataset'}, {'word': 'Á®ÄÁº∫', 'pinyin': 'xƒ´quƒì', 'trans': 'scarce'}, {'word': 'ËÅîÂêàËÆ≠ÁªÉÁ≠ñÁï•', 'pinyin': 'li√°nh√© x√πnli√†n c√®l√º√®', 'trans': 'joint training strategy'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠shƒìng', 'trans': 'improve'}, {'word': 'Ê≥õÂåñËÉΩÂäõ', 'pinyin': 'f√†nhu√† n√©ngl√¨', 'trans': 'generalization capability'}]
[01.04.2025 13:23] Renaming previous Chinese page.
[01.04.2025 13:23] Renaming previous data. zh.html to ./d/2025-03-31_zh_reading_task.html
[01.04.2025 13:23] Writing Chinese reading task.
[01.04.2025 13:23] Writing result.
[01.04.2025 13:23] Renaming log file.
[01.04.2025 13:23] Renaming previous data. log.txt to ./logs/2025-04-01_last_log.txt
