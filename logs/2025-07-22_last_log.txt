[22.07.2025 07:16] Read previous papers.
[22.07.2025 07:16] Generating top page (month).
[22.07.2025 07:16] Writing top page (month).
[22.07.2025 08:17] Read previous papers.
[22.07.2025 08:17] Get feed.
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14683
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15846
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14843
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15061
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11061
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15493
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15852
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15629
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15028
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15778
[22.07.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2507.15597
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15375
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11539
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15815
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15856
[22.07.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2507.15640
[22.07.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2507.10935
[22.07.2025 08:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.07.2025 08:17] No deleted papers detected.
[22.07.2025 08:17] Downloading and parsing papers (pdf, html). Total: 17.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.14683.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.14683.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.14683.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15846.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15846.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15846.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.14843.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.14843.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.14843.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15061.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15061.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15061.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.11061.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.11061.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.11061.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15493.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15493.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15493.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15852.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15852.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15852.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15629.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15629.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15629.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15028.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15028.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15028.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15778.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15778.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15778.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15597.
[22.07.2025 08:17] Downloading paper 2507.15597 from http://arxiv.org/pdf/2507.15597v1...
[22.07.2025 08:18] Extracting affiliations from text.
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos Hao Luo1,3, Yicheng Feng1,3, Wanpeng Zhang1,3, Ye Wang2,3 Haoqi Yuan1,3 Sipeng Zheng3, Jiazheng Liu1 Chaoyi Xu3 Qin Jin2 Zongqing Lu1,3, 1Peking University 2Renmin University of China 3BeingBeyond https://beingbeyond.github.io/Being-H0 5 2 0 2 1 2 ] . [ 1 7 9 5 5 1 . 7 0 5 2 : r Figure 1: Being-H0 acquires dexterous manipulation skills by learning from large-scale human videos in the UniHand dataset via physical instruction tuning. By explicitly modeling hand motion patterns, the resulting foundation model seamlessly transfers from human hand demonstrations to robotic manipulation. These authors contributed equally to this work. Correspondence to Zongqing Lu <lu@beingbeyond.com>. "
[22.07.2025 08:18] Response: ```python
["Peking University", "Renmin University of China", "BeingBeyond"]
```
[22.07.2025 08:18] Deleting PDF ./assets/pdf/2507.15597.pdf.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15375.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.15375.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.15375.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.11539.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.11539.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.11539.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15815.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.15815.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.15815.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15856.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.15856.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.15856.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15640.
[22.07.2025 08:18] Downloading paper 2507.15640 from http://arxiv.org/pdf/2507.15640v1...
[22.07.2025 08:18] Extracting affiliations from text.
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 0 4 6 5 1 . 7 0 5 2 : r Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training Kailai Yang1* Xiao Liu2 Lei Ji2 Hao Li1 Yeyun Gong2 Peng Cheng2 Mao Yang2 1 The University of Manchester 2 Microsoft Research {kailai.yang,hao.li-2}@manchester.ac.uk {xiaoliu2,leiji,yegong,pengc,maoyang}@microsoft.com "
[22.07.2025 08:18] Response: ```python
["The University of Manchester", "Microsoft Research"]
```
[22.07.2025 08:18] Deleting PDF ./assets/pdf/2507.15640.pdf.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.10935.
[22.07.2025 08:18] Downloading paper 2507.10935 from http://arxiv.org/pdf/2507.10935v1...
[22.07.2025 08:18] Extracting affiliations from text.
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization Shaowen Tong1 Zimin Xia2 Alexandre Alahi2 Xuming He1 Yujiao Shi1* 1ShanghaiTech University, China 2 Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland {tongshw2024,hexm,shiyj2}@shanghaitech.edu.cn, {zimin.xia,alexandre.alahi}@epfl.ch 5 2 0 2 5 ] . [ 1 5 3 9 0 1 . 7 0 5 2 : r a "
[22.07.2025 08:18] Response: ```python
["ShanghaiTech University, China", "Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland"]
```
[22.07.2025 08:18] Deleting PDF ./assets/pdf/2507.10935.pdf.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Enriching papers with extra data.
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 0. The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  					AI-generated summary 				 Large language models have recently evolv...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 1. Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 2. Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  					AI-generated summary 				 Recent advances in large reasoning models highlight Reinforcement Learni...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 3. A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  					AI-generated summary 				 The advent of Large Language Model (LLM)-powered agents has revo...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 4. A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  					AI-generated summary 				 Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D co...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 5. A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  					AI-generated summary 				 We report our recent progress towards building generalist robot policies, the...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 6. Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and com...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 7. 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry ...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 8. Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video l...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 9. Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform tr...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 10. Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 11. Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to com...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 12. A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  					AI-generated summary 				 Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fu...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 13. We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Censu...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 14. Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian nois...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 15. Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 16. Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learni...
[22.07.2025 08:18] Read previous papers.
[22.07.2025 08:18] Generating reviews via LLM API.
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#open_source", "#math", "#reasoning"], "emoji": "🧮", "ru": {"title": "Открытые модели для математических рассуждений на новом уровне", "desc": "MiroMind-M1 - это серия открытых языковых моделей для математических рассуждений, достигающих передо
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#reasoning", "#benchmark"], "emoji": "🖱️", "ru": {"title": "Гауссово моделирование для точного взаимодействия с GUI", "desc": "Статья представляет новый подход к обучению моделей машинного обучения для взаимодействия с графическим пользовательским 
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#optimization", "#rl"], "emoji": "🔍", "ru": {"title": "RLVR: повышение точности ценой ограничения исследования", "desc": "Исследование анализирует метод обучения с подкреплением с проверяемыми вознаграждениями (RLVR) в контексте решения сложных логических зада
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#agents", "#dataset", "#synthetic", "#reasoning", "#benchmark"], "emoji": "🕸️", "ru": {"title": "Формализация для синтеза данных: новый подход к обучению ИИ-агентов поиску информации", "desc": "WebShaper - это фреймворк для синтеза наборов данных для задач поиска информации, основан
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#3d"], "emoji": "✏️", "ru": {"title": "Точное локальное 3D-редактирование с помощью робастных масок и улучшенной регуляризации", "desc": "RoMaP - это новая система для точного локального 3D-редактирования, использующая генерацию робастных 3D-масок и улучшенную регуляризацию функции 
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agents", "#training", "#agi"], "emoji": "🤖", "ru": {"title": "GR-3: Шаг к универсальным роботам-помощникам", "desc": "Статья представляет GR-3 - крупномасштабную модель визуально-языкового действия (VLA) для робототехники. Модель демонстрирует исключит
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#benchmark", "#reasoning", "#video"], "emoji": "🎥", "ru": {"title": "Концептуальное понимание для улучшения сегментации объектов в видео", "desc": "Статья представляет новый подход к сегментации объектов в видео под названием Segment Concept (SeC). SeC ис
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#3d"], "emoji": "🎨", "ru": {"title": "Дискретизированное SDF для улучшенного обратного рендеринга с гауссовским сплаттингом", "desc": "Статья представляет новый подход к обратному рендерингу с использованием дискретизированного поля расстояний со знаком (SDF) в контексте 3D гауссовс
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#interpretability", "#security", "#benchmark", "#video"], "emoji": "🎥", "ru": {"title": "Новый рубеж в оценке видео-LLM: человекоподобное понимание реального мира", "desc": "Статья представляет новый тест Video Thinking Test (Video-TT) для оценки способности видео-LLM интерпретирова
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#benchmark"], "emoji": "🎯", "ru": {"title": "Точное обучение с подкреплением: улучшение рассуждений ИИ с помощью энтропийно-адаптивного подхода", "desc": "Статья представляет новый метод обучения с подкреплением для улучшения рассуж
[22.07.2025 08:18] Querying the API.
[22.07.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.
[22.07.2025 08:18] Response: {
  "desc": "Being-H0 - это модель зрения-языка-действия (VLA), обученная на видео с людьми для решения задач манипуляции. Модель использует физическое обучение по инструкциям и токенизацию движений на уровне частей тела для улучшения точности и обобщаемости. Being-H0 демонстрирует превосходные результаты в генерации движений рук и реальной робототехнической манипуляции. Модель обучается на масштабном наборе данных, включающем захват движений, VR и RGB-видео.",
  "emoji": "🤖",
  "title": "Обучение роботов человеческим движениям через видео"
}
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0."

[22.07.2025 08:18] Response: ```python
['DATASET', 'DATA', 'MULTIMODAL', 'ROBOTICS', 'TRAINING']
```
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0."

[22.07.2025 08:18] Response: ```python
["AGI", "OPTIMIZATION"]
```
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications.","title":"Empowering Robots with Human-Like Dexterity through Vision-Language-Action!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications.', title='Empowering Robots with Human-Like Dexterity through Vision-Language-Action!'))
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Being-H0 是一种视觉-语言-动作模型，专注于从人类视频中学习，以解决灵巧性和泛化能力的问题。该模型通过物理指令调优和部件级运动标记化，能够生成精确的手部动作并在真实世界中进行机器人操作。与传统模型相比，Being-H0 更好地处理复杂的操作任务，并能有效适应新场景。我们的研究表明，Being-H0 在手部动作生成和指令跟随方面表现出色，且在实际机器人操作中也取得了显著的进展。","title":"Being-H0：灵巧的视觉-语言-动作模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Being-H0 是一种视觉-语言-动作模型，专注于从人类视频中学习，以解决灵巧性和泛化能力的问题。该模型通过物理指令调优和部件级运动标记化，能够生成精确的手部动作并在真实世界中进行机器人操作。与传统模型相比，Being-H0 更好地处理复杂的操作任务，并能有效适应新场景。我们的研究表明，Being-H0 在手部动作生成和指令跟随方面表现出色，且在实际机器人操作中也取得了显著的进展。', title='Being-H0：灵巧的视觉-语言-动作模型'))
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#training", "#reasoning", "#audio", "#multimodal"], "emoji": "🧠", "ru": {"title": "Stitch: Думай и говори одновременно", "desc": "Статья представляет новый метод генерации речи для разговорных языковых моделей под названием Stitch. Этот метод позволяет моделям осуществлять внутренни
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#benchmark", "#architecture", "#cv"], "emoji": "🔄", "ru": {"title": "Реконструкция 4D-геометрии в реальном времени с помощью потокового трансформера", "desc": "Статья представляет потоковый 4D-трансформер визуальной геометрии для рекон
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#optimization", "#agents", "#agi", "#multimodal", "#rl", "#science"], "emoji": "🤖", "ru": {"title": "Искусственный интеллект как экономист: моделирование и оптимизация экономической политики", "desc": "Статья представляет новую концепцию под названием 'LLM Economist', которая исполь
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "🧹", "ru": {"title": "Шумоподавление как ключ к эффективным визуальным токенизаторам", "desc": "Статья представляет новый подход к разработке визуальных токенизаторов для генеративных моделей. Авторы предлагают 
[22.07.2025 08:18] Querying the API.
[22.07.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data.
[22.07.2025 08:18] Response: {
  "desc": "Data Mixing Agent - это фреймворк на основе обучения с подкреплением для переобучения больших языковых моделей. Он эффективно перевзвешивает обучающие данные для сбалансансировки производительности между исходными и целевыми областями. Этот подход превосходит сильные базовые модели в достижении сбалансированной производительности в задачах математических рассуждений. Data Mixing Agent хорошо обобщается на новые исходные области, целевые модели и пространства доменов без переобучения.",
  "emoji": "🔀",
  "title": "Умное смешивание данных для адаптивного обучения языковых моделей"
}
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."

[22.07.2025 08:18] Response: ```python
["RL", "AGENTS", "TRAINING"]
```
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."

[22.07.2025 08:18] Response: ```python
["TRANSFER_LEARNING", "OPTIMIZATION"]
```
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data.","title":"Reinforcement Learning for Balanced Data Mixing in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data.', title='Reinforcement Learning for Balanced Data Mixing in Language Models'))
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"数据混合代理是一种基于模型的框架，利用强化学习有效地重新加权训练数据，以平衡在持续预训练中源领域和目标领域的性能。该方法解决了在小规模特定任务数据上持续预训练时可能出现的灾难性遗忘问题。通过提出数据混合代理，研究者证明了更通用的启发式方法可以被参数化，从而实现端到端的学习。实验结果表明，该代理在数学推理的持续预训练中表现优于强基线，并且在未见过的源领域和目标模型上具有良好的泛化能力。","title":"数据混合代理：平衡源与目标领域的智能学习"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='数据混合代理是一种基于模型的框架，利用强化学习有效地重新加权训练数据，以平衡在持续预训练中源领域和目标领域的性能。该方法解决了在小规模特定任务数据上持续预训练时可能出现的灾难性遗忘问题。通过提出数据混合代理，研究者证明了更通用的启发式方法可以被参数化，从而实现端到端的学习。实验结果表明，该代理在数学推理的持续预训练中表现优于强基线，并且在未见过的源领域和目标模型上具有良好的泛化能力。', title='数据混合代理：平衡源与目标领域的智能学习'))
[22.07.2025 08:18] Querying the API.
[22.07.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.
[22.07.2025 08:18] Response: {
  "desc": "GeoDistill - это новый подход к кросс-видовой локализации, использующий слабо контролируемое самообучение на основе геометрии. Метод применяет обучение по схеме учитель-ученик с маскированием по полю зрения для улучшения извлечения локальных признаков. Учитель локализует панорамное изображение, а ученик предсказывает положение по ограниченному полю зрения. Такой подход позволяет модели фокусироваться на ключевых особенностях и игнорировать бестекстурные области, повышая точность локализации.",
  "emoji": "🌍",
  "title": "GeoDistill: геометрия на службе кросс-видовой локализации"
}
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill."

[22.07.2025 08:18] Response: ```python
["CV", "RL", "TRAINING"]
```
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill."

[22.07.2025 08:18] Response: ```python
["OPTIMIZATION"]
```
[22.07.2025 08:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera\'s 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation.","title":"GeoDistill: Enhancing Cross-View Localization with Weak Supervision"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera's 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation.", title='GeoDistill: Enhancing Cross-View Localization with Weak Supervision'))
[22.07.2025 08:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"跨视角定位是通过将地面图像与卫星图像对齐来估计相机的三自由度姿态，这在自动导航和增强现实等大规模户外应用中至关重要。现有方法通常依赖于完全监督学习，这需要昂贵的真实姿态标注。我们提出了GeoDistill，一个几何引导的弱监督自蒸馏框架，利用教师-学生学习和基于视场(FoV)的掩蔽来增强局部特征学习，从而实现稳健的跨视角定位。GeoDistill通过对齐学生模型和教师模型的预测，帮助学生模型专注于关键特征，提高了定位精度并减少了不确定性。","title":"GeoDistill：高效的跨视角定位解决方案"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='跨视角定位是通过将地面图像与卫星图像对齐来估计相机的三自由度姿态，这在自动导航和增强现实等大规模户外应用中至关重要。现有方法通常依赖于完全监督学习，这需要昂贵的真实姿态标注。我们提出了GeoDistill，一个几何引导的弱监督自蒸馏框架，利用教师-学生学习和基于视场(FoV)的掩蔽来增强局部特征学习，从而实现稳健的跨视角定位。GeoDistill通过对齐学生模型和教师模型的预测，帮助学生模型专注于关键特征，提高了定位精度并减少了不确定性。', title='GeoDistill：高效的跨视角定位解决方案'))
[22.07.2025 08:19] Renaming data file.
[22.07.2025 08:19] Renaming previous data. hf_papers.json to ./d/2025-07-22.json
[22.07.2025 08:19] Saving new data file.
[22.07.2025 08:19] Generating page.
[22.07.2025 08:19] Renaming previous page.
[22.07.2025 08:19] Renaming previous data. index.html to ./d/2025-07-22.html
[22.07.2025 08:19] Writing result.
[22.07.2025 08:19] Renaming log file.
[22.07.2025 08:19] Renaming previous data. log.txt to ./logs/2025-07-22_last_log.txt
