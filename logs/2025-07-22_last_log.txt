[22.07.2025 07:16] Read previous papers.
[22.07.2025 07:16] Generating top page (month).
[22.07.2025 07:16] Writing top page (month).
[22.07.2025 08:17] Read previous papers.
[22.07.2025 08:17] Get feed.
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14683
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15846
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14843
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15061
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11061
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15493
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15852
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15629
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15028
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15778
[22.07.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2507.15597
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15375
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11539
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15815
[22.07.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15856
[22.07.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2507.15640
[22.07.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2507.10935
[22.07.2025 08:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.07.2025 08:17] No deleted papers detected.
[22.07.2025 08:17] Downloading and parsing papers (pdf, html). Total: 17.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.14683.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.14683.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.14683.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15846.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15846.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15846.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.14843.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.14843.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.14843.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15061.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15061.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15061.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.11061.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.11061.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.11061.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15493.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15493.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15493.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15852.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15852.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15852.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15629.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15629.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15629.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15028.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15028.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15028.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15778.
[22.07.2025 08:17] Extra JSON file exists (./assets/json/2507.15778.json), skip PDF parsing.
[22.07.2025 08:17] Paper image links file exists (./assets/img_data/2507.15778.json), skip HTML parsing.
[22.07.2025 08:17] Success.
[22.07.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2507.15597.
[22.07.2025 08:17] Downloading paper 2507.15597 from http://arxiv.org/pdf/2507.15597v1...
[22.07.2025 08:18] Extracting affiliations from text.
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos Hao Luo1,3, Yicheng Feng1,3, Wanpeng Zhang1,3, Ye Wang2,3 Haoqi Yuan1,3 Sipeng Zheng3, Jiazheng Liu1 Chaoyi Xu3 Qin Jin2 Zongqing Lu1,3, 1Peking University 2Renmin University of China 3BeingBeyond https://beingbeyond.github.io/Being-H0 5 2 0 2 1 2 ] . [ 1 7 9 5 5 1 . 7 0 5 2 : r Figure 1: Being-H0 acquires dexterous manipulation skills by learning from large-scale human videos in the UniHand dataset via physical instruction tuning. By explicitly modeling hand motion patterns, the resulting foundation model seamlessly transfers from human hand demonstrations to robotic manipulation. These authors contributed equally to this work. Correspondence to Zongqing Lu <lu@beingbeyond.com>. "
[22.07.2025 08:18] Response: ```python
["Peking University", "Renmin University of China", "BeingBeyond"]
```
[22.07.2025 08:18] Deleting PDF ./assets/pdf/2507.15597.pdf.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15375.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.15375.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.15375.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.11539.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.11539.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.11539.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15815.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.15815.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.15815.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15856.
[22.07.2025 08:18] Extra JSON file exists (./assets/json/2507.15856.json), skip PDF parsing.
[22.07.2025 08:18] Paper image links file exists (./assets/img_data/2507.15856.json), skip HTML parsing.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.15640.
[22.07.2025 08:18] Downloading paper 2507.15640 from http://arxiv.org/pdf/2507.15640v1...
[22.07.2025 08:18] Extracting affiliations from text.
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 0 4 6 5 1 . 7 0 5 2 : r Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training Kailai Yang1* Xiao Liu2 Lei Ji2 Hao Li1 Yeyun Gong2 Peng Cheng2 Mao Yang2 1 The University of Manchester 2 Microsoft Research {kailai.yang,hao.li-2}@manchester.ac.uk {xiaoliu2,leiji,yegong,pengc,maoyang}@microsoft.com "
[22.07.2025 08:18] Response: ```python
["The University of Manchester", "Microsoft Research"]
```
[22.07.2025 08:18] Deleting PDF ./assets/pdf/2507.15640.pdf.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2507.10935.
[22.07.2025 08:18] Downloading paper 2507.10935 from http://arxiv.org/pdf/2507.10935v1...
[22.07.2025 08:18] Extracting affiliations from text.
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization Shaowen Tong1 Zimin Xia2 Alexandre Alahi2 Xuming He1 Yujiao Shi1* 1ShanghaiTech University, China 2 Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland {tongshw2024,hexm,shiyj2}@shanghaitech.edu.cn, {zimin.xia,alexandre.alahi}@epfl.ch 5 2 0 2 5 ] . [ 1 5 3 9 0 1 . 7 0 5 2 : r a "
[22.07.2025 08:18] Response: ```python
["ShanghaiTech University, China", "Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland"]
```
[22.07.2025 08:18] Deleting PDF ./assets/pdf/2507.10935.pdf.
[22.07.2025 08:18] Success.
[22.07.2025 08:18] Enriching papers with extra data.
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 0. The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  					AI-generated summary 				 Large language models have recently evolv...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 1. Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 2. Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  					AI-generated summary 				 Recent advances in large reasoning models highlight Reinforcement Learni...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 3. A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  					AI-generated summary 				 The advent of Large Language Model (LLM)-powered agents has revo...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 4. A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  					AI-generated summary 				 Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D co...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 5. A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  					AI-generated summary 				 We report our recent progress towards building generalist robot policies, the...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 6. Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and com...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 7. 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry ...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 8. Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video l...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 9. Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform tr...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 10. Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 11. Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to com...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 12. A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  					AI-generated summary 				 Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fu...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 13. We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Censu...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 14. Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian nois...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 15. Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific...
[22.07.2025 08:18] ********************************************************************************
[22.07.2025 08:18] Abstract 16. Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learni...
[22.07.2025 08:18] Read previous papers.
[22.07.2025 08:18] Generating reviews via LLM API.
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#open_source", "#math", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "MiroMind-M1 - —ç—Ç–æ —Å–µ—Ä–∏—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏—Ö –ø–µ—Ä–µ–¥–æ
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#reasoning", "#benchmark"], "emoji": "üñ±Ô∏è", "ru": {"title": "–ì–∞—É—Å—Å–æ–≤–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å GUI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º 
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#optimization", "#rl"], "emoji": "üîç", "ru": {"title": "RLVR: –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ü–µ–Ω–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#agents", "#dataset", "#synthetic", "#reasoning", "#benchmark"], "emoji": "üï∏Ô∏è", "ru": {"title": "–§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "WebShaper - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#3d"], "emoji": "‚úèÔ∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö –º–∞—Å–æ–∫ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏", "desc": "RoMaP - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö 3D-–º–∞—Å–æ–∫ –∏ —É–ª—É—á—à–µ–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Ñ—É–Ω–∫—Ü–∏–∏ 
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agents", "#training", "#agi"], "emoji": "ü§ñ", "ru": {"title": "GR-3: –®–∞–≥ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º —Ä–æ–±–æ—Ç–∞–º-–ø–æ–º–æ—â–Ω–∏–∫–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GR-3 - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è (VLA) –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#benchmark", "#reasoning", "#video"], "emoji": "üé•", "ru": {"title": "–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Segment Concept (SeC). SeC –∏—Å
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#3d"], "emoji": "üé®", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ SDF –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–º —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞—Ç–Ω–æ–º—É —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π —Å–æ –∑–Ω–∞–∫–æ–º (SDF) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ 3D –≥–∞—É—Å—Å–æ–≤—Å
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#interpretability", "#security", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ-LLM: —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ç–µ—Å—Ç Video Thinking Test (Video-TT) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-LLM –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: —É–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —Å –ø–æ–º–æ—â—å—é —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂
[22.07.2025 08:18] Querying the API.
[22.07.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.
[22.07.2025 08:18] Response: {
  "desc": "Being-H0 - —ç—Ç–æ –º–æ–¥–µ–ª—å –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA), –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–∞—Å—Ç–µ–π —Ç–µ–ª–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏. Being-H0 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —Ä—É–∫ –∏ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º –∑–∞—Ö–≤–∞—Ç –¥–≤–∏–∂–µ–Ω–∏–π, VR –∏ RGB-–≤–∏–¥–µ–æ.",
  "emoji": "ü§ñ",
  "title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –¥–≤–∏–∂–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ"
}
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0."

[22.07.2025 08:18] Response: ```python
['DATASET', 'DATA', 'MULTIMODAL', 'ROBOTICS', 'TRAINING']
```
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0."

[22.07.2025 08:18] Response: ```python
["AGI", "OPTIMIZATION"]
```
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications.","title":"Empowering Robots with Human-Like Dexterity through Vision-Language-Action!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications.', title='Empowering Robots with Human-Like Dexterity through Vision-Language-Action!'))
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Being-H0 ÊòØ‰∏ÄÁßçËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫é‰ªé‰∫∫Á±ªËßÜÈ¢ë‰∏≠Â≠¶‰π†Ôºå‰ª•Ëß£ÂÜ≥ÁÅµÂ∑ßÊÄßÂíåÊ≥õÂåñËÉΩÂäõÁöÑÈóÆÈ¢ò„ÄÇËØ•Ê®°ÂûãÈÄöËøáÁâ©ÁêÜÊåá‰ª§Ë∞É‰ºòÂíåÈÉ®‰ª∂Á∫ßËøêÂä®Ê†áËÆ∞ÂåñÔºåËÉΩÂ§üÁîüÊàêÁ≤æÁ°ÆÁöÑÊâãÈÉ®Âä®‰ΩúÂπ∂Âú®ÁúüÂÆû‰∏ñÁïå‰∏≠ËøõË°åÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇ‰∏é‰º†ÁªüÊ®°ÂûãÁõ∏ÊØîÔºåBeing-H0 Êõ¥Â•ΩÂú∞Â§ÑÁêÜÂ§çÊùÇÁöÑÊìç‰Ωú‰ªªÂä°ÔºåÂπ∂ËÉΩÊúâÊïàÈÄÇÂ∫îÊñ∞Âú∫ÊôØ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåBeing-H0 Âú®ÊâãÈÉ®Âä®‰ΩúÁîüÊàêÂíåÊåá‰ª§Ë∑üÈöèÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰∏îÂú®ÂÆûÈôÖÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠‰πüÂèñÂæó‰∫ÜÊòæËëóÁöÑËøõÂ±ï„ÄÇ","title":"Being-H0ÔºöÁÅµÂ∑ßÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Being-H0 ÊòØ‰∏ÄÁßçËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫é‰ªé‰∫∫Á±ªËßÜÈ¢ë‰∏≠Â≠¶‰π†Ôºå‰ª•Ëß£ÂÜ≥ÁÅµÂ∑ßÊÄßÂíåÊ≥õÂåñËÉΩÂäõÁöÑÈóÆÈ¢ò„ÄÇËØ•Ê®°ÂûãÈÄöËøáÁâ©ÁêÜÊåá‰ª§Ë∞É‰ºòÂíåÈÉ®‰ª∂Á∫ßËøêÂä®Ê†áËÆ∞ÂåñÔºåËÉΩÂ§üÁîüÊàêÁ≤æÁ°ÆÁöÑÊâãÈÉ®Âä®‰ΩúÂπ∂Âú®ÁúüÂÆû‰∏ñÁïå‰∏≠ËøõË°åÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇ‰∏é‰º†ÁªüÊ®°ÂûãÁõ∏ÊØîÔºåBeing-H0 Êõ¥Â•ΩÂú∞Â§ÑÁêÜÂ§çÊùÇÁöÑÊìç‰Ωú‰ªªÂä°ÔºåÂπ∂ËÉΩÊúâÊïàÈÄÇÂ∫îÊñ∞Âú∫ÊôØ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåBeing-H0 Âú®ÊâãÈÉ®Âä®‰ΩúÁîüÊàêÂíåÊåá‰ª§Ë∑üÈöèÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰∏îÂú®ÂÆûÈôÖÊú∫Âô®‰∫∫Êìç‰Ωú‰∏≠‰πüÂèñÂæó‰∫ÜÊòæËëóÁöÑËøõÂ±ï„ÄÇ', title='Being-H0ÔºöÁÅµÂ∑ßÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°Âûã'))
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#training", "#reasoning", "#audio", "#multimodal"], "emoji": "üß†", "ru": {"title": "Stitch: –î—É–º–∞–π –∏ –≥–æ–≤–æ—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Stitch. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#benchmark", "#architecture", "#cv"], "emoji": "üîÑ", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 4D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ–º–æ—â—å—é –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–π 4D-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –¥–ª—è —Ä–µ–∫–æ–Ω
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#optimization", "#agents", "#agi", "#multimodal", "#rl", "#science"], "emoji": "ü§ñ", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∫–∞–∫ —ç–∫–æ–Ω–æ–º–∏—Å—Ç: –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'LLM Economist', –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å
[22.07.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "üßπ", "ru": {"title": "–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç 
[22.07.2025 08:18] Querying the API.
[22.07.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data.
[22.07.2025 08:18] Response: {
  "desc": "Data Mixing Agent - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–≤–∑–≤–µ—à–∏–≤–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∏ —Ü–µ–ª–µ–≤—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–∏–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. Data Mixing Agent —Ö–æ—Ä–æ—à–æ –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–æ–º–µ–Ω–æ–≤ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.",
  "emoji": "üîÄ",
  "title": "–£–º–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."

[22.07.2025 08:18] Response: ```python
["RL", "AGENTS", "TRAINING"]
```
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."

[22.07.2025 08:18] Response: ```python
["TRANSFER_LEARNING", "OPTIMIZATION"]
```
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data.","title":"Reinforcement Learning for Balanced Data Mixing in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data.', title='Reinforcement Learning for Balanced Data Mixing in Language Models'))
[22.07.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êï∞ÊçÆÊ∑∑Âêà‰ª£ÁêÜÊòØ‰∏ÄÁßçÂü∫‰∫éÊ®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÊúâÊïàÂú∞ÈáçÊñ∞Âä†ÊùÉËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ª•Âπ≥Ë°°Âú®ÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰∏≠Ê∫êÈ¢ÜÂüüÂíåÁõÆÊ†áÈ¢ÜÂüüÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂú®Â∞èËßÑÊ®°ÁâπÂÆö‰ªªÂä°Êï∞ÊçÆ‰∏äÊåÅÁª≠È¢ÑËÆ≠ÁªÉÊó∂ÂèØËÉΩÂá∫Áé∞ÁöÑÁÅæÈöæÊÄßÈÅóÂøòÈóÆÈ¢ò„ÄÇÈÄöËøáÊèêÂá∫Êï∞ÊçÆÊ∑∑Âêà‰ª£ÁêÜÔºåÁ†îÁ©∂ËÄÖËØÅÊòé‰∫ÜÊõ¥ÈÄöÁî®ÁöÑÂêØÂèëÂºèÊñπÊ≥ïÂèØ‰ª•Ë¢´ÂèÇÊï∞ÂåñÔºå‰ªéËÄåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•‰ª£ÁêÜÂú®Êï∞Â≠¶Êé®ÁêÜÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰∏≠Ë°®Áé∞‰ºò‰∫éÂº∫Âü∫Á∫øÔºåÂπ∂‰∏îÂú®Êú™ËßÅËøáÁöÑÊ∫êÈ¢ÜÂüüÂíåÁõÆÊ†áÊ®°Âûã‰∏äÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ","title":"Êï∞ÊçÆÊ∑∑Âêà‰ª£ÁêÜÔºöÂπ≥Ë°°Ê∫ê‰∏éÁõÆÊ†áÈ¢ÜÂüüÁöÑÊô∫ËÉΩÂ≠¶‰π†"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êï∞ÊçÆÊ∑∑Âêà‰ª£ÁêÜÊòØ‰∏ÄÁßçÂü∫‰∫éÊ®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÊúâÊïàÂú∞ÈáçÊñ∞Âä†ÊùÉËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ª•Âπ≥Ë°°Âú®ÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰∏≠Ê∫êÈ¢ÜÂüüÂíåÁõÆÊ†áÈ¢ÜÂüüÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂú®Â∞èËßÑÊ®°ÁâπÂÆö‰ªªÂä°Êï∞ÊçÆ‰∏äÊåÅÁª≠È¢ÑËÆ≠ÁªÉÊó∂ÂèØËÉΩÂá∫Áé∞ÁöÑÁÅæÈöæÊÄßÈÅóÂøòÈóÆÈ¢ò„ÄÇÈÄöËøáÊèêÂá∫Êï∞ÊçÆÊ∑∑Âêà‰ª£ÁêÜÔºåÁ†îÁ©∂ËÄÖËØÅÊòé‰∫ÜÊõ¥ÈÄöÁî®ÁöÑÂêØÂèëÂºèÊñπÊ≥ïÂèØ‰ª•Ë¢´ÂèÇÊï∞ÂåñÔºå‰ªéËÄåÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•‰ª£ÁêÜÂú®Êï∞Â≠¶Êé®ÁêÜÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰∏≠Ë°®Áé∞‰ºò‰∫éÂº∫Âü∫Á∫øÔºåÂπ∂‰∏îÂú®Êú™ËßÅËøáÁöÑÊ∫êÈ¢ÜÂüüÂíåÁõÆÊ†áÊ®°Âûã‰∏äÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ', title='Êï∞ÊçÆÊ∑∑Âêà‰ª£ÁêÜÔºöÂπ≥Ë°°Ê∫ê‰∏éÁõÆÊ†áÈ¢ÜÂüüÁöÑÊô∫ËÉΩÂ≠¶‰π†'))
[22.07.2025 08:18] Querying the API.
[22.07.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.
[22.07.2025 08:18] Response: {
  "desc": "GeoDistill - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫—Ä–æ—Å—Å-–≤–∏–¥–æ–≤–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–ª–∞–±–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø–æ —Å—Ö–µ–º–µ —É—á–∏—Ç–µ–ª—å-—É—á–µ–Ω–∏–∫ —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ –ø–æ–ª—é –∑—Ä–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –£—á–∏—Ç–µ–ª—å –ª–æ–∫–∞–ª–∏–∑—É–µ—Ç –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∞ —É—á–µ–Ω–∏–∫ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–ª–æ–∂–µ–Ω–∏–µ –ø–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º—É –ø–æ–ª—é –∑—Ä–µ–Ω–∏—è. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è—Ö –∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–µ—Å—Ç–µ–∫—Å—Ç—É—Ä–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –ø–æ–≤—ã—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏.",
  "emoji": "üåç",
  "title": "GeoDistill: –≥–µ–æ–º–µ—Ç—Ä–∏—è –Ω–∞ —Å–ª—É–∂–±–µ –∫—Ä–æ—Å—Å-–≤–∏–¥–æ–≤–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏"
}
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill."

[22.07.2025 08:18] Response: ```python
["CV", "RL", "TRAINING"]
```
[22.07.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill."

[22.07.2025 08:18] Response: ```python
["OPTIMIZATION"]
```
[22.07.2025 08:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera\'s 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation.","title":"GeoDistill: Enhancing Cross-View Localization with Weak Supervision"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera's 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation.", title='GeoDistill: Enhancing Cross-View Localization with Weak Supervision'))
[22.07.2025 08:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Ë∑®ËßÜËßíÂÆö‰ΩçÊòØÈÄöËøáÂ∞ÜÂú∞Èù¢ÂõæÂÉè‰∏éÂç´ÊòüÂõæÂÉèÂØπÈΩêÊù•‰º∞ËÆ°Áõ∏Êú∫ÁöÑ‰∏âËá™Áî±Â∫¶ÂßøÊÄÅÔºåËøôÂú®Ëá™Âä®ÂØºËà™ÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÂ§ßËßÑÊ®°Êà∑Â§ñÂ∫îÁî®‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂÆåÂÖ®ÁõëÁù£Â≠¶‰π†ÔºåËøôÈúÄË¶ÅÊòÇË¥µÁöÑÁúüÂÆûÂßøÊÄÅÊ†áÊ≥®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜGeoDistillÔºå‰∏Ä‰∏™Âá†‰ΩïÂºïÂØºÁöÑÂº±ÁõëÁù£Ëá™Ëí∏È¶èÊ°ÜÊû∂ÔºåÂà©Áî®ÊïôÂ∏à-Â≠¶ÁîüÂ≠¶‰π†ÂíåÂü∫‰∫éËßÜÂú∫(FoV)ÁöÑÊé©ËîΩÊù•Â¢ûÂº∫Â±ÄÈÉ®ÁâπÂæÅÂ≠¶‰π†Ôºå‰ªéËÄåÂÆûÁé∞Á®≥ÂÅ•ÁöÑË∑®ËßÜËßíÂÆö‰Ωç„ÄÇGeoDistillÈÄöËøáÂØπÈΩêÂ≠¶ÁîüÊ®°ÂûãÂíåÊïôÂ∏àÊ®°ÂûãÁöÑÈ¢ÑÊµãÔºåÂ∏ÆÂä©Â≠¶ÁîüÊ®°Âûã‰∏ìÊ≥®‰∫éÂÖ≥ÈîÆÁâπÂæÅÔºåÊèêÈ´ò‰∫ÜÂÆö‰ΩçÁ≤æÂ∫¶Âπ∂ÂáèÂ∞ë‰∫Ü‰∏çÁ°ÆÂÆöÊÄß„ÄÇ","title":"GeoDistillÔºöÈ´òÊïàÁöÑË∑®ËßÜËßíÂÆö‰ΩçËß£ÂÜ≥ÊñπÊ°à"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Ë∑®ËßÜËßíÂÆö‰ΩçÊòØÈÄöËøáÂ∞ÜÂú∞Èù¢ÂõæÂÉè‰∏éÂç´ÊòüÂõæÂÉèÂØπÈΩêÊù•‰º∞ËÆ°Áõ∏Êú∫ÁöÑ‰∏âËá™Áî±Â∫¶ÂßøÊÄÅÔºåËøôÂú®Ëá™Âä®ÂØºËà™ÂíåÂ¢ûÂº∫Áé∞ÂÆûÁ≠âÂ§ßËßÑÊ®°Êà∑Â§ñÂ∫îÁî®‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂÆåÂÖ®ÁõëÁù£Â≠¶‰π†ÔºåËøôÈúÄË¶ÅÊòÇË¥µÁöÑÁúüÂÆûÂßøÊÄÅÊ†áÊ≥®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜGeoDistillÔºå‰∏Ä‰∏™Âá†‰ΩïÂºïÂØºÁöÑÂº±ÁõëÁù£Ëá™Ëí∏È¶èÊ°ÜÊû∂ÔºåÂà©Áî®ÊïôÂ∏à-Â≠¶ÁîüÂ≠¶‰π†ÂíåÂü∫‰∫éËßÜÂú∫(FoV)ÁöÑÊé©ËîΩÊù•Â¢ûÂº∫Â±ÄÈÉ®ÁâπÂæÅÂ≠¶‰π†Ôºå‰ªéËÄåÂÆûÁé∞Á®≥ÂÅ•ÁöÑË∑®ËßÜËßíÂÆö‰Ωç„ÄÇGeoDistillÈÄöËøáÂØπÈΩêÂ≠¶ÁîüÊ®°ÂûãÂíåÊïôÂ∏àÊ®°ÂûãÁöÑÈ¢ÑÊµãÔºåÂ∏ÆÂä©Â≠¶ÁîüÊ®°Âûã‰∏ìÊ≥®‰∫éÂÖ≥ÈîÆÁâπÂæÅÔºåÊèêÈ´ò‰∫ÜÂÆö‰ΩçÁ≤æÂ∫¶Âπ∂ÂáèÂ∞ë‰∫Ü‰∏çÁ°ÆÂÆöÊÄß„ÄÇ', title='GeoDistillÔºöÈ´òÊïàÁöÑË∑®ËßÜËßíÂÆö‰ΩçËß£ÂÜ≥ÊñπÊ°à'))
[22.07.2025 08:19] Renaming data file.
[22.07.2025 08:19] Renaming previous data. hf_papers.json to ./d/2025-07-22.json
[22.07.2025 08:19] Saving new data file.
[22.07.2025 08:19] Generating page.
[22.07.2025 08:19] Renaming previous page.
[22.07.2025 08:19] Renaming previous data. index.html to ./d/2025-07-22.html
[22.07.2025 08:19] Writing result.
[22.07.2025 08:19] Renaming log file.
[22.07.2025 08:19] Renaming previous data. log.txt to ./logs/2025-07-22_last_log.txt
