[22.07.2025 15:13] Read previous papers.
[22.07.2025 15:13] Generating top page (month).
[22.07.2025 15:13] Writing top page (month).
[22.07.2025 16:15] Read previous papers.
[22.07.2025 16:15] Get feed.
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15846
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14683
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14843
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14119
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15061
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11061
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15493
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15852
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15597
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15778
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14417
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15028
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15629
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15375
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13428
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11539
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15856
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15815
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15640
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15550
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14295
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15728
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10935
[22.07.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14102
[22.07.2025 16:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.07.2025 16:15] No deleted papers detected.
[22.07.2025 16:15] Downloading and parsing papers (pdf, html). Total: 24.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15846.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15846.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15846.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.14683.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.14683.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.14683.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.14843.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.14843.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.14843.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.14119.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.14119.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.14119.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15061.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15061.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15061.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.11061.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.11061.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.11061.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15493.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15493.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15493.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15852.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15852.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15852.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15597.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15597.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15597.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15778.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15778.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15778.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.14417.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.14417.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.14417.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15028.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15028.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15028.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15629.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15629.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15629.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15375.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15375.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15375.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.13428.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.13428.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.13428.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.11539.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.11539.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.11539.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15856.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15856.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15856.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15815.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15815.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15815.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15640.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15640.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15640.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15550.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15550.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15550.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.14295.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.14295.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.14295.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.15728.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.15728.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.15728.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.10935.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.10935.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.10935.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2507.14102.
[22.07.2025 16:15] Extra JSON file exists (./assets/json/2507.14102.json), skip PDF parsing.
[22.07.2025 16:15] Paper image links file exists (./assets/img_data/2507.14102.json), skip HTML parsing.
[22.07.2025 16:15] Success.
[22.07.2025 16:15] Enriching papers with extra data.
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 0. Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 1. The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  					AI-generated summary 				 Large language models have recently evolv...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 2. Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  					AI-generated summary 				 Recent advances in large reasoning models highlight Reinforcement Learni...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 3. An automated pipeline mines high-fidelity image editing triplets using generative models and a task-tuned validator, enabling large-scale training without human labeling.  					AI-generated summary 				 Recent advances in generative modeling enable image editing assistants that follow natural langua...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 4. A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  					AI-generated summary 				 The advent of Large Language Model (LLM)-powered agents has revo...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 5. A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  					AI-generated summary 				 Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D co...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 6. A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  					AI-generated summary 				 We report our recent progress towards building generalist robot policies, the...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 7. Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and com...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 8. Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 9. Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform tr...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 10. Evaluating Large Reasoning Models across different reasoning lengths reveals that increased test-time compute can degrade performance and exacerbate specific reasoning failures.  					AI-generated summary 				 We construct evaluation tasks where extending the reasoning length of Large Reasoning Mode...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 11. Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video l...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 12. 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry ...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 13. Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to com...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 14. Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate ...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 15. A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  					AI-generated summary 				 Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fu...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 16. Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian nois...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 17. We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Censu...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 18. Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 19. PhysGym, a new benchmark suite, evaluates large language model-based agents' scientific reasoning in interactive physics environments, focusing on their handling of complexity and prior knowledge.  					AI-generated summary 				 Evaluating the scientific discovery capabilities of large language mode...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 20. Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that model...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 21. Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framewo...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 22. Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learni...
[22.07.2025 16:15] ********************************************************************************
[22.07.2025 16:15] Abstract 23. Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to de...
[22.07.2025 16:15] Read previous papers.
[22.07.2025 16:15] Generating reviews via LLM API.
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#reasoning", "#benchmark"], "emoji": "üñ±Ô∏è", "ru": {"title": "–ì–∞—É—Å—Å–æ–≤–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å GUI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º 
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#open_source", "#math", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "MiroMind-M1 - —ç—Ç–æ —Å–µ—Ä–∏—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏—Ö –ø–µ—Ä–µ–¥–æ
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#optimization", "#rl"], "emoji": "üîç", "ru": {"title": "RLVR: –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ü–µ–Ω–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#cv", "#diffusion", "#open_source", "#training", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#dataset", "#synthetic", "#reasoning", "#benchmark"], "emoji": "üï∏Ô∏è", "ru": {"title": "–§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "WebShaper - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#3d"], "emoji": "‚úèÔ∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö –º–∞—Å–æ–∫ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏", "desc": "RoMaP - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö 3D-–º–∞—Å–æ–∫ –∏ —É–ª—É—á—à–µ–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Ñ—É–Ω–∫—Ü–∏–∏ 
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agents", "#training", "#agi"], "emoji": "ü§ñ", "ru": {"title": "GR-3: –®–∞–≥ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º —Ä–æ–±–æ—Ç–∞–º-–ø–æ–º–æ—â–Ω–∏–∫–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GR-3 - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è (VLA) –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#benchmark", "#reasoning", "#video"], "emoji": "üé•", "ru": {"title": "–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Segment Concept (SeC). SeC –∏—Å
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#agi", "#robotics", "#dataset", "#training", "#optimization", "#multimodal", "#data"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –¥–≤–∏–∂–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ", "desc": "Being-H0 - —ç—Ç–æ –º–æ–¥–µ–ª—å –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA), –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –º–∞–Ω–∏–ø
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: —É–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —Å –ø–æ–º–æ—â—å—é —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#hallucinations", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–ë–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π - –Ω–µ –≤—Å–µ–≥–¥–∞ –ª—É—á—à–µ: –ø–∞—Ä–∞–¥–æ–∫—Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π 
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#interpretability", "#security", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ-LLM: —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ç–µ—Å—Ç Video Thinking Test (Video-TT) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-LLM –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#3d"], "emoji": "üé®", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ SDF –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–º —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞—Ç–Ω–æ–º—É —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π —Å–æ –∑–Ω–∞–∫–æ–º (SDF) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ 3D –≥–∞—É—Å—Å–æ–≤—Å
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#audio", "#multimodal"], "emoji": "üß†", "ru": {"title": "Stitch: –î—É–º–∞–π –∏ –≥–æ–≤–æ—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Stitch. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#games", "#interpretability", "#optimization", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–§–∏–∑–∏–∫–∞ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º –º–∏—Ä–µ: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PhyWorldBench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#benchmark", "#architecture", "#cv"], "emoji": "üîÑ", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 4D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ–º–æ—â—å—é –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–π 4D-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –¥–ª—è —Ä–µ–∫–æ–Ω
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "üßπ", "ru": {"title": "–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç 
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#agents", "#agi", "#multimodal", "#rl", "#science"], "emoji": "ü§ñ", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∫–∞–∫ —ç–∫–æ–Ω–æ–º–∏—Å—Ç: –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'LLM Economist', –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#training", "#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "üîÄ", "ru": {"title": "–£–º–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Data Mixing Agent - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#science"], "emoji": "üß†", "ru": {"title": "PhysGym: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º –º–∏—Ä–µ", "desc": "PhysGym - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö 
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —Å –ø–æ–º–æ—â—å—é –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –≤ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–º —Ä–µ–∂–∏–º–µ 
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#diffusion", "#story_generation", "#long_context", "#multimodal", "#video", "#inference"], "emoji": "üé¨", "ru": {"title": "TokensGen: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å–∂–∞—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤", "desc": "TokensGen - —ç—Ç–æ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—é—â
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#cv"], "emoji": "üåç", "ru": {"title": "GeoDistill: –≥–µ–æ–º–µ—Ç—Ä–∏—è –Ω–∞ —Å–ª—É–∂–±–µ –∫—Ä–æ—Å—Å-–≤–∏–¥–æ–≤–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "GeoDistill - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫—Ä–æ—Å—Å-–≤–∏–¥–æ–≤–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–ª–∞–±–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω
[22.07.2025 16:15] Using data from previous issue: {"categories": ["#healthcare", "#training", "#data"], "emoji": "üî¨", "ru": {"title": "UGPL: –£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ö–¢ –æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É", "desc": "UGPL - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –ö–¢-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø—Ä–∏–Ω—Ü–∏–ø –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É. –°–∏—Å—Ç–µ–º–∞ —Å–Ω–∞—á–∞–ª–∞ –≤—ã—è–≤–ª—è–µ—Ç –æ–±–ª–∞—Å—Ç–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ
[22.07.2025 16:15] Renaming data file.
[22.07.2025 16:15] Renaming previous data. hf_papers.json to ./d/2025-07-22.json
[22.07.2025 16:15] Saving new data file.
[22.07.2025 16:15] Generating page.
[22.07.2025 16:15] Renaming previous page.
[22.07.2025 16:15] Renaming previous data. index.html to ./d/2025-07-22.html
[22.07.2025 16:15] Writing result.
[22.07.2025 16:15] Renaming log file.
[22.07.2025 16:15] Renaming previous data. log.txt to ./logs/2025-07-22_last_log.txt
