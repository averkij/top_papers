[22.07.2025 06:20] Read previous papers.
[22.07.2025 06:20] Generating top page (month).
[22.07.2025 06:20] Writing top page (month).
[22.07.2025 07:16] Read previous papers.
[22.07.2025 07:16] Get feed.
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14683
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15846
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15061
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14843
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11061
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15493
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15852
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15629
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15778
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15028
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11539
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15815
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15375
[22.07.2025 07:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15856
[22.07.2025 07:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.07.2025 07:16] No deleted papers detected.
[22.07.2025 07:16] Downloading and parsing papers (pdf, html). Total: 14.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.14683.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.14683.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.14683.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15846.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15846.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15846.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15061.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15061.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15061.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.14843.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.14843.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.14843.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.11061.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.11061.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.11061.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15493.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15493.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15493.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15852.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15852.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15852.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15629.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15629.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15629.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15778.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15778.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15778.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15028.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15028.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15028.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.11539.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.11539.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.11539.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15815.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15815.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15815.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15375.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15375.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15375.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Downloading and parsing paper https://huggingface.co/papers/2507.15856.
[22.07.2025 07:16] Extra JSON file exists (./assets/json/2507.15856.json), skip PDF parsing.
[22.07.2025 07:16] Paper image links file exists (./assets/img_data/2507.15856.json), skip HTML parsing.
[22.07.2025 07:16] Success.
[22.07.2025 07:16] Enriching papers with extra data.
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 0. The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  					AI-generated summary 				 Large language models have recently evolv...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 1. Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 2. A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  					AI-generated summary 				 The advent of Large Language Model (LLM)-powered agents has revo...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 3. Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  					AI-generated summary 				 Recent advances in large reasoning models highlight Reinforcement Learni...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 4. A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  					AI-generated summary 				 Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D co...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 5. A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  					AI-generated summary 				 We report our recent progress towards building generalist robot policies, the...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 6. Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and com...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 7. 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry ...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 8. Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform tr...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 9. Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video l...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 10. A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  					AI-generated summary 				 Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fu...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 11. We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Censu...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 12. Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to com...
[22.07.2025 07:16] ********************************************************************************
[22.07.2025 07:16] Abstract 13. Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian nois...
[22.07.2025 07:16] Read previous papers.
[22.07.2025 07:16] Generating reviews via LLM API.
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#open_source", "#math", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "MiroMind-M1 - —ç—Ç–æ —Å–µ—Ä–∏—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—é—â–∏—Ö –ø–µ—Ä–µ–¥–æ
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#reasoning", "#benchmark"], "emoji": "üñ±Ô∏è", "ru": {"title": "–ì–∞—É—Å—Å–æ–≤–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å GUI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º 
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#agents", "#dataset", "#synthetic", "#reasoning", "#benchmark"], "emoji": "üï∏Ô∏è", "ru": {"title": "–§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "WebShaper - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#optimization", "#rl"], "emoji": "üîç", "ru": {"title": "RLVR: –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ü–µ–Ω–æ–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#3d"], "emoji": "‚úèÔ∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö –º–∞—Å–æ–∫ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏", "desc": "RoMaP - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö 3D-–º–∞—Å–æ–∫ –∏ —É–ª—É—á—à–µ–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Ñ—É–Ω–∫—Ü–∏–∏ 
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agents", "#training", "#agi"], "emoji": "ü§ñ", "ru": {"title": "GR-3: –®–∞–≥ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º —Ä–æ–±–æ—Ç–∞–º-–ø–æ–º–æ—â–Ω–∏–∫–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GR-3 - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è (VLA) –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#benchmark", "#reasoning", "#video"], "emoji": "üé•", "ru": {"title": "–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Segment Concept (SeC). SeC –∏—Å
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#3d"], "emoji": "üé®", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ SDF –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–º —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞—Ç–Ω–æ–º—É —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π —Å–æ –∑–Ω–∞–∫–æ–º (SDF) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ 3D –≥–∞—É—Å—Å–æ–≤—Å
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: —É–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —Å –ø–æ–º–æ—â—å—é —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#interpretability", "#security", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ-LLM: —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ç–µ—Å—Ç Video Thinking Test (Video-TT) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-LLM –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#benchmark", "#architecture", "#cv"], "emoji": "üîÑ", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 4D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ–º–æ—â—å—é –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–π 4D-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –¥–ª—è —Ä–µ–∫–æ–Ω
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#optimization", "#agents", "#agi", "#multimodal", "#rl", "#science"], "emoji": "ü§ñ", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∫–∞–∫ —ç–∫–æ–Ω–æ–º–∏—Å—Ç: –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'LLM Economist', –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#training", "#reasoning", "#audio", "#multimodal"], "emoji": "üß†", "ru": {"title": "Stitch: –î—É–º–∞–π –∏ –≥–æ–≤–æ—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Stitch. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏
[22.07.2025 07:16] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "üßπ", "ru": {"title": "–®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç 
[22.07.2025 07:16] Renaming data file.
[22.07.2025 07:16] Renaming previous data. hf_papers.json to ./d/2025-07-22.json
[22.07.2025 07:16] Saving new data file.
[22.07.2025 07:16] Generating page.
[22.07.2025 07:16] Renaming previous page.
[22.07.2025 07:16] Renaming previous data. index.html to ./d/2025-07-22.html
[22.07.2025 07:16] Writing result.
[22.07.2025 07:16] Renaming log file.
[22.07.2025 07:16] Renaming previous data. log.txt to ./logs/2025-07-22_last_log.txt
