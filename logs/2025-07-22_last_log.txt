[22.07.2025 12:23] Read previous papers.
[22.07.2025 12:23] Generating top page (month).
[22.07.2025 12:23] Writing top page (month).
[22.07.2025 13:32] Read previous papers.
[22.07.2025 13:32] Get feed.
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14683
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15846
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14843
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15061
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11061
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15852
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15493
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14119
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15597
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15778
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15028
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14417
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15629
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15375
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11539
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15856
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15815
[22.07.2025 13:32] Extract page data from URL. URL: https://huggingface.co/papers/2507.15550
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13428
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15640
[22.07.2025 13:32] Extract page data from URL. URL: https://huggingface.co/papers/2507.15728
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14295
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10935
[22.07.2025 13:32] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14102
[22.07.2025 13:32] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.07.2025 13:32] No deleted papers detected.
[22.07.2025 13:32] Downloading and parsing papers (pdf, html). Total: 24.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.14683.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.14683.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.14683.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15846.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15846.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15846.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.14843.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.14843.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.14843.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15061.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15061.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15061.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.11061.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.11061.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.11061.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15852.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15852.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15852.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15493.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15493.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15493.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.14119.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.14119.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.14119.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15597.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15597.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15597.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15778.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15778.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15778.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15028.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15028.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15028.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.14417.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.14417.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.14417.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15629.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15629.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15629.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15375.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15375.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15375.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.11539.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.11539.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.11539.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15856.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15856.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15856.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15815.
[22.07.2025 13:32] Extra JSON file exists (./assets/json/2507.15815.json), skip PDF parsing.
[22.07.2025 13:32] Paper image links file exists (./assets/img_data/2507.15815.json), skip HTML parsing.
[22.07.2025 13:32] Success.
[22.07.2025 13:32] Downloading and parsing paper https://huggingface.co/papers/2507.15550.
[22.07.2025 13:32] Downloading paper 2507.15550 from http://arxiv.org/pdf/2507.15550v1...
[22.07.2025 13:33] Extracting affiliations from text.
[22.07.2025 13:33] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 0 5 5 5 1 . 7 0 5 2 : r PHYSGYM: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors Yimeng Chen1, Piotr Piekos1, Mateusz Ostaszewski1, Firas Laakom1, JÃ¼rgen Schmidhuber1,2,3 1Center of Excellence for Generative AI, KAUST 2The Swiss AI Lab, IDSIA-USI/SUPSI 3NNAISENSE "
[22.07.2025 13:33] Response: ```python
["Center of Excellence for Generative AI, KAUST", "The Swiss AI Lab, IDSIA-USI/SUPSI", "NNAISENSE"]
```
[22.07.2025 13:33] Deleting PDF ./assets/pdf/2507.15550.pdf.
[22.07.2025 13:33] Success.
[22.07.2025 13:33] Downloading and parsing paper https://huggingface.co/papers/2507.13428.
[22.07.2025 13:33] Extra JSON file exists (./assets/json/2507.13428.json), skip PDF parsing.
[22.07.2025 13:33] Paper image links file exists (./assets/img_data/2507.13428.json), skip HTML parsing.
[22.07.2025 13:33] Success.
[22.07.2025 13:33] Downloading and parsing paper https://huggingface.co/papers/2507.15640.
[22.07.2025 13:33] Extra JSON file exists (./assets/json/2507.15640.json), skip PDF parsing.
[22.07.2025 13:33] Paper image links file exists (./assets/img_data/2507.15640.json), skip HTML parsing.
[22.07.2025 13:33] Success.
[22.07.2025 13:33] Downloading and parsing paper https://huggingface.co/papers/2507.15728.
[22.07.2025 13:33] Downloading paper 2507.15728 from http://arxiv.org/pdf/2507.15728v1...
[22.07.2025 13:33] Extracting affiliations from text.
[22.07.2025 13:33] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TokensGen: Harnessing Condensed Tokens for Long Video Generation Wenqi Ouyang1, Shuai Yang3, 1S-Lab, Nanyang Technological University, Zeqi Xiao1, Lei Yang2, Danni Yang2, Jianlou Si2, Yifan Zhou1, Xingang Pan1 2SenseTime Research, 3Wangxuan Institute of Computer Technology, Peking University https://vicky0522.github.io/tokensgen-webpage/ 5 2 0 2 1 2 ] . [ 1 8 2 7 5 1 . 7 0 5 2 : r Figure 1. Given the text prompt, TokensGen generates long videos of up to 2 minutes, maintaining consistent motions and content. Moreover, TokensGen supports zero-shot prompt-guided video-to-video editing for long videos. "
[22.07.2025 13:33] Response: ```python
[
    "S-Lab, Nanyang Technological University",
    "SenseTime Research",
    "Wangxuan Institute of Computer Technology, Peking University"
]
```
[22.07.2025 13:33] Deleting PDF ./assets/pdf/2507.15728.pdf.
[22.07.2025 13:33] Success.
[22.07.2025 13:33] Downloading and parsing paper https://huggingface.co/papers/2507.14295.
[22.07.2025 13:33] Extra JSON file exists (./assets/json/2507.14295.json), skip PDF parsing.
[22.07.2025 13:33] Paper image links file exists (./assets/img_data/2507.14295.json), skip HTML parsing.
[22.07.2025 13:33] Success.
[22.07.2025 13:33] Downloading and parsing paper https://huggingface.co/papers/2507.10935.
[22.07.2025 13:33] Extra JSON file exists (./assets/json/2507.10935.json), skip PDF parsing.
[22.07.2025 13:33] Paper image links file exists (./assets/img_data/2507.10935.json), skip HTML parsing.
[22.07.2025 13:33] Success.
[22.07.2025 13:33] Downloading and parsing paper https://huggingface.co/papers/2507.14102.
[22.07.2025 13:33] Extra JSON file exists (./assets/json/2507.14102.json), skip PDF parsing.
[22.07.2025 13:33] Paper image links file exists (./assets/img_data/2507.14102.json), skip HTML parsing.
[22.07.2025 13:33] Success.
[22.07.2025 13:33] Enriching papers with extra data.
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 0. The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  					AI-generated summary 				 Large language models have recently evolv...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 1. Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 2. Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  					AI-generated summary 				 Recent advances in large reasoning models highlight Reinforcement Learni...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 3. A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  					AI-generated summary 				 The advent of Large Language Model (LLM)-powered agents has revo...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 4. A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  					AI-generated summary 				 Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D co...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 5. Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and com...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 6. A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  					AI-generated summary 				 We report our recent progress towards building generalist robot policies, the...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 7. An automated pipeline mines high-fidelity image editing triplets using generative models and a task-tuned validator, enabling large-scale training without human labeling.  					AI-generated summary 				 Recent advances in generative modeling enable image editing assistants that follow natural langua...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 8. Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  					AI-generated summary 				 We...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 9. Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform tr...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 10. Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video l...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 11. Evaluating Large Reasoning Models across different reasoning lengths reveals that increased test-time compute can degrade performance and exacerbate specific reasoning failures.  					AI-generated summary 				 We construct evaluation tasks where extending the reasoning length of Large Reasoning Mode...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 12. 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry ...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 13. Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to com...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 14. A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  					AI-generated summary 				 Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fu...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 15. Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian nois...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 16. We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Censu...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 17. PhysGym, a new benchmark suite, evaluates large language model-based agents' scientific reasoning in interactive physics environments, focusing on their handling of complexity and prior knowledge.  					AI-generated summary 				 Evaluating the scientific discovery capabilities of large language mode...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 18. Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate ...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 19. Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  					AI-generated summary 				 Continual pre-training on small-scale task-specific...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 20. Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framewo...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 21. Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that model...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 22. Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learni...
[22.07.2025 13:33] ********************************************************************************
[22.07.2025 13:33] Abstract 23. Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to de...
[22.07.2025 13:33] Read previous papers.
[22.07.2025 13:33] Generating reviews via LLM API.
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#open_source", "#math", "#reasoning"], "emoji": "ð§®", "ru": {"title": "ÐÑÐºÑÑÑÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐµÑÐºÐ¸Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð½Ð° Ð½Ð¾Ð²Ð¾Ð¼ ÑÑÐ¾Ð²Ð½Ðµ", "desc": "MiroMind-M1 - ÑÑÐ¾ ÑÐµÑÐ¸Ñ Ð¾ÑÐºÑÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð¼Ð°ÑÐµÐ¼Ð°ÑÐ¸ÑÐµÑÐºÐ¸Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹, Ð´Ð¾ÑÑÐ¸Ð³Ð°ÑÑÐ¸Ñ Ð¿ÐµÑÐµÐ´Ð¾
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#reasoning", "#benchmark"], "emoji": "ð±ï¸", "ru": {"title": "ÐÐ°ÑÑÑÐ¾Ð²Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ ÑÐ¾ÑÐ½Ð¾Ð³Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑÐ²Ð¸Ñ Ñ GUI", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð°ÑÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑÐ²Ð¸Ñ Ñ Ð³ÑÐ°ÑÐ¸ÑÐµÑÐºÐ¸Ð¼ Ð¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÐµÐ»ÑÑÐºÐ¸Ð¼ 
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#optimization", "#rl"], "emoji": "ð", "ru": {"title": "RLVR: Ð¿Ð¾Ð²ÑÑÐµÐ½Ð¸Ðµ ÑÐ¾ÑÐ½Ð¾ÑÑÐ¸ ÑÐµÐ½Ð¾Ð¹ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸ÑÑÐµÑ Ð¼ÐµÑÐ¾Ð´ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ñ Ð¿ÑÐ¾Ð²ÐµÑÑÐµÐ¼ÑÐ¼Ð¸ Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½Ð¸ÑÐ¼Ð¸ (RLVR) Ð² ÐºÐ¾Ð½ÑÐµÐºÑÑÐµ ÑÐµÑÐµÐ½Ð¸Ñ ÑÐ»Ð¾Ð¶Ð½ÑÑ Ð»Ð¾Ð³Ð¸ÑÐµÑÐºÐ¸Ñ Ð·Ð°Ð´Ð°
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#agents", "#dataset", "#synthetic", "#reasoning", "#benchmark"], "emoji": "ð¸ï¸", "ru": {"title": "Ð¤Ð¾ÑÐ¼Ð°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Ð´Ð»Ñ ÑÐ¸Ð½ÑÐµÐ·Ð° Ð´Ð°Ð½Ð½ÑÑ: Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ ÐÐ-Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð¿Ð¾Ð¸ÑÐºÑ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸", "desc": "WebShaper - ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ ÑÐ¸Ð½ÑÐµÐ·Ð° Ð½Ð°Ð±Ð¾ÑÐ¾Ð² Ð´Ð°Ð½Ð½ÑÑ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#3d"], "emoji": "âï¸", "ru": {"title": "Ð¢Ð¾ÑÐ½Ð¾Ðµ Ð»Ð¾ÐºÐ°Ð»ÑÐ½Ð¾Ðµ 3D-ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ ÑÐ¾Ð±Ð°ÑÑÐ½ÑÑ Ð¼Ð°ÑÐ¾Ðº Ð¸ ÑÐ»ÑÑÑÐµÐ½Ð½Ð¾Ð¹ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ð¸", "desc": "RoMaP - ÑÑÐ¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ¾ÑÐ½Ð¾Ð³Ð¾ Ð»Ð¾ÐºÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ 3D-ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ°Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÑÐ¾Ð±Ð°ÑÑÐ½ÑÑ 3D-Ð¼Ð°ÑÐ¾Ðº Ð¸ ÑÐ»ÑÑÑÐµÐ½Ð½ÑÑ ÑÐµÐ³ÑÐ»ÑÑÐ¸Ð·Ð°ÑÐ¸Ñ ÑÑÐ½ÐºÑÐ¸Ð¸ 
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#benchmark", "#reasoning", "#video"], "emoji": "ð¥", "ru": {"title": "ÐÐ¾Ð½ÑÐµÐ¿ÑÑÐ°Ð»ÑÐ½Ð¾Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐµÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ð¸ Ð¾Ð±ÑÐµÐºÑÐ¾Ð² Ð² Ð²Ð¸Ð´ÐµÐ¾", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº ÑÐµÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ð¸ Ð¾Ð±ÑÐµÐºÑÐ¾Ð² Ð² Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Segment Concept (SeC). SeC Ð¸Ñ
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agents", "#training", "#agi"], "emoji": "ð¤", "ru": {"title": "GR-3: Ð¨Ð°Ð³ Ðº ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½ÑÐ¼ ÑÐ¾Ð±Ð¾ÑÐ°Ð¼-Ð¿Ð¾Ð¼Ð¾ÑÐ½Ð¸ÐºÐ°Ð¼", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ GR-3 - ÐºÑÑÐ¿Ð½Ð¾Ð¼Ð°ÑÑÑÐ°Ð±Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾-ÑÐ·ÑÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð´ÐµÐ¹ÑÑÐ²Ð¸Ñ (VLA) Ð´Ð»Ñ ÑÐ¾Ð±Ð¾ÑÐ¾ÑÐµÑÐ½Ð¸ÐºÐ¸. ÐÐ¾Ð´ÐµÐ»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑÑÐ¸ÑÑÐµÑ Ð¸ÑÐºÐ»ÑÑÐ¸Ñ
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#dataset", "#cv", "#diffusion", "#open_source", "#training", "#data"], "emoji": "ð¼ï¸", "ru": {"title": "ÐÐ²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ Ð´Ð»Ñ ÐÐ-ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð²ÑÑÐ¾ÐºÐ¾ÐºÐ°ÑÐµÑÑÐ²ÐµÐ½Ð½ÑÑ ÑÑÐ¸Ð¿Ð»ÐµÑÐ¾Ð² Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#agi", "#robotics", "#dataset", "#training", "#optimization", "#multimodal", "#data"], "emoji": "ð¤", "ru": {"title": "ÐÐ±ÑÑÐµÐ½Ð¸Ðµ ÑÐ¾Ð±Ð¾ÑÐ¾Ð² ÑÐµÐ»Ð¾Ð²ÐµÑÐµÑÐºÐ¸Ð¼ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸ÑÐ¼ ÑÐµÑÐµÐ· Ð²Ð¸Ð´ÐµÐ¾", "desc": "Being-H0 - ÑÑÐ¾ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð·ÑÐµÐ½Ð¸Ñ-ÑÐ·ÑÐºÐ°-Ð´ÐµÐ¹ÑÑÐ²Ð¸Ñ (VLA), Ð¾Ð±ÑÑÐµÐ½Ð½Ð°Ñ Ð½Ð° Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð»ÑÐ´ÑÐ¼Ð¸ Ð´Ð»Ñ ÑÐµÑÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ Ð¼Ð°Ð½Ð¸Ð¿
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#benchmark"], "emoji": "ð¯", "ru": {"title": "Ð¢Ð¾ÑÐ½Ð¾Ðµ Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼: ÑÐ»ÑÑÑÐµÐ½Ð¸Ðµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ ÐÐ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ ÑÐ½ÑÑÐ¾Ð¿Ð¸Ð¹Ð½Ð¾-Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´ÑÐ¾Ð´Ð°", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ°ÑÑÑÐ¶
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#interpretability", "#security", "#benchmark", "#video"], "emoji": "ð¥", "ru": {"title": "ÐÐ¾Ð²ÑÐ¹ ÑÑÐ±ÐµÐ¶ Ð² Ð¾ÑÐµÐ½ÐºÐµ Ð²Ð¸Ð´ÐµÐ¾-LLM: ÑÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð¾Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ ÑÐµÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¼Ð¸ÑÐ°", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ ÑÐµÑÑ Video Thinking Test (Video-TT) Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð²Ð¸Ð´ÐµÐ¾-LLM Ð¸Ð½ÑÐµÑÐ¿ÑÐµÑÐ¸ÑÐ¾Ð²Ð°
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#benchmark", "#hallucinations", "#reasoning", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÐ¾Ð»ÑÑÐµ Ð²ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ð¹ - Ð½Ðµ Ð²ÑÐµÐ³Ð´Ð° Ð»ÑÑÑÐµ: Ð¿Ð°ÑÐ°Ð´Ð¾ÐºÑ Ð¼Ð°ÑÑÑÐ°Ð±Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÐµÑ, ÑÑÐ¾ ÑÐ²ÐµÐ»Ð¸ÑÐµÐ½Ð¸Ðµ Ð²ÑÑÐ¸ÑÐ»Ð¸ÑÐµÐ»ÑÐ½Ð¾Ð¹ Ð¼Ð¾ÑÐ½Ð¾ÑÑÐ¸ Ð¿ÑÐ¸ ÑÐµÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ð¸ ÐºÑÑÐ¿Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ 
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#3d"], "emoji": "ð¨", "ru": {"title": "ÐÐ¸ÑÐºÑÐµÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ðµ SDF Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÐ°ÑÐ½Ð¾Ð³Ð¾ ÑÐµÐ½Ð´ÐµÑÐ¸Ð½Ð³Ð° Ñ Ð³Ð°ÑÑÑÐ¾Ð²ÑÐºÐ¸Ð¼ ÑÐ¿Ð»Ð°ÑÑÐ¸Ð½Ð³Ð¾Ð¼", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¾Ð±ÑÐ°ÑÐ½Ð¾Ð¼Ñ ÑÐµÐ½Ð´ÐµÑÐ¸Ð½Ð³Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð´Ð¸ÑÐºÑÐµÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð»Ñ ÑÐ°ÑÑÑÐ¾ÑÐ½Ð¸Ð¹ ÑÐ¾ Ð·Ð½Ð°ÐºÐ¾Ð¼ (SDF) Ð² ÐºÐ¾Ð½ÑÐµÐºÑÑÐµ 3D Ð³Ð°ÑÑÑÐ¾Ð²Ñ
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#training", "#reasoning", "#audio", "#multimodal"], "emoji": "ð§ ", "ru": {"title": "Stitch: ÐÑÐ¼Ð°Ð¹ Ð¸ Ð³Ð¾Ð²Ð¾ÑÐ¸ Ð¾Ð´Ð½Ð¾Ð²ÑÐµÐ¼ÐµÐ½Ð½Ð¾", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ ÑÐµÑÐ¸ Ð´Ð»Ñ ÑÐ°Ð·Ð³Ð¾Ð²Ð¾ÑÐ½ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Stitch. Ð­ÑÐ¾Ñ Ð¼ÐµÑÐ¾Ð´ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼ Ð¾ÑÑÑÐµÑÑÐ²Ð»ÑÑÑ Ð²Ð½ÑÑÑÐµÐ½Ð½Ð¸
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#benchmark", "#architecture", "#cv"], "emoji": "ð", "ru": {"title": "Ð ÐµÐºÐ¾Ð½ÑÑÑÑÐºÑÐ¸Ñ 4D-Ð³ÐµÐ¾Ð¼ÐµÑÑÐ¸Ð¸ Ð² ÑÐµÐ°Ð»ÑÐ½Ð¾Ð¼ Ð²ÑÐµÐ¼ÐµÐ½Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð¿Ð¾ÑÐ¾ÐºÐ¾Ð²Ð¾Ð³Ð¾ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÐ°", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð¿Ð¾ÑÐ¾ÐºÐ¾Ð²ÑÐ¹ 4D-ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾Ð¹ Ð³ÐµÐ¾Ð¼ÐµÑÑÐ¸Ð¸ Ð´Ð»Ñ ÑÐµÐºÐ¾Ð½
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#diffusion", "#dataset"], "emoji": "ð§¹", "ru": {"title": "Ð¨ÑÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ°Ðº ÐºÐ»ÑÑ Ðº ÑÑÑÐµÐºÑÐ¸Ð²Ð½ÑÐ¼ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÐ¼ ÑÐ¾ÐºÐµÐ½Ð¸Ð·Ð°ÑÐ¾ÑÐ°Ð¼", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐºÐµ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ ÑÐ¾ÐºÐµÐ½Ð¸Ð·Ð°ÑÐ¾ÑÐ¾Ð² Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð²Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÑÑ 
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#optimization", "#agents", "#agi", "#multimodal", "#rl", "#science"], "emoji": "ð¤", "ru": {"title": "ÐÑÐºÑÑÑÑÐ²ÐµÐ½Ð½ÑÐ¹ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑ ÐºÐ°Ðº ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÑÑ: Ð¼Ð¾Ð´ÐµÐ»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÑÐµÑÐºÐ¾Ð¹ Ð¿Ð¾Ð»Ð¸ÑÐ¸ÐºÐ¸", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÑ ÐºÐ¾Ð½ÑÐµÐ¿ÑÐ¸Ñ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ 'LLM Economist', ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¸ÑÐ¿Ð¾Ð»Ñ
[22.07.2025 13:33] Querying the API.
[22.07.2025 13:33] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PhysGym, a new benchmark suite, evaluates large language model-based agents' scientific reasoning in interactive physics environments, focusing on their handling of complexity and prior knowledge.  					AI-generated summary 				 Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity.
[22.07.2025 13:33] Response: {
  "desc": "PhysGym - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð½Ð°Ð±Ð¾Ñ ÑÐµÑÑÐ¾Ð² Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð½Ð°ÑÑÐ½Ð¾Ð³Ð¾ Ð¼ÑÑÐ»ÐµÐ½Ð¸Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¸Ð½ÑÐµÑÐ°ÐºÑÐ¸Ð²Ð½ÑÑ ÑÐ¸Ð·Ð¸ÑÐµÑÐºÐ¸Ñ ÑÑÐµÐ´Ð°Ñ. ÐÐ½ ÑÐ¾ÐºÑÑÐ¸ÑÑÐµÑÑÑ Ð½Ð° ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð°Ð³ÐµÐ½ÑÐ¾Ð² ÑÐ¿ÑÐ°Ð²Ð»ÑÑÑÑÑ ÑÐ¾ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÑÑ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÑ Ð¿ÑÐµÐ´Ð²Ð°ÑÐ¸ÑÐµÐ»ÑÐ½ÑÐµ Ð·Ð½Ð°Ð½Ð¸Ñ. PhysGym Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ ÐºÐ¾Ð½ÑÐ¾Ð²Ð°ÑÐµÐ»ÑÐ¼ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸ÑÐ¾Ð²Ð°ÑÑ Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÑ Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑÐ¸ Ð¾Ñ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸ Ð·Ð°Ð´Ð°ÑÐ¸ Ð¸ ÑÑÐ¾Ð²Ð½Ñ Ð¿ÑÐµÐ´Ð²Ð°ÑÐ¸ÑÐµÐ»ÑÐ½ÑÑ Ð·Ð½Ð°Ð½Ð¸Ð¹. ÐÐ²ÐºÐ»ÑÑÐ°ÐµÑ Ð¸Ð½ÑÐµÑÐ°ÐºÑÐ¸Ð²Ð½ÑÐµ ÑÐ¸Ð¼ÑÐ»ÑÑÐ¸Ð¸, Ð³Ð´Ðµ Ð°Ð³ÐµÐ½ÑÑ Ð´Ð¾Ð»Ð¶Ð½Ñ Ð°ÐºÑÐ¸Ð²Ð½Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÑ ÑÑÐµÐ´Ñ, ÑÐ¾Ð±Ð¸ÑÐ°ÑÑ Ð´Ð°Ð½Ð½ÑÐµ Ð¸ ÑÐ¾ÑÐ¼ÑÐ»Ð¸ÑÐ¾Ð²Ð°ÑÑ Ð³Ð¸Ð¿Ð¾ÑÐµÐ·Ñ Ð¾ ÑÐ¸Ð·Ð¸ÑÐµÑÐºÐ¸Ñ Ð·Ð°ÐºÐ¾Ð½Ð°Ñ.",

  "emoji": "ð§ ",

  "title": "PhysGym: Ð¸ÑÐ¿ÑÑÐ°Ð½Ð¸Ðµ Ð½Ð°ÑÑÐ½Ð¾Ð³Ð¾ Ð¼ÑÑÐ»ÐµÐ½Ð¸Ñ ÐÐ Ð² ÑÐ¸Ð·Ð¸ÑÐµÑÐºÐ¾Ð¼ Ð¼Ð¸ÑÐµ"
}
[22.07.2025 13:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PhysGym, a new benchmark suite, evaluates large language model-based agents' scientific reasoning in interactive physics environments, focusing on their handling of complexity and prior knowledge.  					AI-generated summary 				 Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity."

[22.07.2025 13:33] Response: ```python
['BENCHMARK', 'AGENTS']
```
[22.07.2025 13:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PhysGym, a new benchmark suite, evaluates large language model-based agents' scientific reasoning in interactive physics environments, focusing on their handling of complexity and prior knowledge.  					AI-generated summary 				 Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity."

[22.07.2025 13:33] Response: ```python
['REASONING', 'SCIENCE']
```
[22.07.2025 13:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PhysGym is a new benchmark suite designed to evaluate how well large language model (LLM) agents can reason scientifically in interactive physics settings. It focuses on how these agents manage different levels of complexity in their environments and how they use prior knowledge to solve problems. The benchmark includes a variety of simulations where agents must explore, collect data, and form hypotheses about physical laws. By providing standardized evaluation metrics, PhysGym helps researchers understand the performance of LLMs based on their prior knowledge and the complexity of tasks they face.","title":"PhysGym: Benchmarking Scientific Reasoning in Physics with LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PhysGym is a new benchmark suite designed to evaluate how well large language model (LLM) agents can reason scientifically in interactive physics settings. It focuses on how these agents manage different levels of complexity in their environments and how they use prior knowledge to solve problems. The benchmark includes a variety of simulations where agents must explore, collect data, and form hypotheses about physical laws. By providing standardized evaluation metrics, PhysGym helps researchers understand the performance of LLMs based on their prior knowledge and the complexity of tasks they face.', title='PhysGym: Benchmarking Scientific Reasoning in Physics with LLMs'))
[22.07.2025 13:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PhysGymæ¯ä¸ä¸ªæ°çåºåå¥ä»¶ï¼ç¨äºè¯ä¼°åºäºå¤§åè¯­è¨æ¨¡åçæºè½ä½å¨äºå¨ç©çç¯å¢ä¸­çç§å­¦æ¨çè½åãå®ä¸æ³¨äºæºè½ä½å¦ä½å¤çå¤ææ§åå©ç¨ååç¥è¯ãPhysGymçä¸»è¦è´¡ç®å¨äºå¯¹æä¾ç»æºè½ä½çååç¥è¯æ°´å¹³è¿è¡ç²¾ç»æ§å¶ï¼ä»èå¸®å©ç ç©¶äººååææºè½ä½å¨ä¸åé®é¢å¤ææ§åç¥è¯æ°´å¹³ä¸çè¡¨ç°ãè¯¥åºååæ¬ä¸ç³»åäºå¨æ¨¡æï¼æºè½ä½éè¦å¨çº¦ææ¡ä»¶ä¸ä¸»å¨æ¢æµç¯å¢ãæ¶éæ°æ®å¹¶å½¢æå³äºç©çæ³åçåè®¾ã","title":"PhysGymï¼è¯ä¼°æºè½ä½ç§å­¦æ¨ççæ°åºå"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PhysGymæ¯ä¸ä¸ªæ°çåºåå¥ä»¶ï¼ç¨äºè¯ä¼°åºäºå¤§åè¯­è¨æ¨¡åçæºè½ä½å¨äºå¨ç©çç¯å¢ä¸­çç§å­¦æ¨çè½åãå®ä¸æ³¨äºæºè½ä½å¦ä½å¤çå¤ææ§åå©ç¨ååç¥è¯ãPhysGymçä¸»è¦è´¡ç®å¨äºå¯¹æä¾ç»æºè½ä½çååç¥è¯æ°´å¹³è¿è¡ç²¾ç»æ§å¶ï¼ä»èå¸®å©ç ç©¶äººååææºè½ä½å¨ä¸åé®é¢å¤ææ§åç¥è¯æ°´å¹³ä¸çè¡¨ç°ãè¯¥åºååæ¬ä¸ç³»åäºå¨æ¨¡æï¼æºè½ä½éè¦å¨çº¦ææ¡ä»¶ä¸ä¸»å¨æ¢æµç¯å¢ãæ¶éæ°æ®å¹¶å½¢æå³äºç©çæ³åçåè®¾ã', title='PhysGymï¼è¯ä¼°æºè½ä½ç§å­¦æ¨ççæ°åºå'))
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#games", "#interpretability", "#optimization", "#benchmark", "#video"], "emoji": "ð¥", "ru": {"title": "Ð¤Ð¸Ð·Ð¸ÐºÐ° Ð² Ð²Ð¸ÑÑÑÐ°Ð»ÑÐ½Ð¾Ð¼ Ð¼Ð¸ÑÐµ: Ð½Ð¾Ð²ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐµÐ°Ð»Ð¸ÑÑÐ¸ÑÐ½Ð¾ÑÑÐ¸ Ð²Ð¸Ð´ÐµÐ¾Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ PhyWorldBench - ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ 
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#training", "#optimization", "#transfer_learning", "#agents", "#rl"], "emoji": "ð", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ ÑÐ¼ÐµÑÐ¸Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½ÑÑ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Data Mixing Agent - ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð¿ÐµÑÐµÐ¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾
[22.07.2025 13:33] Querying the API.
[22.07.2025 13:33] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ .
[22.07.2025 13:33] Response: {
  "desc": "TokensGen - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð´Ð²ÑÑÑÑÐ°Ð¿Ð½ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð´Ð»Ð¸Ð½Ð½ÑÑ Ð²Ð¸Ð´ÐµÐ¾, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ¸Ð¹ ÑÐ¶Ð°ÑÑÐµ ÑÐ¾ÐºÐµÐ½Ñ. ÐÐ¾Ð´ÐµÐ»Ñ To2V Ð³ÐµÐ½ÐµÑÐ¸ÑÑÐµÑ ÐºÐ¾ÑÐ¾ÑÐºÐ¸Ðµ ÐºÐ»Ð¸Ð¿Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐµÐºÑÑÐ° Ð¸ Ð²Ð¸Ð´ÐµÐ¾ÑÐ¾ÐºÐµÐ½Ð¾Ð², Ð° T2To ÑÐ¾Ð·Ð´Ð°ÐµÑ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»ÑÐ½Ð¾ÑÑÑ ÑÐ¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±ÐµÑÐ¿ÐµÑÐµÐ½Ð¸Ñ Ð³Ð»Ð¾Ð±Ð°Ð»ÑÐ½Ð¾Ð¹ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑÐ¸. Ð¡ÑÑÐ°ÑÐµÐ³Ð¸Ñ FIFO-Diffusion Ð¿Ð»Ð°Ð²Ð½Ð¾ ÑÐ¾ÐµÐ´Ð¸Ð½ÑÐµÑ ÑÐ¾ÑÐµÐ´Ð½Ð¸Ðµ ÐºÐ»Ð¸Ð¿Ñ, ÑÐ¼ÐµÐ½ÑÑÐ°Ñ Ð°ÑÑÐµÑÐ°ÐºÑÑ Ð½Ð° Ð³ÑÐ°Ð½Ð¸ÑÐ°Ñ. Ð­ÑÐ¾Ñ Ð¼ÐµÑÐ¾Ð´ Ð·Ð½Ð°ÑÐ¸ÑÐµÐ»ÑÐ½Ð¾ ÑÐ»ÑÑÑÐ°ÐµÑ Ð´Ð¾Ð»Ð³Ð¾ÑÑÐ¾ÑÐ½ÑÑ Ð²ÑÐµÐ¼ÐµÐ½Ð½ÑÑ Ð¸ ÑÐ¾Ð´ÐµÑÐ¶Ð°ÑÐµÐ»ÑÐ½ÑÑ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑÑ Ð²Ð¸Ð´ÐµÐ¾ Ð±ÐµÐ· ÑÑÐµÐ·Ð¼ÐµÑÐ½ÑÑ Ð²ÑÑÐ¸ÑÐ»Ð¸ÑÐµÐ»ÑÐ½ÑÑ Ð·Ð°ÑÑÐ°Ñ.",
  "emoji": "ð¬",
  "title": "TokensGen: Ð ÐµÐ²Ð¾Ð»ÑÑÐ¸Ñ Ð² Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð´Ð»Ð¸Ð½Ð½ÑÑ Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ ÑÐ¶Ð°ÑÑÑ ÑÐ¾ÐºÐµÐ½Ð¾Ð²"
}
[22.07.2025 13:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ ."

[22.07.2025 13:33] Response: ```python
['VIDEO', 'MULTIMODAL', 'INFERENCE']
```
[22.07.2025 13:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ ."

[22.07.2025 13:33] Response: ```python
['DIFFUSION', 'LONG_CONTEXT', 'STORY_GENERATION']
```
[22.07.2025 13:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces TokensGen, a two-stage framework designed to generate long videos more effectively. It addresses challenges like memory limitations and inconsistencies in long video generation by using condensed tokens. The method involves training a short video diffusion model and a video token diffusion transformer to ensure semantic richness and global consistency. The results show that TokensGen improves the coherence of long videos while maintaining manageable computational demands, making it suitable for various applications in storytelling and simulations.","title":"TokensGen: Seamless Long Video Generation with Condensed Tokens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces TokensGen, a two-stage framework designed to generate long videos more effectively. It addresses challenges like memory limitations and inconsistencies in long video generation by using condensed tokens. The method involves training a short video diffusion model and a video token diffusion transformer to ensure semantic richness and global consistency. The results show that TokensGen improves the coherence of long videos while maintaining manageable computational demands, making it suitable for various applications in storytelling and simulations.', title='TokensGen: Seamless Long Video Generation with Condensed Tokens'))
[22.07.2025 13:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"çæä¸è´çé¿è§é¢æ¯ä¸ä¸ªå¤æçææãè½ç¶åºäºæ©æ£ççææ¨¡åå¯ä»¥çæè§è§ä¸ä»¤äººå°è±¡æ·±å»çç­çï¼ä½æ©å±å°æ´é¿çæ¶é¿å¸¸å¸¸ä¼å¯¼è´åå­ç¶é¢åé¿æä¸ä¸è´æ§ãæä»¬æåºäºä¸ç§åä¸ºTokensGençæ°åä¸¤é¶æ®µæ¡æ¶ï¼éè¿æµç¼©çæ è®°æ¥è§£å³è¿äºé®é¢ãæä»¬çç ç©¶è¡¨æï¼è¯¥æ¹æ³å¨é¿è§é¢çæä¸­æ¾èæé«äºæ¶é´ååå®¹çä¸è´æ§ï¼åæ¶é¿åäºè¿é«çè®¡ç®å¼éã","title":"TokensGenï¼é¿è§é¢çæçæ°è§£å³æ¹æ¡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='çæä¸è´çé¿è§é¢æ¯ä¸ä¸ªå¤æçææãè½ç¶åºäºæ©æ£ççææ¨¡åå¯ä»¥çæè§è§ä¸ä»¤äººå°è±¡æ·±å»çç­çï¼ä½æ©å±å°æ´é¿çæ¶é¿å¸¸å¸¸ä¼å¯¼è´åå­ç¶é¢åé¿æä¸ä¸è´æ§ãæä»¬æåºäºä¸ç§åä¸ºTokensGençæ°åä¸¤é¶æ®µæ¡æ¶ï¼éè¿æµç¼©çæ è®°æ¥è§£å³è¿äºé®é¢ãæä»¬çç ç©¶è¡¨æï¼è¯¥æ¹æ³å¨é¿è§é¢çæä¸­æ¾èæé«äºæ¶é´ååå®¹çä¸è´æ§ï¼åæ¶é¿åäºè¿é«çè®¡ç®å¼éã', title='TokensGenï¼é¿è§é¢çæçæ°è§£å³æ¹æ¡'))
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "ð§ ", "ru": {"title": "Ð£Ð»ÑÑÑÐµÐ½Ð¸Ðµ Ð¼Ð½Ð¾Ð³Ð¾ÑÑÐ°Ð¿Ð½ÑÑ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ ÐÐ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑÐ½Ð¾Ð¹ Ð¾Ð±ÑÐ°ÑÐ½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿Ð¾ÑÐ²ÑÑÐµÐ½Ð° ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) ÑÐµÑÐ°ÑÑ Ð·Ð°Ð´Ð°ÑÐ¸ Ð² Ð¼Ð½Ð¾Ð³Ð¾ÑÑÐ°Ð¿Ð½Ð¾Ð¼ ÑÐµÐ¶Ð¸Ð¼Ðµ 
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#cv"], "emoji": "ð", "ru": {"title": "GeoDistill: Ð³ÐµÐ¾Ð¼ÐµÑÑÐ¸Ñ Ð½Ð° ÑÐ»ÑÐ¶Ð±Ðµ ÐºÑÐ¾ÑÑ-Ð²Ð¸Ð´Ð¾Ð²Ð¾Ð¹ Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ð¸", "desc": "GeoDistill - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº ÐºÑÐ¾ÑÑ-Ð²Ð¸Ð´Ð¾Ð²Ð¾Ð¹ Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ¸Ð¹ ÑÐ»Ð°Ð±Ð¾ ÐºÐ¾Ð½ÑÑÐ¾Ð»Ð¸ÑÑÐµÐ¼Ð¾Ðµ ÑÐ°Ð¼Ð¾Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð³ÐµÐ¾Ð¼ÐµÑÑÐ¸Ð¸. ÐÐµÑÐ¾Ð´ Ð¿ÑÐ¸Ð¼ÐµÐ½
[22.07.2025 13:33] Using data from previous issue: {"categories": ["#healthcare", "#training", "#data"], "emoji": "ð¬", "ru": {"title": "UGPL: Ð£Ð¼Ð½ÑÐ¹ Ð°Ð½Ð°Ð»Ð¸Ð· ÐÐ¢ Ð¾Ñ Ð¾Ð±ÑÐµÐ³Ð¾ Ðº ÑÐ°ÑÑÐ½Ð¾Ð¼Ñ", "desc": "UGPL - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÐÐ¢-Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ¸Ð¹ Ð¿ÑÐ¸Ð½ÑÐ¸Ð¿ Ð¿ÑÐ¾Ð³ÑÐµÑÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¾Ñ Ð¾Ð±ÑÐµÐ³Ð¾ Ðº ÑÐ°ÑÑÐ½Ð¾Ð¼Ñ. Ð¡Ð¸ÑÑÐµÐ¼Ð° ÑÐ½Ð°ÑÐ°Ð»Ð° Ð²ÑÑÐ²Ð»ÑÐµÑ Ð¾Ð±Ð»Ð°ÑÑÐ¸ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑÐ¸ÑÐµÑÐºÐ¾
[22.07.2025 13:33] Renaming data file.
[22.07.2025 13:33] Renaming previous data. hf_papers.json to ./d/2025-07-22.json
[22.07.2025 13:33] Saving new data file.
[22.07.2025 13:33] Generating page.
[22.07.2025 13:33] Renaming previous page.
[22.07.2025 13:33] Renaming previous data. index.html to ./d/2025-07-22.html
[22.07.2025 13:33] Writing result.
[22.07.2025 13:33] Renaming log file.
[22.07.2025 13:33] Renaming previous data. log.txt to ./logs/2025-07-22_last_log.txt
