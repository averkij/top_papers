[16.12.2025 13:41] Read previous papers.
[16.12.2025 13:41] Generating top page (month).
[16.12.2025 13:41] Writing top page (month).
[16.12.2025 14:28] Read previous papers.
[16.12.2025 14:28] Get feed.
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13687
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13564
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13604
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13168
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13586
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12730
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12967
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13313
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12602
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09636
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10071
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13080
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12799
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11995
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13250
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12692
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11891
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13592
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12751
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11883
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13674
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13421
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13689
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13672
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13281
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.07186
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.05272
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13690
[16.12.2025 14:28] Extract page data from URL. URL: https://huggingface.co/papers/2512.13006
[16.12.2025 14:28] Extract page data from URL. URL: https://huggingface.co/papers/2512.11438
[16.12.2025 14:28] Extract page data from URL. URL: https://huggingface.co/papers/2512.10655
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13330
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12777
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08405
[16.12.2025 14:28] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08400
[16.12.2025 14:28] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.12.2025 14:28] No deleted papers detected.
[16.12.2025 14:28] Downloading and parsing papers (pdf, html). Total: 35.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13687.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13687.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13687.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13564.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13564.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13564.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13604.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13604.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13604.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13168.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13168.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13168.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13586.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13586.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13586.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.12730.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.12730.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.12730.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.12967.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.12967.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.12967.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13313.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13313.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13313.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.12602.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.12602.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.12602.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.09636.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.09636.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.09636.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.10071.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.10071.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.10071.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13080.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13080.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13080.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.12799.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.12799.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.12799.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.11995.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.11995.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.11995.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13250.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13250.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13250.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.12692.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.12692.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.12692.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.11891.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.11891.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.11891.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13592.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13592.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13592.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.12751.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.12751.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.12751.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.11883.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.11883.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.11883.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13674.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13674.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13674.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13421.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13421.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13421.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13689.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13689.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13689.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13672.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13672.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13672.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13281.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13281.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13281.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.07186.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.07186.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.07186.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.05272.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.05272.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.05272.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13690.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13690.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13690.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13006.
[16.12.2025 14:28] Downloading paper 2512.13006 from https://arxiv.org/pdf/2512.13006v1...
[16.12.2025 14:28] Extracting affiliations from text.
[16.12.2025 14:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 6 0 0 3 1 . 2 1 5 2 : r Few-Step Distillation for Text-to-Image Generation: Practical Guide Yifan Pu1,2, Yizeng Han2, Zhiwei Tang2, Jiasheng Tang2,3, Fan Wang2, Bohan Zhuang2,4, Gao Huang1 1Tsinghua University 2DAMO Academy, Alibaba Group 3Hupan Lab 4Zhejiang University Equal contribution, Corresponding author Diffusiondistillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on strong T2I teacher model, FLUX.1-lite. By casting existing methods into unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill. Date: December 16, In recent years, large-scale Diffusion Models (DMs) (Ho et al., 2020; Sohl-Dickstein et al., 2015) have achieved unprecedented success in the field of text-to-image synthesis, with generation quality that rivals or even surpasses human creation (Rombach et al., 2022; Saharia et al., 2022). Models such as Flux (Black Forest Labs et al., 2025), Qwen-Image (Wu et al., 2025) and Imagen (Google DeepMind, 2025), trained on massive image-text datasets, can generate high-fidelity, high-resolution images from complex textual descriptions. However, this remarkable performance comes at significant computational cost. These models rely on an iterative sampling process, progressively converting Gaussian noise into clear image through hundreds of N"
[16.12.2025 14:28] Response: ```python
[
    "Tsinghua University",
    "DAMO Academy, Alibaba Group",
    "Hupan Lab",
    "Zhejiang University"
]
```
[16.12.2025 14:28] Deleting PDF ./assets/pdf/2512.13006.pdf.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.11438.
[16.12.2025 14:28] Downloading paper 2512.11438 from https://arxiv.org/pdf/2512.11438v1...
[16.12.2025 14:28] Extracting affiliations from text.
[16.12.2025 14:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 8 3 4 1 1 . 2 1 5 2 : r Flowception: Temporally Expansive Flow Matching for Video Generation Tariq Berrada Ifriqi1,2, John Nguyen1, Karteek Alahari2, Jakob Verbeek1, Ricky Chen1 1FAIR at Meta, 2Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, France We present Flowception, novel non-autoregressive and variable-length video generation framework. Flowception learns probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation. Correspondence: First Author at tariqberrada@meta.com Figure 1 Examples of image-to-video (I2V) generation and video interpolation with Flowception . Input frames marked by dashed boundaries. Flowception enables variable-length non-autoregressive generation by learning to both denoise and insert frames in any order. Recent advances in diffusion and flow-matching have unlocked high-fidelity image generation Esser et al. (2024); Labs (2024) and are rapidly migrating to video. Current video generation models typically adopt one of two paradigms: full-sequence generation denoises all frames jointly with full attention Wan et al. (2025); HaCohen et al. (2024); Zheng et al. (2024), while temporal autoregressive (AR) generation produces frames (or"
[16.12.2025 14:28] Response: ```python
[
    "FAIR at Meta",
    "Univ. Grenoble Alpes",
    "Inria",
    "CNRS",
    "Grenoble INP",
    "LJK"
]
```
[16.12.2025 14:28] Deleting PDF ./assets/pdf/2512.11438.pdf.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.10655.
[16.12.2025 14:28] Downloading paper 2512.10655 from https://arxiv.org/pdf/2512.10655v1...
[16.12.2025 14:28] Extracting affiliations from text.
[16.12.2025 14:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CAPTAIN: SEMANTIC FEATURE INJECTION FOR MEMORIZATION MITIGATION IN TEXT-TO-IMAGE DIFFUSION MODELS 5 2 0 2 1 1 ] . [ 1 5 5 6 0 1 . 2 1 5 2 : r Tong Zhang, Carlos Hinojosa, Bernard Ghanem {tong.zhang.1; carlos.hinojosa; bernard.ghanem}@kaust.edu.sa King Abdullah University of Science and Technology "
[16.12.2025 14:28] Response: ```python
["King Abdullah University of Science and Technology"]
```
[16.12.2025 14:28] Deleting PDF ./assets/pdf/2512.10655.pdf.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.13330.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.13330.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.13330.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.12777.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.12777.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.12777.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.08405.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.08405.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.08405.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Downloading and parsing paper https://huggingface.co/papers/2512.08400.
[16.12.2025 14:28] Extra JSON file exists (./assets/json/2512.08400.json), skip PDF parsing.
[16.12.2025 14:28] Paper image links file exists (./assets/img_data/2512.08400.json), skip HTML parsing.
[16.12.2025 14:28] Success.
[16.12.2025 14:28] Enriching papers with extra data.
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 0. A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.  					AI-generated summary 				 The qua...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 1. This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.  					AI-generated summary 				 Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As r...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 2. LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.  					AI-generated summary 				 Building video world models upon pretrained video generation systems represents an im...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 3. Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.  					AI-generated summary 				 We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on ...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 4. ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.  					AI-generated summary 				 Autoregressive models (ARMs) are hindered by slow...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 5. NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.  					AI-generated summary 				 Recent advances in coding agents suggest rapid progress toward autonomous softw...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 6. QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.  					AI-generated summary 				 We introduce QwenLong-L1.5, a model that achieves superior lon...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 7. KlingAvatar 2.0 addresses inefficiencies in generating long-duration, high-resolution videos by using a spatio-temporal cascade framework with a Co-Reasoning Director and Negative Director for improved multimodal instruction alignment.  					AI-generated summary 				 Avatar video generation models h...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 8. Error-Free Linear Attention (EFLA) is a stable, parallelizable, and theoretically sound linear-time attention mechanism that outperforms DeltaNet in language modeling and downstream tasks.  					AI-generated summary 				 Linear-time attention and State Space Models (SSMs) promise to solve the quadra...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 9. MentraSuite, a unified framework, advances reliable mental health reasoning using Mindora, a post-trained model with hybrid SFT-RL, evaluated via MentraBench, a benchmark assessing task performance and reasoning quality.  					AI-generated summary 				 Mental health disorders affect hundreds of mill...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 10. A solution for the 2025 BEHAVIOR Challenge in everyday household tasks using pre-training and post-training techniques substantially outperforms other submissions.  					AI-generated summary 				 The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks b...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 11. A Spatial-Aware VLA Pretraining paradigm improves 3D spatial understanding in robots by aligning 2D visual inputs with 3D actions using dual-encoder architecture with a 3D visual encoder.  					AI-generated summary 				 Vision-Language-Action (VLA) models provide a promising paradigm for robot learn...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 12. DrivePI, a spatial-aware 4D multi-modal large language model, achieves state-of-the-art performance in 3D perception, prediction, and planning for autonomous driving by integrating point clouds, images, and language instructions.  					AI-generated summary 				 Although multi-modal large language mo...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 13. The V-REX evaluation suite assesses vision-language models' multi-step reasoning and exploration capabilities through a Chain-of-Questions framework, revealing their strengths and weaknesses in planning and following.  					AI-generated summary 				 While many vision-language models (VLMs) are devel...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 14. VG-AVS, a task and framework fine-tunes VLMs to select the most informative next viewpoint for visual question answering, enhancing performance and generalization.  					AI-generated summary 				 Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vi...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 15. WebOperator is a tree-search framework that enhances web-based agents by enabling reliable backtracking and strategic exploration, addressing the limitations of existing methods in handling irreversible actions and partial observability.  					AI-generated summary 				 LLM-based agents often operate...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 16. AEGIS, a Vision-Language-Safe Action architecture with a plug-and-play safety constraint layer using control barrier functions, enhances safety and performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models have demonstrated remarkable capabilities...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 17. Diffusion Preview uses ConsistencySolver, a high-order trainable solver, to improve quality and consistency in low-step image generation, enhancing interactive user experiences.  					AI-generated summary 				 The slow inference process of image diffusion models significantly degrades interactive us...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 18. GenieDrive uses a 4D occupancy-based approach with a VAE and Mutual Control Attention for physics-aware driving video generation, improving forecasting accuracy and video quality.  					AI-generated summary 				 Physics-aware driving world model is essential for drive planning, out-of-distribution d...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 19. State-of-the-art image generation and reward models exhibit bias towards conventional aesthetics, often failing to produce anti-aesthetic images as requested, thus compromising user autonomy and aesthetic diversity.  					AI-generated summary 				 Over-aligning image generation models to a generaliz...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 20. Interactive Intelligence, realized through Mio framework, enables advanced digital humans with personality, adaptive interactions, and self-evolution, surpassing current benchmarks.  					AI-generated summary 				 We introduce Interactive Intelligence, a novel paradigm of digital human that is capab...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 21. RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.  					AI-generated summary 				 Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 22. A new 3D point cloud backbone model, LitePT, uses convolutions for early stages and attention for deeper layers, incorporating PointROPE for positional encoding, achieving efficient performance with fewer resources.  					AI-generated summary 				 Modern neural architectures for 3D point cloud proce...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 23. Directional Textual Inversion (DTI) improves text-to-image personalization by constraining learned tokens to unit magnitude, enhancing prompt conditioning and enabling smooth interpolation between concepts.  					AI-generated summary 				 Textual Inversion (TI) is an efficient approach to text-to-im...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 24. The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  					AI-generated summary 				 Recent advances ...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 25. START enhances multimodal large language models by integrating spatial and textual learning through chart-element grounding and chart-to-code generation, improving chart understanding and performance across benchmarks.  					AI-generated summary 				 Chart understanding is crucial for deploying mult...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 26. COM4D predicts the structure and spatio-temporal configuration of 4D/3D objects from 2D video without using 4D compositional training data, achieving state-of-the-art results in 4D object and composed 3D reconstruction.  					AI-generated summary 				 Scenes in the real world are often composed of s...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 27. DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.  					AI-generated summary 				 Video diffusion models have revolutionized generative video synthesis, but they are imprecise, sl...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 28. A systematic study adapts diffusion distillation techniques to text-to-image generation, providing guidelines for successful implementation and deployment.  					AI-generated summary 				 Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to ...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 29. Flowception, a non-autoregressive video generation framework, interleaves discrete frame insertions with continuous denoising, improving efficiency and performance over existing methods.  					AI-generated summary 				 We present Flowception, a novel non-autoregressive and variable-length video gene...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 30. CAPTAIN, a training-free framework, mitigates memorization in diffusion models by modifying latent features during denoising, ensuring prompt fidelity and visual quality.  					AI-generated summary 				 Diffusion models can unintentionally reproduce training examples, raising privacy and copyright c...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 31. FIN-bench-v2 is a unified benchmark suite for evaluating Finnish large language models, incorporating diverse datasets and evaluation criteria.  					AI-generated summary 				 We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolida...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 32. The State over Tokens (SoT) framework reinterprets reasoning tokens in large language models as computational states rather than linguistic narratives, highlighting the need for a new focus in research.  					AI-generated summary 				 Large Language Models (LLMs) can generate reasoning tokens before...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 33. A generative latent flow matching model is proposed to predict future audio for robotic manipulation tasks, improving performance over methods without future lookahead by accurately capturing intrinsic rhythmic patterns.  					AI-generated summary 				 World models have demonstrated impressive perfo...
[16.12.2025 14:28] ********************************************************************************
[16.12.2025 14:28] Abstract 34. The study presents an optimized deep learning pipeline using the AutoFish dataset and Swin-T architecture to improve fish re-identification metrics in electronic monitoring systems.  					AI-generated summary 				 Accurate fisheries data are crucial for effective and sustainable marine resource mana...
[16.12.2025 14:28] Read previous papers.
[16.12.2025 14:28] Generating reviews via LLM API.
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#training", "#architecture", "#optimization", "#cv"], "emoji": "üé®", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ —Å–º—ã—Å–ª—É: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—èÊ°ÜÊû∂VTP –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#long_context", "#rl", "#agents", "#benchmark", "#rag"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–∞–º—è—Ç–∏ –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#video", "#optimization", "#training", "#benchmark", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢—Ä–∏ —ç—Ç–∞–ø–∞ –∫ –∏–¥–µ–∞–ª—å–Ω–æ–π –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "LongVie 2 ‚Äî —ç—Ç–æ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ç—Ä—ë—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–≤–æ–π
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#agents", "#science", "#multimodal", "#dataset", "#benchmark"], "emoji": "üìä", "ru": {"title": "–†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö AI-–ø–æ–º–æ—â–Ω–∏–∫–æ–≤: –±–µ–Ω—á–º–∞—Ä–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫ Finch –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#inference", "#optimization", "#benchmark", "#diffusion", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∫ —Å–ª–æ—Ç–∞–º: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "ReFusion ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–∞–∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#agents", "#reasoning", "#plp"], "emoji": "üèóÔ∏è", "ru": {"title": "–î–æ–ª–≥–æ—ç—Ç–∞–ø–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∫–∞–∫ –≥–ª–∞–≤–Ω—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤", "desc": "NL2Repo Bench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤-–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ Pyth
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#long_context", "#synthetic", "#rl", "#agents", "#benchmark", "#data", "#architecture"], "emoji": "üß†", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "QwenLong-L1.5 ‚Äî —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã–¥–∞—é—â–∏—Ö—Å—è
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#video", "#multimodal", "#story_generation", "#alignment", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã—Å–æ–∫–æ—Ä–µ–∑–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "KlingAvatar 2.0 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ø–∞—Ü–∏–æ-—Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã–π –∫–∞—Å–∫–∞–¥–Ω—ã–π —Ñ
[16.12.2025 14:28] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –±–µ–∑ –æ—à–∏–±–æ–∫: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (EFLA), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ–ª–Ω—É—é –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –∑–∞–¥–∞—á—É –æ–±—É—á–µ–Ω–∏—è –∫
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rlhf", "#hallucinations", "#healthcare", "#science", "#training", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–∞–¥—ë–∂–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#training", "#benchmark", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–æ–º–∞—à–Ω–∏—Ö —Ä–æ–±–æ—Ç–æ–≤: –æ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è BEHAVIOR Challenge 2025, –≥–¥–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∞–≥–µ–Ω—Ç—ã —Ä–µ—à–∞—é—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#multimodal", "#training", "#architecture", "#3d", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ú–æ—Å—Ç –º–µ–∂–¥—É –¥–≤—É–º–µ—Ä–Ω—ã–º –≤–∏–¥–µ–Ω–∏–µ–º –∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#robotics", "#multimodal", "#dataset", "#3d"], "emoji": "üöó", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏", "desc": "DrivePI ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–∞—è —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#interpretability", "#cv", "#benchmark", "#dataset"], "emoji": "üîç", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤", "desc": "V-REX ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º—É
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#multimodal", "#training", "#rl", "#agents", "#dataset", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ä–∞–∫—É—Ä—Å–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VG-AVS ‚Äî –∑–∞–¥–∞—á—É –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤—ã–±–∏—Ä–∞—Ç—å 
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "üå≥", "ru": {"title": "–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –æ—Ç–∫–∞—Ç–æ–º –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "WebOperator ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∂–∞–¥–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π, –ø–æ–∑–≤–æ–ª—è—è –∞
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#architecture", "#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –±–∞—Ä—å–µ—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AEGIS ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision-Language-Safe Action, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–±–∞–≤–ª—è–µ—Ç 
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –∫–∞—á–µ—Å—Ç–≤–∞: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π —Ä–µ—à–∞—Ç–µ–ª—å –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Diffusion Preview –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#video", "#architecture", "#3d", "#robotics"], "emoji": "üöó", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤–æ–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ 4D –æ–±—ä—ë–º–Ω—É—é –∑–∞–Ω—è—Ç–æ—Å—Ç—å", "desc": "GenieDrive ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤–æ–∂–¥–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º —Ñ–∏–∑–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4D –æ–±—ä—ë–º–Ω—É—é –∑–∞–Ω—è—Ç–æ—Å—Ç—å –∫–∞–∫
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "üé®", "ru": {"title": "–ö–æ–≥–¥–∞ –∫—Ä–∞—Å–æ—Ç–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ü–µ–Ω–∑—É—Ä–æ–π: –±–æ—Ä—å–±–∞ –∑–∞ –∞–≤—Ç–æ–Ω–æ–º–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ reward models –∏–º–µ—é—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ —Å—Ç–æ
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –ª—é–¥–∏ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º –∏ —ç–≤–æ–ª—é—Ü–∏–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–∞—Ä–∞–¥–∏–≥–º–∞ Interactive Intelligence –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ª—é–¥–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –≤—ã—Ä–∞–∂–µ–Ω–∏—é, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–º—É
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#open_source", "#architecture"], "emoji": "üé®", "ru": {"title": "–ë–æ–≥–∞—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –≤ –≤—ã—Å–æ–∫–æ–º–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RecTok —Ä–µ—à–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –ª–∞—Ç–µ–Ω
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#3d"], "emoji": "‚ö°", "ru": {"title": "–°–≤—ë—Ä—Ç–∫–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ: –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ 3D –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç LitePT, –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –≤ 3D, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤—ë—Ä—Ç–∫–∏ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö 
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#optimization"], "emoji": "üß≠", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Directional Textual Inversion (DTI), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é text-to-im
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∞–ª–∏–∑–º–∞: –∫–∞–∫ AI-–≤–∏–¥–µ–æ –æ–±–º–∞–Ω—ã–≤–∞—é—Ç –¥–∞–∂–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ Video Reality Test –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ AI-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ ASMR –∫
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#multimodal", "#training", "#synthetic", "#science", "#open_source", "#benchmark", "#dataset"], "emoji": "üìä", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "START ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥–∏–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#video"], "emoji": "üé¨", "ru": {"title": "–ß–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ —Å—Ü–µ–Ω—ã –∏–∑ –¥–≤—É–º–µ—Ä–Ω–æ–≥–æ –≤–∏–¥–µ–æ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "COM4D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏
[16.12.2025 14:28] Using data from previous issue: {"categories": ["#video", "#diffusion", "#inference", "#interpretability", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "DiffusionBrowser ‚Äî —ç—Ç–æ –ª—ë–≥–∫–∏–π –¥–µ–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–∏–¥–µ—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø
[16.12.2025 14:28] Querying the API.
[16.12.2025 14:28] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A systematic study adapts diffusion distillation techniques to text-to-image generation, providing guidelines for successful implementation and deployment.  					AI-generated summary 				 Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.
[16.12.2025 14:28] Response: ```json
{
  "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∫ —Å–∏–ª—å–Ω–æ–π —É—á–∏—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ–ª–∏ FLUX.1-lite –∏ –≤—ã—è–≤–ª—è—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –æ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤—ã—Ö –º–µ—Ç–æ–∫ –∫ —Å–≤–æ–±–æ–¥–Ω–æ—Ñ–æ—Ä–º–∞—Ç–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–∞–º. –ù–∞ –æ—Å–Ω–æ–≤–µ –µ–¥–∏–Ω–æ–≥–æ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ framework –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –±—ã—Å—Ç—Ä—ã—Ö, –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —Ä–µ—Å—É—Ä—Å–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.",
  "emoji": "‚ö°",
  "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
```
[16.12.2025 14:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic study adapts diffusion distillation techniques to text-to-image generation, providing guidelines for successful implementation and deployment.  					AI-generated summary 				 Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill."

[16.12.2025 14:28] Response: ```python
["INFERENCE", "TRAINING", "ARCHITECTURE", "MULTIMODAL"]
```

**Justification:**
- **INFERENCE**: The paper focuses on distillation techniques to accelerate model deployment and enable "fast, high-fidelity, and resource-efficient" generation, which is core to inference optimization.
- **TRAINING**: The paper presents systematic study of distillation techniques and provides guidelines for training student models from teacher models.
- **ARCHITECTURE**: The paper discusses network architecture considerations and proposes adaptations of distillation methods, including architectural guidelines.
- **MULTIMODAL**: The paper addresses text-to-image generation, which combines text (language prompts) and image modalities.
[16.12.2025 14:28] Error. Failed to parse JSON from LLM. ["INFERENCE", "TRAINING", "ARCHITECTURE", "MULTIMODAL"]


**Justification:**
- **INFERENCE**: The paper focuses on distillation techniques to accelerate model deployment and enable "fast, high-fidelity, and resource-efficient" generation, which is core to inference optimization.
- **TRAINING**: The paper presents systematic study of distillation techniques and provides guidelines for training student models from teacher models.
- **ARCHITECTURE**: The paper discusses network architecture considerations and proposes adaptations of distillation methods, including architectural guidelines.
- **MULTIMODAL**: The paper addresses text-to-image generation, which combines text (language prompts) and image modalities.
[16.12.2025 14:28] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic study adapts diffusion distillation techniques to text-to-image generation, providing guidelines for successful implementation and deployment.  					AI-generated summary 				 Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill."

[16.12.2025 14:29] Response: ```python
["DIFFUSION", "OPTIMIZATION", "OPEN_SOURCE"]
```
[16.12.2025 14:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how diffusion distillation techniques can be effectively used in text-to-image (T2I) generation. It presents a systematic study that adapts these techniques to work with a powerful T2I model called FLUX.1-lite, highlighting the challenges of translating free-form language prompts into image generation. The authors provide practical guidelines on various aspects like input scaling and network architecture, along with an open-source implementation to facilitate real-world applications.","title":"Accelerating Text-to-Image Generation with Diffusion Distillation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how diffusion distillation techniques can be effectively used in text-to-image (T2I) generation. It presents a systematic study that adapts these techniques to work with a powerful T2I model called FLUX.1-lite, highlighting the challenges of translating free-form language prompts into image generation. The authors provide practical guidelines on various aspects like input scaling and network architecture, along with an open-source implementation to facilitate real-world applications.', title='Accelerating Text-to-Image Generation with Diffusion Distillation'))
[16.12.2025 14:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂Á≥ªÁªüÂú∞Êé¢ËÆ®‰∫ÜÊâ©Êï£Ëí∏È¶èÊäÄÊúØÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ÁöÑÂ∫îÁî®ÔºåÊèê‰æõ‰∫ÜÊàêÂäüÂÆûÊñΩÂíåÈÉ®ÁΩ≤ÁöÑÊåáÂØº„ÄÇÊàë‰ª¨È¶ñÊ¨°Â∞ÜÊúÄÂÖàËøõÁöÑËí∏È¶èÊäÄÊúØÈÄÇÈÖç‰∫éÂº∫Â§ßÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÊïôÂ∏àÊ®°ÂûãFLUX.1-liteÔºåÂπ∂ËøõË°åÊØîËæÉ„ÄÇÈÄöËøáÂ∞ÜÁé∞ÊúâÊñπÊ≥ïÊï¥ÂêàÂà∞Áªü‰∏ÄÊ°ÜÊû∂‰∏≠ÔºåÊàë‰ª¨ËØÜÂà´‰∫Ü‰ªéÁ¶ªÊï£Á±ªÂà´Ê†áÁ≠æÂà∞Ëá™Áî±ÂΩ¢ÂºèËØ≠Ë®ÄÊèêÁ§∫Êó∂ÊâÄÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÈöúÁ¢ç„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫Âú®ÂÆûÈôÖÊñáÊú¨Âà∞ÂõæÂÉèÂ∫îÁî®‰∏≠ÈÉ®ÁΩ≤Âø´ÈÄü„ÄÅÈ´ò‰øùÁúüÂíåËµÑÊ∫êÈ´òÊïàÁöÑÊâ©Êï£ÁîüÊàêÂô®Â•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇ","title":"Êâ©Êï£Ëí∏È¶èÔºöÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Á≥ªÁªüÂú∞Êé¢ËÆ®‰∫ÜÊâ©Êï£Ëí∏È¶èÊäÄÊúØÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ÁöÑÂ∫îÁî®ÔºåÊèê‰æõ‰∫ÜÊàêÂäüÂÆûÊñΩÂíåÈÉ®ÁΩ≤ÁöÑÊåáÂØº„ÄÇÊàë‰ª¨È¶ñÊ¨°Â∞ÜÊúÄÂÖàËøõÁöÑËí∏È¶èÊäÄÊúØÈÄÇÈÖç‰∫éÂº∫Â§ßÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÊïôÂ∏àÊ®°ÂûãFLUX.1-liteÔºåÂπ∂ËøõË°åÊØîËæÉ„ÄÇÈÄöËøáÂ∞ÜÁé∞ÊúâÊñπÊ≥ïÊï¥ÂêàÂà∞Áªü‰∏ÄÊ°ÜÊû∂‰∏≠ÔºåÊàë‰ª¨ËØÜÂà´‰∫Ü‰ªéÁ¶ªÊï£Á±ªÂà´Ê†áÁ≠æÂà∞Ëá™Áî±ÂΩ¢ÂºèËØ≠Ë®ÄÊèêÁ§∫Êó∂ÊâÄÈù¢‰∏¥ÁöÑÂÖ≥ÈîÆÈöúÁ¢ç„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫Âú®ÂÆûÈôÖÊñáÊú¨Âà∞ÂõæÂÉèÂ∫îÁî®‰∏≠ÈÉ®ÁΩ≤Âø´ÈÄü„ÄÅÈ´ò‰øùÁúüÂíåËµÑÊ∫êÈ´òÊïàÁöÑÊâ©Êï£ÁîüÊàêÂô®Â•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇ', title='Êâ©Êï£Ëí∏È¶èÔºöÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥'))
[16.12.2025 14:29] Querying the API.
[16.12.2025 14:29] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Flowception, a non-autoregressive video generation framework, interleaves discrete frame insertions with continuous denoising, improving efficiency and performance over existing methods.  					AI-generated summary 				 We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.
[16.12.2025 14:29] Response: ```json
{
  "desc": "Flowception ‚Äî —ç—Ç–æ –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —á–µ—Ä–µ–¥—É–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—Å—Ç–∞–≤–∫–∏ –∫–∞–¥—Ä–æ–≤ —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–µ–π. –ú–µ—Ç–æ–¥ –∏–∑–±–µ–≥–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ–≥–æ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –±–ª–∞–≥–æ–¥–∞—Ä—è –º–µ—Ö–∞–Ω–∏–∑–º—É –≤—Å—Ç–∞–≤–∫–∏ –∫–∞–¥—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–∂–∏–º–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ—Ç–æ–∫–∞–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –ø–æ–¥—Ö–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤ —Ç—Ä–∏ —Ä–∞–∑–∞ –∏ –ª—É—á—à–µ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å –ª–æ–∫–∞–ª—å–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º. –ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á: –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤.",
  "emoji": "üé¨",
  "title": "–ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ –≤—Å—Ç–∞–≤–∫–∏ –∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ"
}
```
[16.12.2025 14:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Flowception, a non-autoregressive video generation framework, interleaves discrete frame insertions with continuous denoising, improving efficiency and performance over existing methods.  					AI-generated summary 				 We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation."

[16.12.2025 14:29] Response: ```python
["VIDEO", "ARCHITECTURE", "TRAINING"]
```
[16.12.2025 14:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Flowception, a non-autoregressive video generation framework, interleaves discrete frame insertions with continuous denoising, improving efficiency and performance over existing methods.  					AI-generated summary 				 We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation."

[16.12.2025 14:29] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```

**Justification:**

1. **DIFFUSION**: The paper explicitly discusses a framework that uses "continuous frame denoising" as a core component, which is a fundamental characteristic of diffusion-based generative models. The interleaving of discrete operations with continuous denoising is a diffusion-based approach.

2. **OPTIMIZATION**: The paper emphasizes efficiency improvements, stating it "reduces FLOPs for training three-fold" and discusses being "more amenable to local attention variants," which are optimization-related contributions to training efficiency and computational performance.
[16.12.2025 14:29] Error. Failed to parse JSON from LLM. ["DIFFUSION", "OPTIMIZATION"]


**Justification:**

1. **DIFFUSION**: The paper explicitly discusses a framework that uses "continuous frame denoising" as a core component, which is a fundamental characteristic of diffusion-based generative models. The interleaving of discrete operations with continuous denoising is a diffusion-based approach.

2. **OPTIMIZATION**: The paper emphasizes efficiency improvements, stating it "reduces FLOPs for training three-fold" and discusses being "more amenable to local attention variants," which are optimization-related contributions to training efficiency and computational performance.
[16.12.2025 14:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Flowception is a new framework for generating videos that does not rely on traditional autoregressive methods. It combines two techniques: inserting discrete frames and continuously denoising them, which helps improve both efficiency and performance. This approach reduces the computational load during training and minimizes errors that can accumulate over time. Additionally, Flowception can adaptively learn the length of videos while generating their content, making it versatile for various video-related tasks.","title":"Flowception: Efficient Video Generation with Frame Insertion and Denoising"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Flowception is a new framework for generating videos that does not rely on traditional autoregressive methods. It combines two techniques: inserting discrete frames and continuously denoising them, which helps improve both efficiency and performance. This approach reduces the computational load during training and minimizes errors that can accumulate over time. Additionally, Flowception can adaptively learn the length of videos while generating their content, making it versatile for various video-related tasks.', title='Flowception: Efficient Video Generation with Frame Insertion and Denoising'))
[16.12.2025 14:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FlowceptionÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÈùûËá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåÂÆÉÈÄöËøáÂ∞ÜÁ¶ªÊï£Â∏ßÊèíÂÖ•‰∏éËøûÁª≠ÂéªÂô™Áõ∏ÁªìÂêàÔºåÊèêÈ´ò‰∫ÜÁîüÊàêÊïàÁéáÂíåÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂ≠¶‰π†‰∫Ü‰∏ÄÊù°Ê¶ÇÁéáË∑ØÂæÑÔºåÂú®ÈááÊ†∑ËøáÁ®ã‰∏≠ÈÄöËøáÂ∏ßÊèíÂÖ•Êú∫Âà∂ÊúâÊïàÂ§ÑÁêÜÈïøÊúü‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÂáèËΩª‰∫ÜËØØÂ∑ÆÁ¥ØÁßØÈóÆÈ¢ò„ÄÇ‰∏éÂÖ®Â∫èÂàóÊµÅÁõ∏ÊØîÔºåFlowceptionÂú®ËÆ≠ÁªÉÊó∂ÂáèÂ∞ë‰∫Ü‰∏âÂÄçÁöÑËÆ°ÁÆóÈáèÔºåÂπ∂‰∏îÊõ¥ÈÄÇÂêàÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÂèò‰ΩìÔºåÂêåÊó∂ËÉΩÂ§ü‰∏éÂÜÖÂÆπÂÖ±ÂêåÂ≠¶‰π†ËßÜÈ¢ëÁöÑÈïøÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlowceptionÂú®FVDÂíåVBenchÊåáÊ†á‰∏ä‰ºò‰∫éËá™ÂõûÂΩíÂíåÂÖ®Â∫èÂàóÂü∫Á∫øÔºå‰∏îÂú®ÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÂíåËßÜÈ¢ëÊèíÂÄºÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ","title":"FlowceptionÔºöÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàêÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FlowceptionÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÈùûËá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂ÔºåÂÆÉÈÄöËøáÂ∞ÜÁ¶ªÊï£Â∏ßÊèíÂÖ•‰∏éËøûÁª≠ÂéªÂô™Áõ∏ÁªìÂêàÔºåÊèêÈ´ò‰∫ÜÁîüÊàêÊïàÁéáÂíåÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂ≠¶‰π†‰∫Ü‰∏ÄÊù°Ê¶ÇÁéáË∑ØÂæÑÔºåÂú®ÈááÊ†∑ËøáÁ®ã‰∏≠ÈÄöËøáÂ∏ßÊèíÂÖ•Êú∫Âà∂ÊúâÊïàÂ§ÑÁêÜÈïøÊúü‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÂáèËΩª‰∫ÜËØØÂ∑ÆÁ¥ØÁßØÈóÆÈ¢ò„ÄÇ‰∏éÂÖ®Â∫èÂàóÊµÅÁõ∏ÊØîÔºåFlowceptionÂú®ËÆ≠ÁªÉÊó∂ÂáèÂ∞ë‰∫Ü‰∏âÂÄçÁöÑËÆ°ÁÆóÈáèÔºåÂπ∂‰∏îÊõ¥ÈÄÇÂêàÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÂèò‰ΩìÔºåÂêåÊó∂ËÉΩÂ§ü‰∏éÂÜÖÂÆπÂÖ±ÂêåÂ≠¶‰π†ËßÜÈ¢ëÁöÑÈïøÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlowceptionÂú®FVDÂíåVBenchÊåáÊ†á‰∏ä‰ºò‰∫éËá™ÂõûÂΩíÂíåÂÖ®Â∫èÂàóÂü∫Á∫øÔºå‰∏îÂú®ÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÂíåËßÜÈ¢ëÊèíÂÄºÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ', title='FlowceptionÔºöÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàêÊñ∞ÊñπÊ≥ï'))
[16.12.2025 14:29] Querying the API.
[16.12.2025 14:29] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CAPTAIN, a training-free framework, mitigates memorization in diffusion models by modifying latent features during denoising, ensuring prompt fidelity and visual quality.  					AI-generated summary 				 Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.
[16.12.2025 14:29] Response: ```json
{
  "desc": "CAPTAIN ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, –Ω–µ —Ç—Ä–µ–±—É—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —à—É–º–∞ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö, –∑–∞—Ç–µ–º –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ª–æ–∫–∞–ª–∏–∑—É—é—Ç—Å—è –æ–±–ª–∞—Å—Ç–∏ —Å –∑–∞–ø–æ–º–Ω–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π. –ù–∞–∫–æ–Ω–µ—Ü, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –Ω–µ–∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–Ω–æ—Å—è—Ç—Å—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–æ–¥–∞–≤–ª—è—è –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–æ–º–ø—Ç—É –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.",
  "emoji": "üîê",
  "title": "–ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
}
```
[16.12.2025 14:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CAPTAIN, a training-free framework, mitigates memorization in diffusion models by modifying latent features during denoising, ensuring prompt fidelity and visual quality.  					AI-generated summary 				 Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt."

[16.12.2025 14:29] Response: ```python
["INFERENCE"]
```
[16.12.2025 14:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CAPTAIN, a training-free framework, mitigates memorization in diffusion models by modifying latent features during denoising, ensuring prompt fidelity and visual quality.  					AI-generated summary 				 Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt."

[16.12.2025 14:29] Response: ```python
["DIFFUSION", "SECURITY", "HALLUCINATIONS"]
```

**Justification:**

1. **DIFFUSION**: The paper explicitly focuses on diffusion models and proposes CAPTAIN, a framework for mitigating memorization in diffusion-based generative models.

2. **SECURITY**: The paper addresses privacy and copyright concerns related to diffusion models unintentionally reproducing training examples, which are security/privacy issues.

3. **HALLUCINATIONS**: While not using the exact term "hallucinations," the paper addresses memorization in diffusion models‚Äîthe unintended reproduction of training examples‚Äîwhich is conceptually related to hallucinations as unwanted model behaviors that need mitigation.
[16.12.2025 14:29] Error. Failed to parse JSON from LLM. ["DIFFUSION", "SECURITY", "HALLUCINATIONS"]


**Justification:**

1. **DIFFUSION**: The paper explicitly focuses on diffusion models and proposes CAPTAIN, a framework for mitigating memorization in diffusion-based generative models.

2. **SECURITY**: The paper addresses privacy and copyright concerns related to diffusion models unintentionally reproducing training examples, which are security/privacy issues.

3. **HALLUCINATIONS**: While not using the exact term "hallucinations," the paper addresses memorization in diffusion models‚Äîthe unintended reproduction of training examples‚Äîwhich is conceptually related to hallucinations as unwanted model behaviors that need mitigation.
[16.12.2025 14:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces CAPTAIN, a novel framework designed to reduce memorization in diffusion models without requiring additional training. It addresses the issue of these models unintentionally reproducing training data, which raises privacy and copyright concerns. CAPTAIN modifies latent features during the denoising process by using frequency-based noise initialization and strategically injecting features from non-memorized images. This approach effectively suppresses memorization while ensuring that the generated outputs remain aligned with the original prompts and maintain high visual quality.","title":"CAPTAIN: Combatting Memorization in Diffusion Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces CAPTAIN, a novel framework designed to reduce memorization in diffusion models without requiring additional training. It addresses the issue of these models unintentionally reproducing training data, which raises privacy and copyright concerns. CAPTAIN modifies latent features during the denoising process by using frequency-based noise initialization and strategically injecting features from non-memorized images. This approach effectively suppresses memorization while ensuring that the generated outputs remain aligned with the original prompts and maintain high visual quality.', title='CAPTAIN: Combatting Memorization in Diffusion Models'))
[16.12.2025 14:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CAPTAINÊòØ‰∏Ä‰∏™Êó†ËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂú®ÂéªÂô™ËøáÁ®ã‰∏≠‰øÆÊîπÊΩúÂú®ÁâπÂæÅÊù•ÂáèËΩªÊâ©Êï£Ê®°Âûã‰∏≠ÁöÑËÆ∞ÂøÜÂåñÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈ¢ëÁéáÂü∫Á°ÄÁöÑÂô™Â£∞ÂàùÂßãÂåñÔºåÂáèÂ∞ë‰∫ÜÂú®ÂéªÂô™Êó©ÊúüÂ§çÂà∂ËÆ∞ÂøÜÊ®°ÂºèÁöÑÂÄæÂêë„ÄÇCAPTAINËøòËØÜÂà´ÊúÄ‰Ω≥ÁöÑÂéªÂô™Êó∂Èó¥Ê≠•Ôºå‰ª•‰æøÂú®Â±ÄÈÉ®ÂåñÁöÑÊΩúÂú®Âå∫ÂüüÊ≥®ÂÖ•ËØ≠‰πâÂØπÈΩêÁöÑÁâπÂæÅ„ÄÇÂÆûÈ™åË°®ÊòéÔºåCAPTAINÂú®ÂáèÂ∞ëËÆ∞ÂøÜÂåñÊñπÈù¢ÊòæËëó‰ºò‰∫éÂü∫‰∫éCFGÁöÑÂü∫Á∫øÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éÊù°‰ª∂ÊèêÁ§∫ÁöÑÂº∫ÂØπÈΩê„ÄÇ","title":"CAPTAINÔºöÂáèËΩªÊâ©Êï£Ê®°ÂûãËÆ∞ÂøÜÂåñÁöÑÂàõÊñ∞Ê°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CAPTAINÊòØ‰∏Ä‰∏™Êó†ËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂú®ÂéªÂô™ËøáÁ®ã‰∏≠‰øÆÊîπÊΩúÂú®ÁâπÂæÅÊù•ÂáèËΩªÊâ©Êï£Ê®°Âûã‰∏≠ÁöÑËÆ∞ÂøÜÂåñÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈ¢ëÁéáÂü∫Á°ÄÁöÑÂô™Â£∞ÂàùÂßãÂåñÔºåÂáèÂ∞ë‰∫ÜÂú®ÂéªÂô™Êó©ÊúüÂ§çÂà∂ËÆ∞ÂøÜÊ®°ÂºèÁöÑÂÄæÂêë„ÄÇCAPTAINËøòËØÜÂà´ÊúÄ‰Ω≥ÁöÑÂéªÂô™Êó∂Èó¥Ê≠•Ôºå‰ª•‰æøÂú®Â±ÄÈÉ®ÂåñÁöÑÊΩúÂú®Âå∫ÂüüÊ≥®ÂÖ•ËØ≠‰πâÂØπÈΩêÁöÑÁâπÂæÅ„ÄÇÂÆûÈ™åË°®ÊòéÔºåCAPTAINÂú®ÂáèÂ∞ëËÆ∞ÂøÜÂåñÊñπÈù¢ÊòæËëó‰ºò‰∫éÂü∫‰∫éCFGÁöÑÂü∫Á∫øÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰∏éÊù°‰ª∂ÊèêÁ§∫ÁöÑÂº∫ÂØπÈΩê„ÄÇ', title='CAPTAINÔºöÂáèËΩªÊâ©Êï£Ê®°ÂûãËÆ∞ÂøÜÂåñÁöÑÂàõÊñ∞Ê°ÜÊû∂'))
[16.12.2025 14:29] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#survey", "#dataset", "#low_resource", "#multilingual"], "emoji": "üá´üáÆ", "ru": {"title": "–ù–∞–¥–µ–∂–Ω—ã–π —Ñ–∏–Ω—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ FIN-bench-v2 ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞
[16.12.2025 14:29] Using data from previous issue: {"categories": ["#reasoning", "#interpretability"], "emoji": "üß†", "ru": {"title": "–¢–æ–∫–µ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∞ –Ω–µ —Ç–µ–∫—Å—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–∞—è —Ä–∞–º–∫–∞ State over Tokens (SoT), –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç
[16.12.2025 14:29] Using data from previous issue: {"categories": ["#audio", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –∑–≤—É–∫–∞ –¥–ª—è —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ flow matching", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ flow matching –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –∑
[16.12.2025 14:29] Using data from previous issue: {"categories": [], "emoji": "üêü", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø—Ä–æ—Ç–∏–≤ —Å–≤–µ—Ä—Ç–æ–∫: Swin-T –ø–æ–±–µ–∂–¥–∞–µ—Ç –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–±", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ AutoFish –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Swin-T. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç 
[16.12.2025 14:29] Renaming data file.
[16.12.2025 14:29] Renaming previous data. hf_papers.json to ./d/2025-12-16.json
[16.12.2025 14:29] Saving new data file.
[16.12.2025 14:29] Generating page.
[16.12.2025 14:29] Renaming previous page.
[16.12.2025 14:29] Renaming previous data. index.html to ./d/2025-12-16.html
[16.12.2025 14:29] Writing result.
[16.12.2025 14:29] Renaming log file.
[16.12.2025 14:29] Renaming previous data. log.txt to ./logs/2025-12-16_last_log.txt
