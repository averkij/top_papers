[16.12.2025 05:26] Read previous papers.
[16.12.2025 05:26] Generating top page (month).
[16.12.2025 05:26] Writing top page (month).
[16.12.2025 06:36] Read previous papers.
[16.12.2025 06:36] Get feed.
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13604
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13564
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13586
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12602
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13313
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12967
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12730
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09636
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10071
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13080
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11995
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13250
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11891
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13689
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13592
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13281
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11883
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.07186
[16.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13674
[16.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.13421
[16.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.13690
[16.12.2025 06:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.12.2025 06:36] No deleted papers detected.
[16.12.2025 06:36] Downloading and parsing papers (pdf, html). Total: 21.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13604.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13604.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13604.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13564.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13564.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13564.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13586.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13586.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13586.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.12602.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.12602.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.12602.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13313.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13313.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13313.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.12967.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.12967.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.12967.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.12730.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.12730.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.12730.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.09636.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.09636.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.09636.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.10071.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.10071.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.10071.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13080.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13080.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13080.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.11995.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.11995.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.11995.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13250.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13250.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13250.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.11891.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.11891.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.11891.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13689.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13689.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13689.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13592.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13592.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13592.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13281.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13281.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13281.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.11883.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.11883.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.11883.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.07186.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.07186.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.07186.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13674.
[16.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13674.json), skip PDF parsing.
[16.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13674.json), skip HTML parsing.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13421.
[16.12.2025 06:36] Downloading paper 2512.13421 from https://arxiv.org/pdf/2512.13421v1...
[16.12.2025 06:36] Extracting affiliations from text.
[16.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RecTok: Reconstruction Distillation along Rectified Flow Qingyu Shi1,3, Size Wu2, Jinbin Bai1, Kaidong Yu3, Yujing Wang1, Yunhai Tong1, Xiangtai Li2, Xuelong Li3 1Peking University 2Nanyang Technological University 3TeleAI 5 2 0 D 5 1 ] . [ 1 1 2 4 3 1 . 2 1 5 2 : r Figure 1. (a) presents the core insights of our approach. Unlike previous works, we enhance semantic information along the forward pass of the rectified flow via reconstruction distillation. Fig. (b) shows that increasing the latent space dimension consistently improves the generation performance of RecTok, indicating that the dimensional bottleneck no longer constrains the semantic information encoded in the latent features. (c) compares the gFID convergence across training epochs, where our method converges 7.75 faster than prior works and achieves final gFID of 1.34 without classifier-free guidance, the state-of-the-art gFID performance to date. "
[16.12.2025 06:36] Response: ```python
["Peking University", "Nanyang Technological University", "TeleAI"]
```
[16.12.2025 06:36] Deleting PDF ./assets/pdf/2512.13421.pdf.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13690.
[16.12.2025 06:36] Downloading paper 2512.13690 from https://arxiv.org/pdf/2512.13690v1...
[16.12.2025 06:36] Extracting affiliations from text.
[16.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders Susung Hong1, Chongjian Ge2 1 University of Washington Zhifei Zhang2 Jui-Hsien Wang2 2 Adobe Research https://susunghong.github.io/DiffusionBrowser 5 2 0 2 5 1 ] . [ 1 0 9 6 3 1 . 2 1 5 2 : r Figure 1. DiffusionBrowser is plug-and-play model that enables interactive previews anywhere in the multi-step diffusion process, which allows users to make decisions about whether to continue denoising or modify prompts. In addition, DiffusionBrowser provides multiple variation generation mechanisms that guide users to explore the creative generation space in tree-like structure. Our novel, efficient multibranch decoder architecture preserves the full capacity of the base model, and can generate rich multi-modal previews for each timestep in < 1s, adding negligible overhead at inference time. "
[16.12.2025 06:36] Response: ```python
[
    "University of Washington",
    "Adobe Research"
]
```
[16.12.2025 06:36] Deleting PDF ./assets/pdf/2512.13690.pdf.
[16.12.2025 06:36] Success.
[16.12.2025 06:36] Enriching papers with extra data.
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 0. LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.  					AI-generated summary 				 Building video world models upon pretrained video generation systems represents an im...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 1. This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.  					AI-generated summary 				 Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As r...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 2. ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.  					AI-generated summary 				 Autoregressive models (ARMs) are hindered by slow...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 3. Error-Free Linear Attention (EFLA) is a stable, parallelizable, and theoretically sound linear-time attention mechanism that outperforms DeltaNet in language modeling and downstream tasks.  					AI-generated summary 				 Linear-time attention and State Space Models (SSMs) promise to solve the quadra...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 4. KlingAvatar 2.0 addresses inefficiencies in generating long-duration, high-resolution videos by using a spatio-temporal cascade framework with a Co-Reasoning Director and Negative Director for improved multimodal instruction alignment.  					AI-generated summary 				 Avatar video generation models h...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 5. QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.  					AI-generated summary 				 We introduce QwenLong-L1.5, a model that achieves superior lon...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 6. NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.  					AI-generated summary 				 Recent advances in coding agents suggest rapid progress toward autonomous softw...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 7. MentraSuite, a unified framework, advances reliable mental health reasoning using Mindora, a post-trained model with hybrid SFT-RL, evaluated via MentraBench, a benchmark assessing task performance and reasoning quality.  					AI-generated summary 				 Mental health disorders affect hundreds of mill...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 8. A solution for the 2025 BEHAVIOR Challenge in everyday household tasks using pre-training and post-training techniques substantially outperforms other submissions.  					AI-generated summary 				 The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks b...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 9. A Spatial-Aware VLA Pretraining paradigm improves 3D spatial understanding in robots by aligning 2D visual inputs with 3D actions using dual-encoder architecture with a 3D visual encoder.  					AI-generated summary 				 Vision-Language-Action (VLA) models provide a promising paradigm for robot learn...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 10. The V-REX evaluation suite assesses vision-language models' multi-step reasoning and exploration capabilities through a Chain-of-Questions framework, revealing their strengths and weaknesses in planning and following.  					AI-generated summary 				 While many vision-language models (VLMs) are devel...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 11. VG-AVS, a task and framework fine-tunes VLMs to select the most informative next viewpoint for visual question answering, enhancing performance and generalization.  					AI-generated summary 				 Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vi...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 12. AEGIS, a Vision-Language-Safe Action architecture with a plug-and-play safety constraint layer using control barrier functions, enhances safety and performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models have demonstrated remarkable capabilities...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 13. A new 3D point cloud backbone model, LitePT, uses convolutions for early stages and attention for deeper layers, incorporating PointROPE for positional encoding, achieving efficient performance with fewer resources.  					AI-generated summary 				 Modern neural architectures for 3D point cloud proce...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 14. Diffusion Preview uses ConsistencySolver, a high-order trainable solver, to improve quality and consistency in low-step image generation, enhancing interactive user experiences.  					AI-generated summary 				 The slow inference process of image diffusion models significantly degrades interactive us...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 15. The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  					AI-generated summary 				 Recent advances ...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 16. State-of-the-art image generation and reward models exhibit bias towards conventional aesthetics, often failing to produce anti-aesthetic images as requested, thus compromising user autonomy and aesthetic diversity.  					AI-generated summary 				 Over-aligning image generation models to a generaliz...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 17. START enhances multimodal large language models by integrating spatial and textual learning through chart-element grounding and chart-to-code generation, improving chart understanding and performance across benchmarks.  					AI-generated summary 				 Chart understanding is crucial for deploying mult...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 18. Interactive Intelligence, realized through Mio framework, enables advanced digital humans with personality, adaptive interactions, and self-evolution, surpassing current benchmarks.  					AI-generated summary 				 We introduce Interactive Intelligence, a novel paradigm of digital human that is capab...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 19. RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.  					AI-generated summary 				 Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space...
[16.12.2025 06:36] ********************************************************************************
[16.12.2025 06:36] Abstract 20. DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.  					AI-generated summary 				 Video diffusion models have revolutionized generative video synthesis, but they are imprecise, sl...
[16.12.2025 06:36] Read previous papers.
[16.12.2025 06:36] Generating reviews via LLM API.
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#video", "#optimization", "#training", "#benchmark", "#architecture"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð¢Ñ€Ð¸ ÑÑ‚Ð°Ð¿Ð° Ðº Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð²Ð¸Ð´ÐµÐ¾-Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¸Ñ€Ð°", "desc": "LongVie 2 â€” ÑÑ‚Ð¾ Ð°Ð²Ñ‚Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¼Ð¸Ñ€Ð° Ð²Ð¸Ð´ÐµÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð² Ñ‚Ñ€Ð¸ ÑÑ‚Ð°Ð¿Ð° Ð´Ð»Ñ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ Ñ‚Ñ€Ñ‘Ñ… ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ²Ð¾Ð¹
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#long_context", "#rl", "#agents", "#benchmark", "#rag"], "emoji": "ðŸ§ ", "ru": {"title": "ÐŸÐ°Ð¼ÑÑ‚ÑŒ ÐºÐ°Ðº Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ð¾Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð² Ð°Ð³ÐµÐ½Ñ‚Ð°Ñ…, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð½Ð° Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#inference", "#optimization", "#benchmark", "#diffusion", "#architecture"], "emoji": "âš¡", "ru": {"title": "ÐžÑ‚ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ðº ÑÐ»Ð¾Ñ‚Ð°Ð¼: Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ñ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾ÐºÐ¾Ð»ÐµÐ½Ð¸Ñ", "desc": "ReFusion â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¼Ð°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ñ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ²Ð°ÐµÑ‚ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ ÐºÐ°Ðº Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ñ‹Ñ…
[16.12.2025 06:36] Using data from previous issue: {"categories": [], "emoji": "âš¡", "ru": {"title": "Ð›Ð¸Ð½ÐµÐ¹Ð½Ð¾Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð±ÐµÐ· Ð¾ÑˆÐ¸Ð±Ð¾Ðº: Ð¾Ñ‚ Ñ‚ÐµÐ¾Ñ€Ð¸Ð¸ Ðº Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ Ð»Ð¸Ð½ÐµÐ¹Ð½Ð¾Ð³Ð¾ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ (EFLA), ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ ÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð¾Ð², ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð¿Ð¾Ð»Ð½ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÑŽÑ‚ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ðº
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#video", "#multimodal", "#story_generation", "#alignment", "#architecture"], "emoji": "ðŸŽ¬", "ru": {"title": "ÐšÐ°ÑÐºÐ°Ð´Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ñ€ÐµÐ·Ð¾Ð»ÑŽÑ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð²Ð¸Ð´ÐµÐ¾ Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¼ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸ÐµÐ¼", "desc": "KlingAvatar 2.0 Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¿Ð°Ñ†Ð¸Ð¾-Ñ‚ÐµÐ¼Ð¿Ð¾Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ°ÑÐºÐ°Ð´Ð½Ñ‹Ð¹ Ñ„
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#long_context", "#synthetic", "#rl", "#agents", "#benchmark", "#data", "#architecture"], "emoji": "ðŸ§ ", "ru": {"title": "Ð‘ÐµÑÐºÐ¾Ð½ÐµÑ‡Ð½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð´Ð»Ñ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ", "desc": "QwenLong-L1.5 â€” ÑÑ‚Ð¾ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ð²Ñ‹Ð´Ð°ÑŽÑ‰Ð¸Ñ…ÑÑ
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#agents", "#reasoning", "#plp"], "emoji": "ðŸ—ï¸", "ru": {"title": "Ð”Ð¾Ð»Ð³Ð¾ÑÑ‚Ð°Ð¿Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ ÐºÐ°Ðº Ð³Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²-Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¾Ð²", "desc": "NL2Repo Bench â€” ÑÑ‚Ð¾ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²-ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸ÐºÐ¾Ð² Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð»Ð½Ñ‹Ðµ Pyth
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rlhf", "#hallucinations", "#healthcare", "#science", "#training", "#reasoning", "#dataset"], "emoji": "ðŸ§ ", "ru": {"title": "ÐÐ°Ð´Ñ‘Ð¶Ð½Ñ‹Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð¿ÑÐ¸Ñ…Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ Ñ‡ÐµÑ€ÐµÐ· Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#training", "#benchmark", "#robotics"], "emoji": "ðŸ¤–", "ru": {"title": "ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð´Ð¾Ð¼Ð°ÑˆÐ½Ð¸Ñ… Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ð²: Ð¾Ñ‚ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ðº Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼Ñƒ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸ÑŽ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð´Ð»Ñ BEHAVIOR Challenge 2025, Ð³Ð´Ðµ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð°Ð³ÐµÐ½Ñ‚Ñ‹ Ñ€ÐµÑˆÐ°ÑŽÑ‚ Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ðµ Ð·Ð°
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#multimodal", "#training", "#architecture", "#3d", "#robotics"], "emoji": "ðŸ¤–", "ru": {"title": "ÐœÐ¾ÑÑ‚ Ð¼ÐµÐ¶Ð´Ñƒ Ð´Ð²ÑƒÐ¼ÐµÑ€Ð½Ñ‹Ð¼ Ð²Ð¸Ð´ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ñ‚Ñ€Ñ‘Ñ…Ð¼ÐµÑ€Ð½Ñ‹Ð¼ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÐµÐ¼ Ð² Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ñ‚ÐµÑ…Ð½Ð¸ÐºÐµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Ð½Ð¾Ð²ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼Ñƒ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Vision-Language-Action Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ñ‚ÐµÑ…Ð½Ð¸Ðº
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#interpretability", "#cv", "#benchmark", "#dataset"], "emoji": "ðŸ”", "ru": {"title": "ÐžÑ†ÐµÐ½ÐºÐ° Ð¼Ð½Ð¾Ð³Ð¾ÑˆÐ°Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð²", "desc": "V-REX â€” ÑÑ‚Ð¾ Ð½Ð°Ð±Ð¾Ñ€ Ñ‚ÐµÑÑ‚Ð¾Ð² Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ðº Ð¼Ð½Ð¾Ð³Ð¾ÑˆÐ°Ð³Ð¾Ð²Ð¾Ð¼Ñƒ
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#multimodal", "#training", "#rl", "#agents", "#dataset", "#robotics"], "emoji": "ðŸ¤–", "ru": {"title": "ÐÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ Ñ€Ð°ÐºÑƒÑ€ÑÐ° Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð³Ð¾", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ VG-AVS â€” Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð¸ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð·Ñ€ÐµÐ½Ð¸Ñ Ð¸ ÑÐ·Ñ‹ÐºÐ° Ð²Ñ‹Ð±Ð¸Ñ€Ð°Ñ‚ÑŒ 
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#architecture", "#benchmark", "#cv", "#robotics"], "emoji": "ðŸ¤–", "ru": {"title": "Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ñ€Ð¾Ð±Ð¾Ñ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ð±Ð°Ñ€ÑŒÐµÑ€Ð½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ", "desc": "Ð Ð°Ð±Ð¾Ñ‚Ð° Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ AEGIS â€” Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Vision-Language-Safe Action, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ 
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#3d"], "emoji": "âš¡", "ru": {"title": "Ð¡Ð²Ñ‘Ñ€Ñ‚ÐºÐ¸ Ð¸ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ: Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¾Ñ‡ÐµÑ‚Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ 3D Ð¾Ð±Ð»Ð°ÐºÐ¾Ð² Ñ‚Ð¾Ñ‡ÐµÐº", "desc": "ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ LitePT, Ð½Ð¾Ð²ÑƒÑŽ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¾Ð±Ð»Ð°ÐºÐ¾Ð² Ñ‚Ð¾Ñ‡ÐµÐº Ð² 3D, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑÐ²Ñ‘Ñ€Ñ‚ÐºÐ¸ Ð½Ð° Ñ€Ð°Ð½Ð½Ð¸Ñ… 
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "âš¡", "ru": {"title": "Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ñ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸ÐµÐ¹ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°: ÐºÐ¾Ð½ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ Ñ€ÐµÑˆÐ°Ñ‚ÐµÐ»ÑŒ Ð´Ð»Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Diffusion Preview Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð´Ð¸Ñ„Ñ„
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#multimodal"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð“Ñ€Ð°Ð½Ð¸Ñ†Ð° Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¼Ð°: ÐºÐ°Ðº AI-Ð²Ð¸Ð´ÐµÐ¾ Ð¾Ð±Ð¼Ð°Ð½Ñ‹Ð²Ð°ÑŽÑ‚ Ð´Ð°Ð¶Ðµ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Video Reality Test Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ AI-Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð°ÑƒÐ´Ð¸Ð¾ Ð½Ð° Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ ASMR Ðº
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "ðŸŽ¨", "ru": {"title": "ÐšÐ¾Ð³Ð´Ð° ÐºÑ€Ð°ÑÐ¾Ñ‚Ð° ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ Ñ†ÐµÐ½Ð·ÑƒÑ€Ð¾Ð¹: Ð±Ð¾Ñ€ÑŒÐ±Ð° Ð·Ð° Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð¸ÑŽ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ reward models Ð¸Ð¼ÐµÑŽÑ‚ Ð²ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ð¾Ðµ ÑÐ¼ÐµÑ‰ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ð¾
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#multimodal", "#training", "#synthetic", "#science", "#open_source", "#benchmark", "#dataset"], "emoji": "ðŸ“Š", "ru": {"title": "ÐŸÑ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸", "desc": "START â€” ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ð¸ÐºÐ° Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ…
[16.12.2025 06:36] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#architecture"], "emoji": "ðŸ¤–", "ru": {"title": "Ð˜Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ñ‹Ðµ Ð»ÑŽÐ´Ð¸ Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÐµÐ¼ Ð¸ ÑÐ²Ð¾Ð»ÑŽÑ†Ð¸ÐµÐ¹", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼Ð° Interactive Intelligence Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ñ‹Ñ… Ð»ÑŽÐ´ÐµÐ¹, ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ñ… Ðº Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑŽ, ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¼Ñƒ
[16.12.2025 06:36] Querying the API.
[16.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.  					AI-generated summary 				 Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.
[16.12.2025 06:37] Response: ```json
{
  "desc": "RecTok Ñ€ÐµÑˆÐ°ÐµÑ‚ Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾Ñ€ÐµÑ‡Ð¸Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð° Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾Ð¼ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð´Ð²Ð° ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ð¸Ð½Ð½Ð¾Ð²Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°: Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸ÑŽ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÐ¸ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð¸ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸ÑŽ. ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð¸Ð´ÐµÑ Ð·Ð°ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ÑÑ Ð² Ð¾Ð±Ð¾Ð³Ð°Ñ‰ÐµÐ½Ð¸Ð¸ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÐ¸ Ð² ÑÐ°Ð¼Ñ‹Ñ… Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑÑ… flow matching, Ð° Ð½Ðµ Ð² Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ, Ð¿ÑƒÑ‚Ñ‘Ð¼ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð° ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð·Ñ€ÐµÐ½Ð¸Ñ. ÐœÐµÑ‚Ð¾Ð´ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¿Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ñƒ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸, Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸ Ð´Ð¸ÑÐºÑ€Ð¸Ð¼Ð¸Ð½Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ñ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸ Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð°.",
  "emoji": "ðŸŽ¨",
  "title": "Ð‘Ð¾Ð³Ð°Ñ‚Ð°Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÐ° Ð² Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¼ÐµÑ€Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð°Ñ… Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"
}
```
[16.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.  					AI-generated summary 				 Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io."

[16.12.2025 06:37] Response: ```python
["CV", "ARCHITECTURE", "TRAINING"]
```
[16.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.  					AI-generated summary 				 Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io."

[16.12.2025 06:37] Response: ```python
["DIFFUSION", "OPEN_SOURCE"]
```
[16.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RecTok is a novel approach that enhances diffusion models by improving the semantics of forward flow and reconstruction processes. It addresses the trade-off between high-dimensional latent spaces and generation quality, which has limited the performance of existing methods. By introducing flow semantic distillation and reconstruction-alignment distillation, RecTok enriches the training space for diffusion transformers, leading to better image reconstruction and generation quality. The method achieves state-of-the-art results on benchmark datasets while maintaining a semantically rich latent space, demonstrating consistent improvements with increased dimensionality.","title":"Enhancing Diffusion Models with RecTok: Bridging Semantics and Reconstruction"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RecTok is a novel approach that enhances diffusion models by improving the semantics of forward flow and reconstruction processes. It addresses the trade-off between high-dimensional latent spaces and generation quality, which has limited the performance of existing methods. By introducing flow semantic distillation and reconstruction-alignment distillation, RecTok enriches the training space for diffusion transformers, leading to better image reconstruction and generation quality. The method achieves state-of-the-art results on benchmark datasets while maintaining a semantically rich latent space, demonstrating consistent improvements with increased dimensionality.', title='Enhancing Diffusion Models with RecTok: Bridging Semantics and Reconstruction'))
[16.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RecTokæ˜¯ä¸€ç§æ”¹è¿›æ‰©æ•£æ¨¡åž‹çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸°å¯Œå‰å‘æµçš„è¯­ä¹‰å’Œå¢žå¼ºé‡å»ºæ•ˆæžœï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æžœã€‚è¯¥æ–¹æ³•è§£å†³äº†é«˜ç»´è§†è§‰æ ‡è®°å™¨åœ¨ç”Ÿæˆè´¨é‡ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†æµè¯­ä¹‰è’¸é¦å’Œé‡å»ºå¯¹é½è’¸é¦ä¸¤é¡¹åˆ›æ–°ã€‚RecTokçš„å…³é”®åœ¨äºŽä½¿å‰å‘æµåœ¨æµåŒ¹é…ä¸­å…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰ï¼Œä»Žè€Œä½œä¸ºæ‰©æ•£å˜æ¢å™¨çš„è®­ç»ƒç©ºé—´ã€‚é€šè¿‡å¼•å…¥æŽ©ç ç‰¹å¾é‡å»ºæŸå¤±ï¼ŒRecTokåœ¨å›¾åƒé‡å»ºã€ç”Ÿæˆè´¨é‡å’Œåˆ¤åˆ«æ€§èƒ½ä¸Šå‡è¡¨çŽ°ä¼˜å¼‚ã€‚","title":"RecTokï¼šæå‡æ‰©æ•£æ¨¡åž‹çš„é«˜ç»´è§†è§‰æ ‡è®°å™¨"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RecTokæ˜¯ä¸€ç§æ”¹è¿›æ‰©æ•£æ¨¡åž‹çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸°å¯Œå‰å‘æµçš„è¯­ä¹‰å’Œå¢žå¼ºé‡å»ºæ•ˆæžœï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æžœã€‚è¯¥æ–¹æ³•è§£å†³äº†é«˜ç»´è§†è§‰æ ‡è®°å™¨åœ¨ç”Ÿæˆè´¨é‡ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†æµè¯­ä¹‰è’¸é¦å’Œé‡å»ºå¯¹é½è’¸é¦ä¸¤é¡¹åˆ›æ–°ã€‚RecTokçš„å…³é”®åœ¨äºŽä½¿å‰å‘æµåœ¨æµåŒ¹é…ä¸­å…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰ï¼Œä»Žè€Œä½œä¸ºæ‰©æ•£å˜æ¢å™¨çš„è®­ç»ƒç©ºé—´ã€‚é€šè¿‡å¼•å…¥æŽ©ç ç‰¹å¾é‡å»ºæŸå¤±ï¼ŒRecTokåœ¨å›¾åƒé‡å»ºã€ç”Ÿæˆè´¨é‡å’Œåˆ¤åˆ«æ€§èƒ½ä¸Šå‡è¡¨çŽ°ä¼˜å¼‚ã€‚', title='RecTokï¼šæå‡æ‰©æ•£æ¨¡åž‹çš„é«˜ç»´è§†è§‰æ ‡è®°å™¨'))
[16.12.2025 06:37] Querying the API.
[16.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.  					AI-generated summary 				 Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.
[16.12.2025 06:37] Response: ```json
{
  "desc": "DiffusionBrowser â€” ÑÑ‚Ð¾ Ð»Ñ‘Ð³ÐºÐ¸Ð¹ Ð´ÐµÐºÐ¾Ð´ÐµÑ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ð²Ð¸Ð´ÐµÑ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ñ€ÐµÐ²ÑŒÑŽ Ð²Ð¸Ð´ÐµÐ¾ Ð½Ð° Ð»ÑŽÐ±Ð¾Ð¼ ÑÑ‚Ð°Ð¿Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð´ÐµÐ½Ð¾Ð¹Ð·Ð¸Ð½Ð³Ð°, Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ Ð² 4 Ñ€Ð°Ð·Ð° Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. Ð¤Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ñ, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ RGB Ð¸ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ ÑÑ†ÐµÐ½Ñ‹, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð²Ð½ÐµÑˆÐ½ÐµÐ³Ð¾ Ð²Ð¸Ð´Ð° Ð¸ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ñ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¼ Ð²Ð¸Ð´ÐµÐ¾. Ð¡ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð´ÐµÐºÐ¾Ð´ÐµÑ€Ð° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÑ‚ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð½Ð° Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ñ… ÑˆÐ°Ð³Ð°Ñ… Ñ‡ÐµÑ€ÐµÐ· Ð¿ÐµÑ€ÐµÐ¸Ð½ÑŠÐµÐºÑ†Ð¸ÑŽ ÑÑ‚Ð¾Ñ…Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒÑŽ. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÐµÐµ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ñ€Ð°ÑÐºÑ€Ñ‹Ð²Ð°Ñ, ÐºÐ°Ðº ÑÑ†ÐµÐ½Ð°, Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð´ÐµÑ‚Ð°Ð»Ð¸ ÑÐ¾Ð±Ð¸Ñ€Ð°ÑŽÑ‚ÑÑ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð´ÐµÐ½Ð¾Ð¹Ð·Ð¸Ð½Ð³Ð°.",
  "emoji": "ðŸŽ¬",
  "title": "ÐŸÑ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ð¾Ðµ Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸ÐµÐ¹ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸"
}
```
[16.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.  					AI-generated summary 				 Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process."

[16.12.2025 06:37] Response: ```python
["VIDEO", "ARCHITECTURE", "INFERENCE"]
```
[16.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.  					AI-generated summary 				 Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process."

[16.12.2025 06:37] Response: ```python
["DIFFUSION", "INTERPRETABILITY"]
```
[16.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DiffusionBrowser is a new tool that improves how users interact with video generation models by allowing them to see previews during the denoising process. It works with any video diffusion model and generates previews quickly, showing both the video and important details about the scene. Users can control the generation process by guiding it at different stages, which helps them understand how the final video is created. This approach not only speeds up the preview generation but also makes the process more transparent and user-friendly.","title":"Interactive Control for Faster Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DiffusionBrowser is a new tool that improves how users interact with video generation models by allowing them to see previews during the denoising process. It works with any video diffusion model and generates previews quickly, showing both the video and important details about the scene. Users can control the generation process by guiding it at different stages, which helps them understand how the final video is created. This approach not only speeds up the preview generation but also makes the process more transparent and user-friendly.', title='Interactive Control for Faster Video Generation'))
[16.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DiffusionBrowser æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§£ç æ¡†æž¶ï¼Œå…è®¸ç”¨æˆ·åœ¨åŽ»å™ªè¿‡ç¨‹ä¸­å®žæ—¶ç”Ÿæˆè§†é¢‘é¢„è§ˆã€‚å®ƒæ”¯æŒåœ¨åŽ»å™ªçš„ä»»æ„æ—¶åˆ»ç”Ÿæˆå¤šæ¨¡æ€é¢„è§ˆï¼ŒåŒ…æ‹¬ RGB å’Œåœºæ™¯å†…åœ¨ç‰¹å¾ï¼Œé€Ÿåº¦è¶…è¿‡çœŸå®žæ—¶é—´çš„å››å€ã€‚é€šè¿‡éšæœºæ€§å†æ³¨å…¥å’Œæ¨¡æ€å¼•å¯¼ï¼Œç”¨æˆ·å¯ä»¥åœ¨ä¸­é—´å™ªå£°æ­¥éª¤ä¸­äº’åŠ¨åœ°æŽ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¨¡åž‹è¿˜æ­ç¤ºäº†åœ¨åŽ»å™ªè¿‡ç¨‹ä¸­åœºæ™¯ã€ç‰©ä½“ç­‰ç»†èŠ‚çš„ç»„æˆå’Œç»„è£…æ–¹å¼ï¼Œæå‡äº†ç”¨æˆ·ä½“éªŒã€‚","title":"å®žæ—¶è§†é¢‘é¢„è§ˆä¸ŽæŽ§åˆ¶çš„åˆ›æ–°"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DiffusionBrowser æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§£ç æ¡†æž¶ï¼Œå…è®¸ç”¨æˆ·åœ¨åŽ»å™ªè¿‡ç¨‹ä¸­å®žæ—¶ç”Ÿæˆè§†é¢‘é¢„è§ˆã€‚å®ƒæ”¯æŒåœ¨åŽ»å™ªçš„ä»»æ„æ—¶åˆ»ç”Ÿæˆå¤šæ¨¡æ€é¢„è§ˆï¼ŒåŒ…æ‹¬ RGB å’Œåœºæ™¯å†…åœ¨ç‰¹å¾ï¼Œé€Ÿåº¦è¶…è¿‡çœŸå®žæ—¶é—´çš„å››å€ã€‚é€šè¿‡éšæœºæ€§å†æ³¨å…¥å’Œæ¨¡æ€å¼•å¯¼ï¼Œç”¨æˆ·å¯ä»¥åœ¨ä¸­é—´å™ªå£°æ­¥éª¤ä¸­äº’åŠ¨åœ°æŽ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¨¡åž‹è¿˜æ­ç¤ºäº†åœ¨åŽ»å™ªè¿‡ç¨‹ä¸­åœºæ™¯ã€ç‰©ä½“ç­‰ç»†èŠ‚çš„ç»„æˆå’Œç»„è£…æ–¹å¼ï¼Œæå‡äº†ç”¨æˆ·ä½“éªŒã€‚', title='å®žæ—¶è§†é¢‘é¢„è§ˆä¸ŽæŽ§åˆ¶çš„åˆ›æ–°'))
[16.12.2025 06:37] Renaming data file.
[16.12.2025 06:37] Renaming previous data. hf_papers.json to ./d/2025-12-16.json
[16.12.2025 06:37] Saving new data file.
[16.12.2025 06:37] Generating page.
[16.12.2025 06:37] Renaming previous page.
[16.12.2025 06:37] Renaming previous data. index.html to ./d/2025-12-16.html
[16.12.2025 06:37] Writing result.
[16.12.2025 06:37] Renaming log file.
[16.12.2025 06:37] Renaming previous data. log.txt to ./logs/2025-12-16_last_log.txt
