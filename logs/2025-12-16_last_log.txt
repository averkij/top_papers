[16.12.2025 11:21] Read previous papers.
[16.12.2025 11:21] Generating top page (month).
[16.12.2025 11:21] Writing top page (month).
[16.12.2025 12:48] Read previous papers.
[16.12.2025 12:48] Get feed.
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13687
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13564
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13604
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13586
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12730
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12602
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13313
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12967
[16.12.2025 12:48] Extract page data from URL. URL: https://huggingface.co/papers/2512.13168
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09636
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10071
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13080
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11995
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13250
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12692
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11891
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13592
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11883
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13421
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12799
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13689
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13674
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13672
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13281
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.07186
[16.12.2025 12:48] Extract page data from URL. URL: https://huggingface.co/papers/2512.05272
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13690
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08405
[16.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08400
[16.12.2025 12:48] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.12.2025 12:48] No deleted papers detected.
[16.12.2025 12:48] Downloading and parsing papers (pdf, html). Total: 29.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13687.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13687.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13687.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13564.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13564.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13564.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13604.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13604.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13604.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13586.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13586.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13586.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.12730.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.12730.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.12730.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.12602.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.12602.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.12602.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13313.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13313.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13313.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.12967.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.12967.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.12967.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13168.
[16.12.2025 12:48] Downloading paper 2512.13168 from https://arxiv.org/pdf/2512.13168v1...
[16.12.2025 12:48] Extracting affiliations from text.
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 8 6 1 3 1 . 2 1 5 2 : r FINCH: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows FINCH members finworkbench@gmail.com Figure 1: Real-world F&A work is messy, spanning heterogeneous and large-scale artifacts such as spreadsheets and PDFs. Its also long-horizon and knowledge-intensive: workflows interleave multiple tasks and span diverse domains such as budgeting, trading, asset management, and operations. "
[16.12.2025 12:48] Response: ```python
[]
```
[16.12.2025 12:48] Extracting affiliations from text.
[16.12.2025 12:48] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 8 6 1 3 1 . 2 1 5 2 : r FINCH: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows FINCH members finworkbench@gmail.com Figure 1: Real-world F&A work is messy, spanning heterogeneous and large-scale artifacts such as spreadsheets and PDFs. Its also long-horizon and knowledge-intensive: workflows interleave multiple tasks and span diverse domains such as budgeting, trading, asset management, and operations.We introduce finance & accounting benchmark (FINCH) for evaluating AI agents on real-world, enterprise-grade professional workflowsinterleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. FINCH is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management. We propose workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domainexpert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work. We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents. Dataset: https://huggingface.co/FinWorkBench Full author list: Appendix A. Preprint.Frontier AI systems are increasingly transforming professional workspaces. AI-assisted tools like ChatGPT [41], Claude [2], Gemini [21], and Copilot [39] are now embedded in daily enterprise workflowshelping professionals draft documents, explore data, manipulate spreadsheets, and generate reports. These tools are particularly impactful in finance and accounting (F&A), highstakes, knowledgeand labor-intensive domain critical to every organization. However, real-world F&A work is inherently messy with substantial contextual complexity: artifacts are interconnected across heterogeneous spreadsheets, PDFs, and more, evolving through multiple versions with collaborative edits [28]; spreadsheets contain large, complex structures [17] with cross-sheet references, intricate layouts, inconsistent formatting, cryptic terms, erroneous formulas, and multimodal artifacts such as charts, images, and code. It is also long-horizon [42]: workflows demand multi-step reasoning spanning data entry, editing, retrieval, calculation, modeling, validation, reporting, and more. This raises key question: Can todays frontier AI agents actually handle the messy, long-horizon, and knowledge-intensive workflows that professionals face daily? To answer this, we introduce FINCH, an enterprise-grade F&A benchmark sourced from authentic enterprise environments. FINCH captures the intrinsic complexity of professional work through: In-the-wild enterprise sourcing: FINCH is built around authentic enterprise spreadsheets, emails, and PDFs from real-world enterprise workspacesprimarily Enron [19] (about 15,000 spreadsheet files and 500,000 emails from 150 executives and employees) and EUSES [20] (about 450 financial spreadsheets from various sources), along with securities and asset management firms, global organizations such as World Bank [4], and Canadian and British governments [11, 24]. Documents are large, cross-referenced, and messy containing rich multimodal artifacts such as tables, formulas, charts, pivots, and images. Rigorous construction process: We propose novel workflow construction pipeline grounded in real collaborative context of emails and versioned artifacts. We induce workflows from enterprise email threads and attachments, where collaborators naturally describe, discuss, and track workflows as part of their daily work. And we propose an LLM-assisted, expert-verified method to derive workflows by analyzing changes across versioned spreadsheets, surfacing the underlying goals that drive professionals work. Annotators must reason over large multi-sheet workbooks, and subtle version deltas to infer what the original analyst was trying to achieve, making the annotation process substantially more difficult than curating QA pairs over isolated tables. We compile 172 meticulously annotated, enterprise-grade workflows built on 1,710 spreadsheets, along with PDFs and other artifacts, collectively capturing the compositional, messy, knowledgeintensive, and collaborative nature of real work. Each workflow spans one or more interdependent tasksdata entry, editing, retrieval, calculation, modeling, validation, translation, visualization, and reportingmirroring how professionals actually work on artifact manipulation and creation. Figure 2: Model pass-rate comparison on FINCH workflows. Bars show overall workflow success rates for product-side agents and API-based models. Detailed settings can be found in Section 3. We evaluate spectrum of frontier AI systemsincluding Claude Sonnet 4.5, GPT 5.1, Gemini 3, Grok 4, and Qwen 3using both expert evaluation and novel automated evaluation pipeline that closely aligns with expert judgments. Our experiments reveal that even the frontier agents pass only fewer than 40% of workflows (GPT 5.1 Pro spends 16.8 minutes per workflow on average), highlighting the substantial challenges that FINCH poses for AI agents. Comprehensive case analyses further surface concrete challenges that real-world enterprise workflows pose for AI agents. 2 Figure 3: Illustration of an end-to-end predictive modeling workflow commonly performed by financial analysts. The workflow involves multiple steps, including web search, data import, crosssheet and cross-file retrieval, calculation and financial modeling, visualization, and report generation. More illustrative examp"
[16.12.2025 12:48] Mistral response. {"id": "01d4f79a7e014cb39c9b8fd53c20ca22", "created": 1765889306, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1441, "total_tokens": 1447, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[16.12.2025 12:48] Response: ```python
[]
```
[16.12.2025 12:48] Deleting PDF ./assets/pdf/2512.13168.pdf.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.09636.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.09636.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.09636.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.10071.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.10071.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.10071.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13080.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13080.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13080.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.11995.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.11995.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.11995.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13250.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13250.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13250.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.12692.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.12692.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.12692.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.11891.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.11891.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.11891.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13592.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13592.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13592.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.11883.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.11883.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.11883.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13421.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13421.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13421.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.12799.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.12799.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.12799.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13689.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13689.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13689.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13674.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13674.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13674.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13672.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13672.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13672.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13281.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13281.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13281.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.07186.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.07186.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.07186.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.05272.
[16.12.2025 12:48] Downloading paper 2512.05272 from https://arxiv.org/pdf/2512.05272v1...
[16.12.2025 12:48] Extracting affiliations from text.
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Inferring Compositional 4D Scenes without Ever Seeing One Ahmet Berke Gokmen Ajad Chhatkuli INSAIT, Sofia University St. Kliment Ohridski, Bulgaria {berke.gokmen, ajad.chhatkuli, luc.vangool, danda.paudel}@insait.ai 5 2 0 2 4 ] . [ 1 2 7 2 5 0 . 2 1 5 2 : r Figure 1. Given single video (bottom), our method reconstructs the entire 3D scene along with the individual dynamic objects (A), while maintaining spatial and temporal consistency through spatio-temporal attention mixing (C). The silhouettes (purple for human and orange for dog) correspond to the beginning of dynamic sequences. Our user study (for spatial correctness and temporal coherence) shows that the reconstructions obtained using the proposed attention mixing mechanism are clearly preferred over the baseline without mixing (B). "
[16.12.2025 12:48] Response: ```python
["INSAIT, Sofia University St. Kliment Ohridski, Bulgaria"]
```
[16.12.2025 12:48] Deleting PDF ./assets/pdf/2512.05272.pdf.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13690.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13690.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13690.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.08405.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.08405.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.08405.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.08400.
[16.12.2025 12:48] Extra JSON file exists (./assets/json/2512.08400.json), skip PDF parsing.
[16.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.08400.json), skip HTML parsing.
[16.12.2025 12:48] Success.
[16.12.2025 12:48] Enriching papers with extra data.
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 0. A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.  					AI-generated summary 				 The qua...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 1. This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.  					AI-generated summary 				 Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As r...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 2. LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.  					AI-generated summary 				 Building video world models upon pretrained video generation systems represents an im...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 3. ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.  					AI-generated summary 				 Autoregressive models (ARMs) are hindered by slow...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 4. NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.  					AI-generated summary 				 Recent advances in coding agents suggest rapid progress toward autonomous softw...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 5. Error-Free Linear Attention (EFLA) is a stable, parallelizable, and theoretically sound linear-time attention mechanism that outperforms DeltaNet in language modeling and downstream tasks.  					AI-generated summary 				 Linear-time attention and State Space Models (SSMs) promise to solve the quadra...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 6. KlingAvatar 2.0 addresses inefficiencies in generating long-duration, high-resolution videos by using a spatio-temporal cascade framework with a Co-Reasoning Director and Negative Director for improved multimodal instruction alignment.  					AI-generated summary 				 Avatar video generation models h...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 7. QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.  					AI-generated summary 				 We introduce QwenLong-L1.5, a model that achieves superior lon...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 8. Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.  					AI-generated summary 				 We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on ...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 9. MentraSuite, a unified framework, advances reliable mental health reasoning using Mindora, a post-trained model with hybrid SFT-RL, evaluated via MentraBench, a benchmark assessing task performance and reasoning quality.  					AI-generated summary 				 Mental health disorders affect hundreds of mill...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 10. A solution for the 2025 BEHAVIOR Challenge in everyday household tasks using pre-training and post-training techniques substantially outperforms other submissions.  					AI-generated summary 				 The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks b...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 11. A Spatial-Aware VLA Pretraining paradigm improves 3D spatial understanding in robots by aligning 2D visual inputs with 3D actions using dual-encoder architecture with a 3D visual encoder.  					AI-generated summary 				 Vision-Language-Action (VLA) models provide a promising paradigm for robot learn...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 12. The V-REX evaluation suite assesses vision-language models' multi-step reasoning and exploration capabilities through a Chain-of-Questions framework, revealing their strengths and weaknesses in planning and following.  					AI-generated summary 				 While many vision-language models (VLMs) are devel...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 13. VG-AVS, a task and framework fine-tunes VLMs to select the most informative next viewpoint for visual question answering, enhancing performance and generalization.  					AI-generated summary 				 Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vi...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 14. WebOperator is a tree-search framework that enhances web-based agents by enabling reliable backtracking and strategic exploration, addressing the limitations of existing methods in handling irreversible actions and partial observability.  					AI-generated summary 				 LLM-based agents often operate...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 15. AEGIS, a Vision-Language-Safe Action architecture with a plug-and-play safety constraint layer using control barrier functions, enhances safety and performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models have demonstrated remarkable capabilities...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 16. Diffusion Preview uses ConsistencySolver, a high-order trainable solver, to improve quality and consistency in low-step image generation, enhancing interactive user experiences.  					AI-generated summary 				 The slow inference process of image diffusion models significantly degrades interactive us...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 17. State-of-the-art image generation and reward models exhibit bias towards conventional aesthetics, often failing to produce anti-aesthetic images as requested, thus compromising user autonomy and aesthetic diversity.  					AI-generated summary 				 Over-aligning image generation models to a generaliz...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 18. RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.  					AI-generated summary 				 Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 19. DrivePI, a spatial-aware 4D multi-modal large language model, achieves state-of-the-art performance in 3D perception, prediction, and planning for autonomous driving by integrating point clouds, images, and language instructions.  					AI-generated summary 				 Although multi-modal large language mo...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 20. A new 3D point cloud backbone model, LitePT, uses convolutions for early stages and attention for deeper layers, incorporating PointROPE for positional encoding, achieving efficient performance with fewer resources.  					AI-generated summary 				 Modern neural architectures for 3D point cloud proce...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 21. Interactive Intelligence, realized through Mio framework, enables advanced digital humans with personality, adaptive interactions, and self-evolution, surpassing current benchmarks.  					AI-generated summary 				 We introduce Interactive Intelligence, a novel paradigm of digital human that is capab...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 22. Directional Textual Inversion (DTI) improves text-to-image personalization by constraining learned tokens to unit magnitude, enhancing prompt conditioning and enabling smooth interpolation between concepts.  					AI-generated summary 				 Textual Inversion (TI) is an efficient approach to text-to-im...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 23. The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  					AI-generated summary 				 Recent advances ...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 24. START enhances multimodal large language models by integrating spatial and textual learning through chart-element grounding and chart-to-code generation, improving chart understanding and performance across benchmarks.  					AI-generated summary 				 Chart understanding is crucial for deploying mult...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 25. COM4D predicts the structure and spatio-temporal configuration of 4D/3D objects from 2D video without using 4D compositional training data, achieving state-of-the-art results in 4D object and composed 3D reconstruction.  					AI-generated summary 				 Scenes in the real world are often composed of s...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 26. DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.  					AI-generated summary 				 Video diffusion models have revolutionized generative video synthesis, but they are imprecise, sl...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 27. A generative latent flow matching model is proposed to predict future audio for robotic manipulation tasks, improving performance over methods without future lookahead by accurately capturing intrinsic rhythmic patterns.  					AI-generated summary 				 World models have demonstrated impressive perfo...
[16.12.2025 12:48] ********************************************************************************
[16.12.2025 12:48] Abstract 28. The study presents an optimized deep learning pipeline using the AutoFish dataset and Swin-T architecture to improve fish re-identification metrics in electronic monitoring systems.  					AI-generated summary 				 Accurate fisheries data are crucial for effective and sustainable marine resource mana...
[16.12.2025 12:48] Read previous papers.
[16.12.2025 12:48] Generating reviews via LLM API.
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#training", "#architecture", "#optimization", "#cv"], "emoji": "üé®", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ —Å–º—ã—Å–ª—É: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—èÊ°ÜÊû∂VTP –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#long_context", "#rl", "#agents", "#benchmark", "#rag"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–∞–º—è—Ç–∏ –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#video", "#optimization", "#training", "#benchmark", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢—Ä–∏ —ç—Ç–∞–ø–∞ –∫ –∏–¥–µ–∞–ª—å–Ω–æ–π –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "LongVie 2 ‚Äî —ç—Ç–æ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ç—Ä—ë—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–≤–æ–π
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#inference", "#optimization", "#benchmark", "#diffusion", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∫ —Å–ª–æ—Ç–∞–º: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "ReFusion ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–∞–∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#agents", "#reasoning", "#plp"], "emoji": "üèóÔ∏è", "ru": {"title": "–î–æ–ª–≥–æ—ç—Ç–∞–ø–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∫–∞–∫ –≥–ª–∞–≤–Ω—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤", "desc": "NL2Repo Bench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤-–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ Pyth
[16.12.2025 12:48] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –±–µ–∑ –æ—à–∏–±–æ–∫: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (EFLA), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ–ª–Ω—É—é –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –∑–∞–¥–∞—á—É –æ–±—É—á–µ–Ω–∏—è –∫
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#video", "#multimodal", "#story_generation", "#alignment", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã—Å–æ–∫–æ—Ä–µ–∑–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "KlingAvatar 2.0 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ø–∞—Ü–∏–æ-—Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã–π –∫–∞—Å–∫–∞–¥–Ω—ã–π —Ñ
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#long_context", "#synthetic", "#rl", "#agents", "#benchmark", "#data", "#architecture"], "emoji": "üß†", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "QwenLong-L1.5 ‚Äî —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã–¥–∞—é—â–∏—Ö—Å—è
[16.12.2025 12:48] Querying the API.
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.  					AI-generated summary 				 We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.   We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.   We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.
[16.12.2025 12:48] Response: ```json
{
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫ Finch –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–±—Ä–∞–Ω –∏–∑ –∞—É—Ç–µ–Ω—Ç–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ Enron –∏ –¥—Ä—É–≥–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏–Ω—Å—Ç–∏—Ç—É—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∏—Ç 172 –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å 384 –∑–∞–¥–∞—á–∞–º–∏, –≤–∫–ª—é—á–∞—é—â–∏–º–∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã, –ø–∏—Å—å–º–∞, PDF –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø—Ä–∏–º–µ–Ω—ë–Ω –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM –∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é, —Ç—Ä–µ–±–æ–≤–∞–≤—à—É—é –±–æ–ª–µ–µ 700 —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤. –û—Ü–µ–Ω–∫–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å–∏—Å—Ç–µ–º—ã —Ç–∏–ø–∞ GPT –∏ Claude —Ä–µ—à–∞—é—Ç –º–µ–Ω–µ–µ 40% —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è AI –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.",
  "emoji": "üìä",
  "title": "–†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö AI-–ø–æ–º–æ—â–Ω–∏–∫–æ–≤: –±–µ–Ω—á–º–∞—Ä–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤"
}
```
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.  					AI-generated summary 				 We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.   We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.   We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents."

[16.12.2025 12:48] Response: ```python
["BENCHMARK", "AGENTS", "DATASET", "MULTIMODAL"]
```
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.  					AI-generated summary 				 We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.   We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.   We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents."

[16.12.2025 12:48] Response: ```python
['SCIENCE']
```
[16.12.2025 12:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Finch, a benchmark designed to evaluate AI agents in the context of enterprise finance and accounting. It utilizes real-world data from Enron and other institutions, encompassing a variety of tasks such as data entry, web search, and reporting, to reflect the complexity of actual workflows. Finch includes 172 workflows and 384 tasks, derived through a combination of large language model (LLM) assistance and expert annotation, capturing the messy and collaborative nature of enterprise work. The evaluation of leading AI systems reveals significant challenges, with top models like GPT 5.1 and Claude Sonnet 4.5 struggling to complete a majority of the workflows successfully.","title":"Finch: Benchmarking AI in Real-World Finance Workflows"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Finch, a benchmark designed to evaluate AI agents in the context of enterprise finance and accounting. It utilizes real-world data from Enron and other institutions, encompassing a variety of tasks such as data entry, web search, and reporting, to reflect the complexity of actual workflows. Finch includes 172 workflows and 384 tasks, derived through a combination of large language model (LLM) assistance and expert annotation, capturing the messy and collaborative nature of enterprise work. The evaluation of leading AI systems reveals significant challenges, with top models like GPT 5.1 and Claude Sonnet 4.5 struggling to complete a majority of the workflows successfully.', title='Finch: Benchmarking AI in Real-World Finance Workflows'))
[16.12.2025 12:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FinchÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞AI‰ª£ÁêÜÂú®‰ºÅ‰∏öË¥¢Âä°Âíå‰ºöËÆ°È¢ÜÂüüË°®Áé∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®Â§ÑÁêÜÂ§çÊùÇÁöÑÁúüÂÆûÂ∑•‰ΩúÊµÅÁ®ã„ÄÇËØ•Âü∫ÂáÜ‰ΩøÁî®Êù•Ëá™EnronÁ≠âÊú∫ÊûÑÁöÑÁúüÂÆûÊï∞ÊçÆÔºåÂåÖÊã¨150ÂêçÂëòÂ∑•ÁöÑ15,000‰∏™ÁîµÂ≠êË°®Ê†ºÂíå500,000Â∞ÅÁîµÂ≠êÈÇÆ‰ª∂ÔºåÂèçÊò†‰∫ÜÁúüÂÆûÂ∑•‰ΩúÁéØÂ¢É‰∏≠ÁöÑÂ§çÊùÇÊÄß„ÄÇFinchÈÄöËøáÁªìÂêàÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËæÖÂä©ÂèëÁé∞Âíå‰∏ìÂÆ∂Ê≥®ÈáäÔºåÊûÑÂª∫‰∫Ü172‰∏™Â§çÂêàÂ∑•‰ΩúÊµÅÁ®ãÔºåÊ∂âÂèä384‰∏™‰ªªÂä°Âíå1710‰∏™ÁîµÂ≠êË°®Ê†º„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑAIÁ≥ªÁªüÂú®Â§ÑÁêÜËøô‰∫õÁúüÂÆû‰ºÅ‰∏öÂ∑•‰ΩúÊµÅÁ®ãÊó∂Èù¢‰∏¥ÊòæËëóÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÊñπÈù¢„ÄÇ","title":"FinchÔºöËØÑ‰º∞AIÂú®‰ºÅ‰∏öË¥¢Âä°‰∏≠ÁöÑÁúüÂÆûË°®Áé∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FinchÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞AI‰ª£ÁêÜÂú®‰ºÅ‰∏öË¥¢Âä°Âíå‰ºöËÆ°È¢ÜÂüüË°®Áé∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®Â§ÑÁêÜÂ§çÊùÇÁöÑÁúüÂÆûÂ∑•‰ΩúÊµÅÁ®ã„ÄÇËØ•Âü∫ÂáÜ‰ΩøÁî®Êù•Ëá™EnronÁ≠âÊú∫ÊûÑÁöÑÁúüÂÆûÊï∞ÊçÆÔºåÂåÖÊã¨150ÂêçÂëòÂ∑•ÁöÑ15,000‰∏™ÁîµÂ≠êË°®Ê†ºÂíå500,000Â∞ÅÁîµÂ≠êÈÇÆ‰ª∂ÔºåÂèçÊò†‰∫ÜÁúüÂÆûÂ∑•‰ΩúÁéØÂ¢É‰∏≠ÁöÑÂ§çÊùÇÊÄß„ÄÇFinchÈÄöËøáÁªìÂêàÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËæÖÂä©ÂèëÁé∞Âíå‰∏ìÂÆ∂Ê≥®ÈáäÔºåÊûÑÂª∫‰∫Ü172‰∏™Â§çÂêàÂ∑•‰ΩúÊµÅÁ®ãÔºåÊ∂âÂèä384‰∏™‰ªªÂä°Âíå1710‰∏™ÁîµÂ≠êË°®Ê†º„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑAIÁ≥ªÁªüÂú®Â§ÑÁêÜËøô‰∫õÁúüÂÆû‰ºÅ‰∏öÂ∑•‰ΩúÊµÅÁ®ãÊó∂Èù¢‰∏¥ÊòæËëóÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÊñπÈù¢„ÄÇ', title='FinchÔºöËØÑ‰º∞AIÂú®‰ºÅ‰∏öË¥¢Âä°‰∏≠ÁöÑÁúüÂÆûË°®Áé∞'))
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rlhf", "#hallucinations", "#healthcare", "#science", "#training", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–∞–¥—ë–∂–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#training", "#benchmark", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–æ–º–∞—à–Ω–∏—Ö —Ä–æ–±–æ—Ç–æ–≤: –æ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è BEHAVIOR Challenge 2025, –≥–¥–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∞–≥–µ–Ω—Ç—ã —Ä–µ—à–∞—é—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#training", "#architecture", "#3d", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ú–æ—Å—Ç –º–µ–∂–¥—É –¥–≤—É–º–µ—Ä–Ω—ã–º –≤–∏–¥–µ–Ω–∏–µ–º –∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#interpretability", "#cv", "#benchmark", "#dataset"], "emoji": "üîç", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤", "desc": "V-REX ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º—É
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#training", "#rl", "#agents", "#dataset", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ä–∞–∫—É—Ä—Å–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VG-AVS ‚Äî –∑–∞–¥–∞—á—É –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤—ã–±–∏—Ä–∞—Ç—å 
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "üå≥", "ru": {"title": "–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –æ—Ç–∫–∞—Ç–æ–º –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "WebOperator ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∂–∞–¥–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π, –ø–æ–∑–≤–æ–ª—è—è –∞
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#architecture", "#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –±–∞—Ä—å–µ—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AEGIS ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision-Language-Safe Action, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–±–∞–≤–ª—è–µ—Ç 
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –∫–∞—á–µ—Å—Ç–≤–∞: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π —Ä–µ—à–∞—Ç–µ–ª—å –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Diffusion Preview –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "üé®", "ru": {"title": "–ö–æ–≥–¥–∞ –∫—Ä–∞—Å–æ—Ç–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ü–µ–Ω–∑—É—Ä–æ–π: –±–æ—Ä—å–±–∞ –∑–∞ –∞–≤—Ç–æ–Ω–æ–º–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ reward models –∏–º–µ—é—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ —Å—Ç–æ
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#open_source", "#architecture"], "emoji": "üé®", "ru": {"title": "–ë–æ–≥–∞—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –≤ –≤—ã—Å–æ–∫–æ–º–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RecTok —Ä–µ—à–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –ª–∞—Ç–µ–Ω
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#robotics", "#multimodal", "#dataset", "#3d"], "emoji": "üöó", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏", "desc": "DrivePI ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–∞—è —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#3d"], "emoji": "‚ö°", "ru": {"title": "–°–≤—ë—Ä—Ç–∫–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ: –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ 3D –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç LitePT, –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –≤ 3D, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤—ë—Ä—Ç–∫–∏ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö 
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –ª—é–¥–∏ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º –∏ —ç–≤–æ–ª—é—Ü–∏–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–∞—Ä–∞–¥–∏–≥–º–∞ Interactive Intelligence –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ª—é–¥–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –≤—ã—Ä–∞–∂–µ–Ω–∏—é, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–º—É
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#optimization"], "emoji": "üß≠", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Directional Textual Inversion (DTI), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é text-to-im
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∞–ª–∏–∑–º–∞: –∫–∞–∫ AI-–≤–∏–¥–µ–æ –æ–±–º–∞–Ω—ã–≤–∞—é—Ç –¥–∞–∂–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ Video Reality Test –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ AI-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ ASMR –∫
[16.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#training", "#synthetic", "#science", "#open_source", "#benchmark", "#dataset"], "emoji": "üìä", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "START ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥–∏–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö
[16.12.2025 12:48] Querying the API.
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

COM4D predicts the structure and spatio-temporal configuration of 4D/3D objects from 2D video without using 4D compositional training data, achieving state-of-the-art results in 4D object and composed 3D reconstruction.  					AI-generated summary 				 Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.
[16.12.2025 12:48] Response: ```json
{
  "desc": "COM4D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏–∑ –¥–≤—É–º–µ—Ä–Ω–æ–≥–æ –≤–∏–¥–µ–æ. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –Ω–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –¥–∏–Ω–∞–º–∏–∫–µ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–±–µ–≥–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö 4D –¥–∞–Ω–Ω—ã—Ö. –ù–∞ —ç—Ç–∞–ø–µ –≤—ã–≤–æ–¥–∞ –º–µ—Ö–∞–Ω–∏–∑–º —Å–º–µ—à–∏–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è (attention mixing) –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –≤—ã—É—á–µ–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö —Å—Ü–µ–Ω. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö 4D-–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–π, —Ä–∞–±–æ—Ç–∞—è —Ç–æ–ª—å–∫–æ —Å –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã–º –≤–∏–¥–µ–æ.",
  "emoji": "üé¨",
  "title": "–ß–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ —Å—Ü–µ–Ω—ã –∏–∑ –¥–≤—É–º–µ—Ä–Ω–æ–≥–æ –≤–∏–¥–µ–æ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
}
```
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"COM4D predicts the structure and spatio-temporal configuration of 4D/3D objects from 2D video without using 4D compositional training data, achieving state-of-the-art results in 4D object and composed 3D reconstruction.  					AI-generated summary 				 Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven."

[16.12.2025 12:48] Response: ```python
['VIDEO', '3D', 'MULTIMODAL']
```
[16.12.2025 12:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"COM4D predicts the structure and spatio-temporal configuration of 4D/3D objects from 2D video without using 4D compositional training data, achieving state-of-the-art results in 4D object and composed 3D reconstruction.  					AI-generated summary 				 Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven."

[16.12.2025 12:49] Response: ```python
['TRANSFER_LEARNING']
```

The paper demonstrates transfer learning by training on separate sources (static multi-object supervision and dynamic single object supervision) and combining the learned knowledge at inference time through an attention mixing mechanism, without requiring 4D compositional training data.
[16.12.2025 12:49] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING"]


The paper demonstrates transfer learning by training on separate sources (static multi-object supervision and dynamic single object supervision) and combining the learned knowledge at inference time through an attention mixing mechanism, without requiring 4D compositional training data.
[16.12.2025 12:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"COM4D is a novel method that predicts the structure and arrangement of 4D and 3D objects from 2D video inputs without needing 4D training data. It effectively learns from both static multi-object scenes and dynamic single-object scenarios by utilizing spatial and temporal attention mechanisms. This approach allows for consistent scene configurations and overcomes limitations of previous models that focused on individual objects. By combining learned attentions during inference, COM4D achieves state-of-the-art results in reconstructing complex 4D scenes and 3D objects.","title":"Revolutionizing 4D Object Reconstruction from 2D Videos"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='COM4D is a novel method that predicts the structure and arrangement of 4D and 3D objects from 2D video inputs without needing 4D training data. It effectively learns from both static multi-object scenes and dynamic single-object scenarios by utilizing spatial and temporal attention mechanisms. This approach allows for consistent scene configurations and overcomes limitations of previous models that focused on individual objects. By combining learned attentions during inference, COM4D achieves state-of-the-art results in reconstructing complex 4D scenes and 3D objects.', title='Revolutionizing 4D Object Reconstruction from 2D Videos'))
[16.12.2025 12:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"COM4DÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÂèØ‰ª•‰ªé2DËßÜÈ¢ë‰∏≠È¢ÑÊµã4D/3DÁâ©‰ΩìÁöÑÁªìÊûÑÂíåÊó∂Á©∫ÈÖçÁΩÆÔºåËÄåÊó†ÈúÄ‰ΩøÁî®4DÁªÑÂêàËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂØπÁ©∫Èó¥ÂíåÊó∂Èó¥Ê≥®ÊÑèÂäõÁöÑÁ≤æÂøÉËÆæËÆ°ËÆ≠ÁªÉÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂ§ö‰∏™ÈùôÊÄÅÁâ©‰ΩìÊàñÂçï‰∏™Âä®ÊÄÅÁâ©‰Ωì„ÄÇCOM4DÂú®Êé®ÁêÜÊó∂ÁªìÂêàÁã¨Á´ãÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÂçïÁõÆËßÜÈ¢ëÈáçÂª∫ÂÆåÊï¥ÁöÑ4DÂú∫ÊôØ„ÄÇÂ∞ΩÁÆ°ÂÆåÂÖ®‰æùËµñÊï∞ÊçÆÈ©±Âä®ÔºåCOM4DÂú®4DÁâ©‰ΩìÂíåÁªÑÂêà3DÈáçÂª∫ÁöÑÁé∞ÊúâÈóÆÈ¢ò‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ","title":"COM4DÔºö‰ªé2DËßÜÈ¢ëÈáçÂª∫4DÁâ©‰ΩìÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='COM4DÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÂèØ‰ª•‰ªé2DËßÜÈ¢ë‰∏≠È¢ÑÊµã4D/3DÁâ©‰ΩìÁöÑÁªìÊûÑÂíåÊó∂Á©∫ÈÖçÁΩÆÔºåËÄåÊó†ÈúÄ‰ΩøÁî®4DÁªÑÂêàËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂØπÁ©∫Èó¥ÂíåÊó∂Èó¥Ê≥®ÊÑèÂäõÁöÑÁ≤æÂøÉËÆæËÆ°ËÆ≠ÁªÉÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂ§ö‰∏™ÈùôÊÄÅÁâ©‰ΩìÊàñÂçï‰∏™Âä®ÊÄÅÁâ©‰Ωì„ÄÇCOM4DÂú®Êé®ÁêÜÊó∂ÁªìÂêàÁã¨Á´ãÂ≠¶‰π†ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÂçïÁõÆËßÜÈ¢ëÈáçÂª∫ÂÆåÊï¥ÁöÑ4DÂú∫ÊôØ„ÄÇÂ∞ΩÁÆ°ÂÆåÂÖ®‰æùËµñÊï∞ÊçÆÈ©±Âä®ÔºåCOM4DÂú®4DÁâ©‰ΩìÂíåÁªÑÂêà3DÈáçÂª∫ÁöÑÁé∞ÊúâÈóÆÈ¢ò‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ', title='COM4DÔºö‰ªé2DËßÜÈ¢ëÈáçÂª∫4DÁâ©‰ΩìÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[16.12.2025 12:49] Using data from previous issue: {"categories": ["#video", "#diffusion", "#inference", "#interpretability", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "DiffusionBrowser ‚Äî —ç—Ç–æ –ª—ë–≥–∫–∏–π –¥–µ–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–∏–¥–µ—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø
[16.12.2025 12:49] Using data from previous issue: {"categories": ["#audio", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –∑–≤—É–∫–∞ –¥–ª—è —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ flow matching", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ flow matching –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –∑
[16.12.2025 12:49] Using data from previous issue: {"categories": [], "emoji": "üêü", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø—Ä–æ—Ç–∏–≤ —Å–≤–µ—Ä—Ç–æ–∫: Swin-T –ø–æ–±–µ–∂–¥–∞–µ—Ç –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–±", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ AutoFish –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Swin-T. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç 
[16.12.2025 12:49] Renaming data file.
[16.12.2025 12:49] Renaming previous data. hf_papers.json to ./d/2025-12-16.json
[16.12.2025 12:49] Saving new data file.
[16.12.2025 12:49] Generating page.
[16.12.2025 12:49] Renaming previous page.
[16.12.2025 12:49] Renaming previous data. index.html to ./d/2025-12-16.html
[16.12.2025 12:49] Writing result.
[16.12.2025 12:49] Renaming log file.
[16.12.2025 12:49] Renaming previous data. log.txt to ./logs/2025-12-16_last_log.txt
