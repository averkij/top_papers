[16.12.2025 19:21] Read previous papers.
[16.12.2025 19:21] Generating top page (month).
[16.12.2025 19:21] Writing top page (month).
[16.12.2025 20:27] Read previous papers.
[16.12.2025 20:27] Get feed.
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13586
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13687
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13564
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12967
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13604
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13168
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12730
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12602
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13313
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09636
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10071
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13080
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12692
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11995
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12799
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13250
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11891
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13592
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12751
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11883
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13674
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13421
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10655
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13689
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13672
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13281
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11438
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10794
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.07186
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.05272
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13690
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13006
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12777
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11470
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10927
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08405
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13330
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09069
[16.12.2025 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2512.08400
[16.12.2025 20:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.12.2025 20:27] No deleted papers detected.
[16.12.2025 20:27] Downloading and parsing papers (pdf, html). Total: 39.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13586.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13586.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13586.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13687.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13687.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13687.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13564.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13564.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13564.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.12967.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.12967.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.12967.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13604.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13604.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13604.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13168.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13168.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13168.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.12730.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.12730.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.12730.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.12602.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.12602.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.12602.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13313.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13313.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13313.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.09636.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.09636.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.09636.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.10071.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.10071.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.10071.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13080.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13080.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13080.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.12692.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.12692.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.12692.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.11995.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.11995.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.11995.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.12799.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.12799.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.12799.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13250.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13250.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13250.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.11891.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.11891.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.11891.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13592.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13592.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13592.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.12751.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.12751.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.12751.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.11883.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.11883.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.11883.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13674.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13674.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13674.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13421.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13421.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13421.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.10655.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.10655.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.10655.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13689.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13689.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13689.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13672.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13672.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13672.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13281.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13281.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13281.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.11438.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.11438.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.11438.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.10794.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.10794.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.10794.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.07186.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.07186.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.07186.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.05272.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.05272.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.05272.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13690.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13690.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13690.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13006.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13006.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13006.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.12777.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.12777.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.12777.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.11470.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.11470.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.11470.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.10927.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.10927.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.10927.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.08405.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.08405.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.08405.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.13330.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.13330.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.13330.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.09069.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.09069.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.09069.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Downloading and parsing paper https://huggingface.co/papers/2512.08400.
[16.12.2025 20:27] Extra JSON file exists (./assets/json/2512.08400.json), skip PDF parsing.
[16.12.2025 20:27] Paper image links file exists (./assets/img_data/2512.08400.json), skip HTML parsing.
[16.12.2025 20:27] Success.
[16.12.2025 20:27] Enriching papers with extra data.
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 0. ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.  					AI-generated summary 				 Autoregressive models (ARMs) are hindered by slow...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 1. A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.  					AI-generated summary 				 The qua...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 2. This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.  					AI-generated summary 				 Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As r...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 3. QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.  					AI-generated summary 				 We introduce QwenLong-L1.5, a model that achieves superior lon...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 4. LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.  					AI-generated summary 				 Building video world models upon pretrained video generation systems represents an im...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 5. Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.  					AI-generated summary 				 We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on ...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 6. NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.  					AI-generated summary 				 Recent advances in coding agents suggest rapid progress toward autonomous softw...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 7. Error-Free Linear Attention (EFLA) is a stable, parallelizable, and theoretically sound linear-time attention mechanism that outperforms DeltaNet in language modeling and downstream tasks.  					AI-generated summary 				 Linear-time attention and State Space Models (SSMs) promise to solve the quadra...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 8. KlingAvatar 2.0 addresses inefficiencies in generating long-duration, high-resolution videos by using a spatio-temporal cascade framework with a Co-Reasoning Director and Negative Director for improved multimodal instruction alignment.  					AI-generated summary 				 Avatar video generation models h...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 9. MentraSuite, a unified framework, advances reliable mental health reasoning using Mindora, a post-trained model with hybrid SFT-RL, evaluated via MentraBench, a benchmark assessing task performance and reasoning quality.  					AI-generated summary 				 Mental health disorders affect hundreds of mill...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 10. A solution for the 2025 BEHAVIOR Challenge in everyday household tasks using pre-training and post-training techniques substantially outperforms other submissions.  					AI-generated summary 				 The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks b...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 11. A Spatial-Aware VLA Pretraining paradigm improves 3D spatial understanding in robots by aligning 2D visual inputs with 3D actions using dual-encoder architecture with a 3D visual encoder.  					AI-generated summary 				 Vision-Language-Action (VLA) models provide a promising paradigm for robot learn...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 12. WebOperator is a tree-search framework that enhances web-based agents by enabling reliable backtracking and strategic exploration, addressing the limitations of existing methods in handling irreversible actions and partial observability.  					AI-generated summary 				 LLM-based agents often operate...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 13. The V-REX evaluation suite assesses vision-language models' multi-step reasoning and exploration capabilities through a Chain-of-Questions framework, revealing their strengths and weaknesses in planning and following.  					AI-generated summary 				 While many vision-language models (VLMs) are devel...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 14. DrivePI, a spatial-aware 4D multi-modal large language model, achieves state-of-the-art performance in 3D perception, prediction, and planning for autonomous driving by integrating point clouds, images, and language instructions.  					AI-generated summary 				 Although multi-modal large language mo...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 15. VG-AVS, a task and framework fine-tunes VLMs to select the most informative next viewpoint for visual question answering, enhancing performance and generalization.  					AI-generated summary 				 Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vi...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 16. AEGIS, a Vision-Language-Safe Action architecture with a plug-and-play safety constraint layer using control barrier functions, enhances safety and performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models have demonstrated remarkable capabilities...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 17. Diffusion Preview uses ConsistencySolver, a high-order trainable solver, to improve quality and consistency in low-step image generation, enhancing interactive user experiences.  					AI-generated summary 				 The slow inference process of image diffusion models significantly degrades interactive us...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 18. GenieDrive uses a 4D occupancy-based approach with a VAE and Mutual Control Attention for physics-aware driving video generation, improving forecasting accuracy and video quality.  					AI-generated summary 				 Physics-aware driving world model is essential for drive planning, out-of-distribution d...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 19. State-of-the-art image generation and reward models exhibit bias towards conventional aesthetics, often failing to produce anti-aesthetic images as requested, thus compromising user autonomy and aesthetic diversity.  					AI-generated summary 				 Over-aligning image generation models to a generaliz...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 20. Interactive Intelligence, realized through Mio framework, enables advanced digital humans with personality, adaptive interactions, and self-evolution, surpassing current benchmarks.  					AI-generated summary 				 We introduce Interactive Intelligence, a novel paradigm of digital human that is capab...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 21. RecTok improves diffusion models by enriching forward flow semantics and enhancing reconstruction, achieving state-of-the-art results with high-dimensional visual tokenizers.  					AI-generated summary 				 Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 22. CAPTAIN, a training-free framework, mitigates memorization in diffusion models by modifying latent features during denoising, ensuring prompt fidelity and visual quality.  					AI-generated summary 				 Diffusion models can unintentionally reproduce training examples, raising privacy and copyright c...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 23. A new 3D point cloud backbone model, LitePT, uses convolutions for early stages and attention for deeper layers, incorporating PointROPE for positional encoding, achieving efficient performance with fewer resources.  					AI-generated summary 				 Modern neural architectures for 3D point cloud proce...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 24. Directional Textual Inversion (DTI) improves text-to-image personalization by constraining learned tokens to unit magnitude, enhancing prompt conditioning and enabling smooth interpolation between concepts.  					AI-generated summary 				 Textual Inversion (TI) is an efficient approach to text-to-im...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 25. The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  					AI-generated summary 				 Recent advances ...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 26. Flowception, a non-autoregressive video generation framework, interleaves discrete frame insertions with continuous denoising, improving efficiency and performance over existing methods.  					AI-generated summary 				 We present Flowception, a novel non-autoregressive and variable-length video gene...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 27. Representation alignment enhances generative training by transferring spatial structure from pretrained vision encoders to diffusion models, surpassing the importance of global semantic performance.  					AI-generated summary 				 Representation alignment (REPA) guides generative training by distill...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 28. START enhances multimodal large language models by integrating spatial and textual learning through chart-element grounding and chart-to-code generation, improving chart understanding and performance across benchmarks.  					AI-generated summary 				 Chart understanding is crucial for deploying mult...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 29. COM4D predicts the structure and spatio-temporal configuration of 4D/3D objects from 2D video without using 4D compositional training data, achieving state-of-the-art results in 4D object and composed 3D reconstruction.  					AI-generated summary 				 Scenes in the real world are often composed of s...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 30. DiffusionBrowser enables interactive video preview generation and control during the denoising process, enhancing user experience and revealing model composition details.  					AI-generated summary 				 Video diffusion models have revolutionized generative video synthesis, but they are imprecise, sl...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 31. A systematic study adapts diffusion distillation techniques to text-to-image generation, providing guidelines for successful implementation and deployment.  					AI-generated summary 				 Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to ...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 32. The State over Tokens (SoT) framework reinterprets reasoning tokens in large language models as computational states rather than linguistic narratives, highlighting the need for a new focus in research.  					AI-generated summary 				 Large Language Models (LLMs) can generate reasoning tokens before...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 33. The Sequential SFT-then-RL pipeline is identified as optimal for integrating expert trajectories, with guidelines for scaling and trajectory selection based on performance metrics.  					AI-generated summary 				 While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 34. FoundationMotion is an automated pipeline for creating large-scale motion datasets using object detection, trajectory extraction, and LLM-generated captions, improving motion understanding in models.  					AI-generated summary 				 Motion understanding is fundamental to physical reasoning, enabling ...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 35. A generative latent flow matching model is proposed to predict future audio for robotic manipulation tasks, improving performance over methods without future lookahead by accurately capturing intrinsic rhythmic patterns.  					AI-generated summary 				 World models have demonstrated impressive perfo...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 36. FIN-bench-v2 is a unified benchmark suite for evaluating Finnish large language models, incorporating diverse datasets and evaluation criteria.  					AI-generated summary 				 We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolida...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 37. A novel knowledge distillation framework compresses a high-performance ConvNeXtV2-Large model into a lightweight EfficientNet-B2 for efficient AMD and CNV classification in real-time clinical settings.  					AI-generated summary 				 Age-related macular degeneration (AMD) and choroidal neovasculariz...
[16.12.2025 20:27] ********************************************************************************
[16.12.2025 20:27] Abstract 38. The study presents an optimized deep learning pipeline using the AutoFish dataset and Swin-T architecture to improve fish re-identification metrics in electronic monitoring systems.  					AI-generated summary 				 Accurate fisheries data are crucial for effective and sustainable marine resource mana...
[16.12.2025 20:27] Read previous papers.
[16.12.2025 20:27] Generating reviews via LLM API.
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#inference", "#optimization", "#benchmark", "#diffusion", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∫ —Å–ª–æ—Ç–∞–º: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "ReFusion ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–∞–∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#training", "#architecture", "#optimization", "#cv"], "emoji": "üé®", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ —Å–º—ã—Å–ª—É: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—èÊ°ÜÊû∂VTP –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#long_context", "#rl", "#agents", "#benchmark", "#rag"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–∞–º—è—Ç–∏ –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#long_context", "#synthetic", "#rl", "#agents", "#benchmark", "#data", "#architecture"], "emoji": "üß†", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "QwenLong-L1.5 ‚Äî —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã–¥–∞—é—â–∏—Ö—Å—è
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#video", "#optimization", "#training", "#benchmark", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢—Ä–∏ —ç—Ç–∞–ø–∞ –∫ –∏–¥–µ–∞–ª—å–Ω–æ–π –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", "desc": "LongVie 2 ‚Äî —ç—Ç–æ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ç—Ä—ë—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–≤–æ–π
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#agents", "#science", "#multimodal", "#dataset", "#benchmark"], "emoji": "üìä", "ru": {"title": "–†–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö AI-–ø–æ–º–æ—â–Ω–∏–∫–æ–≤: –±–µ–Ω—á–º–∞—Ä–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫ Finch –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#agents", "#reasoning", "#plp"], "emoji": "üèóÔ∏è", "ru": {"title": "–î–æ–ª–≥–æ—ç—Ç–∞–ø–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∫–∞–∫ –≥–ª–∞–≤–Ω—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤", "desc": "NL2Repo Bench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤-–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ Pyth
[16.12.2025 20:27] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –±–µ–∑ –æ—à–∏–±–æ–∫: –æ—Ç —Ç–µ–æ—Ä–∏–∏ –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (EFLA), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ–ª–Ω—É—é –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –∑–∞–¥–∞—á—É –æ–±—É—á–µ–Ω–∏—è –∫
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#video", "#multimodal", "#story_generation", "#alignment", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã—Å–æ–∫–æ—Ä–µ–∑–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º", "desc": "KlingAvatar 2.0 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ø–∞—Ü–∏–æ-—Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã–π –∫–∞—Å–∫–∞–¥–Ω—ã–π —Ñ
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rlhf", "#hallucinations", "#healthcare", "#science", "#training", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–∞–¥—ë–∂–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#training", "#benchmark", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–æ–º–∞—à–Ω–∏—Ö —Ä–æ–±–æ—Ç–æ–≤: –æ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è BEHAVIOR Challenge 2025, –≥–¥–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∞–≥–µ–Ω—Ç—ã —Ä–µ—à–∞—é—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#multimodal", "#training", "#architecture", "#3d", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ú–æ—Å—Ç –º–µ–∂–¥—É –¥–≤—É–º–µ—Ä–Ω—ã–º –≤–∏–¥–µ–Ω–∏–µ–º –∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "üå≥", "ru": {"title": "–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –æ—Ç–∫–∞—Ç–æ–º –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "WebOperator ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∂–∞–¥–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π, –ø–æ–∑–≤–æ–ª—è—è –∞
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#interpretability", "#cv", "#benchmark", "#dataset"], "emoji": "üîç", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤", "desc": "V-REX ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º—É
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#robotics", "#multimodal", "#dataset", "#3d"], "emoji": "üöó", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏", "desc": "DrivePI ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–∞—è —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#multimodal", "#training", "#rl", "#agents", "#dataset", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ä–∞–∫—É—Ä—Å–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VG-AVS ‚Äî –∑–∞–¥–∞—á—É –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤—ã–±–∏—Ä–∞—Ç—å 
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#architecture", "#benchmark", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –±–∞—Ä—å–µ—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AEGIS ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision-Language-Safe Action, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–±–∞–≤–ª—è–µ—Ç 
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –∫–∞—á–µ—Å—Ç–≤–∞: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π —Ä–µ—à–∞—Ç–µ–ª—å –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Diffusion Preview –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#video", "#architecture", "#3d", "#robotics"], "emoji": "üöó", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤–æ–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ 4D –æ–±—ä—ë–º–Ω—É—é –∑–∞–Ω—è—Ç–æ—Å—Ç—å", "desc": "GenieDrive ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤–æ–∂–¥–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º —Ñ–∏–∑–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4D –æ–±—ä—ë–º–Ω—É—é –∑–∞–Ω—è—Ç–æ—Å—Ç—å –∫–∞–∫
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "üé®", "ru": {"title": "–ö–æ–≥–¥–∞ –∫—Ä–∞—Å–æ—Ç–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ü–µ–Ω–∑—É—Ä–æ–π: –±–æ—Ä—å–±–∞ –∑–∞ –∞–≤—Ç–æ–Ω–æ–º–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ reward models –∏–º–µ—é—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ —Å—Ç–æ
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –ª—é–¥–∏ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º –∏ —ç–≤–æ–ª—é—Ü–∏–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–∞—Ä–∞–¥–∏–≥–º–∞ Interactive Intelligence –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ª—é–¥–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –≤—ã—Ä–∞–∂–µ–Ω–∏—é, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–º—É
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#open_source", "#architecture"], "emoji": "üé®", "ru": {"title": "–ë–æ–≥–∞—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –≤ –≤—ã—Å–æ–∫–æ–º–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RecTok —Ä–µ—à–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –ª–∞—Ç–µ–Ω
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#inference"], "emoji": "üîê", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "CAPTAIN ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#3d"], "emoji": "‚ö°", "ru": {"title": "–°–≤—ë—Ä—Ç–∫–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ: –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ 3D –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç LitePT, –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –≤ 3D, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤—ë—Ä—Ç–∫–∏ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö 
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#architecture", "#training", "#multimodal", "#optimization"], "emoji": "üß≠", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Directional Textual Inversion (DTI), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é text-to-im
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∞–ª–∏–∑–º–∞: –∫–∞–∫ AI-–≤–∏–¥–µ–æ –æ–±–º–∞–Ω—ã–≤–∞—é—Ç –¥–∞–∂–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ Video Reality Test –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ AI-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ ASMR –∫
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#training", "#architecture", "#video"], "emoji": "üé¨", "ru": {"title": "–ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ –≤—Å—Ç–∞–≤–∫–∏ –∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "Flowception ‚Äî —ç—Ç–æ –Ω–µ–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —á–µ—Ä–µ–¥—É–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—Å—Ç–∞–≤–∫–∏ –∫–∞–¥—Ä–æ–≤ —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –¥
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#training", "#architecture", "#open_source", "#diffusion", "#optimization", "#transfer_learning", "#cv"], "emoji": "üé®", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤–∞–∂–Ω–µ–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –ø–µ—Ä–µ–¥–∞—á–∞ 
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#multimodal", "#training", "#synthetic", "#science", "#open_source", "#benchmark", "#dataset"], "emoji": "üìä", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "START ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥–∏–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#video"], "emoji": "üé¨", "ru": {"title": "–ß–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ —Å—Ü–µ–Ω—ã –∏–∑ –¥–≤—É–º–µ—Ä–Ω–æ–≥–æ –≤–∏–¥–µ–æ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "COM4D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#video", "#diffusion", "#inference", "#interpretability", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "DiffusionBrowser ‚Äî —ç—Ç–æ –ª—ë–≥–∫–∏–π –¥–µ–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–∏–¥–µ—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#reasoning", "#interpretability"], "emoji": "üß†", "ru": {"title": "–¢–æ–∫–µ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∞ –Ω–µ —Ç–µ–∫—Å—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–∞—è —Ä–∞–º–∫–∞ State over Tokens (SoT), –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#training", "#rl", "#benchmark"], "emoji": "üìà", "ru": {"title": "–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π SFT-then-RL: –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Plasticity-Ceiling Framework –¥–ª—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ SFT (supervised fine-tuning) –∏ RL (rei
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#data", "#video", "#dataset", "#training", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥–µ—Ç–µ–∫—Ü–∏—é –∏ LLM", "desc": "FoundationMotion ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#audio", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –∑–≤—É–∫–∞ –¥–ª—è —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ flow matching", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ flow matching –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –∑
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#survey", "#dataset", "#low_resource", "#multilingual"], "emoji": "üá´üáÆ", "ru": {"title": "–ù–∞–¥–µ–∂–Ω—ã–π —Ñ–∏–Ω—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ FIN-bench-v2 ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞
[16.12.2025 20:27] Using data from previous issue: {"categories": ["#cv", "#inference", "#training", "#small_models", "#healthcare"], "emoji": "üîç", "ru": {"title": "–°–∂–∞—Ç–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –≥–ª–∞–∑–Ω—ã—Ö –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–¥–∞—á–∏ –∑–Ω–∞–Ω–∏–π (knowledge distillation), –∫–æ—Ç–æ—Ä–∞—è —Å–∂–∏–º–∞–µ—Ç –º–æ—â–Ω—É—é —É—á–∏—Ç–µ–ª—å
[16.12.2025 20:27] Using data from previous issue: {"categories": [], "emoji": "üêü", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø—Ä–æ—Ç–∏–≤ —Å–≤–µ—Ä—Ç–æ–∫: Swin-T –ø–æ–±–µ–∂–¥–∞–µ—Ç –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–±", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–± –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ AutoFish –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Swin-T. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç 
[16.12.2025 20:27] Renaming data file.
[16.12.2025 20:27] Renaming previous data. hf_papers.json to ./d/2025-12-16.json
[16.12.2025 20:27] Saving new data file.
[16.12.2025 20:27] Generating page.
[16.12.2025 20:27] Renaming previous page.
[16.12.2025 20:27] Renaming previous data. index.html to ./d/2025-12-16.html
[16.12.2025 20:27] Writing result.
[16.12.2025 20:27] Renaming log file.
[16.12.2025 20:27] Renaming previous data. log.txt to ./logs/2025-12-16_last_log.txt
