[24.09.2025 05:12] Read previous papers.
[24.09.2025 05:12] Generating top page (month).
[24.09.2025 05:12] Writing top page (month).
[24.09.2025 06:17] Read previous papers.
[24.09.2025 06:17] Get feed.
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19249
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18154
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18824
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19296
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19284
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19297
[24.09.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2509.18644
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18849
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19300
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19087
[24.09.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2509.17321
[24.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17083
[24.09.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2509.18174
[24.09.2025 06:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.09.2025 06:17] No deleted papers detected.
[24.09.2025 06:17] Downloading and parsing papers (pdf, html). Total: 13.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.19249.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.19249.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.19249.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.18154.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.18154.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.18154.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.18824.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.18824.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.18824.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.19296.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.19296.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.19296.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.19284.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.19284.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.19284.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.19297.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.19297.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.19297.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.18644.
[24.09.2025 06:17] Downloading paper 2509.18644 from http://arxiv.org/pdf/2509.18644v1...
[24.09.2025 06:17] Extracting affiliations from text.
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Do You Need Proprioceptive States in Visuomotor Policies? Juntu Zhao1,2, Wenbo Lu2,4, Di Zhang2,5, Yufeng Liu1,2, Yushen Liang4, Tianluo Zhang4, Yifeng Cao2, Junyuan Xie2, Yingdong Hu2,3, Shengjie Wang4, Junliang Guo2, Dequan Wang1 and Yang Gao2,3 5 2 0 2 3 2 ] . [ 1 4 4 6 8 1 . 9 0 5 2 : r Abstract Imitation-learningbased visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment. I. INTRODUCTION Imitation-learning-based visuomotor policies [1, 2, 3, 4, 5] have been widely used in robotic manipulation. Leveraging large-scale demonstration datasets [6, 7, 8, 9] and fine-tuning powerful pre-trained policies have enabled robots to achieve remarkable performance across diverse real-world tasks. For precise and reliable control, these visuomotor policies typical"
[24.09.2025 06:17] Response: ```python
[]
```
[24.09.2025 06:17] Extracting affiliations from text.
[24.09.2025 06:17] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Do You Need Proprioceptive States in Visuomotor Policies? Juntu Zhao1,2, Wenbo Lu2,4, Di Zhang2,5, Yufeng Liu1,2, Yushen Liang4, Tianluo Zhang4, Yifeng Cao2, Junyuan Xie2, Yingdong Hu2,3, Shengjie Wang4, Junliang Guo2, Dequan Wang1 and Yang Gao2,3 5 2 0 2 3 2 ] . [ 1 4 4 6 8 1 . 9 0 5 2 : r Abstract Imitation-learningbased visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment. I. INTRODUCTION Imitation-learning-based visuomotor policies [1, 2, 3, 4, 5] have been widely used in robotic manipulation. Leveraging large-scale demonstration datasets [6, 7, 8, 9] and fine-tuning powerful pre-trained policies have enabled robots to achieve remarkable performance across diverse real-world tasks. For precise and reliable control, these visuomotor policies typically incorporate not only visual observations of the task environment but also proprioceptive state (hereafter referred to as state) inputs [3, 10, 11], such as end-effector poses and joint angles. The state inputs provide compact and the robots configuration, but accurate information about they also make the policy prone to overfitting by simply memorizing the training trajectories. Therefore it severely limits spatial generalization [12, 13, 14] if the training data lacks diversity [15]. In todays context, where collecting demonstration data with wide state coverage (i.e., diverse locations of task-relevant objects) is prohibitively spatial expensive, this has become critical bottleneck for the development of visuomotor policies. In this study, we propose to completely remove the state input in visuomotor policies to enhance their spatial generalization ability, hereafter referred to as State-free Policies. This design is built upon two conditions: Equal Contribution. This work was done during the internship at Spirit AI. Corresponding authors. Project leader. 1Shanghai Jiao Tong University, 2Spirit AI, 3Tsinghua University, 4New York University Shanghai, 5Tongji University. Fig. 1: With relative EEF action space and full task observation, State-free Policies demonstrate significantly improved spatial generalization ability compared to state-based policies. To handle complex scenarios, the full task observation is implemented by dual wide-angle wrist-cameras. Relative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies. Full task observation: Another key condition for effective State-free Policies is to ensure sufficient taskrelevant visual information, which we term full task observation. This enables visuomotor policies to fully see the task-relevant objects in the task. As shown in Figure 1, under the relative EEF action space, we ensure full task observation with dual wide-angle wristcameras (field of view approximately 120 120) mounted on the top and bottom of the end-effector, which provides sufficient task-relevant visual information for State-free Policies even in some complex scenarios. This mechanism of State-free Policies forces the policy to develop deeper understanding of the task environment rather than simply memorizing the trajectories, thereby enabling State-free Policies to achieve advantages that statebased policies cannot provide: Spatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories. Therefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant objects location in the vertical direction, and horizontal refers to variations of the objects location in the 2D plane. Data efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often major bottleneck in deploying real-world robots. with limited gains [14]. While such methods can be effective, they introduce additional cost or require non-trivial algorithmic design, highlighting the need for simpler and more practical solution to spatial generalization. Cross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps. We have conducted extensive experiments across diverse range of tasks, robot embodiments, and policy architectures. In both real-world and simulation environments, State-free Policies achieve comparably great in-domain performance to state-based policies. Most importantly, when trained on strictly collected real-world demonstration data (i.e., the taskrelevant object location has constrained initial distribution range), State-free Policies exhibit significantly stronger spatial generalization"
[24.09.2025 06:17] Mistral response. {"id": "86762b60693145f49133c770139b19b9", "created": 1758694628, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1369, "total_tokens": 1412, "completion_tokens": 43}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Shanghai Jiao Tong University\",\n    \"Spirit AI\",\n    \"Tsinghua University\",\n    \"New York University Shanghai\",\n    \"Tongji University\"\n]\n```"}}]}
[24.09.2025 06:17] Response: ```python
[
    "Shanghai Jiao Tong University",
    "Spirit AI",
    "Tsinghua University",
    "New York University Shanghai",
    "Tongji University"
]
```
[24.09.2025 06:17] Deleting PDF ./assets/pdf/2509.18644.pdf.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.18849.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.18849.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.18849.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.19300.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.19300.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.19300.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.19087.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.19087.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.19087.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.17321.
[24.09.2025 06:17] Downloading paper 2509.17321 from http://arxiv.org/pdf/2509.17321v1...
[24.09.2025 06:17] Extracting affiliations from text.
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 1 2 3 7 1 . 9 0 5 2 : r OpenGVL - Benchmarking Visual Temporal Progress for Data Curation Pawe≈Ç Budzianowski1 Emilia Wisnios Gracjan Goral1 Igor Kulakov3 Viktor Petrenko3 Krzysztof Walas2,4 1University of Warsaw 2IDEAS Research Institute 3Simple Automation 4Poznan University of Technology Abstract: Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available opensource foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately 70% of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at OpenGVL. Advancements in hardware and modeling have accelerated progress in robotics. Various embodiments have recently been proposed with decreasing bill-of-material costs, leading to wider availability [1, 2, 3]. variety of Vision-Language-Action (VLA) models are being created and opensourced [4, 5, 6]. Furthermore, new benchmarks, repositories, and communities are being formed [7, 8]. These hardware innovations have led to different data collecti"
[24.09.2025 06:17] Response: ```python
[
    "University of Warsaw",
    "IDEAS Research Institute",
    "Simple Automation",
    "Poznan University of Technology"
]
```
[24.09.2025 06:17] Deleting PDF ./assets/pdf/2509.17321.pdf.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.17083.
[24.09.2025 06:17] Extra JSON file exists (./assets/json/2509.17083.json), skip PDF parsing.
[24.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.17083.json), skip HTML parsing.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.18174.
[24.09.2025 06:17] Downloading paper 2509.18174 from http://arxiv.org/pdf/2509.18174v1...
[24.09.2025 06:17] Extracting affiliations from text.
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 1 ] . [ 1 4 7 1 8 1 . 9 0 5 2 : r Baseer: Vision-Language Model for Arabic Document-to-Markdown OCR Khalil Hennara, Muhammad Hreden, Mohamed Motasim Hamed, Ahmad Bastati, Zeina Aldallal, Sara Chrouf, and Safwan AlModhayan Khobar, Saudi Arabia hennara,hreden,hamed,bastati,aldallal,chrouf,safwan@misraj.ai Abstract Arabic document OCR remains challenging task due to the languages cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, vision-language model finetuned specifically for Arabic document OCR. Leveraging large-scale dataset combining synthetic and real-world documents, Baseer is trained using decoder-only fine-tuning strategy to adapt pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving WER of 0.25 and establishing new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish strong baseline for high-accuracy OCR on morphologically rich languages like Arabic. The recent and rapid advancements in Multimodal Large Language Models (MLLMs) have fundamentally reshaped the landscape of how machines perceive and process complex visual and textual data Hurst et al. [2024]; Comanici et al. [2025]; Zhu et al. [2025]; Bai et al. [2025]. Among the myriad applications of these models, Optical Character Recognition (OCR) and comprehensive document understanding continue to present significant challenges. This is particularly true for languages that are morphologically rich and structurally complex, such as"
[24.09.2025 06:17] Response: ```python
["Misraj.ai"]
```
[24.09.2025 06:17] Deleting PDF ./assets/pdf/2509.18174.pdf.
[24.09.2025 06:17] Success.
[24.09.2025 06:17] Enriching papers with extra data.
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 0. Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.  					AI-generated summary 				 The growing disparity between the exponential ...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 1. MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 2. Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.  					AI-generated summary 				 Unified multimodal models have recently attracted considerable attenti...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 3. A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.  					AI-generated summary 				 The ability to generate virtual environments is crucial for applications r...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 4. Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.  					AI-generated summary 				 Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, b...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 5. VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.  					AI-generated summary 				 Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view s...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 6. A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  					AI-generated summary 				 Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where ...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 7. Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.  					AI-generated summary 				 Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimi...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 8. Condition-Aware Reparameterization for Flow Matching (CAR-Flow) enhances conditional generative modeling by repositioning distributions, leading to faster training and improved performance on image data.  					AI-generated summary 				 Conditional generative modeling aims to learn a conditional data...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 9. A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.  					AI-generated summary 				 Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use cl...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 10. OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  					AI-generated summary 				 Data scarcity remains one of the most limiting factors in driving...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 11. Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.  					AI-generated summary 				 Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling r...
[24.09.2025 06:17] ********************************************************************************
[24.09.2025 06:17] Abstract 12. Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  					AI-generated summary 				 Arabic document OCR remains a challenging task due t...
[24.09.2025 06:17] Read previous papers.
[24.09.2025 06:17] Generating reviews via LLM API.
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning", "#benchmark", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "RLPT: –£—Å–∏–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ú–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (RLPT) –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –±–æ–ª—å—à–∏
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#architecture", "#rl", "#agi", "#benchmark", "#optimization", "#multimodal", "#training"], "emoji": "üöÄ", "ru": {"title": "MiniCPM-V 4.5: –ö–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "MiniCPM-V 4.5 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 8 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#training"], "emoji": "üöÄ", "ru": {"title": "Hyper-Bagel: –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Hyper-Bagel - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –û–Ω–∞ –∏
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#3d", "#robotics", "#games", "#synthetic", "#video"], "emoji": "üé≠", "ru": {"title": "3D-–º–∏—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ä–µ–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ—è–≤–Ω—ã–µ 3D-–∑–Ω–∞–Ω–∏—è –∏–∑ –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ 
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#math", "#rl", "#reasoning", "#interpretability", "#training"], "emoji": "üß†", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–æ, –∞ –Ω–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought, CoT) –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö 
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#benchmark", "#3d", "#games"], "emoji": "üîç", "ru": {"title": "VolSplat: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ —á–µ—Ä–µ–∑ –≤–æ–∫—Å–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤", "desc": "VolSplat - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–æ–∫—Å–µ–ª—å–Ω–æ-–≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å
[24.09.2025 06:17] Querying the API.
[24.09.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  					AI-generated summary 				 Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\% to 85\% in height generalization and from 6\% to 64\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.
[24.09.2025 06:17] Response: {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ª–∏—Ç–∏–∫–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ª–∏—Ç–∏–∫–∞ –±–µ–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å –¥–≤—É—Ö —à–∏—Ä–æ–∫–æ—É–≥–æ–ª—å–Ω—ã—Ö –∫–∞–º–µ—Ä –Ω–∞ –∑–∞–ø—è—Å—Ç—å–µ —Ä–æ–±–æ—Ç–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ –ø—Ä–∏ –æ–±–æ–±—â–µ–Ω–∏–∏ –ø–æ –≤—ã—Å–æ—Ç–µ –∏ –≤ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏.",
  "emoji": "ü§ñ",
  "title": "–ó—Ä–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–æ–±–æ—Ç–æ–≤-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤"
}
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  					AI-generated summary 				 Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\% to 85\% in height generalization and from 6\% to 64\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment."

[24.09.2025 06:17] Response: ```python
['AGENTS', 'ROBOTICS', 'CV']
```
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  					AI-generated summary 				 Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\% to 85\% in height generalization and from 6\% to 64\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment."

[24.09.2025 06:17] Response: ```python
["OPTIMIZATION"]
```
[24.09.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a State-free Policy for robot manipulation that relies solely on visual observations instead of combining them with proprioceptive states. The authors argue that traditional state-based policies can lead to overfitting and poor generalization across different spatial contexts. By focusing on visual inputs, the State-free Policy demonstrates improved spatial generalization and data efficiency in various tasks, such as pick-and-place and shirt-folding. Empirical results show significant improvements in success rates for height and horizontal generalization, making this approach more effective for real-world applications.","title":"Visual-Only Policies: Better Generalization in Robot Manipulation!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a State-free Policy for robot manipulation that relies solely on visual observations instead of combining them with proprioceptive states. The authors argue that traditional state-based policies can lead to overfitting and poor generalization across different spatial contexts. By focusing on visual inputs, the State-free Policy demonstrates improved spatial generalization and data efficiency in various tasks, such as pick-and-place and shirt-folding. Empirical results show significant improvements in success rates for height and horizontal generalization, making this approach more effective for real-world applications.', title='Visual-Only Policies: Better Generalization in Robot Manipulation!'))
[24.09.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†Áä∂ÊÄÅÁ≠ñÁï•Ôºå‰ªÖ‰æùËµñËßÜËßâËßÇÂØüÊù•ËøõË°åÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÁä∂ÊÄÅÁöÑÁ≠ñÁï•Áõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÂú®Á©∫Èó¥Ê≥õÂåñËÉΩÂäõÂíåÊï∞ÊçÆÊïàÁéá‰∏äË°®Áé∞Êõ¥‰Ω≥„ÄÇÈÄöËøáÂéªÈô§ÂØπÊú¨‰ΩìÁä∂ÊÄÅËæìÂÖ•ÁöÑ‰æùËµñÔºåÈÅøÂÖç‰∫ÜËøáÊãüÂêàËÆ≠ÁªÉËΩ®ËøπÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊó†Áä∂ÊÄÅÁ≠ñÁï•Âú®Â§öÁßçÂÆûÈôÖ‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊàêÂäüÁéáÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÁúüÂÆû‰∏ñÁïåÂ∫îÁî®‰∏≠ÁöÑÂÆûÁî®ÊÄß„ÄÇ","title":"Êó†Áä∂ÊÄÅÁ≠ñÁï•ÔºöÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÁ©∫Èó¥Ê≥õÂåñËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†Áä∂ÊÄÅÁ≠ñÁï•Ôºå‰ªÖ‰æùËµñËßÜËßâËßÇÂØüÊù•ËøõË°åÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÁä∂ÊÄÅÁöÑÁ≠ñÁï•Áõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÂú®Á©∫Èó¥Ê≥õÂåñËÉΩÂäõÂíåÊï∞ÊçÆÊïàÁéá‰∏äË°®Áé∞Êõ¥‰Ω≥„ÄÇÈÄöËøáÂéªÈô§ÂØπÊú¨‰ΩìÁä∂ÊÄÅËæìÂÖ•ÁöÑ‰æùËµñÔºåÈÅøÂÖç‰∫ÜËøáÊãüÂêàËÆ≠ÁªÉËΩ®ËøπÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÊó†Áä∂ÊÄÅÁ≠ñÁï•Âú®Â§öÁßçÂÆûÈôÖ‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊàêÂäüÁéáÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÁúüÂÆû‰∏ñÁïåÂ∫îÁî®‰∏≠ÁöÑÂÆûÁî®ÊÄß„ÄÇ', title='Êó†Áä∂ÊÄÅÁ≠ñÁï•ÔºöÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÁ©∫Èó¥Ê≥õÂåñËÉΩÂäõ'))
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#rlhf", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MAPO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–≤–∑–≤–µ
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#cv", "#data", "#training", "#synthetic", "#diffusion"], "emoji": "üîÄ", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "CAR-Flow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ —É—Å–ª–æ–≤–Ω–æ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#science", "#dataset", "#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏—Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –¥–ª—è –ò–ò –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º—É–ª—å—Ç–∏—Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –∏–∑–æ
[24.09.2025 06:17] Querying the API.
[24.09.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  					AI-generated summary 				 Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately 70% of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at github.com/budzianowski/opengvl{OpenGVL}.
[24.09.2025 06:17] Response: {
  "desc": "OpenGVL - —ç—Ç–æ —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∑–∞–¥–∞—á –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∏—Ö –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ —É—Å—Ç—É–ø–∞—é—Ç –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–∫—Ä—ã—Ç—ã–º –∞–Ω–∞–ª–æ–≥–∞–º. OpenGVL –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –∫–∞–∫ —Ä–æ–±–æ—Ç–∞–º–∏, —Ç–∞–∫ –∏ –ª—é–¥—å–º–∏.",
  "emoji": "ü§ñ",
  "title": "OpenGVL: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ"
}
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  					AI-generated summary 				 Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately 70% of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at github.com/budzianowski/opengvl{OpenGVL}."

[24.09.2025 06:17] Response: ```python
['BENCHMARK', 'DATASET', 'ROBOTICS', 'DATA']
```
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  					AI-generated summary 				 Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately 70% of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at github.com/budzianowski/opengvl{OpenGVL}."

[24.09.2025 06:17] Response: ```python
['OPEN_SOURCE', 'SCIENCE']
```
[24.09.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OpenGVL is a benchmark designed to improve task progress prediction in robotics by utilizing vision-language models (VLMs). It highlights the performance gap between open-source and closed-source models, with open-source models achieving only about 70% of the effectiveness of their closed-source counterparts. The benchmark aims to facilitate automated data curation, allowing for better annotation and quality assessment of large robotics datasets. By leveraging the Generative Value Learning (GVL) approach, OpenGVL provides a structured way to evaluate and enhance the capabilities of models in predicting task completion from visual inputs.","title":"OpenGVL: Bridging the Gap in Robotics Task Prediction"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OpenGVL is a benchmark designed to improve task progress prediction in robotics by utilizing vision-language models (VLMs). It highlights the performance gap between open-source and closed-source models, with open-source models achieving only about 70% of the effectiveness of their closed-source counterparts. The benchmark aims to facilitate automated data curation, allowing for better annotation and quality assessment of large robotics datasets. By leveraging the Generative Value Learning (GVL) approach, OpenGVL provides a structured way to evaluate and enhance the capabilities of models in predicting task completion from visual inputs.', title='OpenGVL: Bridging the Gap in Robotics Task Prediction'))
[24.09.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OpenGVLÊòØ‰∏Ä‰∏™Áî®‰∫éÊú∫Âô®‰∫∫‰ªªÂä°ËøõÂ∫¶È¢ÑÊµãÁöÑÂü∫ÂáÜÔºåÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËøõË°åÁ†îÁ©∂„ÄÇËØ•Á†îÁ©∂Ë°®ÊòéÔºåÂºÄÊ∫êÊ®°ÂûãÂú®‰ªªÂä°ËøõÂ∫¶È¢ÑÊµãÊñπÈù¢ÁöÑË°®Áé∞ÊòéÊòæ‰Ωé‰∫éÈó≠Ê∫êÊ®°ÂûãÔºå‰ªÖËææÂà∞ÂêéËÄÖÊÄßËÉΩÁöÑ70%Â∑¶Âè≥„ÄÇOpenGVL‰∏ç‰ªÖÊèê‰æõ‰∫ÜÂ§öÊ†∑ÂåñÁöÑÊåëÊàòÊÄßÊìç‰Ωú‰ªªÂä°ÁöÑËØÑ‰º∞ÔºåËøòËÉΩ‰Ωú‰∏∫Ëá™Âä®ÂåñÊï∞ÊçÆÊï¥ÁêÜÂíåËøáÊª§ÁöÑÂÆûÁî®Â∑•ÂÖ∑ÔºåÂ∏ÆÂä©È´òÊïàËØÑ‰º∞Â§ßËßÑÊ®°Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜÁöÑË¥®Èáè„ÄÇÈÄöËøáÂà©Áî®‰∏çÊñ≠Â¢ûÈïøÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆÔºåOpenGVL‰∏∫Êú∫Âô®‰∫∫È¢ÜÂüüÁöÑËøõÊ≠•Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊú∫‰ºö„ÄÇ","title":"OpenGVLÔºöÊèêÂçáÊú∫Âô®‰∫∫‰ªªÂä°È¢ÑÊµãÁöÑÂü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OpenGVLÊòØ‰∏Ä‰∏™Áî®‰∫éÊú∫Âô®‰∫∫‰ªªÂä°ËøõÂ∫¶È¢ÑÊµãÁöÑÂü∫ÂáÜÔºåÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËøõË°åÁ†îÁ©∂„ÄÇËØ•Á†îÁ©∂Ë°®ÊòéÔºåÂºÄÊ∫êÊ®°ÂûãÂú®‰ªªÂä°ËøõÂ∫¶È¢ÑÊµãÊñπÈù¢ÁöÑË°®Áé∞ÊòéÊòæ‰Ωé‰∫éÈó≠Ê∫êÊ®°ÂûãÔºå‰ªÖËææÂà∞ÂêéËÄÖÊÄßËÉΩÁöÑ70%Â∑¶Âè≥„ÄÇOpenGVL‰∏ç‰ªÖÊèê‰æõ‰∫ÜÂ§öÊ†∑ÂåñÁöÑÊåëÊàòÊÄßÊìç‰Ωú‰ªªÂä°ÁöÑËØÑ‰º∞ÔºåËøòËÉΩ‰Ωú‰∏∫Ëá™Âä®ÂåñÊï∞ÊçÆÊï¥ÁêÜÂíåËøáÊª§ÁöÑÂÆûÁî®Â∑•ÂÖ∑ÔºåÂ∏ÆÂä©È´òÊïàËØÑ‰º∞Â§ßËßÑÊ®°Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜÁöÑË¥®Èáè„ÄÇÈÄöËøáÂà©Áî®‰∏çÊñ≠Â¢ûÈïøÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆÔºåOpenGVL‰∏∫Êú∫Âô®‰∫∫È¢ÜÂüüÁöÑËøõÊ≠•Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊú∫‰ºö„ÄÇ', title='OpenGVLÔºöÊèêÂçáÊú∫Âô®‰∫∫‰ªªÂä°È¢ÑÊµãÁöÑÂü∫ÂáÜ'))
[24.09.2025 06:17] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#3d"], "emoji": "üåà", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–∞–¥–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è: –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ 3D —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ", "desc": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–∞–¥–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è (HyRF) –æ–±—ä–µ–¥–∏–Ω—è—é—Ç —è–≤–Ω—ã–µ –≥–∞—É—Å—Å–∏–∞–Ω—ã –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å —É–º–µ–Ω—å—à
[24.09.2025 06:17] Querying the API.
[24.09.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  					AI-generated summary 				 Arabic document OCR remains a challenging task due to the language's cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic.
[24.09.2025 06:17] Response: {
  "desc": "Baseer - —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –¥–ª—è –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∞—Ä–∞–±—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–µ–∫–æ–¥–µ—Ä–∞ –∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –¥–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. Baseer –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–ª–æ–≤ (WER) 0,25. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã—Ö —è–∑—ã–∫–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∞—Ä–∞–±—Å–∫–∏–π.",
  "emoji": "üìú",
  "title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∞—Ä–∞–±—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤: Baseer —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç OCR"
}
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  					AI-generated summary 				 Arabic document OCR remains a challenging task due to the language's cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic."

[24.09.2025 06:17] Response: ```python
['DATASET', 'CV', 'BENCHMARK', 'MULTIMODAL']
```
[24.09.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  					AI-generated summary 				 Arabic document OCR remains a challenging task due to the language's cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic."

[24.09.2025 06:17] Response: ```python
['LOW_RESOURCE', 'OPEN_SOURCE', 'SYNTHETIC']
```
[24.09.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Baseer is a vision-language model specifically designed for Optical Character Recognition (OCR) of Arabic documents. It utilizes a decoder-only fine-tuning strategy on a large-scale dataset that includes both synthetic and real-world documents. This model achieves a remarkable Word Error Rate (WER) of 0.25, surpassing existing OCR solutions for Arabic. Additionally, the introduction of the Misraj-DocOCR benchmark provides a robust framework for evaluating the performance of Arabic OCR systems.","title":"Baseer: Revolutionizing Arabic Document OCR with State-of-the-Art Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Baseer is a vision-language model specifically designed for Optical Character Recognition (OCR) of Arabic documents. It utilizes a decoder-only fine-tuning strategy on a large-scale dataset that includes both synthetic and real-world documents. This model achieves a remarkable Word Error Rate (WER) of 0.25, surpassing existing OCR solutions for Arabic. Additionally, the introduction of the Misraj-DocOCR benchmark provides a robust framework for evaluating the performance of Arabic OCR systems.', title='Baseer: Revolutionizing Arabic Document OCR with State-of-the-Art Performance'))
[24.09.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BaseerÊòØ‰∏ÄÁßç‰∏ìÈó®‰∏∫ÈòøÊãâ‰ºØÊñáÊ°£OCRÔºàÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºâÂæÆË∞ÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆÉÈááÁî®Ëß£Á†ÅÂô®Á≠ñÁï•ÔºåÂπ∂Âà©Áî®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈòøÊãâ‰ºØÊñáÊ°£ÁöÑËØÜÂà´ÂáÜÁ°ÆÁéáÔºåËææÂà∞‰∫Ü0.25ÁöÑÂ≠óÈîôËØØÁéáÔºàWERÔºâ„ÄÇËØ•Ê®°ÂûãÂú®Â§ÑÁêÜÈòøÊãâ‰ºØËØ≠ÁöÑËøûÂÜô„ÄÅ‰∏çÂêåÂ≠ó‰ΩìÂíåÂè≥Âà∞Â∑¶ÁöÑ‰π¶ÂÜôÊñπÂêëÁ≠âÊåëÊàòÊó∂Ë°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÈíàÂØπÁâπÂÆöÈ¢ÜÂüüÁöÑÂæÆË∞ÉÂèØ‰ª•ÊòæËëóÊèêÂçáÈÄöÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÈòøÊãâ‰ºØÊñáOCR‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ","title":"BaseerÔºöÈòøÊãâ‰ºØÊñáÊ°£OCRÁöÑÊñ∞Ê†áÊùÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BaseerÊòØ‰∏ÄÁßç‰∏ìÈó®‰∏∫ÈòøÊãâ‰ºØÊñáÊ°£OCRÔºàÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´ÔºâÂæÆË∞ÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆÉÈááÁî®Ëß£Á†ÅÂô®Á≠ñÁï•ÔºåÂπ∂Âà©Áî®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈòøÊãâ‰ºØÊñáÊ°£ÁöÑËØÜÂà´ÂáÜÁ°ÆÁéáÔºåËææÂà∞‰∫Ü0.25ÁöÑÂ≠óÈîôËØØÁéáÔºàWERÔºâ„ÄÇËØ•Ê®°ÂûãÂú®Â§ÑÁêÜÈòøÊãâ‰ºØËØ≠ÁöÑËøûÂÜô„ÄÅ‰∏çÂêåÂ≠ó‰ΩìÂíåÂè≥Âà∞Â∑¶ÁöÑ‰π¶ÂÜôÊñπÂêëÁ≠âÊåëÊàòÊó∂Ë°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÈíàÂØπÁâπÂÆöÈ¢ÜÂüüÁöÑÂæÆË∞ÉÂèØ‰ª•ÊòæËëóÊèêÂçáÈÄöÁî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÈòøÊãâ‰ºØÊñáOCR‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ', title='BaseerÔºöÈòøÊãâ‰ºØÊñáÊ°£OCRÁöÑÊñ∞Ê†áÊùÜ'))
[24.09.2025 06:17] Renaming data file.
[24.09.2025 06:17] Renaming previous data. hf_papers.json to ./d/2025-09-24.json
[24.09.2025 06:17] Saving new data file.
[24.09.2025 06:17] Generating page.
[24.09.2025 06:17] Renaming previous page.
[24.09.2025 06:17] Renaming previous data. index.html to ./d/2025-09-24.html
[24.09.2025 06:17] Writing result.
[24.09.2025 06:17] Renaming log file.
[24.09.2025 06:17] Renaming previous data. log.txt to ./logs/2025-09-24_last_log.txt
