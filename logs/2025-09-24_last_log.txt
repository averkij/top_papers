[24.09.2025 16:14] Read previous papers.
[24.09.2025 16:14] Generating top page (month).
[24.09.2025 16:14] Writing top page (month).
[24.09.2025 17:10] Read previous papers.
[24.09.2025 17:10] Get feed.
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18174
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18644
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19249
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18154
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18849
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18824
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19297
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19296
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19284
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13835
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17321
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17083
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19300
[24.09.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2509.19170
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19087
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19002
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18090
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17349
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16506
[24.09.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19274
[24.09.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2509.18282
[24.09.2025 17:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.09.2025 17:10] No deleted papers detected.
[24.09.2025 17:10] Downloading and parsing papers (pdf, html). Total: 21.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.18174.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.18174.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.18174.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.18644.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.18644.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.18644.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.19249.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.19249.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.19249.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.18154.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.18154.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.18154.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.18849.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.18849.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.18849.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.18824.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.18824.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.18824.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.19297.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.19297.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.19297.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.19296.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.19296.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.19296.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.19284.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.19284.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.19284.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.13835.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.13835.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.13835.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.17321.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.17321.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.17321.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.17083.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.17083.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.17083.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.19300.
[24.09.2025 17:10] Extra JSON file exists (./assets/json/2509.19300.json), skip PDF parsing.
[24.09.2025 17:10] Paper image links file exists (./assets/img_data/2509.19300.json), skip HTML parsing.
[24.09.2025 17:10] Success.
[24.09.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.19170.
[24.09.2025 17:10] Downloading paper 2509.19170 from http://arxiv.org/pdf/2509.19170v1...
[24.09.2025 17:14] Extracting affiliations from text.
[24.09.2025 17:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 0 7 1 9 1 . 9 0 5 2 : r Soft Tokens, Hard Truths Natasha Butt1,, Ariel Kwiatkowski2, Ismail Labiad2, Julia Kempe2,3,, Yann Ollivier2, 1University of Amsterdam, 2Meta FAIR, 3New York University Work done during an internship at Meta, Joint senior authors The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that continuous mixture of discrete tokens could simulate superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use soft tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the soft models can be deployed in standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing softer touch to the base model. Date: September 24, 2025 Correspondence: Natasha Butt "
[24.09.2025 17:14] Response: ```python
["University of Amsterdam", "Meta FAIR", "New York University"]
```
[24.09.2025 17:14] Deleting PDF ./assets/pdf/2509.19170.pdf.
[24.09.2025 17:14] Success.
[24.09.2025 17:14] Downloading and parsing paper https://huggingface.co/papers/2509.19087.
[24.09.2025 17:14] Extra JSON file exists (./assets/json/2509.19087.json), skip PDF parsing.
[24.09.2025 17:14] Paper image links file exists (./assets/img_data/2509.19087.json), skip HTML parsing.
[24.09.2025 17:14] Success.
[24.09.2025 17:14] Downloading and parsing paper https://huggingface.co/papers/2509.19002.
[24.09.2025 17:14] Extra JSON file exists (./assets/json/2509.19002.json), skip PDF parsing.
[24.09.2025 17:14] Paper image links file exists (./assets/img_data/2509.19002.json), skip HTML parsing.
[24.09.2025 17:14] Success.
[24.09.2025 17:14] Downloading and parsing paper https://huggingface.co/papers/2509.18090.
[24.09.2025 17:14] Extra JSON file exists (./assets/json/2509.18090.json), skip PDF parsing.
[24.09.2025 17:14] Paper image links file exists (./assets/img_data/2509.18090.json), skip HTML parsing.
[24.09.2025 17:14] Success.
[24.09.2025 17:14] Downloading and parsing paper https://huggingface.co/papers/2509.17349.
[24.09.2025 17:14] Extra JSON file exists (./assets/json/2509.17349.json), skip PDF parsing.
[24.09.2025 17:14] Paper image links file exists (./assets/img_data/2509.17349.json), skip HTML parsing.
[24.09.2025 17:14] Success.
[24.09.2025 17:14] Downloading and parsing paper https://huggingface.co/papers/2509.16506.
[24.09.2025 17:14] Downloading paper 2509.16506 from http://arxiv.org/pdf/2509.16506v1...
[24.09.2025 17:14] Extracting affiliations from text.
[24.09.2025 17:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"COMMONFORMS: Large, Diverse Dataset for Form Field Detection Joe Barrow Independent Researcher joseph.d.barrow@gmail.com 5 2 0 2 0 2 ] . [ 1 6 0 5 6 1 . 9 0 5 2 : r a "
[24.09.2025 17:14] Response: []
[24.09.2025 17:14] Extracting affiliations from text.
[24.09.2025 17:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"COMMONFORMS: Large, Diverse Dataset for Form Field Detection Joe Barrow Independent Researcher joseph.d.barrow@gmail.com 5 2 0 2 0 2 ] . [ 1 6 0 5 6 1 . 9 0 5 2 : r aThis paper introduces CommonForms, web-scale dataset for form field detection. It casts the problem of form field detection as object detection: given an image of page, predict the location and type (Text Input, Choice Button, Signature) of form fields. The dataset is constructed by filtering Common Crawl to find PDFs that have fillable elements. Starting with 8 million documents, the filtering process is used to arrive at final dataset of roughly 55k documents that have more than 450k pages. Analysis shows that the dataset contains diverse mixture of languages and domains; one third of the pages are nonEnglish, and among the 14 classified domains, no domain makes up more than 25% of the dataset. In addition, this paper presents family of form field detectors, FFDNet-Small and FFDNet-Large, which attain very high average precision on the CommonForms test set. Each model cost less than $500 to train. Ablation results show that high-resolution inputs are crucial for highquality form field detection, and that the cleaning process improves data efficiency over using all PDFs that have fillable fields in Common Crawl. qualitative analysis shows that they outperform popular and commercially available PDF reader that can prepare forms. Unlike the most popular commercially available solutions, FFDNet can predict checkboxes in addition to text and signature fields. This is, to our knowledge, the first large-scale dataset released for form field detection, as well as the first open-source models. The dataset, models, and code will be released at https://github.com/jbarrow/commonforms. 1. Introduction Despite decades of digitalization, large volume of realworld transactions still center around paper forms: insurance claims, municipal paperwork, school permission slips, and many, many others. These documents are often distributed as scans or non-fillable (flat) PDFs, and require either printing, tedious software workarounds, or turning to proprietary solution like Adobe Acrobat or Apple Preview. In order to be digitally filled, flat form must be prepared by having interactive form fields inserted. Although proprietary solutions may employ machine learning to prepare forms, there is not yet high-quality open-source machine learning-based system that can automatically and reliably prepare fillable forms. Converting flat PDF to an accessible form one that can be understood by screen reader, or filled automatically typically requires two steps: 1. form field detection: detecting the locations and types (e.g. text or checkbox) of fillable elements; and 2. form enrichment: grouping the fillable elements and their labels based on the semantics of the form. Previous work has primarily focused on the second [1, 2]. In this work, we tackle the first problem by detecting where the fillable elements should go visually in PDF. To this end, we construct and release both large-scale dataset, COMMONFORMS, consisting of more than 480k pages from more than 59k document forms. We also train and release family of form field detection models trained on this dataset, FFDNet-Small and FFDNetLarge. Training each of these models costs roughly $500 of compute or less, and yet these models are capable of highquality form field detection. They achieve high average precision of more than 80 on the COMMONFORMS test set. We perform qualitative analysis of these models against Adobe Acrobat and show that FFDNet has better recall and precision than Acrobat. COMMONFORMS is drawn from large collection of interactive PDF forms scraped from the Internet. The core insight of this paper is that quantity has quality all its own, and that we can leverage existing fillable forms as training signal. We use Common Crawl as wellspring of PDFs and apply rigorous cleaning process. This cleaning process results in improved data efficiency compared to using every PDF with form field. To train the FFDNet family of models, we cast the problem of form field detection as pure object detection problem. Given page image, the goal is to predict the location and types of each form element. The types are drawn from teh following set: {Choice Button, Text Figure 1. Overview of form field detection and preparation. Input, Signature}. This means that, unlike Adobe Acrobat and Apple Preview, FFDNet can predict the location of choice buttons (i.e. checkboxes and radio buttons). As of the time of publication, these tools only predict text form fields. The FFDNet models are high-resolution (1216px) YOLO11 [11] object detectors. Our ablation results show that high-resolution inputs are crucial to quality; models trained on 10k pages at varying resolutions show range of roughly 20 mean average precision points. Our contributions are as follows: we prepare release COMMONFORMS, large, diverse, and high quality dataset for form field detection; we train and release family of form field detection models on COMMONFORMS, FFDNet-Small and FFDNet-Large1; we provide an in depth analysis of COMMONFORMS to identify the represented languages and domains; and we compare the FFDNet models against the most popular commercial system, Adobe Acrobat, and show that they produce substantially higher quality forms. 2. Related Work Machine-learning-based work on document forms can be viewed as two broad categories: form preparation and form understanding. Form preparation is the task of making form fillable, whether that is detecting widgets or building semantic model of the form. Form understanding, also 1supported by LambdaLabs compute grant known as key information extraction (KIE), is an information extraction task on filled-out document forms. This paper is an instance of form preparation, which is the smaller of the two subfields. Form Preparation and Understanding Representative work on form preparation includes the line of work on Form2Seq [1, 2] and FormA11y [18]. Form2Seq formalizes semantic form model, treating it as joint classification and grouping task. Lower-level elements like widgets and text blocks are first classified into, e.g. ChoiceButton or TextField, and then grouped into higher-order categories like ChoiceGroup. However, it is assumed that the interactive form widgets have already been correctly detected. FormA11y takes human-in-the-loop approach in which users match labels to widgets to c"
[24.09.2025 17:14] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "service_tier_capacity_exceeded", "param": null, "code": "3505"}
[24.09.2025 17:14] Failed to download and parse paper https://huggingface.co/papers/2509.16506: 'choices'
[24.09.2025 17:14] Downloading and parsing paper https://huggingface.co/papers/2509.19274.
[24.09.2025 17:14] Extra JSON file exists (./assets/json/2509.19274.json), skip PDF parsing.
[24.09.2025 17:14] Paper image links file exists (./assets/img_data/2509.19274.json), skip HTML parsing.
[24.09.2025 17:14] Success.
[24.09.2025 17:14] Downloading and parsing paper https://huggingface.co/papers/2509.18282.
[24.09.2025 17:14] Downloading paper 2509.18282 from http://arxiv.org/pdf/2509.18282v1...
[24.09.2025 17:14] Extracting affiliations from text.
[24.09.2025 17:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies Jesse Zhang1,2,3, Marius Memmel1,2, Kevin Kim3, Dieter Fox1,4, Jesse Thomason3, Fabio Ramos2, Erdem Bƒ±yƒ±k3, Abhishek Gupta1, Anqi Li2 5 2 0 2 2 2 ] . [ 1 2 8 2 8 1 . 9 0 5 2 : r Abstract Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict unified point-based intermediate representation: (1) endeffector paths specifying what actions to take, and (2) taskrelevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including 41.4 real-world improvement for 3D policy trained only in simulation, and 23.5 gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they needwhere, what, and how. Website at https://peek-robot.github.io. I. INTRODUCTION Imagine walking through crowded store when your child suddenly cries out, want the Labubu! Though youve never heard the word before, context clues guide your eyes to the fuzzy toy on the shelf, and you effortlessly weave through the crowd to grab it. What makes this possible is not raw perception ability, but the ability to interpret ambiguous instructions and distill them into just the right cueswhere to focu"
[24.09.2025 17:14] Response: ```python
[]
```
[24.09.2025 17:14] Extracting affiliations from text.
[24.09.2025 17:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies Jesse Zhang1,2,3, Marius Memmel1,2, Kevin Kim3, Dieter Fox1,4, Jesse Thomason3, Fabio Ramos2, Erdem Bƒ±yƒ±k3, Abhishek Gupta1, Anqi Li2 5 2 0 2 2 2 ] . [ 1 2 8 2 8 1 . 9 0 5 2 : r Abstract Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict unified point-based intermediate representation: (1) endeffector paths specifying what actions to take, and (2) taskrelevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including 41.4 real-world improvement for 3D policy trained only in simulation, and 23.5 gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they needwhere, what, and how. Website at https://peek-robot.github.io. I. INTRODUCTION Imagine walking through crowded store when your child suddenly cries out, want the Labubu! Though youve never heard the word before, context clues guide your eyes to the fuzzy toy on the shelf, and you effortlessly weave through the crowd to grab it. What makes this possible is not raw perception ability, but the ability to interpret ambiguous instructions and distill them into just the right cueswhere to focus, what actions to take, and how to perform these actions at the low level. Similarly, if given where to focus and what motions to take, robot manipulation policy should be able to achieve the visual robustness and semantic generalization necessary for open-world deployment by focusing only on how to perform actions.training manipulation policies is through imitation learning of human-collected robotics data [1][4], which attempts to learn the where, what, and how all at the same time. Yet their performance degrades on novel objects, clutter, or semantic variations [5], [6], since the policy alone bears the burden of handling task, semantic, and visual complexity. Such failures often entangle the axes of where, what, and howfor example, grasping distractor simultaneously reflects misplaced attention, an incorrect object choice, and wrong motion. Co-first authors, Equal Advising, 1University of Washington, 2NVIDIA, 3University of Southern California, 4Allen Institute for AI Fig. 1: PEEK enables policy generalization by modulating minimal representations of where to focus and what to do for robust policy learning. Our key idea is to offload high-level reasoning to visionlanguage models (VLMs), which can excel at semantic and visual generalization [7], [8], leaving the policy to determine how low-level behavior should be executed. Instead of forcing the policy to directly parse raw images and instructions, high-level VLM modulates the input representation to the low-level policy by providing: (1) path that encodes what the policy should do, and (2) masks showing where to attend. By absorbing semantic and visual variation, the VLM provides the policy simplified, annotated peek of the scene that gives the what and the where, while the policy only needs to learn how to perform the low-level actions. This intermediate representation helps policy execution inherit many of the VLMs semantic and visual generalization capabilities. Our VLM-modulated representation is naturally policy-agnostic, allowing it to be applied to arbitrary imageinput robot manipulation policies, including state-of-the-art RGB and 3D manipulation policies [1], [3], [9]. To concretely instantiate this insight into practical algorithm, we introduce PEEK (Policy-agnostic Extraction of Essential Keypoints), which proposes unified, pointbased intermediate representation that trains VLMs to predict what policies should do and where to focus on. Specifically, we propose to finetune pretrained VLMs [10] to predict sequence of points corresponding to (1) path that guides the robot end-effector in what actions to take and (2) set of task-relevant masking points that show the policy where to focus on (see Figure 1). During low-level visuomotor policy training and inference, we modulate the policys image observations by directly drawing these VLMpredicted paths and masks onto the image, allowing the policy to simply focus on how to act, rather than learning all three simultaneously. Doing so significantly bolsters policy generalization, combining the generality of high-level VLM predictions with the precision of low-level policy learning. In this paper, we instantiate full-stack implementation of PEEK, from devising scalable data annotation scheme that enables large-scale VLM finetuning on robotic datasets to representation-modulated training of low-level robot policies from simulation and real world data. In 535 real-world evaluations across 17 task variations, we demonstrate that PEEK consistently boosts zero-shot policy generalization: 3D policy (3DDA [9]) trained only in simulation achieves 41.4 higher success in the real world when guided by PEEK, and both large-scale vision-languageaction models (œÄ0 [3]) and small transformer-based policies [1] see 23.5 success rate improvements. These results demonstrate the power of using high-level VLMs to absorb task complexity, providing low-level policies with exactly the minimal cues they need for generalizable manipulation. II. RELATED WORKS Object-Centric Representations. One approach to improving the visual generalization of imitation learning (IL) policies is to build object-centric representations [11][17]. Earlier works relied on human-selected abstractions or manual annotation [11], while more recent methods leverage pretrained, open-vocabulary segmentation models to visually isolate task-relevant objects [12][15], [17]. Among these, to our work, proposing policyARRO [13] is closest agnostic masking scheme using"
[24.09.2025 17:14] Mistral response. {"id": "df5799b8360f494cb7a9c6d74ae26929", "created": 1758734079, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1424, "total_tokens": 1457, "completion_tokens": 33}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of Washington\",\n    \"NVIDIA\",\n    \"University of Southern California\",\n    \"Allen Institute for AI\"\n]\n```"}}]}
[24.09.2025 17:14] Response: ```python
[
    "University of Washington",
    "NVIDIA",
    "University of Southern California",
    "Allen Institute for AI"
]
```
[24.09.2025 17:14] Deleting PDF ./assets/pdf/2509.18282.pdf.
[24.09.2025 17:14] Success.
[24.09.2025 17:14] Enriching papers with extra data.
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 0. Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  					AI-generated summary 				 Arabic document OCR remains a challenging task due t...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 1. A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  					AI-generated summary 				 Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where ...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 2. Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.  					AI-generated summary 				 The growing disparity between the exponential ...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 3. MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 4. Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.  					AI-generated summary 				 Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimi...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 5. Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.  					AI-generated summary 				 Unified multimodal models have recently attracted considerable attenti...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 6. VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.  					AI-generated summary 				 Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view s...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 7. A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.  					AI-generated summary 				 The ability to generate virtual environments is crucial for applications r...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 8. Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.  					AI-generated summary 				 Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, b...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 9. Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.  					AI-generated summary 				 Dialects represent a significant component of human culture and are found across all region...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 10. OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  					AI-generated summary 				 Data scarcity remains one of the most limiting factors in driving...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 11. Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.  					AI-generated summary 				 Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling r...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 12. Condition-Aware Reparameterization for Flow Matching (CAR-Flow) enhances conditional generative modeling by repositioning distributions, leading to faster training and improved performance on image data.  					AI-generated summary 				 Conditional generative modeling aims to learn a conditional data...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 13. A scalable reinforcement learning method for learning continuous chain-of-thought tokens in large language models improves performance and diversity over discrete tokens while preserving out-of-domain predictions.  					AI-generated summary 				 The use of continuous instead of discrete tokens durin...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 14. A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.  					AI-generated summary 				 Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use cl...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 15. VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have significantly enhanced video ...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 16. GeoSVR, a voxel-based framework, improves surface reconstruction accuracy and detail using sparse voxels with depth constraints and surface regularization.  					AI-generated summary 				 Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However,...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 17. The paper analyzes SimulST latency metrics, identifies segmentation bias, and introduces YAAL and LongYAAL for more accurate latency evaluation, along with SoftSegmenter for improved alignment quality.  					AI-generated summary 				 Simultaneous speech-to-text translation (SimulST) systems have to ...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 18. A web-scale dataset and models for form field detection are introduced, achieving high precision and supporting diverse languages and domains.  					AI-generated summary 				 This paper introduces CommonForms, a web-scale dataset for form field detection. It casts the problem of form field detection...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 19. DRISHTIKON is a multimodal and multilingual benchmark for evaluating generative AI systems' cultural understanding across India's diverse regions and languages.  					AI-generated summary 				 We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual benchmark centered exclusively on ...
[24.09.2025 17:14] ********************************************************************************
[24.09.2025 17:14] Abstract 20. PEEK fine-tunes vision-language models to predict essential keypoints for robotic manipulation, enhancing zero-shot generalization across different policies and robot embodiments.  					AI-generated summary 				 Robotic manipulation policies often fail to generalize because they must simultaneously ...
[24.09.2025 17:14] Read previous papers.
[24.09.2025 17:14] Generating reviews via LLM API.
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#cv", "#low_resource", "#multimodal", "#synthetic", "#dataset"], "emoji": "üìú", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∞—Ä–∞–±—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤: Baseer —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç OCR", "desc": "Baseer - —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#cv", "#robotics", "#agents", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ó—Ä–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–æ–±–æ—Ç–æ–≤-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–æ–ª–∏—Ç–∏–∫–∞ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning", "#benchmark", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "RLPT: –£—Å–∏–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ú–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (RLPT) –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –±–æ–ª—å—à–∏
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#architecture", "#rl", "#agi", "#benchmark", "#optimization", "#multimodal", "#training"], "emoji": "üöÄ", "ru": {"title": "MiniCPM-V 4.5: –ö–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "MiniCPM-V 4.5 - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 8 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#rlhf", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MAPO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–≤–∑–≤–µ
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#training"], "emoji": "üöÄ", "ru": {"title": "Hyper-Bagel: –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Hyper-Bagel - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –û–Ω–∞ –∏
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#benchmark", "#3d", "#games"], "emoji": "üîç", "ru": {"title": "VolSplat: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ —á–µ—Ä–µ–∑ –≤–æ–∫—Å–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤", "desc": "VolSplat - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–æ–∫—Å–µ–ª—å–Ω–æ-–≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#3d", "#robotics", "#games", "#synthetic", "#video"], "emoji": "üé≠", "ru": {"title": "3D-–º–∏—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ä–µ–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ—è–≤–Ω—ã–µ 3D-–∑–Ω–∞–Ω–∏—è –∏–∑ –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ 
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#math", "#rl", "#reasoning", "#interpretability", "#training"], "emoji": "üß†", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–æ, –∞ –Ω–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought, CoT) –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö 
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#multilingual", "#dataset"], "emoji": "üó£Ô∏è", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø—ã –æ –¥–∏–∞–ª–µ–∫—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –Ω–æ—Å–∏—Ç–µ–ª—è–º –Ω–µ–º–µ—Ü–∫–∏—Ö –¥–∏–∞–ª–µ
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#data", "#robotics", "#dataset", "#science"], "emoji": "ü§ñ", "ru": {"title": "OpenGVL: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "OpenGVL - —ç—Ç–æ —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∑–∞–¥–∞—á –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#3d"], "emoji": "üåà", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–∞–¥–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è: –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ 3D —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ", "desc": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–∞–¥–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è (HyRF) –æ–±—ä–µ–¥–∏–Ω—è—é—Ç —è–≤–Ω—ã–µ –≥–∞—É—Å—Å–∏–∞–Ω—ã –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å —É–º–µ–Ω—å—à
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#cv", "#data", "#training", "#synthetic", "#diffusion"], "emoji": "üîÄ", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "CAR-Flow - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ —É—Å–ª–æ–≤–Ω–æ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±
[24.09.2025 17:14] Querying the API.
[24.09.2025 17:14] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A scalable reinforcement learning method for learning continuous chain-of-thought tokens in large language models improves performance and diversity over discrete tokens while preserving out-of-domain predictions.  					AI-generated summary 				 The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.   This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the "soft" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.
[24.09.2025 17:14] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ Chain-of-Thought —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ reinforcement learning –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç \"–º—è–≥–∫–∏–µ\" —Ç–æ–∫–µ–Ω—ã - —Å–º–µ—Å–∏ —Ç–æ–∫–µ–Ω–æ–≤ —Å —à—É–º–æ–º –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –±–æ–ª—å—à–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ–ø—ã—Ç–∫–∞—Ö. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤–Ω–µ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–æ–º–µ–Ω–∞.",
  "emoji": "üåä",
  "title": "–ú—è–≥–∫–∏–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM"
}
```
[24.09.2025 17:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A scalable reinforcement learning method for learning continuous chain-of-thought tokens in large language models improves performance and diversity over discrete tokens while preserving out-of-domain predictions.  					AI-generated summary 				 The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.   This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the "soft" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model."

[24.09.2025 17:14] Response: ```python
['RL', 'TRAINING']
```
[24.09.2025 17:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A scalable reinforcement learning method for learning continuous chain-of-thought tokens in large language models improves performance and diversity over discrete tokens while preserving out-of-domain predictions.  					AI-generated summary 				 The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.   This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the "soft" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model."

[24.09.2025 17:14] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[24.09.2025 17:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method for using continuous tokens in reinforcement learning to enhance the performance of large language models during reasoning tasks. By employing a scalable approach, the authors demonstrate that continuous chain-of-thought (CoT) tokens can improve both the efficiency and diversity of reasoning compared to traditional discrete tokens. The method allows for the training of models with hundreds of continuous tokens while minimizing computational costs, thus overcoming previous limitations. Results show that models trained with continuous CoTs not only match but exceed the performance of those using discrete tokens, especially in out-of-domain predictions.","title":"Unlocking Reasoning with Continuous Tokens in RL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method for using continuous tokens in reinforcement learning to enhance the performance of large language models during reasoning tasks. By employing a scalable approach, the authors demonstrate that continuous chain-of-thought (CoT) tokens can improve both the efficiency and diversity of reasoning compared to traditional discrete tokens. The method allows for the training of models with hundreds of continuous tokens while minimizing computational costs, thus overcoming previous limitations. Results show that models trained with continuous CoTs not only match but exceed the performance of those using discrete tokens, especially in out-of-domain predictions.', title='Unlocking Reasoning with Continuous Tokens in RL'))
[24.09.2025 17:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÁî®‰∫éÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠Â≠¶‰π†ËøûÁª≠ÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÊ†áËÆ∞„ÄÇ‰∏éÁ¶ªÊï£Ê†áËÆ∞Áõ∏ÊØîÔºåËøûÁª≠Ê†áËÆ∞Âú®Êé®ÁêÜËøáÁ®ã‰∏≠ËÉΩÂ§üÊèêÈ´òÊÄßËÉΩÂíåÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂‰øùÊåÅÂØπÂüüÂ§ñÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËøûÁª≠Ê†áËÆ∞ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑË°®ËææËÉΩÂäõÔºåÂπ∂ËÉΩÊõ¥È´òÊïàÂú∞Ëß£ÂÜ≥ÁâπÂÆöÈóÆÈ¢ò„ÄÇÈÄöËøá‰ΩøÁî®‚ÄúËΩØ‚ÄùÊ†áËÆ∞ÂíåËæìÂÖ•ÂµåÂÖ•ÁöÑÂô™Â£∞ÔºåËØ•ÊñπÊ≥ïÂÖãÊúç‰∫Ü‰ª•ÂæÄËÆ≠ÁªÉËøûÁª≠Ê†áËÆ∞ÁöÑÂõ∞ÈöæÔºå‰ΩøÂæóÂú®Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁ¶ªÊï£Ê†áËÆ∞„ÄÇ","title":"ËøûÁª≠ÊÄùÁª¥ÈìæÔºöÊèêÂçáÊé®ÁêÜÊÄßËÉΩ‰∏éÂ§öÊ†∑ÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÁî®‰∫éÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠Â≠¶‰π†ËøûÁª≠ÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÊ†áËÆ∞„ÄÇ‰∏éÁ¶ªÊï£Ê†áËÆ∞Áõ∏ÊØîÔºåËøûÁª≠Ê†áËÆ∞Âú®Êé®ÁêÜËøáÁ®ã‰∏≠ËÉΩÂ§üÊèêÈ´òÊÄßËÉΩÂíåÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂‰øùÊåÅÂØπÂüüÂ§ñÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËøûÁª≠Ê†áËÆ∞ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑË°®ËææËÉΩÂäõÔºåÂπ∂ËÉΩÊõ¥È´òÊïàÂú∞Ëß£ÂÜ≥ÁâπÂÆöÈóÆÈ¢ò„ÄÇÈÄöËøá‰ΩøÁî®‚ÄúËΩØ‚ÄùÊ†áËÆ∞ÂíåËæìÂÖ•ÂµåÂÖ•ÁöÑÂô™Â£∞ÔºåËØ•ÊñπÊ≥ïÂÖãÊúç‰∫Ü‰ª•ÂæÄËÆ≠ÁªÉËøûÁª≠Ê†áËÆ∞ÁöÑÂõ∞ÈöæÔºå‰ΩøÂæóÂú®Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁ¶ªÊï£Ê†áËÆ∞„ÄÇ', title='ËøûÁª≠ÊÄùÁª¥ÈìæÔºöÊèêÂçáÊé®ÁêÜÊÄßËÉΩ‰∏éÂ§öÊ†∑ÊÄß'))
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#science", "#dataset", "#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏—Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –¥–ª—è –ò–ò –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º—É–ª—å—Ç–∏—Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –∏–∑–æ
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#benchmark", "#video", "#science", "#games", "#agents", "#multimodal"], "emoji": "üåé", "ru": {"title": "VIR-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è—Ö –¥–ª—è –ò–ò", "desc": "VIR-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–æ–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "üßä", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –≤–æ–∫—Å–µ–ª–∏ –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω GeoSVR - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–æ–∫—Å–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–µ—Ç–æ–¥—ã –Ω–∞ –±–∞–∑–µ Gaussian Splatting. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#audio", "#data", "#machine_translation", "#alignment", "#benchmark"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–¥–µ—Ä–∂–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç (SimulST),
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#open_source", "#data", "#cv"], "emoji": "üìã", "ru": {"title": "–ü–µ—Ä–≤—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ–ª–µ–π –≤ —Ñ–æ—Ä–º–∞—Ö", "desc": "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω CommonForms - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–ª–µ–π —Ñ–æ—Ä–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 55 —Ç
[24.09.2025 17:14] Using data from previous issue: {"categories": ["#alignment", "#low_resource", "#dataset", "#benchmark", "#multimodal", "#multilingual", "#games"], "emoji": "üèõÔ∏è", "ru": {"title": "–ü–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è AI –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –ò–Ω–¥–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ DRISHTIKON - –ø–µ—Ä–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∏ –º–Ω–æ–≥–æ—è–∑—ã
[24.09.2025 17:14] Querying the API.
[24.09.2025 17:14] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PEEK fine-tunes vision-language models to predict essential keypoints for robotic manipulation, enhancing zero-shot generalization across different policies and robot embodiments.  					AI-generated summary 				 Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/.
[24.09.2025 17:15] Response: ```json
{
  "desc": "PEEK ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç vision-language –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –ú–æ–¥–µ–ª—å –≤—ã–¥–µ–ª—è–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–æ—Ä–∞ –∏ –º–∞—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π, —Å–æ–∑–¥–∞–≤–∞—è –µ–¥–∏–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–∏—Ç–∏–∫–∞–º —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π, –∞ –Ω–µ –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å—Ü–µ–Ω—ã. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ zero-shot –æ–±–æ–±—â–µ–Ω–∏—è ‚Äî –¥–æ 41.4 —Ä–∞–∑–∞ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö.",
  "emoji": "ü§ñ",
  "title": "–ö–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –∫–∞–∫ –º–æ—Å—Ç –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –¥–µ–π—Å—Ç–≤–∏–µ–º –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ"
}
```
[24.09.2025 17:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PEEK fine-tunes vision-language models to predict essential keypoints for robotic manipulation, enhancing zero-shot generalization across different policies and robot embodiments.  					AI-generated summary 				 Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/."

[24.09.2025 17:15] Response: ```python
['AGENTS', 'CV', 'ROBOTICS', 'DATASET', 'TRAINING']
```
[24.09.2025 17:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PEEK fine-tunes vision-language models to predict essential keypoints for robotic manipulation, enhancing zero-shot generalization across different policies and robot embodiments.  					AI-generated summary 				 Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/."

[24.09.2025 17:15] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION']
```
[24.09.2025 17:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PEEK is a method that improves how robots understand and perform tasks by using vision-language models (VLMs) to identify key points for manipulation. It allows robots to focus on essential actions and locations without needing to learn everything from scratch. By fine-tuning VLMs to create a simple point-based representation, PEEK helps robots generalize better across different tasks and robot types. The approach includes an automatic annotation system that generates training data, leading to significant improvements in real-world performance, especially for robots trained primarily in simulations.","title":"Enhancing Robotic Manipulation with PEEK: Keypoints for Zero-Shot Generalization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PEEK is a method that improves how robots understand and perform tasks by using vision-language models (VLMs) to identify key points for manipulation. It allows robots to focus on essential actions and locations without needing to learn everything from scratch. By fine-tuning VLMs to create a simple point-based representation, PEEK helps robots generalize better across different tasks and robot types. The approach includes an automatic annotation system that generates training data, leading to significant improvements in real-world performance, especially for robots trained primarily in simulations.', title='Enhancing Robotic Manipulation with PEEK: Keypoints for Zero-Shot Generalization'))
[24.09.2025 17:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PEEKÊòØ‰∏ÄÁßçÂæÆË∞ÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÊó®Âú®È¢ÑÊµãÊú∫Âô®‰∫∫Êìç‰ΩúÊâÄÈúÄÁöÑÂÖ≥ÈîÆÁÇπÔºå‰ªéËÄåÊèêÈ´ò‰∏çÂêåÁ≠ñÁï•ÂíåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÁöÑÈõ∂-shotÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÈ´òÂ±ÇÊ¨°ÁöÑÊé®ÁêÜ‰ªªÂä°ÂàÜÈÖçÁªôËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂÖÅËÆ∏Á≠ñÁï•‰∏ìÊ≥®‰∫éÂ¶Ç‰ΩïÊâßË°åÊìç‰Ωú„ÄÇPEEKÈÄöËøáÁîüÊàêÁªü‰∏ÄÁöÑÂü∫‰∫éÁÇπÁöÑ‰∏≠Èó¥Ë°®Á§∫ÔºåÊèê‰æõ‰∫ÜÊâßË°åË∑ØÂæÑÂíå‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂÖ≥Ê≥®Âå∫ÂüüÔºå‰ΩøÂæóË°®Á§∫‰∏éÁ≠ñÁï•Êó†ÂÖ≥ÔºåËÉΩÂ§üÂú®‰∏çÂêåÊû∂ÊûÑÈó¥ËΩ¨Áßª„ÄÇÈÄöËøáËá™Âä®Ê≥®ÈáäÁÆ°ÈÅìÔºåPEEKÂú®20Â§ö‰∏™Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ‰∏äÁîüÊàêÊ†áÊ≥®Êï∞ÊçÆÔºåÊòæËëóÊèêÂçá‰∫ÜÂú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÁöÑË°®Áé∞„ÄÇ","title":"PEEKÔºöÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂÖ≥ÈîÆÁÇπÈ¢ÑÊµã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PEEKÊòØ‰∏ÄÁßçÂæÆË∞ÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÊó®Âú®È¢ÑÊµãÊú∫Âô®‰∫∫Êìç‰ΩúÊâÄÈúÄÁöÑÂÖ≥ÈîÆÁÇπÔºå‰ªéËÄåÊèêÈ´ò‰∏çÂêåÁ≠ñÁï•ÂíåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÁöÑÈõ∂-shotÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÈ´òÂ±ÇÊ¨°ÁöÑÊé®ÁêÜ‰ªªÂä°ÂàÜÈÖçÁªôËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂÖÅËÆ∏Á≠ñÁï•‰∏ìÊ≥®‰∫éÂ¶Ç‰ΩïÊâßË°åÊìç‰Ωú„ÄÇPEEKÈÄöËøáÁîüÊàêÁªü‰∏ÄÁöÑÂü∫‰∫éÁÇπÁöÑ‰∏≠Èó¥Ë°®Á§∫ÔºåÊèê‰æõ‰∫ÜÊâßË°åË∑ØÂæÑÂíå‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÂÖ≥Ê≥®Âå∫ÂüüÔºå‰ΩøÂæóË°®Á§∫‰∏éÁ≠ñÁï•Êó†ÂÖ≥ÔºåËÉΩÂ§üÂú®‰∏çÂêåÊû∂ÊûÑÈó¥ËΩ¨Áßª„ÄÇÈÄöËøáËá™Âä®Ê≥®ÈáäÁÆ°ÈÅìÔºåPEEKÂú®20Â§ö‰∏™Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ‰∏äÁîüÊàêÊ†áÊ≥®Êï∞ÊçÆÔºåÊòæËëóÊèêÂçá‰∫ÜÂú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÁöÑË°®Áé∞„ÄÇ', title='PEEKÔºöÊèêÂçáÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂÖ≥ÈîÆÁÇπÈ¢ÑÊµã'))
[24.09.2025 17:15] Renaming data file.
[24.09.2025 17:15] Renaming previous data. hf_papers.json to ./d/2025-09-24.json
[24.09.2025 17:15] Saving new data file.
[24.09.2025 17:15] Generating page.
[24.09.2025 17:15] Renaming previous page.
[24.09.2025 17:15] Renaming previous data. index.html to ./d/2025-09-24.html
[24.09.2025 17:15] Writing result.
[24.09.2025 17:15] Renaming log file.
[24.09.2025 17:15] Renaming previous data. log.txt to ./logs/2025-09-24_last_log.txt
