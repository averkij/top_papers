[24.09.2025 06:17] Read previous papers.
[24.09.2025 06:17] Generating top page (month).
[24.09.2025 06:17] Writing top page (month).
[24.09.2025 07:11] Read previous papers.
[24.09.2025 07:11] Get feed.
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19249
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18174
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18644
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18154
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19297
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18849
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18824
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19296
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19284
[24.09.2025 07:11] Extract page data from URL. URL: https://huggingface.co/papers/2509.13835
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19300
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19087
[24.09.2025 07:11] Extract page data from URL. URL: https://huggingface.co/papers/2509.19002
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17321
[24.09.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17083
[24.09.2025 07:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.09.2025 07:11] No deleted papers detected.
[24.09.2025 07:11] Downloading and parsing papers (pdf, html). Total: 15.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.19249.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.19249.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.19249.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.18174.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.18174.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.18174.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.18644.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.18644.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.18644.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.18154.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.18154.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.18154.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.19297.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.19297.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.19297.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.18849.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.18849.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.18849.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.18824.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.18824.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.18824.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.19296.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.19296.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.19296.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.19284.
[24.09.2025 07:11] Extra JSON file exists (./assets/json/2509.19284.json), skip PDF parsing.
[24.09.2025 07:11] Paper image links file exists (./assets/img_data/2509.19284.json), skip HTML parsing.
[24.09.2025 07:11] Success.
[24.09.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2509.13835.
[24.09.2025 07:11] Downloading paper 2509.13835 from http://arxiv.org/pdf/2509.13835v1...
[24.09.2025 07:12] Extracting affiliations from text.
[24.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Minh Duc Bui*1 Carolin Holtermann*2 Valentin Hofmann3,4 Katharina von der Wense1,5 Anne Lauscher2 1Johannes Gutenberg University Mainz, Germany 2Data Science Group, University of Hamburg, Germany 3Allen Institute for AI 4University of Washington, USA 5University of Colorado Boulder, USA minhducbui@uni-mainz.de, carolin.holtermann@uni-hamburg.de 5 2 0 2 7 1 ] . [ 1 5 3 8 3 1 . 9 0 5 2 : r a "
[24.09.2025 07:12] Response: ```python
[
    "Johannes Gutenberg University Mainz, Germany",
    "Data Science Group, University of Hamburg, Germany",
    "Allen Institute for AI",
    "University of Washington, USA",
    "University of Colorado Boulder, USA"
]
```
[24.09.2025 07:12] Deleting PDF ./assets/pdf/2509.13835.pdf.
[24.09.2025 07:12] Success.
[24.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.19300.
[24.09.2025 07:12] Extra JSON file exists (./assets/json/2509.19300.json), skip PDF parsing.
[24.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.19300.json), skip HTML parsing.
[24.09.2025 07:12] Success.
[24.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.19087.
[24.09.2025 07:12] Extra JSON file exists (./assets/json/2509.19087.json), skip PDF parsing.
[24.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.19087.json), skip HTML parsing.
[24.09.2025 07:12] Success.
[24.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.19002.
[24.09.2025 07:12] Downloading paper 2509.19002 from http://arxiv.org/pdf/2509.19002v1...
[24.09.2025 07:12] Extracting affiliations from text.
[24.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 2 0 0 9 1 . 9 0 5 2 : r VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction Hao Wang 1 Eiki Murata 2,3 Lingfang Zhang1 Ayako Sato2 So Fukuda1 Ziqi Yin1 Wentao Hu1 Keisuke Nakao1 Yusuke Nakamura1 Sebastian Zwirner1 Yi-Chia Chen1 Hiroyuki Otomo2 Hiroki Ouchi4,2 Daisuke Kawahara 1Waseda University 2CyberAgent, Inc. 3AI Shift, Inc. 4Nara Institute of Science and Technology https://github.com/nlp-waseda/VIR-Bench "
[24.09.2025 07:12] Response: ```python
["Waseda University", "CyberAgent, Inc.", "AI Shift, Inc.", "Nara Institute of Science and Technology"]
```
[24.09.2025 07:12] Deleting PDF ./assets/pdf/2509.19002.pdf.
[24.09.2025 07:12] Success.
[24.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.17321.
[24.09.2025 07:12] Extra JSON file exists (./assets/json/2509.17321.json), skip PDF parsing.
[24.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.17321.json), skip HTML parsing.
[24.09.2025 07:12] Success.
[24.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.17083.
[24.09.2025 07:12] Extra JSON file exists (./assets/json/2509.17083.json), skip PDF parsing.
[24.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.17083.json), skip HTML parsing.
[24.09.2025 07:12] Success.
[24.09.2025 07:12] Enriching papers with extra data.
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 0. Reinforcement Learning on Pre-Training data (RLPT) optimizes large language models by autonomously exploring meaningful trajectories in pre-training data, improving generalizable reasoning skills without human annotation.  					AI-generated summary 				 The growing disparity between the exponential ...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 1. Baseer, a vision-language model fine-tuned for Arabic document OCR, achieves state-of-the-art performance using a decoder-only strategy and a large-scale dataset, outperforming existing solutions with a WER of 0.25.  					AI-generated summary 				 Arabic document OCR remains a challenging task due t...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 2. A state-free policy using only visual observations achieves better spatial generalization and data efficiency in robot manipulation tasks compared to state-based policies.  					AI-generated summary 				 Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where ...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 3. MiniCPM-V 4.5, a 8B parameter multimodal large language model, achieves high performance and efficiency through a unified 3D-Resampler architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 4. VolSplat, a voxel-aligned Gaussian prediction method, improves novel view synthesis by overcoming pixel alignment limitations and enhancing 3D reconstruction quality.  					AI-generated summary 				 Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view s...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 5. Mixed Advantage Policy Optimization (MAPO) dynamically reweights the advantage function to improve trajectory ranking in reinforcement learning for foundation models.  					AI-generated summary 				 Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimi...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 6. Hyper-Bagel accelerates multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving significant speedups while maintaining high-quality outputs.  					AI-generated summary 				 Unified multimodal models have recently attracted considerable attenti...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 7. A self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.  					AI-generated summary 				 The ability to generate virtual environments is crucial for applications r...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 8. Effective chain-of-thoughts in large reasoning models are characterized by fewer failed steps and better structural quality, not necessarily by length or review.  					AI-generated summary 				 Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, b...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 9. Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.  					AI-generated summary 				 Dialects represent a significant component of human culture and are found across all region...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 10. Condition-Aware Reparameterization for Flow Matching (CAR-Flow) enhances conditional generative modeling by repositioning distributions, leading to faster training and improved performance on image data.  					AI-generated summary 				 Conditional generative modeling aims to learn a conditional data...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 11. A training-free method enables generalist multimodal models to process multi-spectral imagery in a zero-shot manner, enhancing performance on remote sensing tasks.  					AI-generated summary 				 Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use cl...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 12. VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have significantly enhanced video ...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 13. OpenGVL is a benchmark for task progress prediction in robotics using vision-language models, showing open-source models underperform compared to closed-source ones and enabling automated data curation.  					AI-generated summary 				 Data scarcity remains one of the most limiting factors in driving...
[24.09.2025 07:12] ********************************************************************************
[24.09.2025 07:12] Abstract 14. Hybrid Radiance Fields combine explicit Gaussians and neural fields to achieve high-quality rendering with reduced memory usage and real-time performance.  					AI-generated summary 				 Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling r...
[24.09.2025 07:12] Read previous papers.
[24.09.2025 07:12] Generating reviews via LLM API.
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning", "#benchmark", "#optimization", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "RLPT: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (RLPT) Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#cv", "#low_resource", "#multimodal", "#synthetic", "#dataset"], "emoji": "ğŸ“œ", "ru": {"title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Baseer ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ OCR", "desc": "Baseer - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#cv", "#robotics", "#agents", "#optimization"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ—Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#architecture", "#rl", "#agi", "#benchmark", "#optimization", "#multimodal", "#training"], "emoji": "ğŸš€", "ru": {"title": "MiniCPM-V 4.5: ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "MiniCPM-V 4.5 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#benchmark", "#3d", "#games"], "emoji": "ğŸ”", "ru": {"title": "VolSplat: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ²", "desc": "VolSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ñ
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#rlhf", "#training"], "emoji": "âš–ï¸", "ru": {"title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼", "desc": "MAPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²Ğµ
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#training"], "emoji": "ğŸš€", "ru": {"title": "Hyper-Bagel: Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Hyper-Bagel - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ½Ğ° Ğ¸
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#3d", "#robotics", "#games", "#synthetic", "#video"], "emoji": "ğŸ­", "ru": {"title": "3D-Ğ¼Ğ¸Ñ€Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ 3D-Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² 
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#math", "#rl", "#reasoning", "#interpretability", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ° Ğ½Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾: ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought, CoT) Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… 
[24.09.2025 07:12] Querying the API.
[24.09.2025 07:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.  					AI-generated summary 				 Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage.
[24.09.2025 07:12] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ‘Ğ¯Ğœ) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğº Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑĞ¼ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¸Ñ… Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ² Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¸Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½ĞµĞµ, Ñ‡ĞµĞ¼ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ‘Ğ¯Ğœ.",
  "emoji": "ğŸ—£ï¸",
  "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°Ñ…"
}
[24.09.2025 07:12] Renaming some terms.
[24.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.  					AI-generated summary 				 Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage."

[24.09.2025 07:12] Response: ```python
['DATASET', 'BENCHMARK', 'MULTILINGUAL']
```
[24.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models exhibit significant dialect naming and usage bias against German dialect speakers, which is amplified when linguistic demographics are explicitly labeled.  					AI-generated summary 				 Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage."

[24.09.2025 07:12] Response: ```python
['ETHICS']
```
[24.09.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the biases present in large language models (LLMs) against speakers of German dialects. It highlights that these models not only associate negative traits with dialect speakers but also exhibit biased decision-making when processing dialect-related content. The research utilizes a novel evaluation corpus to measure dialect naming and usage biases through specific tasks. The findings reveal that explicitly labeling dialect speakers increases bias, contradicting previous studies that suggested minimal bias with demographic mentions.","title":"Unmasking Bias: Language Models and German Dialects"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the biases present in large language models (LLMs) against speakers of German dialects. It highlights that these models not only associate negative traits with dialect speakers but also exhibit biased decision-making when processing dialect-related content. The research utilizes a novel evaluation corpus to measure dialect naming and usage biases through specific tasks. The findings reveal that explicitly labeling dialect speakers increases bias, contradicting previous studies that suggested minimal bias with demographic mentions.', title='Unmasking Bias: Language Models and German Dialects'))
[24.09.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„åè§ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨æ–¹è¨€å‘½åå’Œä½¿ç”¨ä¸Šå­˜åœ¨æ˜¾è‘—çš„åè§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ–¹è¨€ä½¿ç”¨è€…å¸¸å¸¸é¢ä¸´è´Ÿé¢çš„ç¤¾ä¼šåˆ»æ¿å°è±¡ï¼Œè€Œè¿™äº›åˆ»æ¿å°è±¡åœ¨LLMsä¸­å¾—åˆ°äº†åæ˜ ã€‚é€šè¿‡å…³è”ä»»åŠ¡å’Œå†³ç­–ä»»åŠ¡ï¼Œä½œè€…è¯„ä¼°äº†æ¨¡å‹çš„æ–¹è¨€å‘½ååè§å’Œä½¿ç”¨åè§ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°è¯­æ–™åº“ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ˜ç¡®æ ‡è®°è¯­è¨€äººå£ç»Ÿè®¡ä¿¡æ¯ä¼šåŠ å‰§åè§ï¼Œåæ˜ å‡ºå¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„è´Ÿé¢è”æƒ³ã€‚","title":"å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„åè§ç ”ç©¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„åè§ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨æ–¹è¨€å‘½åå’Œä½¿ç”¨ä¸Šå­˜åœ¨æ˜¾è‘—çš„åè§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ–¹è¨€ä½¿ç”¨è€…å¸¸å¸¸é¢ä¸´è´Ÿé¢çš„ç¤¾ä¼šåˆ»æ¿å°è±¡ï¼Œè€Œè¿™äº›åˆ»æ¿å°è±¡åœ¨LLMsä¸­å¾—åˆ°äº†åæ˜ ã€‚é€šè¿‡å…³è”ä»»åŠ¡å’Œå†³ç­–ä»»åŠ¡ï¼Œä½œè€…è¯„ä¼°äº†æ¨¡å‹çš„æ–¹è¨€å‘½ååè§å’Œä½¿ç”¨åè§ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°è¯­æ–™åº“ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ˜ç¡®æ ‡è®°è¯­è¨€äººå£ç»Ÿè®¡ä¿¡æ¯ä¼šåŠ å‰§åè§ï¼Œåæ˜ å‡ºå¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„è´Ÿé¢è”æƒ³ã€‚', title='å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¾·è¯­æ–¹è¨€ä½¿ç”¨è€…çš„åè§ç ”ç©¶'))
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#cv", "#data", "#training", "#synthetic", "#diffusion"], "emoji": "ğŸ”€", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "CAR-Flow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#science", "#dataset", "#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "ğŸ›°ï¸", "ru": {"title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾
[24.09.2025 07:12] Querying the API.
[24.09.2025 07:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.
[24.09.2025 07:12] Response: {
  "desc": "VIR-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 200 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ insights Ğ¸Ğ· VIR-Bench Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ°Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹.",
  "emoji": "ğŸŒ",
  "title": "VIR-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ˜Ğ˜"
}
[24.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications."

[24.09.2025 07:12] Response: ```python
['BENCHMARK', 'VIDEO', 'MULTIMODAL', 'AGENTS']
```
[24.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VIR-Bench, a new benchmark for travel videos, evaluates and enhances MLLMs' geospatial-temporal intelligence, improving itinerary recommendations in real-world applications.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications."

[24.09.2025 07:12] Response: ```python
["GAMES", "SCIENCE"]
```
[24.09.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VIR-Bench is a new benchmark designed to assess and improve the geospatial-temporal intelligence of multimodal large language models (MLLMs) using travel videos. It consists of 200 videos that focus on long-distance travel, a topic that has been largely overlooked in existing benchmarks. The study shows that even advanced MLLMs struggle with the complexities of itinerary reconstruction from these videos. By developing a travel-planning agent based on insights from VIR-Bench, the research demonstrates significant improvements in itinerary recommendations, highlighting the practical benefits of this evaluation framework.","title":"Enhancing Travel Itinerary Recommendations with VIR-Bench"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VIR-Bench is a new benchmark designed to assess and improve the geospatial-temporal intelligence of multimodal large language models (MLLMs) using travel videos. It consists of 200 videos that focus on long-distance travel, a topic that has been largely overlooked in existing benchmarks. The study shows that even advanced MLLMs struggle with the complexities of itinerary reconstruction from these videos. By developing a travel-planning agent based on insights from VIR-Bench, the research demonstrates significant improvements in itinerary recommendations, highlighting the practical benefits of this evaluation framework.', title='Enhancing Travel Itinerary Recommendations with VIR-Bench'))
[24.09.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VIR-Benchæ˜¯ä¸€ä¸ªæ–°çš„æ—…è¡Œè§†é¢‘åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ï¼Œä»è€Œæ”¹å–„ç°å®åº”ç”¨ä¸­çš„è¡Œç¨‹æ¨èã€‚å½“å‰çš„è§†é¢‘åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å®¤å†…åœºæ™¯æˆ–çŸ­é€”æˆ·å¤–æ´»åŠ¨ä¸Šï¼Œé•¿é€”æ—…è¡Œçš„æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æŒæ¡æ‰©å±•çš„åœ°ç†æ—¶ç©ºè½¨è¿¹å¯¹äºä¸‹ä¸€ä»£MLLMsè‡³å…³é‡è¦ï¼Œè¿™æ”¯æŒäº†è¯¸å¦‚å…·èº«äººå·¥æ™ºèƒ½è§„åˆ’å’Œå¯¼èˆªç­‰ç°å®ä»»åŠ¡ã€‚é€šè¿‡VIR-Benchï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªåŒ…å«200ä¸ªæ—…è¡Œè§†é¢‘çš„åŸºå‡†ï¼Œå°†è¡Œç¨‹é‡å»ºä½œä¸ºä¸€é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œä»¥è¯„ä¼°å’Œæ¨åŠ¨MLLMsçš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ã€‚","title":"æå‡æ—…è¡Œè§†é¢‘ç†è§£çš„åŸºå‡†æŒ‘æˆ˜"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VIR-Benchæ˜¯ä¸€ä¸ªæ–°çš„æ—…è¡Œè§†é¢‘åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ï¼Œä»è€Œæ”¹å–„ç°å®åº”ç”¨ä¸­çš„è¡Œç¨‹æ¨èã€‚å½“å‰çš„è§†é¢‘åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å®¤å†…åœºæ™¯æˆ–çŸ­é€”æˆ·å¤–æ´»åŠ¨ä¸Šï¼Œé•¿é€”æ—…è¡Œçš„æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æŒæ¡æ‰©å±•çš„åœ°ç†æ—¶ç©ºè½¨è¿¹å¯¹äºä¸‹ä¸€ä»£MLLMsè‡³å…³é‡è¦ï¼Œè¿™æ”¯æŒäº†è¯¸å¦‚å…·èº«äººå·¥æ™ºèƒ½è§„åˆ’å’Œå¯¼èˆªç­‰ç°å®ä»»åŠ¡ã€‚é€šè¿‡VIR-Benchï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªåŒ…å«200ä¸ªæ—…è¡Œè§†é¢‘çš„åŸºå‡†ï¼Œå°†è¡Œç¨‹é‡å»ºä½œä¸ºä¸€é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œä»¥è¯„ä¼°å’Œæ¨åŠ¨MLLMsçš„åœ°ç†æ—¶ç©ºæ™ºèƒ½ã€‚', title='æå‡æ—…è¡Œè§†é¢‘ç†è§£çš„åŸºå‡†æŒ‘æˆ˜'))
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#data", "#robotics", "#dataset", "#science"], "emoji": "ğŸ¤–", "ru": {"title": "OpenGVL: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ", "desc": "OpenGVL - ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğµ
[24.09.2025 07:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#3d"], "emoji": "ğŸŒˆ", "ru": {"title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² 3D Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğµ", "desc": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ (HyRF) Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆ
[24.09.2025 07:12] Renaming data file.
[24.09.2025 07:12] Renaming previous data. hf_papers.json to ./d/2025-09-24.json
[24.09.2025 07:12] Saving new data file.
[24.09.2025 07:12] Generating page.
[24.09.2025 07:12] Renaming previous page.
[24.09.2025 07:12] Renaming previous data. index.html to ./d/2025-09-24.html
[24.09.2025 07:12] Writing result.
[24.09.2025 07:12] Renaming log file.
[24.09.2025 07:12] Renaming previous data. log.txt to ./logs/2025-09-24_last_log.txt
