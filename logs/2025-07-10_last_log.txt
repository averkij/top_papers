[10.07.2025 18:17] Read previous papers.
[10.07.2025 18:17] Generating top page (month).
[10.07.2025 18:17] Writing top page (month).
[10.07.2025 19:10] Read previous papers.
[10.07.2025 19:10] Get feed.
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07105
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07095
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06448
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06920
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06457
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07017
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05687
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06804
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24044
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06853
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05455
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06485
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06260
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10251
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07106
[10.07.2025 19:10] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01702
[10.07.2025 19:10] Extract page data from URL. URL: https://huggingface.co/papers/2507.06415
[10.07.2025 19:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.07.2025 19:10] No deleted papers detected.
[10.07.2025 19:10] Downloading and parsing papers (pdf, html). Total: 17.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.07105.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.07105.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.07105.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.07095.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.07095.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.07095.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06448.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.06448.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.06448.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06920.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.06920.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.06920.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06457.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.06457.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.06457.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.07017.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.07017.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.07017.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.05687.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.05687.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.05687.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06804.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.06804.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.06804.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.24044.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2506.24044.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2506.24044.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06853.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.06853.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.06853.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.05455.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.05455.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.05455.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06485.
[10.07.2025 19:10] Downloading paper 2507.06485 from http://arxiv.org/pdf/2507.06485v1...
[10.07.2025 19:10] Extracting affiliations from text.
[10.07.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VIDEO-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning Ziyang Wang* Jaehong Yoon Md Mohaiminul Islam Gedas Bertasius Mohit Bansal 5 2 0 2 9 ] . [ 1 5 8 4 6 0 . 7 0 5 2 : r a https://sites.google.com/cs.unc.edu/videorts2025/ "
[10.07.2025 19:10] Response: []
[10.07.2025 19:10] Extracting affiliations from text.
[10.07.2025 19:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VIDEO-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning Ziyang Wang* Jaehong Yoon Md Mohaiminul Islam Gedas Bertasius Mohit Bansal5 2 0 2 9 ] . [ 1 5 8 4 6 0 . 7 0 5 2 : r ahttps://sites.google.com/cs.unc.edu/videorts2025/Despite advances in reinforcement learning (RL)-based video reasoning with large language models (LLMs), data collection and finetuning remain significant challenges. These methods often rely on large-scale supervised fine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT) annotations, making them costly and hard to scale. To address this, we present VIDEO-RTS, new approach to improve video reasoning capability with drastically improved data efficiency by combining data-efficient RL with video-adaptive test-time scaling (TTS) strategy. Based on observations about the data scaling of RL samples, we skip the resource-intensive SFT step and employ efficient pure-RL training with output-based rewards, requiring no additional annotations or extensive fine-tuning. Furthermore, to utilize computational resources more efficiently, we introduce sparse-to-dense video TTS strategy that improves inference by iteratively adding frames based on output consistency. We validate our approach on multiple video reasoning benchmarks, showing that VIDEO-RTS surpasses existing video reasoning models by an average of 2.4% in accuracy using only 3.6% training samples. For example, VIDEO-RTS achieves 4.2% improvement on Video-Holmes, recent and challenging video reasoning benchmark, and 2.6% improvement on MMVU. Notably, our pure RL training and adaptive video TTS offer complementary strengths, enabling VIDEO-RTSs strong reasoning performance.Large language models (LLMs) have demonstrated strong problem-solving abilities across diverse domains, enabled by techniques such as Chain-ofThought (CoT) reasoning (Wang et al., 2022; Yao et al., 2023) and multi-agent collaboration (Talebirad and Nadiri, 2023; Chen et al., 2024b). Building *Equal contribution. 1 on advances in the language domain, several approaches (Liu et al., 2025; Fei et al., 2024; Feng et al., 2025; Sun et al., 2025; Li et al., 2025) have recently extended them to improve video reasoning capabilities. However, these methods demand high computational costs and lower training efficiency, typically following an extensive twostage recipe: (i) supervised fine-tuning (SFT) on reasoning-focused prompts with step-by-step chainof-thought annotations, followed by (ii) large-scale reinforcement learning using rewards over massive collections of video question-answering data. This pipeline poses substantial computational overhead, particularly in generating long CoT data for video corpus, which limits its scalability for complex, long-term video reasoning tasks. To overcome these limitations and enable efficient video reasoning, we propose VIDEO-RTS, novel approach that integrates data-efficient reinforcement learning with video-adaptive test-time scaling strategies, significantly enhancing reasoning performance while maintaining efficiency. In training, unlike the existing approaches (Wang et al., 2025a; Feng et al., 2025), which rely on large-scale supervised fine-tuning (SFT) data with long CoT annotation, we skip the data generation step and directly utilize pure RL training on simple video question-answering (QA) data. Specifically, we adapt the outcome-supervised RL (group relative preference optimization, GRPO (Shao et al., 2024)), motivated by DeepSeek-R1Zero (DeepSeek-AI, 2025), for its simplicity and effectiveness in aligning model outputs with answer correctness. With merely 6K video-question pairs for RL, our approach matches the performance of the existing SFT+RL framework (Video-R1 (Feng et al., 2025)), which relies on 165K SFT examples plus 4K RL examples, underscoring the effectiveness and training efficiency of VIDEO-RTS. Furthermore, as illustrated in Fig. 3, scaling to even more video QA samples only brings marginal Figure 1: Training and inference recipe comparison between Video-R1 (Feng et al., 2025) and our VIDEO-RTS. While (a) Video-R1 uses two-stage pipeline with SFT and RL, (b) VIDEO-RTS adapts pure-RL approach with output-based rewards for better data efficiency. We further enhance the reasoning of VIDEO-RTS by proposing dynamic sparse-to-dense video test-time scaling. The format reward is omitted, as both models use it. improvements, suggesting that the RL training saturates quickly on video reasoning data. This matches the recent findings in the language domain (Wang et al., 2025c) that very few RL training samples could bring great improvement on reasoning tasks. Thus, inspired by the test-time scaling works (Wang et al., 2022; Yao et al., 2023; Snell et al., 2024) in the language community, we aim to enhance the video reasoning capability at the inference stage to better allocate the computational resources. To the best of our knowledge, this is the first study to systematically explore the combination of reinforcement learning and test-time inference strategies for improving video reasoning capability. To better allocate the excessive training computation, we propose sparse-to-dense test-time scaling mechanism specifically designed for video reasoning. Specifically, VIDEO-RTS adaptively selects the appropriate temporal context based on output consistency by iteratively adding more frames at the inference stage. Taking advantage of the pureRL training, the model is able to generate diverse deep reasoning process given the challenging video query, which allows us to utilize self-consistency check to decide whether the model obtains sufficient temporal context. The combination of efficient training and adaptive inference enables the model to adapt its computational effort based on the complexity of each input query, producing accurate responses while using only the necessary amount of resources. We evaluate VIDEO-RTS on the five popular video reasoning benchmarks, including VideoHolmes(Cheng et al., 2025), Video-MMMU (Hu et al., 2025), MMVU (Zhao et al., 2025), VideoMME (Fu et al., 2024a) and LongVideoBench (Wu et al., 2024). Results show that across all benchmarks, compared to the recent Video-R1 model (Feng et al., 2025) which trained on 169K samples, VIDEO-RTS, trained with only 6K samples (i.e., 96.4% lesser samples), outperforms by 2.4% in average accuracy while using fewer frames during inference. Specifically, on Video-Holmes, "
[10.07.2025 19:10] Mistral response. {"id": "75a749ed63384c4c98bbe60a1661c1cc", "object": "chat.completion", "created": 1752174609, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1733, "total_tokens": 1735, "completion_tokens": 2}}
[10.07.2025 19:10] Response: []
[10.07.2025 19:10] Deleting PDF ./assets/pdf/2507.06485.pdf.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06260.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.06260.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.06260.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.10251.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2505.10251.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2505.10251.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.07106.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.07106.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.07106.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.01702.
[10.07.2025 19:10] Extra JSON file exists (./assets/json/2507.01702.json), skip PDF parsing.
[10.07.2025 19:10] Paper image links file exists (./assets/img_data/2507.01702.json), skip HTML parsing.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2507.06415.
[10.07.2025 19:10] Downloading paper 2507.06415 from http://arxiv.org/pdf/2507.06415v1...
[10.07.2025 19:10] Extracting affiliations from text.
[10.07.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 5 1 4 6 0 . 7 0 5 2 : r PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning Zeming Chen Angelika Romanou Gail Weiss Antoine Bosselut Department of Computer and Communication Science EPFL Lausanne, Switzerland 1015 {zeming.chen, antoine.bosselut}@epfl.ch "
[10.07.2025 19:10] Response: ```python
["Department of Computer and Communication Science EPFL Lausanne, Switzerland"]
```
[10.07.2025 19:10] Deleting PDF ./assets/pdf/2507.06415.pdf.
[10.07.2025 19:10] Success.
[10.07.2025 19:10] Enriching papers with extra data.
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 0. 4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.  					AI-generated summary 				 We present 4KAgent, a unified agentic super-resolution ...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 1. A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.  					AI-generated summary 				 Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamen...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 2. Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has p...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 3. A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.  					AI-generated summary 				 Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 4. Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.  					AI-generated summary 				 Transformers face quadratic complexity and memor...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 5. FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.  					AI-generated summary 				 Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Larg...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 6. Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting l...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 7. A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.  					AI-generated summary 				 Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) ha...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 8. This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.  					AI-generated summary 				 The rapid progress of multimodal large langua...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 9. DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.  					AI-generated summary 				 Molecular structure elucidation from spectra is a foundational problem in chem...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 10. A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.  					AI-generated summary 				 Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it ...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 11. Video-RTS enhances video reasoning efficiency and accuracy through pure RL training and adaptive test-time scaling, reducing data and computational costs.  					AI-generated summary 				 Despite advances in reinforcement learning (RL)-based video reasoning with large language models (LLMs), data col...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 12. Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of large codebases, 400-page documents, and 90-minute videos in a single prompt. We present the first ...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 13. A hierarchical framework combining high-level task planning and low-level trajectory generation enables autonomous surgical procedures with high success rates in ex vivo experiments.  					AI-generated summary 				 Research on autonomous surgery has largely focused on simple task automation in contr...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 14. Text-to-image diffusion models enhance image-based question-answering by providing semantically rich and instruction-aware visual encodings, complementing CLIP and improving spatial and compositional reasoning.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLM...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 15. AdamMeme, an adaptive agent-based framework, evaluates multimodal Large Language Models' understanding of harmful memes through iterative updates and multi-agent collaboration, revealing model-specific weaknesses.  					AI-generated summary 				 The proliferation of multimodal memes in the social me...
[10.07.2025 19:10] ********************************************************************************
[10.07.2025 19:10] Abstract 16. PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.  					AI-generated summary 				 Long-context reasoning requires accurate...
[10.07.2025 19:10] Read previous papers.
[10.07.2025 19:10] Generating reviews via LLM API.
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#low_resource", "#cv", "#open_source", "#benchmark", "#agents", "#healthcare"], "emoji": "üî¨", "ru": {"title": "4KAgent: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —É–ª—É—á—à–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "4KAgent - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–≤–µ—Ä—Ö–≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∏–∑
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#transfer_learning", "#cv", "#benchmark", "#dataset", "#robotics", "#games"], "emoji": "ü§ñ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MotionMillion –∏ —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ MotionMillion-Eval –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#rl", "#rlhf", "#optimization"], "emoji": "üëÅÔ∏è", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Perception-Aware Policy Optimization (PAPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#dataset", "#training", "#games"], "emoji": "üß™", "ru": {"title": "–ß–µ–ª–æ–≤–µ–∫ –∏ –ò–ò –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SAGA –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –∏ –≤–æ–∑–º–æ–∂
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#open_source", "#benchmark", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤–Ω–∏–º–∞–Ω–∏—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "FR3E: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "FR3E - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#rl"], "emoji": "üöÄ", "ru": {"title": "AutoTriton: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —è–¥–µ—Ä –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é RL", "desc": "AutoTriton - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL), –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Triton. –û–Ω–∞ –∏
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#dataset", "#training", "#math"], "emoji": "üß†", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É —Ç–µ–æ—Ä–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É —Ç–µ–æ—Ä–µ–º, —Ä–∞–∑–¥–µ–ª—è—é—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞—Å—Å—É–∂–¥–µ
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#architecture", "#benchmark", "#survey", "#dataset", "#multimodal", "#alignment", "#interpretability"], "emoji": "üöó", "ru": {"title": "VLA: –ù–æ–≤—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π", "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –ø–∞—Ä–∞–¥–∏–≥–º 
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#science", "#multimodal", "#data", "#diffusion", "#3d"], "emoji": "üß™", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–ª–µ–∫—É–ª —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "DiffSpectra - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è 2D –∏ 3D —Å—Ç—Ä—É–∫—Ç—É—Ä –º–æ–ª–µ–∫—É–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–ø–µ–∫—Ç—Ä–∞
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#data", "#open_source", "#ethics", "#dataset", "#training"], "emoji": "üó®Ô∏è", "ru": {"title": "–ò–Ω–∫–ª—é–∑–∏–≤–Ω–∞—è –º–æ–¥–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: —É—á–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç MODELCITIZENS –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ —è–∑—ã–∫–∞, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#video", "#rl", "#reasoning", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é RL –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "Video-RTS - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ –≤–∏–¥–µ–æ —Å drastically –ø–æ–≤—ã—à
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#healthcare", "#benchmark", "#multimodal", "#alignment", "#security"], "emoji": "üî¨", "ru": {"title": "Nova Premier: –º–æ—â–Ω–∞—è –∏ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ò–ò –æ—Ç Amazon", "desc": "Amazon –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ Nova Premier - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω—É—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#robotics", "#agents", "#optimization", "#science", "#agi"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Ö–∏—Ä—É—Ä–≥–∏—è: –æ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∫ —Ç–æ—á–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏—è–º", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Ä–æ–±–æ—Ç–æ–º. –°–∏—Å—Ç–µ–º–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –ø–ª–∞
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#cv", "#benchmark", "#leakage", "#reasoning"], "emoji": "üñºÔ∏è", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–æ–≥—É—Ç —É–ª—É—á—à–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç
[10.07.2025 19:10] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#benchmark", "#reasoning", "#interpretability", "#alignment"], "emoji": "üïµÔ∏è", "ru": {"title": "AdamMeme: –£–º–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–∏–≤ –≤ –º–∏—Ä–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –º–µ–º–æ–≤", "desc": "AdamMeme - —ç—Ç–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –º–µ–º
[10.07.2025 19:10] Querying the API.
[10.07.2025 19:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.  					AI-generated summary 				 Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference.
[10.07.2025 19:10] Response: {
  "desc": "PERK - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –ø–æ–¥—Ö–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –û–Ω –∫–æ–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –≤ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–æ–≤. PERK –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ü–∏–∫–ª–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –∫–æ–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –≤ –∞–¥–∞–ø—Ç–µ—Ä LoRA, –∞ –≤–Ω–µ—à–Ω–∏–π —É—á–∏—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PERK –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –ª–∏–Ω–∏–∏ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –±–æ–ª—å—à—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –¥–ª–∏–Ω—ã.",

  "emoji": "üß†",

  "title": "PERK: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"
}
[10.07.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.  					AI-generated summary 				 Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference."

[10.07.2025 19:10] Response: ```python
['TRAINING', 'ARCHITECTURE', 'SMALL_MODELS']
```
[10.07.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.  					AI-generated summary 				 Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference."

[10.07.2025 19:10] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION", "REASONING"]
```
[10.07.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PERK (Parameter Efficient Reasoning over Knowledge) is a novel approach that enhances long-context reasoning by using lightweight model adapters during test time. It effectively encodes extensive input contexts into a low-rank adapter, allowing for efficient memory usage and improved reasoning capabilities. The method employs two optimization loops: one for quickly adapting the model to the context and another for refining the reasoning process. Evaluations demonstrate that PERK significantly outperforms traditional prompt-based methods, achieving substantial performance gains across various long-context reasoning tasks.","title":"Enhancing Long-Context Reasoning with Efficient Adapters"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PERK (Parameter Efficient Reasoning over Knowledge) is a novel approach that enhances long-context reasoning by using lightweight model adapters during test time. It effectively encodes extensive input contexts into a low-rank adapter, allowing for efficient memory usage and improved reasoning capabilities. The method employs two optimization loops: one for quickly adapting the model to the context and another for refining the reasoning process. Evaluations demonstrate that PERK significantly outperforms traditional prompt-based methods, achieving substantial performance gains across various long-context reasoning tasks.', title='Enhancing Long-Context Reasoning with Efficient Adapters'))
[10.07.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PERKÔºàÂèÇÊï∞È´òÊïàÁü•ËØÜÊé®ÁêÜÔºâÊòØ‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂú®ÊµãËØïÊó∂‰ΩøÁî®ËΩªÈáèÁ∫ßÊ®°ÂûãÈÄÇÈÖçÂô®Êù•Â¢ûÂº∫Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊ¢ØÂ∫¶Êõ¥Êñ∞Âø´ÈÄüÂ∞Ü‰∏ä‰∏ãÊñáÁºñÁ†ÅÂà∞‰ΩéÁß©ÈÄÇÈÖçÂô®‰∏≠Ôºå‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÁöÑÈ´òÊïàËÆ∞ÂøÜÊ®°Âùó„ÄÇPERKÂú®Â§ö‰∏™Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂÖ∂ÊÄßËÉΩÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÂü∫‰∫éÊèêÁ§∫ÁöÑÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Â∞èÂûãÊ®°ÂûãÔºàÂ¶ÇGPT-2Ôºâ‰∏äÔºåÊÄßËÉΩÊèêÂçáÂèØËææ90%„ÄÇÂ∞ΩÁÆ°Âú®ËÆ≠ÁªÉÊó∂ÂÜÖÂ≠òÊ∂àËÄóËæÉÂ§ßÔºå‰ΩÜÂú®Êé®ÁêÜÊó∂ÔºåPERKÁöÑÊïàÁéá‰ºò‰∫éÂü∫‰∫éÊèêÁ§∫ÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜ„ÄÇ","title":"È´òÊïàÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PERKÔºàÂèÇÊï∞È´òÊïàÁü•ËØÜÊé®ÁêÜÔºâÊòØ‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂú®ÊµãËØïÊó∂‰ΩøÁî®ËΩªÈáèÁ∫ßÊ®°ÂûãÈÄÇÈÖçÂô®Êù•Â¢ûÂº∫Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊ¢ØÂ∫¶Êõ¥Êñ∞Âø´ÈÄüÂ∞Ü‰∏ä‰∏ãÊñáÁºñÁ†ÅÂà∞‰ΩéÁß©ÈÄÇÈÖçÂô®‰∏≠Ôºå‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÁöÑÈ´òÊïàËÆ∞ÂøÜÊ®°Âùó„ÄÇPERKÂú®Â§ö‰∏™Èïø‰∏ä‰∏ãÊñáÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂÖ∂ÊÄßËÉΩÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑÂü∫‰∫éÊèêÁ§∫ÁöÑÊñπÊ≥ïÔºåÂ∞§ÂÖ∂Âú®Â∞èÂûãÊ®°ÂûãÔºàÂ¶ÇGPT-2Ôºâ‰∏äÔºåÊÄßËÉΩÊèêÂçáÂèØËææ90%„ÄÇÂ∞ΩÁÆ°Âú®ËÆ≠ÁªÉÊó∂ÂÜÖÂ≠òÊ∂àËÄóËæÉÂ§ßÔºå‰ΩÜÂú®Êé®ÁêÜÊó∂ÔºåPERKÁöÑÊïàÁéá‰ºò‰∫éÂü∫‰∫éÊèêÁ§∫ÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜ„ÄÇ', title='È´òÊïàÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï'))
[10.07.2025 19:10] Renaming data file.
[10.07.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-07-10.json
[10.07.2025 19:10] Saving new data file.
[10.07.2025 19:10] Generating page.
[10.07.2025 19:10] Renaming previous page.
[10.07.2025 19:10] Renaming previous data. index.html to ./d/2025-07-10.html
[10.07.2025 19:10] Writing result.
[10.07.2025 19:10] Renaming log file.
[10.07.2025 19:10] Renaming previous data. log.txt to ./logs/2025-07-10_last_log.txt
