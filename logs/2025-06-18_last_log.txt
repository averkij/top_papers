[18.06.2025 22:11] Read previous papers.
[18.06.2025 22:11] Generating top page (month).
[18.06.2025 22:11] Writing top page (month).
[18.06.2025 23:11] Read previous papers.
[18.06.2025 23:11] Get feed.
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14028
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12928
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12285
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14429
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14245
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14234
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13363
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13642
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14758
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12278
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14603
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12860
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14606
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13977
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09985
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13651
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10100
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14002
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10038
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05336
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14755
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09033
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14761
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14731
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14702
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13599
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05426
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13901
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13387
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12880
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14629
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13922
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12015
[18.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03939
[18.06.2025 23:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.06.2025 23:11] No deleted papers detected.
[18.06.2025 23:11] Downloading and parsing papers (pdf, html). Total: 34.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14028.
[18.06.2025 23:11] Downloading paper 2506.14028 from None...
[18.06.2025 23:11] Error downloading PDF (None). Details: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14028.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.12928.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.12928.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.12928.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.12285.
[18.06.2025 23:11] Downloading paper 2506.12285 from None...
[18.06.2025 23:11] Error downloading PDF (None). Details: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.12285.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14429.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14429.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14429.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14245.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14245.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14245.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14234.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14234.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14234.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13363.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13363.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13363.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13642.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13642.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13642.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14758.
[18.06.2025 23:11] Downloading paper 2506.14758 from None...
[18.06.2025 23:11] Error downloading PDF (None). Details: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14758.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.12278.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.12278.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.12278.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14603.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14603.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14603.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.12860.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.12860.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.12860.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14606.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14606.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14606.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13977.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13977.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13977.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.09985.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.09985.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.09985.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13651.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13651.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13651.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.10100.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.10100.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.10100.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14002.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14002.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14002.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.10038.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.10038.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.10038.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.05336.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.05336.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.05336.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14755.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14755.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14755.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.09033.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.09033.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.09033.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14761.
[18.06.2025 23:11] Downloading paper 2506.14761 from None...
[18.06.2025 23:11] Error downloading PDF (None). Details: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14761.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14731.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14731.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14731.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14702.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14702.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14702.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13599.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13599.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13599.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.05426.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.05426.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.05426.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13901.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13901.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13901.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13387.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13387.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13387.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.12880.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.12880.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.12880.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14629.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14629.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14629.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13922.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13922.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13922.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.12015.
[18.06.2025 23:11] Extra JSON file exists (./assets/json/2506.12015.json), skip PDF parsing.
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.12015.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.03939.
[18.06.2025 23:11] Downloading paper 2506.03939 from None...
[18.06.2025 23:11] Error downloading PDF (None). Details: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
[18.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.03939.json), skip HTML parsing.
[18.06.2025 23:11] Success.
[18.06.2025 23:11] Enriching papers with extra data.
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 0. MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.  					AI-generated summary 				 Recent advances in large language models (LL...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 1. Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  					AI-generated summary 				 Scaling test time ...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 2. CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.  					AI-generated summary 				 Recent advances in audio-text large language models (LLMs) have opened new possibilities for music unders...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 3. This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  					AI-generated summary 				 Large Language Diffusion Models, or diffusion LL...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 4. RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for ad...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 5. Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  					AI-generated summary 				 Despite impressive progress on complex r...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 6. An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  					AI-generated summary 				 Visual Information Extraction (VIE) converts unstructured document image...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 7. Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  					AI-generated summary 				 The emergence of GPT-4o...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 8. Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  					AI-generated summary 				 Balancing exploration and exploitation is a central goal in reinforceme...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 9. TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  					AI-generated summary 				 We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm pro...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 10. Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  					AI-generated summary 				 Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, bu...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 11. Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  					AI-generated summary 				 Recent advancements in Long Chain-of...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 12. A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  					AI-generated summary 				 The hardware ecosystem is rapidly evolving, with increasing interest in translating l...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 13. A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  					AI-generated summary 				 The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range o...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 14. A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  					AI-generated summary 				 A major challenge...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 15. We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professio...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 16. EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  					AI-generated summary 				 Vision-Language-Action (VLA) models, particularly diffusion-based architectures,...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 17. A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  					AI-generated summary 				 We study the challenge of achieving theoreti...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 18. Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  					AI-generated summary 				 We show how to use low-quality, synthetic, and out-of-distribution ima...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 19. VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  					AI-generated summary 				 Spatio-temporal localization is vital for precise interactions ...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 20. LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  					AI-generated summary 				 Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unneces...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 21. Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.  					AI-generated summary 				 The rapid emergence of diverse large language models (LLMs) has spurred ...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 22. An autoregressive U-Net learns to embed its own tokens during training, enabling a multi-scale view of text sequences and improved handling of character-level tasks and low-resource languages.  					AI-generated summary 				 Tokenization imposes a fixed granularity on the input text, freezing how a ...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 23. Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  					AI-generated summary 				 We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model op...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 24. A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  					AI-generated summary 				 One of the most profound challenges of modern machine learning is performing well on...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 25. CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  					AI-generated summary 				 Human mobility simulation plays a crucial role in various real-world applications. Recentl...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 26. T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.  					AI-generated summary 				 In-context reinforcement learning (ICRL) has emerged as a promising paradigm for ada...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 27. A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  					AI-generated summary 				 ...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 28. A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  					AI-generated summary 				 This work presents a generalizable framework to transfer relative depth to met...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 29. Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.  					AI-generated summary 				 We study suffix-based jailbreaksx2013a powerful...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 30. VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.  					AI-generated summary 				 Mosquito-borne diseases pose a major global health ris...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 31. DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  					AI-generated summary 				 Deploying large, complex polici...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 32. EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.  					AI-generated summary 				 Open-source foundation models have seen rapid adoption and development, enabling powerful genera...
[18.06.2025 23:11] ********************************************************************************
[18.06.2025 23:11] Abstract 33. Graph Counselor enhances Large Language Models by using multi-agent collaboration and adaptive reasoning to integrate knowledge effectively, improving factual accuracy and generation quality in specialized domains.  					AI-generated summary 				 Graph Retrieval Augmented Generation (GraphRAG) effec...
[18.06.2025 23:11] Read previous papers.
[18.06.2025 23:11] Generating reviews via LLM API.
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#multimodal", "#reasoning", "#financial", "#dataset"], "emoji": "üíπ", "ru": {"title": "MultiFinBen: –ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è —è–∑—ã–∫–æ–≤—ã–µ –∏ –º–æ–¥–∞–ª—å–Ω—ã–µ –±–∞—Ä—å–µ—Ä—ã –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º –ò–ò", "desc": "MultiFinBen - —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∑–∞–¥–∞—á –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agents", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–∫–∞–∑
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#open_source", "#survey", "#audio", "#benchmark", "#ethics"], "emoji": "üéµ", "ru": {"title": "CMI-Bench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö LLM", "desc": "CMI-Bench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞—É–¥–∏–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#training", "#long_context", "#architecture", "#benchmark", "#diffusion", "#rl"], "emoji": "üî¨", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "RLVR: –ø—É—Ç—å –∫ –ª–æ–≥–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Reinforcement Learning with Verifiable Rewards (RLVR). 
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#training", "#agents", "#agi", "#open_source", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "Xolver: –û–ø—ã—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "Xolver - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#optimization", "#healthcare", "#reasoning", "#rl", "#multimodal"], "emoji": "üè•", "ru": {"title": "RLVR: –ü—Ä–æ—Ä—ã–≤ –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ RLVR –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2.5-VL-7B –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–∑—É–∞
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#audio", "#transfer_learning", "#cv", "#benchmark", "#agi"], "emoji": "üîÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –º–æ—â–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π", "desc": "Stream-Omni - —ç—Ç–æ –∫—Ä—É–ø–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ä–µ—á—å. –û
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠–Ω—Ç—Ä–æ–ø–∏—è –∫–∞–∫ –∫–ª—é—á –∫ –≥–ª—É–±–æ–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#benchmark"], "emoji": "üß™", "ru": {"title": "TestCase-Eval: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –Ø–ú –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤", "desc": "TestCase-Eval - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ø–ú) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#optimization", "#diffusion", "#small_models"], "emoji": "üåä", "ru": {"title": "Flow maps: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'flow maps'. 
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#training", "#math", "#long_context", "#reasoning", "#low_resource"], "emoji": "üß†", "ru": {"title": "QFFT: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò –≥–∏–±–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - Question-Free Fine-Tuning (QFFT). QFFT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —ç—Ñ—Ñ–µ
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#benchmark", "#data", "#science"], "emoji": "üîÑ", "ru": {"title": "–Ø–ú–ë –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º –º–µ–∂–¥—É —Ä–∞–∑–ª–∏
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#dataset", "#optimization"], "emoji": "üõ†Ô∏è", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏", "desc": "CRITICTOOL - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ –æ–±
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#games", "#transfer_learning", "#dataset", "#agi", "#cv", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º –º–∏—Ä–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π –º–∞—Å—à—Ç–∞–±–Ω—ã–µ 
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "üìä", "ru": {"title": "xbench: –æ—Ü–µ–Ω–∫–∞ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω xbench - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, 
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#architecture", "#inference", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "EfficientVLA: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π VLA –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "EfficientVLA - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–µ–π Vision-Language-Action (VLA). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –æ—Å
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#interpretability", "#math", "#optimization"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#training", "#cv", "#dataset", "#synthetic", "#diffusion", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª—å–∑—ã –∏–∑ —à—É–º–∞: —É–ª—É—á—à–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Ambient Diffusion Omni, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#interpretability", "#benchmark", "#open_source", "#reasoning", "#video", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "VideoMolmo - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#architecture", "#benchmark", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "LC-R1: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "LC-R1 - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#multimodal", "#optimization", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "Router-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#architecture", "#data", "#optimization", "#low_resource", "#multilingual"], "emoji": "üß†", "ru": {"title": "–ì–∏–±–∫–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–∞—Å—Å–∏–≤–Ω—É—é U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –≤—Å—Ç—Ä–∞–∏–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#reasoning", "#open_source", "#rl", "#architecture"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –º–µ–Ω—å—à–∏–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏", "desc": "Ring-lite - —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts (MoE), –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#training"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ–¥–∫–∏—Ö —Å–ª—É—á–∞–µ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∏ –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#agents", "#synthetic", "#reasoning", "#multimodal"], "emoji": "üèôÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥—Å–∫–æ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ –≥–æ—Ä–æ
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#rl", "#games", "#multimodal", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "T2MIR: –°–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ", "desc": "T2MIR - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (ICRL), –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Å–º–µ—Å—å —ç–∫—Å
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#security", "#rlhf", "#alignment", "#benchmark", "#dataset", "#open_source"], "emoji": "üéØ", "ru": {"title": "AQI: –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (alignment) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –ò–Ω–¥–µ–∫—Å –ö–∞—á–µ—Å—Ç
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#transfer_learning", "#cv", "#multimodal"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "TR2M - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤ –º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#security", "#hallucinations", "#multimodal", "#open_source", "#alignment", "#data"], "emoji": "üîì", "ru": {"title": "–£—è–∑–≤–∏–º–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∞—Ç–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞—Ç–∞–∫–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ –ø—Ä–æ—Ç–∏–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#games", "#dataset", "#open_source", "#multimodal", "#healthcare", "#reasoning"], "emoji": "ü¶ü", "ru": {"title": "–ò–ò –Ω–∞ —Å—Ç—Ä–∞–∂–µ –∑–¥–æ—Ä–æ–≤—å—è: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ —É–≥—Ä–æ–∑—ã –∫–æ–º–∞—Ä–æ–≤", "desc": "VisText-Mosquito - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∞–≤
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#agents", "#diffusion", "#optimization", "#training", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "DynaGuide: –≥–∏–±–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –≤–Ω–µ—à–Ω–µ–π –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "DynaGuide - —ç—Ç–æ –º–µ—Ç–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –ø–æ–ª–∏—Ç–∏–∫–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–µ–π –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏. –û
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#dataset", "#open_source", "#inference"], "emoji": "üß†", "ru": {"title": "–¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–±—ã—á–Ω–æ–º –ü–ö", "desc": "EMLoC - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ –ø–∞–º—è—Ç–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ-–æ
[18.06.2025 23:11] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#graphs", "#rag"], "emoji": "üß†", "ru": {"title": "–£–º–Ω—ã–µ –≥—Ä–∞—Ñ—ã –¥–ª—è —É–º–Ω—ã—Ö –º–∞—à–∏–Ω: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò", "desc": "Graph Counselor - —ç—Ç–æ –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏ –∞
[18.06.2025 23:11] Renaming data file.
[18.06.2025 23:11] Renaming previous data. hf_papers.json to ./d/2025-06-18.json
[18.06.2025 23:11] Saving new data file.
[18.06.2025 23:11] Generating page.
[18.06.2025 23:11] Renaming previous page.
[18.06.2025 23:11] Renaming previous data. index.html to ./d/2025-06-18.html
[18.06.2025 23:11] Writing result.
[18.06.2025 23:11] Renaming log file.
[18.06.2025 23:11] Renaming previous data. log.txt to ./logs/2025-06-18_last_log.txt
