[18.06.2025 13:29] Read previous papers.
[18.06.2025 13:29] Generating top page (month).
[18.06.2025 13:29] Writing top page (month).
[18.06.2025 14:12] Read previous papers.
[18.06.2025 14:12] Get feed.
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12928
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14429
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14245
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14234
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13363
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13642
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14758
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12860
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14603
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12278
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14606
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13977
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13651
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10100
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05336
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14002
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10038
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14755
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09033
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14731
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14702
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13599
[18.06.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.09985
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05426
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13901
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14629
[18.06.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.13922
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13387
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12880
[18.06.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.12015
[18.06.2025 14:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.06.2025 14:12] No deleted papers detected.
[18.06.2025 14:12] Downloading and parsing papers (pdf, html). Total: 30.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.12928.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.12928.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.12928.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14429.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14429.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14429.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14245.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14245.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14245.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14234.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14234.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14234.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.13363.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.13363.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.13363.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.13642.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.13642.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.13642.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14758.
[18.06.2025 14:12] Downloading paper 2506.14758 from http://arxiv.org/pdf/2506.14758v1...
[18.06.2025 14:12] Failed to download and parse paper https://huggingface.co/papers/2506.14758: 'LTChar' object is not iterable
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.12860.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.12860.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.12860.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14603.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14603.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14603.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.12278.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.12278.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.12278.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14606.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14606.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14606.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.13977.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.13977.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.13977.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.13651.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.13651.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.13651.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.10100.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.10100.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.10100.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.05336.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.05336.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.05336.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14002.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14002.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14002.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.10038.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.10038.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.10038.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14755.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14755.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14755.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.09033.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.09033.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.09033.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14731.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14731.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14731.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.14702.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.14702.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.14702.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.13599.
[18.06.2025 14:12] Extra JSON file exists (./assets/json/2506.13599.json), skip PDF parsing.
[18.06.2025 14:12] Paper image links file exists (./assets/img_data/2506.13599.json), skip HTML parsing.
[18.06.2025 14:12] Success.
[18.06.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2506.09985.
[18.06.2025 14:13] Downloading paper 2506.09985 from http://arxiv.org/pdf/2506.09985v1...
[18.06.2025 14:13] Extracting affiliations from text.
[18.06.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 5 8 9 9 0 . 6 0 5 2 : r V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning Mahmoud Assran1,, Adrien Bardes1,, David Fan1,, Quentin Garrido1,, Russell Howes1,, Mojtaba Komeili1,, Matthew Muckley1,, Ammar Rizvi1,, Claire Roberts1,, Koustuv Sinha1,, Artem Zholus1,2,, Sergio Arnaud1,, Abha Gejji1,, Ada Martin1,, Francois Robert Hogan1,, Daniel Dugas1,, Piotr Bojanowski1, Vasil Khalidov1, Patrick Labatut1, Francisco Massa1, Marc Szafraniec1, Kapil Krishnakumar1, Yong Li1, Xiaodong Ma1, Sarath Chandar2, Franziska Meier1,, Yann LeCun1,, Michael Rabbat1,, Nicolas Ballas1, 1FAIR at Meta, 2Mila Quebec AI Institute and Polytechnique Montr√©al Core Team major challenge for modern AI is to learn to understand the world and learn to act largely by observation (LeCun, 2022). This paper explores self-supervised approach that combines internet-scale video data with small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free jointembedding-predictive architecture, V-JEPA 2, on video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two d"
[18.06.2025 14:13] Response: ```python
["FAIR at Meta", "Mila Quebec AI Institute and Polytechnique Montr√©al"]
```
[18.06.2025 14:13] Deleting PDF ./assets/pdf/2506.09985.pdf.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2506.05426.
[18.06.2025 14:13] Extra JSON file exists (./assets/json/2506.05426.json), skip PDF parsing.
[18.06.2025 14:13] Paper image links file exists (./assets/img_data/2506.05426.json), skip HTML parsing.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2506.13901.
[18.06.2025 14:13] Extra JSON file exists (./assets/json/2506.13901.json), skip PDF parsing.
[18.06.2025 14:13] Paper image links file exists (./assets/img_data/2506.13901.json), skip HTML parsing.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2506.14629.
[18.06.2025 14:13] Extra JSON file exists (./assets/json/2506.14629.json), skip PDF parsing.
[18.06.2025 14:13] Paper image links file exists (./assets/img_data/2506.14629.json), skip HTML parsing.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2506.13922.
[18.06.2025 14:13] Downloading paper 2506.13922 from http://arxiv.org/pdf/2506.13922v1...
[18.06.2025 14:13] Extracting affiliations from text.
[18.06.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 2 2 9 3 1 . 6 0 5 2 : r DynaGuide: Steering Diffusion Polices with Maximilian Du Stanford University maxjdu@stanford.edu Shuran Song Stanford University shuran@stanford.edu dynaguide.github.io "
[18.06.2025 14:13] Response: ```python
["Stanford University"]
```
[18.06.2025 14:13] Deleting PDF ./assets/pdf/2506.13922.pdf.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2506.13387.
[18.06.2025 14:13] Extra JSON file exists (./assets/json/2506.13387.json), skip PDF parsing.
[18.06.2025 14:13] Paper image links file exists (./assets/img_data/2506.13387.json), skip HTML parsing.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2506.12880.
[18.06.2025 14:13] Extra JSON file exists (./assets/json/2506.12880.json), skip PDF parsing.
[18.06.2025 14:13] Paper image links file exists (./assets/img_data/2506.12880.json), skip HTML parsing.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Downloading and parsing paper https://huggingface.co/papers/2506.12015.
[18.06.2025 14:13] Extra JSON file exists (./assets/json/2506.12015.json), skip PDF parsing.
[18.06.2025 14:13] Paper image links file exists (./assets/img_data/2506.12015.json), skip HTML parsing.
[18.06.2025 14:13] Success.
[18.06.2025 14:13] Enriching papers with extra data.
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 0. Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  					AI-generated summary 				 Scaling test time ...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 1. This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  					AI-generated summary 				 Large Language Diffusion Models, or diffusion LL...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 2. RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for ad...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 3. Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  					AI-generated summary 				 Despite impressive progress on complex r...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 4. An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  					AI-generated summary 				 Visual Information Extraction (VIE) converts unstructured document image...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 5. Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  					AI-generated summary 				 The emergence of GPT-4o...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 6. Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  					AI-generated summary 				 Balancing exploration and exploitation is a central goal in reinforceme...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 7. Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  					AI-generated summary 				 Recent advancements in Long Chain-of...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 8. Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  					AI-generated summary 				 Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, bu...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 9. TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  					AI-generated summary 				 We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm pro...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 10. A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  					AI-generated summary 				 The hardware ecosystem is rapidly evolving, with increasing interest in translating l...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 11. A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  					AI-generated summary 				 The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range o...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 12. We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professio...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 13. EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  					AI-generated summary 				 Vision-Language-Action (VLA) models, particularly diffusion-based architectures,...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 14. VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  					AI-generated summary 				 Spatio-temporal localization is vital for precise interactions ...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 15. A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  					AI-generated summary 				 We study the challenge of achieving theoreti...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 16. Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  					AI-generated summary 				 We show how to use low-quality, synthetic, and out-of-distribution ima...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 17. LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  					AI-generated summary 				 Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unneces...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 18. Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.  					AI-generated summary 				 The rapid emergence of diverse large language models (LLMs) has spurred ...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 19. Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  					AI-generated summary 				 We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model op...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 20. A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  					AI-generated summary 				 One of the most profound challenges of modern machine learning is performing well on...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 21. CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  					AI-generated summary 				 Human mobility simulation plays a crucial role in various real-world applications. Recentl...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 22. A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  					AI-generated summary 				 A major challenge...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 23. T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.  					AI-generated summary 				 In-context reinforcement learning (ICRL) has emerged as a promising paradigm for ada...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 24. A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  					AI-generated summary 				 ...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 25. VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.  					AI-generated summary 				 Mosquito-borne diseases pose a major global health ris...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 26. DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  					AI-generated summary 				 Deploying large, complex polici...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 27. A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  					AI-generated summary 				 This work presents a generalizable framework to transfer relative depth to met...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 28. Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.  					AI-generated summary 				 We study suffix-based jailbreaksx2013a powerful...
[18.06.2025 14:13] ********************************************************************************
[18.06.2025 14:13] Abstract 29. EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.  					AI-generated summary 				 Open-source foundation models have seen rapid adoption and development, enabling powerful genera...
[18.06.2025 14:13] Read previous papers.
[18.06.2025 14:13] Generating reviews via LLM API.
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agents", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–∫–∞–∑
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#training", "#long_context", "#architecture", "#benchmark", "#diffusion", "#rl"], "emoji": "üî¨", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "RLVR: –ø—É—Ç—å –∫ –ª–æ–≥–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Reinforcement Learning with Verifiable Rewards (RLVR). 
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#training", "#agents", "#agi", "#open_source", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "Xolver: –û–ø—ã—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "Xolver - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#optimization", "#healthcare", "#reasoning", "#rl", "#multimodal"], "emoji": "üè•", "ru": {"title": "RLVR: –ü—Ä–æ—Ä—ã–≤ –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ RLVR –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2.5-VL-7B –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–∑—É–∞
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#multimodal", "#audio", "#transfer_learning", "#cv", "#benchmark", "#agi"], "emoji": "üîÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –º–æ—â–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π", "desc": "Stream-Omni - —ç—Ç–æ –∫—Ä—É–ø–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ä–µ—á—å. –û
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#training", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠–Ω—Ç—Ä–æ–ø–∏—è –∫–∞–∫ –∫–ª—é—á –∫ –≥–ª—É–±–æ–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#training", "#math", "#long_context", "#reasoning", "#low_resource"], "emoji": "üß†", "ru": {"title": "QFFT: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò –≥–∏–±–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - Question-Free Fine-Tuning (QFFT). QFFT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —ç—Ñ—Ñ–µ
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#optimization", "#diffusion", "#small_models"], "emoji": "üåä", "ru": {"title": "Flow maps: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'flow maps'. 
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#benchmark"], "emoji": "üß™", "ru": {"title": "TestCase-Eval: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –Ø–ú –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤", "desc": "TestCase-Eval - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ø–ú) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#benchmark", "#data", "#science"], "emoji": "üîÑ", "ru": {"title": "–Ø–ú–ë –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç—Ä–∞–Ω—Å–ø–∏–ª—è—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º –º–µ–∂–¥—É —Ä–∞–∑–ª–∏
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#dataset", "#optimization"], "emoji": "üõ†Ô∏è", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏", "desc": "CRITICTOOL - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ –æ–±
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "üìä", "ru": {"title": "xbench: –æ—Ü–µ–Ω–∫–∞ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω xbench - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, 
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#architecture", "#inference", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "EfficientVLA: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π VLA –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "EfficientVLA - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–µ–π Vision-Language-Action (VLA). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –æ—Å
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#interpretability", "#benchmark", "#open_source", "#reasoning", "#video", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "VideoMolmo - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#training", "#architecture", "#interpretability", "#math", "#optimization"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#training", "#cv", "#dataset", "#synthetic", "#diffusion", "#data"], "emoji": "üñºÔ∏è", "ru": {"title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª—å–∑—ã –∏–∑ —à—É–º–∞: —É–ª—É—á—à–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Ambient Diffusion Omni, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#reasoning", "#training", "#architecture", "#benchmark", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "LC-R1: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "LC-R1 - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#multimodal", "#optimization", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "Router-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#reasoning", "#open_source", "#rl", "#architecture"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –º–µ–Ω—å—à–∏–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏", "desc": "Ring-lite - —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts (MoE), –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#training"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ–¥–∫–∏—Ö —Å–ª—É—á–∞–µ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∏ –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#agents", "#synthetic", "#reasoning", "#multimodal"], "emoji": "üèôÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥—Å–∫–æ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ –≥–æ—Ä–æ
[18.06.2025 14:13] Querying the API.
[18.06.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  					AI-generated summary 				 A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.
[18.06.2025 14:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ø–æ–¥—Ö–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π –º–∞—Å—à—Ç–∞–±–Ω—ã–µ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —Ä–æ–±–æ—Ç–∞. –ú–æ–¥–µ–ª—å V-JEPA 2 –ø—Ä–µ–¥–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 1 –º–∏–ª–ª–∏–æ–Ω–µ —á–∞—Å–æ–≤ –≤–∏–¥–µ–æ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. –ü–æ—Å–ª–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é, V-JEPA 2 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–∏–¥–µ–æ. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 62 —á–∞—Å–∞—Ö –≤–∏–¥–µ–æ —Å —Ä–æ–±–æ—Ç–∞–º–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ V-JEPA 2-AC –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤ –±–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –∑–∞–¥–∞—á—É.",
  "emoji": "ü§ñ",
  "title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º –º–∏—Ä–µ"
}
[18.06.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  					AI-generated summary 				 A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world."

[18.06.2025 14:13] Response: ```python
['DATASET', 'CV', 'RL', 'ROBOTICS', 'MULTIMODAL']
```
[18.06.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  					AI-generated summary 				 A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world."

[18.06.2025 14:13] Response: ```python
["AGI", "GAMES", "TRANSFER_LEARNING"]
```
[18.06.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a self-supervised learning method that leverages vast amounts of internet video data alongside minimal robot interaction to enhance motion understanding, action anticipation, and robotic planning. The authors introduce V-JEPA 2, a joint-embedding-predictive architecture pre-trained on over 1 million hours of video, achieving impressive results in various tasks without the need for specific training or rewards. By aligning V-JEPA 2 with a large language model, they also achieve state-of-the-art performance in video question-answering tasks. Furthermore, they demonstrate the application of this approach in robotic planning, enabling robots to perform tasks like object manipulation using learned models without additional data collection.","title":"Learning to Act by Watching: Self-Supervised Motion Understanding and Planning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a self-supervised learning method that leverages vast amounts of internet video data alongside minimal robot interaction to enhance motion understanding, action anticipation, and robotic planning. The authors introduce V-JEPA 2, a joint-embedding-predictive architecture pre-trained on over 1 million hours of video, achieving impressive results in various tasks without the need for specific training or rewards. By aligning V-JEPA 2 with a large language model, they also achieve state-of-the-art performance in video question-answering tasks. Furthermore, they demonstrate the application of this approach in robotic planning, enabling robots to perform tasks like object manipulation using learned models without additional data collection.', title='Learning to Act by Watching: Self-Supervised Motion Understanding and Planning'))
[18.06.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÁªìÂêà‰∫Ü‰∫íËÅîÁΩëËßÜÈ¢ëÊï∞ÊçÆÂíåÂ∞ëÈáèÊú∫Âô®‰∫∫‰∫§‰∫íÊï∞ÊçÆÔºå‰ª•ÂÆûÁé∞ËøêÂä®ÁêÜËß£„ÄÅÂä®‰ΩúÈ¢ÑÊµã„ÄÅËßÜÈ¢ëÈóÆÁ≠îÂíåÊú∫Âô®‰∫∫ËßÑÂàíÁ≠â‰ªªÂä°„ÄÇÊàë‰ª¨È¶ñÂÖàÂú®Ë∂ÖËøá100‰∏áÂ∞èÊó∂ÁöÑËßÜÈ¢ëÊï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Êó†Âä®‰ΩúÁöÑËÅîÂêàÂµåÂÖ•È¢ÑÊµãÊû∂ÊûÑV-JEPA 2ÔºåÂèñÂæó‰∫ÜËøêÂä®ÁêÜËß£Âíå‰∫∫Á±ªÂä®‰ΩúÈ¢ÑÊµãÁöÑ‰ºòÂºÇË°®Áé∞„ÄÇÈÄöËøá‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩêÔºåV-JEPA 2Âú®Â§ö‰∏™ËßÜÈ¢ëÈóÆÁ≠î‰ªªÂä°‰∏ä‰πüËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞ÜËá™ÁõëÁù£Â≠¶‰π†Â∫îÁî®‰∫éÊú∫Âô®‰∫∫ËßÑÂàí‰ªªÂä°ÔºåÊàêÂäüÂÆûÁé∞‰∫ÜÂú®‰∏çÂêåÂÆûÈ™åÂÆ§‰∏≠‰ΩøÁî®V-JEPA 2-ACËøõË°åÁâ©‰ΩìÁöÑÊäìÂèñÂíåÊîæÁΩÆÔºåËÄåÊó†ÈúÄÁâπÂÆö‰ªªÂä°ÁöÑËÆ≠ÁªÉÊàñÂ•ñÂä±„ÄÇ","title":"Ëá™ÁõëÁù£Â≠¶‰π†Ôºö‰ªéËßÜÈ¢ëÂà∞Êú∫Âô®‰∫∫ËßÑÂàíÁöÑÁ™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÁªìÂêà‰∫Ü‰∫íËÅîÁΩëËßÜÈ¢ëÊï∞ÊçÆÂíåÂ∞ëÈáèÊú∫Âô®‰∫∫‰∫§‰∫íÊï∞ÊçÆÔºå‰ª•ÂÆûÁé∞ËøêÂä®ÁêÜËß£„ÄÅÂä®‰ΩúÈ¢ÑÊµã„ÄÅËßÜÈ¢ëÈóÆÁ≠îÂíåÊú∫Âô®‰∫∫ËßÑÂàíÁ≠â‰ªªÂä°„ÄÇÊàë‰ª¨È¶ñÂÖàÂú®Ë∂ÖËøá100‰∏áÂ∞èÊó∂ÁöÑËßÜÈ¢ëÊï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Êó†Âä®‰ΩúÁöÑËÅîÂêàÂµåÂÖ•È¢ÑÊµãÊû∂ÊûÑV-JEPA 2ÔºåÂèñÂæó‰∫ÜËøêÂä®ÁêÜËß£Âíå‰∫∫Á±ªÂä®‰ΩúÈ¢ÑÊµãÁöÑ‰ºòÂºÇË°®Áé∞„ÄÇÈÄöËøá‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩêÔºåV-JEPA 2Âú®Â§ö‰∏™ËßÜÈ¢ëÈóÆÁ≠î‰ªªÂä°‰∏ä‰πüËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞ÜËá™ÁõëÁù£Â≠¶‰π†Â∫îÁî®‰∫éÊú∫Âô®‰∫∫ËßÑÂàí‰ªªÂä°ÔºåÊàêÂäüÂÆûÁé∞‰∫ÜÂú®‰∏çÂêåÂÆûÈ™åÂÆ§‰∏≠‰ΩøÁî®V-JEPA 2-ACËøõË°åÁâ©‰ΩìÁöÑÊäìÂèñÂíåÊîæÁΩÆÔºåËÄåÊó†ÈúÄÁâπÂÆö‰ªªÂä°ÁöÑËÆ≠ÁªÉÊàñÂ•ñÂä±„ÄÇ', title='Ëá™ÁõëÁù£Â≠¶‰π†Ôºö‰ªéËßÜÈ¢ëÂà∞Êú∫Âô®‰∫∫ËßÑÂàíÁöÑÁ™ÅÁ†¥'))
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#optimization", "#rl", "#games", "#multimodal", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "T2MIR: –°–º–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ", "desc": "T2MIR - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (ICRL), –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Å–º–µ—Å—å —ç–∫—Å
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#security", "#rlhf", "#alignment", "#benchmark", "#dataset", "#open_source"], "emoji": "üéØ", "ru": {"title": "AQI: –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (alignment) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –ò–Ω–¥–µ–∫—Å –ö–∞—á–µ—Å—Ç
[18.06.2025 14:13] Using data from previous issue: {"categories": ["#games", "#dataset", "#open_source", "#multimodal", "#healthcare", "#reasoning"], "emoji": "ü¶ü", "ru": {"title": "–ò–ò –Ω–∞ —Å—Ç—Ä–∞–∂–µ –∑–¥–æ—Ä–æ–≤—å—è: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ —É–≥—Ä–æ–∑—ã –∫–æ–º–∞—Ä–æ–≤", "desc": "VisText-Mosquito - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∞–≤
[18.06.2025 14:13] Querying the API.
[18.06.2025 14:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  					AI-generated summary 				 Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io
[18.06.2025 14:14] Response: {
  "desc": "DynaGuide - —ç—Ç–æ –º–µ—Ç–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –ø–æ–ª–∏—Ç–∏–∫–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–µ–π –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏ –∫ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ü–µ–ª—è–º –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏–µ —Ü–µ–ª—è–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–µ–ª—è—Ö. DynaGuide –æ—Ç–¥–µ–ª—è–µ—Ç –º–æ–¥–µ–ª—å –¥–∏–Ω–∞–º–∏–∫–∏ –æ—Ç –±–∞–∑–æ–≤–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏, —á—Ç–æ –¥–∞–µ—Ç —Ä—è–¥ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤. –ú–µ—Ç–æ–¥ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª —Å—Ä–µ–¥–Ω—é—é —É—Å–ø–µ—à–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è 70% –Ω–∞ –Ω–∞–±–æ—Ä–µ –∑–∞–¥–∞—á CALVIN –∏ –ø—Ä–µ–≤–∑–æ—à–µ–ª –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏–µ —Ü–µ–ª—è–º–∏ –≤ 5,4 —Ä–∞–∑–∞ –ø—Ä–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —Å –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏.",
  "emoji": "ü§ñ",
  "title": "DynaGuide: –≥–∏–±–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –≤–Ω–µ—à–Ω–µ–π –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏"
}
[18.06.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  					AI-generated summary 				 Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io"

[18.06.2025 14:14] Response: ```python
['AGENTS', 'ROBOTICS', 'TRAINING']
```
[18.06.2025 14:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  					AI-generated summary 				 Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io"

[18.06.2025 14:14] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[18.06.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DynaGuide is a novel steering method that enhances diffusion policies by utilizing an external dynamics model. This approach allows the policies to adapt to various objectives while ensuring robustness, particularly when dealing with low-quality objectives. Unlike traditional goal-conditioning methods, DynaGuide separates the dynamics model from the base policy, enabling it to steer towards multiple goals and improve underrepresented behaviors. The effectiveness of DynaGuide is demonstrated through experiments, achieving a 70% steering success rate and significantly outperforming existing methods.","title":"Steering Policies with DynaGuide: Adapting to Multiple Goals Robustly!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DynaGuide is a novel steering method that enhances diffusion policies by utilizing an external dynamics model. This approach allows the policies to adapt to various objectives while ensuring robustness, particularly when dealing with low-quality objectives. Unlike traditional goal-conditioning methods, DynaGuide separates the dynamics model from the base policy, enabling it to steer towards multiple goals and improve underrepresented behaviors. The effectiveness of DynaGuide is demonstrated through experiments, achieving a 70% steering success rate and significantly outperforming existing methods.', title='Steering Policies with DynaGuide: Adapting to Multiple Goals Robustly!'))
[18.06.2025 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DynaGuideÊòØ‰∏ÄÁßç‰ΩøÁî®Â§ñÈÉ®Âä®ÊÄÅÊ®°ÂûãÁöÑÂºïÂØºÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∫Êâ©Êï£Á≠ñÁï•ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÂÆÉÂÖÅËÆ∏Á≠ñÁï•ÈÄÇÂ∫îÂ§ö‰∏™ÁõÆÊ†áÔºåÂπ∂Âú®‰ΩéË¥®ÈáèÁõÆÊ†á‰∏ã‰øùÊåÅÈ≤ÅÊ£íÊÄßÔºåË°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁõÆÊ†áÊù°‰ª∂ÊñπÊ≥ï„ÄÇÈÄöËøáÂ∞ÜÂä®ÊÄÅÊ®°Âûã‰∏éÂü∫Á°ÄÁ≠ñÁï•ÂàÜÁ¶ªÔºåDynaGuideËÉΩÂ§üÂºïÂØºÁ≠ñÁï•ÊúùÂêëÂ§ö‰∏™ÁõÆÊ†áÔºåÂπ∂Â¢ûÂº∫Âü∫Á°ÄÁ≠ñÁï•ÁöÑË°®Áé∞„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDynaGuideÂú®Â§öÁßç‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü70%ÁöÑÂºïÂØºÊàêÂäüÁéáÔºåÂ∞§ÂÖ∂Âú®‰ΩéË¥®ÈáèÁõÆÊ†á‰∏ãÊØîÁõÆÊ†áÊù°‰ª∂ÊñπÊ≥ïÊèêÈ´ò‰∫Ü5.4ÂÄçÁöÑÊÄßËÉΩ„ÄÇ","title":"DynaGuideÔºöÂ§öÁõÆÊ†áÂºïÂØºÁöÑÊô∫ËÉΩÁ≠ñÁï•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DynaGuideÊòØ‰∏ÄÁßç‰ΩøÁî®Â§ñÈÉ®Âä®ÊÄÅÊ®°ÂûãÁöÑÂºïÂØºÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∫Êâ©Êï£Á≠ñÁï•ÁöÑÈÄÇÂ∫îÊÄß„ÄÇÂÆÉÂÖÅËÆ∏Á≠ñÁï•ÈÄÇÂ∫îÂ§ö‰∏™ÁõÆÊ†áÔºåÂπ∂Âú®‰ΩéË¥®ÈáèÁõÆÊ†á‰∏ã‰øùÊåÅÈ≤ÅÊ£íÊÄßÔºåË°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÁõÆÊ†áÊù°‰ª∂ÊñπÊ≥ï„ÄÇÈÄöËøáÂ∞ÜÂä®ÊÄÅÊ®°Âûã‰∏éÂü∫Á°ÄÁ≠ñÁï•ÂàÜÁ¶ªÔºåDynaGuideËÉΩÂ§üÂºïÂØºÁ≠ñÁï•ÊúùÂêëÂ§ö‰∏™ÁõÆÊ†áÔºåÂπ∂Â¢ûÂº∫Âü∫Á°ÄÁ≠ñÁï•ÁöÑË°®Áé∞„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåDynaGuideÂú®Â§öÁßç‰ªªÂä°‰∏≠ÂèñÂæó‰∫Ü70%ÁöÑÂºïÂØºÊàêÂäüÁéáÔºåÂ∞§ÂÖ∂Âú®‰ΩéË¥®ÈáèÁõÆÊ†á‰∏ãÊØîÁõÆÊ†áÊù°‰ª∂ÊñπÊ≥ïÊèêÈ´ò‰∫Ü5.4ÂÄçÁöÑÊÄßËÉΩ„ÄÇ', title='DynaGuideÔºöÂ§öÁõÆÊ†áÂºïÂØºÁöÑÊô∫ËÉΩÁ≠ñÁï•'))
[18.06.2025 14:14] Using data from previous issue: {"categories": ["#transfer_learning", "#cv", "#multimodal"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "TR2M - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã –≤ –º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
[18.06.2025 14:14] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#security", "#hallucinations", "#multimodal", "#open_source", "#alignment", "#data"], "emoji": "üîì", "ru": {"title": "–£—è–∑–≤–∏–º–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∞—Ç–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞—Ç–∞–∫–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ –ø—Ä–æ—Ç–∏–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[18.06.2025 14:14] Using data from previous issue: {"categories": ["#optimization", "#training", "#dataset", "#open_source", "#inference"], "emoji": "üß†", "ru": {"title": "–¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–±—ã—á–Ω–æ–º –ü–ö", "desc": "EMLoC - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ –ø–∞–º—è—Ç–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ-–æ
[18.06.2025 14:14] Renaming data file.
[18.06.2025 14:14] Renaming previous data. hf_papers.json to ./d/2025-06-18.json
[18.06.2025 14:14] Saving new data file.
[18.06.2025 14:14] Generating page.
[18.06.2025 14:14] Renaming previous page.
[18.06.2025 14:14] Renaming previous data. index.html to ./d/2025-06-18.html
[18.06.2025 14:14] Writing result.
[18.06.2025 14:14] Renaming log file.
[18.06.2025 14:14] Renaming previous data. log.txt to ./logs/2025-06-18_last_log.txt
