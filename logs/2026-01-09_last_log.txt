[09.01.2026 03:44] Read previous papers.
[09.01.2026 03:44] Generating top page (month).
[09.01.2026 03:44] Writing top page (month).
[09.01.2026 04:42] Read previous papers.
[09.01.2026 04:42] Get feed.
[09.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.05242
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05167
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03425
[09.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.05239
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05138
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05163
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05175
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05111
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05106
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03559
[09.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01887
[09.01.2026 04:42] Extract page data from URL. URL: https://huggingface.co/papers/2512.23628
[09.01.2026 04:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.01.2026 04:42] No deleted papers detected.
[09.01.2026 04:42] Downloading and parsing papers (pdf, html). Total: 12.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05242.
[09.01.2026 04:42] Downloading paper 2601.05242 from https://arxiv.org/pdf/2601.05242v1...
[09.01.2026 04:42] Extracting affiliations from text.
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2026-1-9 GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization Shih-Yang Liu1, Xin Dong*, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng1, Yejin Choi, Jan Kautz, Pavlo Molchanov NVIDIA As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization. Implementations: HF-TRL, verl, Nemo-RL Links: Project, Lab 6 2 0 2 8 ] . [ 1 2 4 2 5 0 . 1 0 6 2 : r (a) An overview of our proposed GDPO (b) Reward trends: GD"
[09.01.2026 04:42] Response: ```python
["NVIDIA"]
```
[09.01.2026 04:42] Deleting PDF ./assets/pdf/2601.05242.pdf.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05167.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.05167.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.05167.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.03425.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.03425.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.03425.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05239.
[09.01.2026 04:42] Downloading paper 2601.05239 from https://arxiv.org/pdf/2601.05239v1...
[09.01.2026 04:42] Extracting affiliations from text.
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Xiao Fu1,2, Shitao Tang1, Min Shi1,3, Xian Liu1, Jinwei Gu1, Ming-Yu Liu1, Dahua Lin2, Chen-Hsuan Lin1 1NVIDIA, 2The Chinese University of Hong Kong, 3Georgia Institute of Technology 6 2 0 2 8 ] . [ 1 9 3 2 5 0 . 1 0 6 2 : r Figure 1. We present PlenopticDreamer, generative framework that re-renders input video under novel camera trajectories while preserving long-term spatio-temporal memory in hallucinated regions across overlapping views, thereby producing coherent plenoptic functions (see robots right side, highlighted in red dashed boxes across three trajectories). Please refer to our website for more results. "
[09.01.2026 04:42] Response: ```python
["NVIDIA", "The Chinese University of Hong Kong", "Georgia Institute of Technology"]
```
[09.01.2026 04:42] Deleting PDF ./assets/pdf/2601.05239.pdf.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05138.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.05138.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.05138.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05163.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.05163.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.05163.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05175.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.05175.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.05175.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05111.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.05111.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.05111.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.05106.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.05106.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.05106.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.03559.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.03559.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.03559.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.01887.
[09.01.2026 04:42] Extra JSON file exists (./assets/json/2601.01887.json), skip PDF parsing.
[09.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.01887.json), skip HTML parsing.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2512.23628.
[09.01.2026 04:42] Downloading paper 2512.23628 from https://arxiv.org/pdf/2512.23628v1...
[09.01.2026 04:42] Extracting affiliations from text.
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Memorization in 3D Shape Generation: An Empirical Study Shu Pu1 Boya Zeng1 Kaichen Zhou2 Mengyu Wang2 2Harvard University 1Princeton University Zhuang Liu1 5 2 0 2 9 ] . [ 1 8 2 6 3 2 . 2 1 5 2 : r a "
[09.01.2026 04:42] Response: ```python
["Princeton University", "Harvard University"]
```
[09.01.2026 04:42] Deleting PDF ./assets/pdf/2512.23628.pdf.
[09.01.2026 04:42] Success.
[09.01.2026 04:42] Enriching papers with extra data.
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 0. Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.  					AI-generated summary 				 As language models become increasingly capable, user...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 1. RelayLLM enables efficient collaborative reasoning between small and large language models through token-level dynamic invocation, achieving high accuracy with minimal computational overhead.  					AI-generated summary 				 Large Language Models (LLMs) for complex reasoning is often hindered by high...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 2. Research challenges the assumption of domain specialization in Mixture of Experts models by identifying a persistent central committee of experts that dominates routing behavior across different domains and architectures.  					AI-generated summary 				 Mixture of Experts models are widely assumed t...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 3. PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.  					AI-generated summary 				 Camera-controlled generative v...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 4. VerseCrafter is a 4D-aware video world model that enables unified control over camera and object dynamics through 4D geometric control representation and video diffusion models.  					AI-generated summary 				 Video world models aim to simulate dynamic, real-world environments, yet existing methods ...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 5. DocDancer is an end-to-end trained open-source document question answering agent that formulates the task as an information-seeking problem and uses a tool-driven framework with exploration and synthesis for training.  					AI-generated summary 				 Document Question Answering (DocQA) focuses on ans...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 6. VideoAuto-R1 framework employs a reason-when-necessary strategy for video understanding, using a Thinking Once, Answering Twice training paradigm with verifiable rewards and confidence-based reasoning activation during inference.  					AI-generated summary 				 Chain-of-thought (CoT) reasoning has e...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 7. Large language models face limitations in evaluating complex, multi-step tasks, prompting the development of agent-based evaluation systems that utilize planning, tool-augmented verification, and multi-agent collaboration for more robust assessments.  					AI-generated summary 				 LLM-as-a-Judge ha...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 8. FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.  					AI-generated summary 				 Large language models (LLMs) exhibit str...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 9. DiffCoT reformulates chain-of-thought reasoning as an iterative denoising process using diffusion principles, enabling unified generation and correction of intermediate steps while maintaining causal consistency.  					AI-generated summary 				 Chain-of-Thought (CoT) reasoning improves multi-step ma...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 10. Safety alignment of large language models can be fully recovered with a single safety example, maintaining utility and achieving convergence in few epochs through identified low-rank gradient structures.  					AI-generated summary 				 Fine-tuning safety-aligned large language models (LLMs) can subs...
[09.01.2026 04:42] ********************************************************************************
[09.01.2026 04:42] Abstract 11. Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.  					AI-generated summary 				 Generative models are increasing...
[09.01.2026 04:42] Read previous papers.
[09.01.2026 04:42] Generating reviews via LLM API.
[09.01.2026 04:42] Querying the API.
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.  					AI-generated summary 				 As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.
[09.01.2026 04:42] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (reinforcement learning) Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° GRPO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ GRPO Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ñ… ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ GDPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GDPO Ğ½Ğ°Ğ´ GRPO.",
  "emoji": "âš–ï¸",
  "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸"
}
```
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.  					AI-generated summary 				 As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization."

[09.01.2026 04:42] Response: ```python
["RL", "RLHF", "TRAINING"]
```
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.  					AI-generated summary 				 As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization."

[09.01.2026 04:42] Response: ```python
["ALIGNMENT", "REASONING", "OPTIMIZATION"]
```
[09.01.2026 04:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges faced in multi-reward reinforcement learning, particularly the issue of reward normalization collapse when using Group Relative Policy Optimization (GRPO). The authors show that GRPO can lead to identical advantage values for different rewards, which diminishes the effectiveness of the training signal and can cause suboptimal learning outcomes. To overcome this, they propose a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO), which separates the normalization of individual rewards to better maintain their differences. The results indicate that GDPO significantly improves training stability and performance across various reasoning tasks compared to GRPO.","title":"Decoupling Rewards for Better Learning in Multi-Reward RL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges faced in multi-reward reinforcement learning, particularly the issue of reward normalization collapse when using Group Relative Policy Optimization (GRPO). The authors show that GRPO can lead to identical advantage values for different rewards, which diminishes the effectiveness of the training signal and can cause suboptimal learning outcomes. To overcome this, they propose a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO), which separates the normalization of individual rewards to better maintain their differences. The results indicate that GDPO significantly improves training stability and performance across various reasoning tasks compared to GRPO.', title='Decoupling Rewards for Better Learning in Multi-Reward RL'))
[09.01.2026 04:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ åœ¨ä½¿ç”¨GRPOæ—¶ä¼šå‡ºç°å¥–åŠ±å½’ä¸€åŒ–å´©æºƒçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•GDPOï¼Œé€šè¿‡è§£è€¦ä¸ªä½“å¥–åŠ±çš„å½’ä¸€åŒ–ï¼Œæ”¹å–„äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚GDPOèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™å¥–åŠ±ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„å¤šå¥–åŠ±ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDPOåœ¨å·¥å…·è°ƒç”¨ã€æ•°å­¦æ¨ç†å’Œç¼–ç æ¨ç†ç­‰ä»»åŠ¡ä¸­å‡ä¼˜äºGRPOã€‚","title":"è§£è€¦å¥–åŠ±å½’ä¸€åŒ–ï¼Œæå‡å¤šå¥–åŠ±å­¦ä¹ æ•ˆæœ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='å¤šå¥–åŠ±å¼ºåŒ–å­¦ä¹ åœ¨ä½¿ç”¨GRPOæ—¶ä¼šå‡ºç°å¥–åŠ±å½’ä¸€åŒ–å´©æºƒçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•GDPOï¼Œé€šè¿‡è§£è€¦ä¸ªä½“å¥–åŠ±çš„å½’ä¸€åŒ–ï¼Œæ”¹å–„äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚GDPOèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™å¥–åŠ±ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„å¤šå¥–åŠ±ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDPOåœ¨å·¥å…·è°ƒç”¨ã€æ•°å­¦æ¨ç†å’Œç¼–ç æ¨ç†ç­‰ä»»åŠ¡ä¸­å‡ä¼˜äºGRPOã€‚', title='è§£è€¦å¥–åŠ±å½’ä¸€åŒ–ï¼Œæå‡å¤šå¥–åŠ±å­¦ä¹ æ•ˆæœ'))
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#training", "#rlhf", "#inference", "#small_models", "#optimization", "#reasoning"], "emoji": "âš¡", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ°: Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…", "desc": "RelayLLM Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#training", "#benchmark", "#interpretability", "#reasoning", "#architecture"], "emoji": "ğŸ¤", "ru": {"title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Mixture of Experts: Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¸Ñ‚ĞµÑ‚Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture of 
[09.01.2026 04:42] Querying the API.
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.  					AI-generated summary 				 Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/
[09.01.2026 04:42] Response: ```json
{
  "desc": "PlenopticDreamer â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ’ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹.",
  "emoji": "ğŸ¬",
  "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸"
}
```
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.  					AI-generated summary 				 Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/"

[09.01.2026 04:42] Response: ```python
['VIDEO', 'MULTIMODAL', 'TRAINING']
```
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PlenopticDreamer enables consistent multi-view video re-rendering through synchronized generative hallucinations, leveraging camera-guided retrieval and progressive training mechanisms for improved temporal coherence and visual fidelity.  					AI-generated summary 				 Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/"

[09.01.2026 04:42] Response: ```python
['HALLUCINATIONS', 'LONG_CONTEXT']
```

**Justification:**

1. **HALLUCINATIONS**: The paper explicitly discusses "generative hallucinations" and "hallucinated regions" as a core challenge, focusing on maintaining consistency in these hallucinated areas across multi-view scenarios.

2. **LONG_CONTEXT**: The paper mentions "long-video conditioning mechanism to support extended video generation" and addresses "error accumulation" in long-range scenarios, which relates to handling extended context in video generation.
[09.01.2026 04:42] Error. Failed to parse JSON from LLM. ["HALLUCINATIONS", "LONG_CONTEXT"]


**Justification:**

1. **HALLUCINATIONS**: The paper explicitly discusses "generative hallucinations" and "hallucinated regions" as a core challenge, focusing on maintaining consistency in these hallucinated areas across multi-view scenarios.

2. **LONG_CONTEXT**: The paper mentions "long-video conditioning mechanism to support extended video generation" and addresses "error accumulation" in long-range scenarios, which relates to handling extended context in video generation.
[09.01.2026 04:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PlenopticDreamer is a novel framework designed for multi-view video re-rendering that ensures consistent visual output across different perspectives. It utilizes synchronized generative hallucinations, which help maintain spatio-temporal coherence in the generated video content. The model employs a camera-guided retrieval system to select relevant video segments from previous generations, enhancing the quality of the output. Additionally, it incorporates progressive training techniques to improve model robustness and visual fidelity, achieving state-of-the-art results in video re-rendering tasks.","title":"Synchronized Generative Hallucinations for Multi-View Video Mastery"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PlenopticDreamer is a novel framework designed for multi-view video re-rendering that ensures consistent visual output across different perspectives. It utilizes synchronized generative hallucinations, which help maintain spatio-temporal coherence in the generated video content. The model employs a camera-guided retrieval system to select relevant video segments from previous generations, enhancing the quality of the output. Additionally, it incorporates progressive training techniques to improve model robustness and visual fidelity, achieving state-of-the-art results in video re-rendering tasks.', title='Synchronized Generative Hallucinations for Multi-View Video Mastery'))
[09.01.2026 04:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PlenopticDreamer æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡åŒæ­¥ç”Ÿæˆçš„å¹»è§‰å®ç°ä¸€è‡´çš„å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“ã€‚å®ƒåˆ©ç”¨ç›¸æœºå¼•å¯¼çš„æ£€ç´¢å’Œæ¸è¿›è®­ç»ƒæœºåˆ¶ï¼Œæ”¹å–„äº†æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªå›å½’æ–¹å¼è®­ç»ƒå¤šè¾“å…¥å•è¾“å‡ºçš„è§†é¢‘æ¡ä»¶æ¨¡å‹ï¼Œå¹¶è‡ªé€‚åº”é€‰æ‹©å‰æœŸç”Ÿæˆçš„æ˜¾è‘—è§†é¢‘ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlenopticDreamer åœ¨è§†é¢‘é‡æ¸²æŸ“æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæä¾›äº†å“è¶Šçš„è§†è§’åŒæ­¥å’Œé«˜ä¿çœŸè§†è§‰æ•ˆæœã€‚","title":"åŒæ­¥ç”Ÿæˆå¹»è§‰ï¼Œå®ç°å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“çš„çªç ´"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PlenopticDreamer æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡åŒæ­¥ç”Ÿæˆçš„å¹»è§‰å®ç°ä¸€è‡´çš„å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“ã€‚å®ƒåˆ©ç”¨ç›¸æœºå¼•å¯¼çš„æ£€ç´¢å’Œæ¸è¿›è®­ç»ƒæœºåˆ¶ï¼Œæ”¹å–„äº†æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªå›å½’æ–¹å¼è®­ç»ƒå¤šè¾“å…¥å•è¾“å‡ºçš„è§†é¢‘æ¡ä»¶æ¨¡å‹ï¼Œå¹¶è‡ªé€‚åº”é€‰æ‹©å‰æœŸç”Ÿæˆçš„æ˜¾è‘—è§†é¢‘ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPlenopticDreamer åœ¨è§†é¢‘é‡æ¸²æŸ“æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæä¾›äº†å“è¶Šçš„è§†è§’åŒæ­¥å’Œé«˜ä¿çœŸè§†è§‰æ•ˆæœã€‚', title='åŒæ­¥ç”Ÿæˆå¹»è§‰ï¼Œå®ç°å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“çš„çªç ´'))
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#3d", "#dataset", "#data", "#video", "#multimodal"], "emoji": "ğŸ¬", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· 4D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ", "desc": "VerseCrafter Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ 4D-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#benchmark", "#dataset", "#data", "#agents", "#synthetic"], "emoji": "ğŸ•º", "ru": {"title": "Ğ¢Ğ°Ğ½ĞµÑ† Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…", "desc": "DocDancer â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#training", "#rlhf", "#benchmark", "#optimization", "#video", "#reasoning", "#multimodal"], "emoji": "ğŸ¬", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° VideoAuto-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ñ
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agents", "#survey"], "emoji": "âš–ï¸", "ru": {"title": "ĞÑ‚ ÑÑƒĞ´ÑŒĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑÑƒĞ´ÑŒĞµ-Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² AI", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ Ğ¾Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ 
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture"], "emoji": "ğŸ”€", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°", "desc": "FusionRoute â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#training", "#diffusion", "#math", "#reasoning", "#architecture"], "emoji": "ğŸ”„", "ru": {"title": "Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ°", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ DiffCoT â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½
[09.01.2026 04:42] Using data from previous issue: {"categories": ["#alignment", "#training", "#optimization", "#rlhf"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "ĞĞ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¼Ğ¾
[09.01.2026 04:42] Querying the API.
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.  					AI-generated summary 				 Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.
[09.01.2026 04:42] Response: ```json
{
  "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ) Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. Ğ§ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Vecset Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² guidance. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑƒÑ‚ĞµÑ‡ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².",
  "emoji": "ğŸ§ ",
  "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¼ĞµĞ¼Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…"
}
```
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.  					AI-generated summary 				 Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem."

[09.01.2026 04:42] Response: ```python
["3D", "BENCHMARK"]
```
[09.01.2026 04:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Researchers develop a framework to measure memorization in 3D generative models and identify factors affecting it, finding that data modality and model design parameters influence how much training data is remembered during generation.  					AI-generated summary 				 Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem."

[09.01.2026 04:42] Response: ```python
['DIFFUSION', 'LEAKAGE', 'OPEN_SOURCE']
```
[09.01.2026 04:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a framework to assess how much 3D generative models memorize their training data during the generation process. The researchers found that factors such as the type of data used and the design of the model significantly affect memorization levels. Their experiments revealed that increased data diversity and finer conditioning lead to higher memorization, while certain modeling techniques can help reduce it. The findings aim to enhance the understanding of memorization in generative models, which can help improve the quality and diversity of generated outputs.","title":"Measuring and Mitigating Memorization in 3D Generative Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a framework to assess how much 3D generative models memorize their training data during the generation process. The researchers found that factors such as the type of data used and the design of the model significantly affect memorization levels. Their experiments revealed that increased data diversity and finer conditioning lead to higher memorization, while certain modeling techniques can help reduce it. The findings aim to enhance the understanding of memorization in generative models, which can help improve the quality and diversity of generated outputs.', title='Measuring and Mitigating Memorization in 3D Generative Models'))
[09.01.2026 04:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºæµ‹é‡3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡ï¼Œå¹¶è¯†åˆ«å½±å“å› ç´ ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°æ®æ¨¡æ€å’Œæ¨¡å‹è®¾è®¡å‚æ•°ä¼šå½±å“è®­ç»ƒæ•°æ®åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è®°å¿†ç¨‹åº¦ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„è¯„ä¼°ï¼Œæˆ‘ä»¬é‡åŒ–äº†è®°å¿†ç°è±¡ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°ï¼Œè®°å¿†ä¸æ•°æ®å¤šæ ·æ€§å’Œç»†ç²’åº¦æ¡ä»¶æœ‰å…³ã€‚æˆ‘ä»¬çš„æ¡†æ¶å’Œåˆ†æä¸ºç†è§£3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†æä¾›äº†å®è¯ä¾æ®ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„å‡å°‘è®°å¿†çš„æ–¹æ³•ã€‚","title":"é‡åŒ–3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºæµ‹é‡3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡ï¼Œå¹¶è¯†åˆ«å½±å“å› ç´ ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°æ®æ¨¡æ€å’Œæ¨¡å‹è®¾è®¡å‚æ•°ä¼šå½±å“è®­ç»ƒæ•°æ®åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è®°å¿†ç¨‹åº¦ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„è¯„ä¼°ï¼Œæˆ‘ä»¬é‡åŒ–äº†è®°å¿†ç°è±¡ï¼Œå¹¶é€šè¿‡å®éªŒå‘ç°ï¼Œè®°å¿†ä¸æ•°æ®å¤šæ ·æ€§å’Œç»†ç²’åº¦æ¡ä»¶æœ‰å…³ã€‚æˆ‘ä»¬çš„æ¡†æ¶å’Œåˆ†æä¸ºç†è§£3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†æä¾›äº†å®è¯ä¾æ®ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„å‡å°‘è®°å¿†çš„æ–¹æ³•ã€‚', title='é‡åŒ–3Dç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†ç°è±¡'))
[09.01.2026 04:42] Renaming data file.
[09.01.2026 04:42] Renaming previous data. hf_papers.json to ./d/2026-01-09.json
[09.01.2026 04:42] Saving new data file.
[09.01.2026 04:42] Generating page.
[09.01.2026 04:42] Renaming previous page.
[09.01.2026 04:42] Renaming previous data. index.html to ./d/2026-01-09.html
[09.01.2026 04:42] Writing result.
[09.01.2026 04:42] Renaming log file.
[09.01.2026 04:42] Renaming previous data. log.txt to ./logs/2026-01-09_last_log.txt
