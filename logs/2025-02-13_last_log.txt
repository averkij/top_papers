[13.02.2025 19:08] Read previous papers.
[13.02.2025 19:08] Generating top page (month).
[13.02.2025 19:08] Writing top page (month).
[13.02.2025 20:11] Read previous papers.
[13.02.2025 20:11] Get feed.
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07870
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07346
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08590
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08127
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08639
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08047
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07864
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07563
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08606
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06533
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08168
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07599
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08524
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.05167
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06145
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06872
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07737
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04411
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.00963
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08213
[13.02.2025 20:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07985
[13.02.2025 20:11] Extract page data from URL. URL: https://huggingface.co/papers/2502.05282
[13.02.2025 20:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.02.2025 20:11] No deleted papers detected.
[13.02.2025 20:11] Downloading and parsing papers (pdf, html). Total: 22.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.07870.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.07870.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.07870.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.07346.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.07346.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.07346.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08590.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08590.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08590.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08127.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08127.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08127.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08639.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08639.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08639.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08047.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08047.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08047.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.07864.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.07864.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.07864.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.07563.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.07563.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.07563.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08606.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08606.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08606.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.06533.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.06533.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.06533.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08168.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08168.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08168.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.07599.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.07599.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.07599.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08524.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08524.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08524.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.05167.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.05167.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.05167.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.06145.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.06145.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.06145.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.06872.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.06872.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.06872.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.07737.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.07737.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.07737.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.04411.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.04411.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.04411.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.00963.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.00963.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.00963.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.08213.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.08213.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.08213.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.07985.
[13.02.2025 20:11] Extra JSON file exists (./assets/json/2502.07985.json), skip PDF parsing.
[13.02.2025 20:11] Paper image links file exists (./assets/img_data/2502.07985.json), skip HTML parsing.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Downloading and parsing paper https://huggingface.co/papers/2502.05282.
[13.02.2025 20:11] Downloading paper 2502.05282 from http://arxiv.org/pdf/2502.05282v1...
[13.02.2025 20:11] Extracting affiliations from text.
[13.02.2025 20:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 ] . [ 1 2 8 2 5 0 . 2 0 5 2 : r JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST Yuting He, Boyu Wang, Rongjun Ge, Yang Chen, Guanyu Yang, Shuo Li AbstractDense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables reliable correspondence discovery for effective dense contrast. We proposes deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate deformable mapping to predict the pixels correspondence under the condition of topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via gradient. We also proposes geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code at companion website. Index TermsMedical image analysis, Dense contrastive representation learning, False positive and negative pairs problem, Homeomorphism prior, Correspondence problem [7] is crucial for medical ENSE contrastive representation learning (DCRL) [1] image dense prediction (MIDP) tasks, e.g., segmentation [8][10]. With the increasing demand for "
[13.02.2025 20:11] Response: ```python
[]
```
[13.02.2025 20:11] Extracting affiliations from text.
[13.02.2025 20:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 ] . [ 1 2 8 2 5 0 . 2 0 5 2 : r JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUSTYuting He, Boyu Wang, Rongjun Ge, Yang Chen, Guanyu Yang, Shuo Li AbstractDense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables reliable correspondence discovery for effective dense contrast. We proposes deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate deformable mapping to predict the pixels correspondence under the condition of topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via gradient. We also proposes geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code at companion website. Index TermsMedical image analysis, Dense contrastive representation learning, False positive and negative pairs problem, Homeomorphism prior, Correspondence problem[7] is crucial for medical ENSE contrastive representation learning (DCRL) [1] image dense prediction (MIDP) tasks, e.g., segmentation [8][10]. With the increasing demand for deep learning in medical image applications [11], the extremely high cost of medical image collection and dense annotation are becoming large bottleneck [12]. The DCRL discovers the corresponding pixel1-wise features [13][16] to drive the learning of consistent or distinct representation for them (Fig.1 A)) which will effectively capture the dense posterior distribution of the underlying explanatory factors for the input. Therefore, it will make models Corresponding authors: G. Yang. (e-mail: yang.list@seu.edu.cn) This research was supported by the National Natural Science Foundation of China (Grant No. 82441021), the Natural Science Foundation of Jiangsu Province (Grant No. BK20210291), the National Natural Science Foundation of China (Grant No. 62101249, T2225025), the Jiangsu Shuangchuang Talent Program (Grant No. JSSCBS20220202). Y. He, Y. Chen and G. Yang are with the Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China, the Centre de Recherche en Information Biomedicale Sino-Francais (CRIBs), and Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China. (e-mail: yang.list@seu.edu.cn) R. Ge is with the School of Instrument Science and Engineering, Southeast University, Nanjing, China (e-mail: rongjun ge@seu.edu.cn) B. Wang is with the Department of Computer Science, Western University, London, ON N6A 3K7, Canada. (e-mail: bwang@csd.uwo.ca) S. Li and Y. He are with the Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA (e-mail: shuo.li11@case.edu). Manuscript received April 19, 2005; revised August 26, 2015. 1. pixel for 2D and voxel for 3D images, we call pixel uniformly Fig. 1: The DCRL with the large-scale FP&N problem. A) The DCRL pulls and pushes the positive and negative feature pairs for consistent or distinct representation. However, B) medical images properties cause unreliable correspondence discovery, resulting in the open problem of large-scale FP&N features pairs in DCRL and extremely limiting the representation learning ability. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 utilize the correspondence from the manual transformation of the same images weakening the FP problem, the diversity of positive pairs is limited (only paired from the same images) and they are still limited by large-scale FN problem. 2) Large-scale false negative (FN) pairs caused by semantic continuity and semantic overlap: a. Different from the imagewise contrastive learning [21][24] whose unit is instance, the DCRL utilizes the pixel as the unit which continuously arranges on images and constructs semantics via numerous pixels due to the continuity of image signal [21] (Fig.1 B) b)). This makes the semantics continuously distributed on images, so it is unreliable to absolutely divide the pixelwise features as different semantics on images as negative pairs. b. Some natural DCRL methods [1], [3], [4] utilize the features from different images as negative pairs, like momentum contrast mechanism [1], [3], [25] which utilizes memory bank to save previous features as negative samples (Fig.1 B) c)). However, due to the consistency of human body, medical images have similar global content which shares numerous same anatomies. This makes numerous overlapped semantics between images, so that it will construct numerous negative pairs with the same semantics resulting in FN pairs. During correspondence discovery, these properties will enlarge the risk of the FP&N feature pairs [18], [19] resulting in large-scale FP&N problem. It will train the networks representation to deviate from reality, making the pre-trained network even worse than the randomly initialized network. Therefore, we seek to answer the following question: How to cope with the FP&N problem for reliable pixel-wise correspondence in the medical image DCRL? Motivation: Inspired by topologie [26]  (Fig.2)  , homeomorphism [27] between medical images [28][30], e.g., CT, MR, X-ray, provides prior knowledge for reliable pixelwise correspondence. An often-repeated mathematical joke is that topologists cannot tell the difference between coffee cup and donut [31] (Fig.2 A)), because the coffee and donut are homeomorphic (have the same topology) and they are able to transform to each other via topologypreserved ma"
[13.02.2025 20:11] Mistral response. {"id": "fd02768dc0fe4a7fa3e2ac7a90e9f066", "object": "chat.completion", "created": 1739477491, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China\",\n    \"Centre de Recherche en Information Biomedicale Sino-Francais (CRIBs)\",\n    \"Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China\",\n    \"School of Instrument Science and Engineering, Southeast University, Nanjing, China\",\n    \"Department of Computer Science, Western University, London, ON N6A 3K7, Canada\",\n    \"Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1671, "total_tokens": 1853, "completion_tokens": 182}}
[13.02.2025 20:11] Response: ```python
[
    "Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China",
    "Centre de Recherche en Information Biomedicale Sino-Francais (CRIBs)",
    "Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China",
    "School of Instrument Science and Engineering, Southeast University, Nanjing, China",
    "Department of Computer Science, Western University, London, ON N6A 3K7, Canada",
    "Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA"
]
```
[13.02.2025 20:11] Deleting PDF ./assets/pdf/2502.05282.pdf.
[13.02.2025 20:11] Success.
[13.02.2025 20:11] Enriching papers with extra data.
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 0. Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text a...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 1. Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages i...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 2. Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video rel...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 3. Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financ...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 4. In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera ...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 5. Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or ...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 6. Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV sta...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 7. Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 8. We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 9. The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as ...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 10. In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expert...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 11. Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has iden...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 12. Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token p...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 13. Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 14. Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To ...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 15. Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 16. Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generati...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 17. Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes exce...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 18. While recent AI-for-math has made strides in pure mathematics, areas of applied mathematics, particularly PDEs, remain underexplored despite their significant real-world applications. We present PDE-Controller, a framework that enables large language models (LLMs) to control systems governed by part...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 19. In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially de...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 20. We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed spe...
[13.02.2025 20:11] ********************************************************************************
[13.02.2025 20:11] Abstract 21. Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspond...
[13.02.2025 20:11] Read previous papers.
[13.02.2025 20:11] Generating reviews via LLM API.
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#long_context", "#benchmark", "#cv"], "emoji": "üìö", "ru": {"title": "TextAtlas5M: –ù–æ–≤—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç TextAtlas5M –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. –î–∞—Ç–∞—Å–µ—Ç
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#long_context", "#low_resource", "#machine_translation", "#multilingual", "#dataset", "#open_source", "#benchmark"], "emoji": "üåê", "ru": {"title": "BenchMAX: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "BenchMAX - —ç—Ç–æ –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#video", "#diffusion", "#cv"], "emoji": "üí°", "ru": {"title": "–ü–ª–∞–≤–Ω–æ–µ –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–∏–µ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Light-A-Video - –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–∏—é –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç–æ–¥–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Å
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#rl", "#open_source", "#training", "#long_context"], "emoji": "üíπ", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –∫–ª—é—á –∫ —É—Å–ø–µ—Ö—É –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º –∞–Ω–∞–ª–∏–∑–µ", "desc": "–í —ç—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å 16 –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#dataset", "#video", "#diffusion", "#3d", "#games"], "emoji": "üé¨", "ru": {"title": "CineMaster: –†–µ–∂–∏—Å—Å–∏—Ä—É–π—Ç–µ —Å–≤–æ–µ –≤–∏–¥–µ–æ –≤ 3D", "desc": "CineMaster - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 3D-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Ç–æ—á–Ω–æ —Ä–∞–∑–º–µ—â–∞
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#agents", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ GUI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WorldGUI - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º –≤ —Ä–∞–∑–ª–∏—á–Ω
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference", "#training", "#long_context"], "emoji": "üöÄ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –≤–Ω–∏–º–∞–Ω–∏—è: MLA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multi-head Latent Attention (MLA), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –Ω–∞ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π LASP-2 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üî¨", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ 
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#rl", "#training", "#long_context", "#small_models", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è LLM —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Ü–µ–ª–µ–π –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#open_source", "#synthetic"], "emoji": "üõ∞Ô∏è", "ru": {"title": "SARChat-2M: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π SAR —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥–æ–≤—ã–π
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#alignment", "#training", "#rlhf"], "emoji": "üîÄ", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DPO-Shift –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#interpretability", "#training", "#architecture", "#benchmark"], "emoji": "üß†", "ru": {"title": "CoCoMix: —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Co
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#benchmark", "#long_context"], "emoji": "üîç", "ru": {"title": "NoLiMa: –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ NoLiMa –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –í –æ—Ç
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#video", "#diffusion"], "emoji": "üé≠", "ru": {"title": "–û–∂–∏–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —É—á–µ—Ç–æ–º –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã", "desc": "–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Animate Anyone 2 - –º–µ—Ç–æ–¥ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —É—á–µ—Ç–æ–º –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#hallucinations", "#interpretability", "#security", "#survey", "#rag", "#ethics"], "emoji": "üß†", "ru": {"title": "–ü—É—Ç—å –∫ –Ω–∞–¥—ë–∂–Ω–æ–º—É –ò–ò: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –º–µ—Ç–æ–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#inference", "#training"], "emoji": "üé¨", "ru": {"title": "NBP: –ë—ã—Å—Ç—Ä–∞—è –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –±–ª–æ–∫–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Next-Block Prediction (NBP). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#model merging", "#training", "#architecture", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#open_source", "#training", "#dataset", "#synthetic", "#math", "#science", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–æ—Ä—è—é—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ PDE-Controller - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) —É–ø
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#small_models", "#transfer_learning", "#optimization", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥—É–ª–µ–π LLM, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è –æ—Ç –±–æ–ª—å—à–æ
[13.02.2025 20:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#security", "#alignment", "#inference"], "emoji": "üõ°Ô∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[13.02.2025 20:11] Querying the API.
[13.02.2025 20:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels' correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: https://github.com/YutingHe-list/GEMINI.
[13.02.2025 20:11] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ø–ª–æ—Ç–Ω—ã–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º (DCRL) –¥–ª—è –∑–∞–¥–∞—á –ø–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥—Ö–æ–¥ GEMINI, –∫–æ—Ç–æ—Ä—ã–π –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≥–æ–º–µ–æ–º–æ—Ä—Ñ–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ DCRL –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ñ–æ—Ä–º–∏—Ä—É–µ–º–æ–º—É –≥–æ–º–µ–æ–º–æ—Ä—Ñ–∏–∑–º—É (DHL) –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (GSS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–µ–º–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.",
  "emoji": "üî¨",
  "title": "GEMINI: –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
}
[13.02.2025 20:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels' correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: https://github.com/YutingHe-list/GEMINI."

[13.02.2025 20:11] Response: ```python
['DATASET', 'CV', 'HEALTHCARE', 'TRAINING']
```
[13.02.2025 20:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels' correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: https://github.com/YutingHe-list/GEMINI."

[13.02.2025 20:11] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```
[13.02.2025 20:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces GEMINI, a novel approach to improve dense contrastive representation learning (DCRL) for medical images. It addresses the challenge of unreliable correspondence discovery by incorporating a homeomorphism prior, which helps in accurately mapping pixel correspondences while preserving the topological structure of the images. The method includes deformable homeomorphism learning (DHL) to reduce false positive and negative pairs, enhancing the learning process. Additionally, geometric semantic similarity (GSS) is utilized to assess the alignment of features, leading to more effective and reliable dense contrast learning.","title":"Enhancing Medical Image Learning with GEMINI: Reliable Correspondence Discovery"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces GEMINI, a novel approach to improve dense contrastive representation learning (DCRL) for medical images. It addresses the challenge of unreliable correspondence discovery by incorporating a homeomorphism prior, which helps in accurately mapping pixel correspondences while preserving the topological structure of the images. The method includes deformable homeomorphism learning (DHL) to reduce false positive and negative pairs, enhancing the learning process. Additionally, geometric semantic similarity (GSS) is utilized to assess the alignment of features, leading to more effective and reliable dense contrast learning.', title='Enhancing Medical Image Learning with GEMINI: Reliable Correspondence Discovery'))
[13.02.2025 20:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ÂØÜÈõÜÂØπÊØîË°®Á§∫Â≠¶‰π†ÔºàDCRLÔºâÂú®ÂõæÂÉèÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÂ≠¶‰π†ÊïàÁéáÔºåÂ∞§ÂÖ∂Âú®ÂåªÁñóÂõæÂÉèÊî∂ÈõÜÂíåÂØÜÈõÜÊ†áÊ≥®ÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂåªÁñóÂõæÂÉèÁöÑÁâπÊÄßÂØºËá¥‰∫Ü‰∏çÂèØÈù†ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂèëÁé∞ÔºåÈÄ†Êàê‰∫ÜÂ§ßÈáèÁöÑÂÅáÈò≥ÊÄßÂíåÂÅáÈò¥ÊÄßÂØπ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂá†‰ΩïËßÜËßâÂØÜÈõÜÁõ∏‰ººÊÄßÔºàGEMINIÔºâÂ≠¶‰π†ÔºåÈÄöËøáÂºïÂÖ•ÂêåËÉöÊÄßÂÖàÈ™åÊù•Â¢ûÂº∫DCRLÁöÑÂèØÈù†ÊÄßÔºå‰ªéËÄåÊúâÊïàÂú∞ÂèëÁé∞ÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÂèØÂèòÂΩ¢ÂêåËÉöÂ≠¶‰π†ÔºàDHLÔºâÂíåÂá†‰ΩïËØ≠‰πâÁõ∏‰ººÊÄßÔºàGSSÔºâÔºåËøô‰∏§ËÄÖÂÖ±ÂêåÊèêÈ´ò‰∫ÜÂØπÂ∫îÂ≠¶‰π†ÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ","title":"ÊèêÂçáÂåªÁñóÂõæÂÉèÂ≠¶‰π†ÊïàÁéáÁöÑÂá†‰ΩïÂØπÊØîÊñπÊ≥ï"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ÂØÜÈõÜÂØπÊØîË°®Á§∫Â≠¶‰π†ÔºàDCRLÔºâÂú®ÂõæÂÉèÂØÜÈõÜÈ¢ÑÊµã‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÂ≠¶‰π†ÊïàÁéáÔºåÂ∞§ÂÖ∂Âú®ÂåªÁñóÂõæÂÉèÊî∂ÈõÜÂíåÂØÜÈõÜÊ†áÊ≥®ÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂåªÁñóÂõæÂÉèÁöÑÁâπÊÄßÂØºËá¥‰∫Ü‰∏çÂèØÈù†ÁöÑÂØπÂ∫îÂÖ≥Á≥ªÂèëÁé∞ÔºåÈÄ†Êàê‰∫ÜÂ§ßÈáèÁöÑÂÅáÈò≥ÊÄßÂíåÂÅáÈò¥ÊÄßÂØπ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂá†‰ΩïËßÜËßâÂØÜÈõÜÁõ∏‰ººÊÄßÔºàGEMINIÔºâÂ≠¶‰π†ÔºåÈÄöËøáÂºïÂÖ•ÂêåËÉöÊÄßÂÖàÈ™åÊù•Â¢ûÂº∫DCRLÁöÑÂèØÈù†ÊÄßÔºå‰ªéËÄåÊúâÊïàÂú∞ÂèëÁé∞ÂØπÂ∫îÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÂèØÂèòÂΩ¢ÂêåËÉöÂ≠¶‰π†ÔºàDHLÔºâÂíåÂá†‰ΩïËØ≠‰πâÁõ∏‰ººÊÄßÔºàGSSÔºâÔºåËøô‰∏§ËÄÖÂÖ±ÂêåÊèêÈ´ò‰∫ÜÂØπÂ∫îÂ≠¶‰π†ÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ', title='ÊèêÂçáÂåªÁñóÂõæÂÉèÂ≠¶‰π†ÊïàÁéáÁöÑÂá†‰ΩïÂØπÊØîÊñπÊ≥ï'))
[13.02.2025 20:11] Loading Chinese text from previous data.
[13.02.2025 20:11] Renaming data file.
[13.02.2025 20:11] Renaming previous data. hf_papers.json to ./d/2025-02-13.json
[13.02.2025 20:11] Saving new data file.
[13.02.2025 20:11] Generating page.
[13.02.2025 20:11] Renaming previous page.
[13.02.2025 20:11] Renaming previous data. index.html to ./d/2025-02-13.html
[13.02.2025 20:11] [Experimental] Generating Chinese page for reading.
[13.02.2025 20:11] Chinese vocab [{'word': 'ÈáçÂÖâ', 'pinyin': 'ch√≥ng guƒÅng', 'trans': 'relighting'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': '‰∏ÄËá¥', 'pinyin': 'yƒ´ zh√¨', 'trans': 'consistent'}, {'word': 'ÂÖâÁÖß', 'pinyin': 'guƒÅng zh√†o', 'trans': 'illumination'}, {'word': 'ÊªûÂêé', 'pinyin': 'zh√¨ h√≤u', 'trans': 'lag behind'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«î y√†o', 'trans': 'mainly'}, {'word': 'Áî±‰∫é', 'pinyin': 'y√≥u y√∫', 'trans': 'due to'}, {'word': 'È´òÊòÇ', 'pinyin': 'gƒÅo √°ng', 'trans': 'high'}, {'word': 'Áº∫‰πè', 'pinyin': 'quƒì f√°', 'trans': 'lack'}, {'word': 'Â§öÊ†∑Âåñ', 'pinyin': 'du≈ç y√†ng hu√†', 'trans': 'diversified'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ÈÄêÂ∏ß', 'pinyin': 'zh√∫ zhƒìn', 'trans': 'frame-by-frame'}, {'word': 'ÂØºËá¥', 'pinyin': 'd«éo zh√¨', 'trans': 'lead to'}, {'word': 'Â§ñËßÇ', 'pinyin': 'w√†i gu«én', 'trans': 'appearance'}, {'word': 'Èó™ÁÉÅ', 'pinyin': 'sh«én shu√≤', 'trans': 'flicker'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Êó†ÈúÄ', 'pinyin': 'w√∫ x≈´', 'trans': 'without needing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Ê®°Âùó', 'pinyin': 'm√≥ ku√†i', 'trans': 'module'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìng qi√°ng', 'trans': 'enhance'}, {'word': 'Á°Æ‰øù', 'pinyin': 'qu√® b«éo', 'trans': 'ensure'}, {'word': 'ËøûË¥ØÊÄß', 'pinyin': 'li√°n gu√†n x√¨ng', 'trans': 'coherence'}]
[13.02.2025 20:11] Renaming previous Chinese page.
[13.02.2025 20:11] Renaming previous data. zh.html to ./d/2025-02-12_zh_reading_task.html
[13.02.2025 20:11] Writing Chinese reading task.
[13.02.2025 20:11] Writing result.
[13.02.2025 20:11] Renaming log file.
[13.02.2025 20:11] Renaming previous data. log.txt to ./logs/2025-02-13_last_log.txt
