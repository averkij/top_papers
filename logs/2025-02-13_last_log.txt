[13.02.2025 15:11] Read previous papers.
[13.02.2025 15:11] Generating top page (month).
[13.02.2025 15:11] Writing top page (month).
[13.02.2025 16:13] Read previous papers.
[13.02.2025 16:13] Get feed.
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07346
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07870
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08590
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08639
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08127
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08047
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07864
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07563
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08606
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08168
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06533
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07599
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08524
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.05167
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06145
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07737
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.04411
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06872
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08213
[13.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07985
[13.02.2025 16:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.02.2025 16:13] No deleted papers detected.
[13.02.2025 16:13] Downloading and parsing papers (pdf, html). Total: 20.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.07346.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.07346.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.07346.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.07870.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.07870.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.07870.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08590.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08590.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08590.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08639.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08639.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08639.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08127.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08127.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08127.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08047.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08047.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08047.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.07864.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.07864.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.07864.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.07563.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.07563.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.07563.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08606.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08606.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08606.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08168.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08168.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08168.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.06533.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.06533.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.06533.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.07599.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.07599.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.07599.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08524.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08524.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08524.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.05167.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.05167.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.05167.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.06145.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.06145.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.06145.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.07737.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.07737.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.07737.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.04411.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.04411.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.04411.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.06872.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.06872.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.06872.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.08213.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.08213.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.08213.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.07985.
[13.02.2025 16:13] Extra JSON file exists (./assets/json/2502.07985.json), skip PDF parsing.
[13.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.07985.json), skip HTML parsing.
[13.02.2025 16:13] Success.
[13.02.2025 16:13] Enriching papers with extra data.
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 0. Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages i...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 1. Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text a...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 2. Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video rel...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 3. In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera ...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 4. Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financ...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 5. Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or ...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 6. Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV sta...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 7. Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 8. We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 9. In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expert...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 10. The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as ...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 11. Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has iden...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 12. Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token p...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 13. Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 14. Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To ...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 15. Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generati...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 16. Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes exce...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 17. Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 18. In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially de...
[13.02.2025 16:13] ********************************************************************************
[13.02.2025 16:13] Abstract 19. We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed spe...
[13.02.2025 16:13] Read previous papers.
[13.02.2025 16:13] Generating reviews via LLM API.
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#long_context", "#low_resource", "#machine_translation", "#multilingual", "#dataset", "#open_source", "#benchmark"], "emoji": "üåê", "ru": {"title": "BenchMAX: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "BenchMAX - —ç—Ç–æ –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#long_context", "#benchmark", "#cv"], "emoji": "üìö", "ru": {"title": "TextAtlas5M: –ù–æ–≤—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç TextAtlas5M –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. –î–∞—Ç–∞—Å–µ—Ç
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#video", "#diffusion", "#cv"], "emoji": "üí°", "ru": {"title": "–ü–ª–∞–≤–Ω–æ–µ –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–∏–µ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Light-A-Video - –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä–µ–æ—Å–≤–µ—â–µ–Ω–∏—é –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç–æ–¥–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Å
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#video", "#diffusion", "#3d", "#games"], "emoji": "üé¨", "ru": {"title": "CineMaster: –†–µ–∂–∏—Å—Å–∏—Ä—É–π—Ç–µ —Å–≤–æ–µ –≤–∏–¥–µ–æ –≤ 3D", "desc": "CineMaster - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 3D-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Ç–æ—á–Ω–æ —Ä–∞–∑–º–µ—â–∞
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#rl", "#open_source", "#training", "#long_context"], "emoji": "üíπ", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –∫–ª—é—á –∫ —É—Å–ø–µ—Ö—É –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º –∞–Ω–∞–ª–∏–∑–µ", "desc": "–í —ç—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å 16 –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#agents", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ GUI", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WorldGUI - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º –≤ —Ä–∞–∑–ª–∏—á–Ω
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference", "#training", "#long_context"], "emoji": "üöÄ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –≤–Ω–∏–º–∞–Ω–∏—è: MLA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multi-head Latent Attention (MLA), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –Ω–∞ —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π LASP-2 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üî¨", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ 
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#open_source", "#synthetic"], "emoji": "üõ∞Ô∏è", "ru": {"title": "SARChat-2M: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π SAR —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥–æ–≤—ã–π
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#rl", "#training", "#long_context", "#small_models", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è LLM —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Ü–µ–ª–µ–π –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#alignment", "#training", "#rlhf"], "emoji": "üîÄ", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DPO-Shift –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#interpretability", "#training", "#architecture", "#benchmark"], "emoji": "üß†", "ru": {"title": "CoCoMix: —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Co
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#benchmark", "#long_context"], "emoji": "üîç", "ru": {"title": "NoLiMa: –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ NoLiMa –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –í –æ—Ç
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#video", "#diffusion"], "emoji": "üé≠", "ru": {"title": "–û–∂–∏–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —É—á–µ—Ç–æ–º –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã", "desc": "–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Animate Anyone 2 - –º–µ—Ç–æ–¥ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —É—á–µ—Ç–æ–º –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#video", "#inference", "#training"], "emoji": "üé¨", "ru": {"title": "NBP: –ë—ã—Å—Ç—Ä–∞—è –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –±–ª–æ–∫–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Next-Block Prediction (NBP). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#model merging", "#training", "#architecture", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#hallucinations", "#interpretability", "#security", "#survey", "#rag", "#ethics"], "emoji": "üß†", "ru": {"title": "–ü—É—Ç—å –∫ –Ω–∞–¥—ë–∂–Ω–æ–º—É –ò–ò: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –º–µ—Ç–æ–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#small_models", "#transfer_learning", "#optimization", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥—É–ª–µ–π LLM, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∑–Ω–∞–Ω–∏—è –æ—Ç –±–æ–ª—å—à–æ
[13.02.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#security", "#alignment", "#inference"], "emoji": "üõ°Ô∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[13.02.2025 16:13] Loading Chinese text from previous data.
[13.02.2025 16:13] Renaming data file.
[13.02.2025 16:13] Renaming previous data. hf_papers.json to ./d/2025-02-13.json
[13.02.2025 16:13] Saving new data file.
[13.02.2025 16:13] Generating page.
[13.02.2025 16:13] Renaming previous page.
[13.02.2025 16:13] Renaming previous data. index.html to ./d/2025-02-13.html
[13.02.2025 16:13] [Experimental] Generating Chinese page for reading.
[13.02.2025 16:13] Chinese vocab [{'word': 'ÈáçÂÖâ', 'pinyin': 'ch√≥ng guƒÅng', 'trans': 'relighting'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': '‰∏ÄËá¥', 'pinyin': 'yƒ´ zh√¨', 'trans': 'consistent'}, {'word': 'ÂÖâÁÖß', 'pinyin': 'guƒÅng zh√†o', 'trans': 'illumination'}, {'word': 'ÊªûÂêé', 'pinyin': 'zh√¨ h√≤u', 'trans': 'lag behind'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«î y√†o', 'trans': 'mainly'}, {'word': 'Áî±‰∫é', 'pinyin': 'y√≥u y√∫', 'trans': 'due to'}, {'word': 'È´òÊòÇ', 'pinyin': 'gƒÅo √°ng', 'trans': 'high'}, {'word': 'Áº∫‰πè', 'pinyin': 'quƒì f√°', 'trans': 'lack'}, {'word': 'Â§öÊ†∑Âåñ', 'pinyin': 'du≈ç y√†ng hu√†', 'trans': 'diversified'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ÈÄêÂ∏ß', 'pinyin': 'zh√∫ zhƒìn', 'trans': 'frame-by-frame'}, {'word': 'ÂØºËá¥', 'pinyin': 'd«éo zh√¨', 'trans': 'lead to'}, {'word': 'Â§ñËßÇ', 'pinyin': 'w√†i gu«én', 'trans': 'appearance'}, {'word': 'Èó™ÁÉÅ', 'pinyin': 'sh«én shu√≤', 'trans': 'flicker'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Êó†ÈúÄ', 'pinyin': 'w√∫ x≈´', 'trans': 'without needing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Ê®°Âùó', 'pinyin': 'm√≥ ku√†i', 'trans': 'module'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìng qi√°ng', 'trans': 'enhance'}, {'word': 'Á°Æ‰øù', 'pinyin': 'qu√® b«éo', 'trans': 'ensure'}, {'word': 'ËøûË¥ØÊÄß', 'pinyin': 'li√°n gu√†n x√¨ng', 'trans': 'coherence'}]
[13.02.2025 16:13] Renaming previous Chinese page.
[13.02.2025 16:13] Renaming previous data. zh.html to ./d/2025-02-12_zh_reading_task.html
[13.02.2025 16:13] Writing Chinese reading task.
[13.02.2025 16:13] Writing result.
[13.02.2025 16:13] Renaming log file.
[13.02.2025 16:13] Renaming previous data. log.txt to ./logs/2025-02-13_last_log.txt
