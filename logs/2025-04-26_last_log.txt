[26.04.2025 06:30] Read previous papers.
[26.04.2025 06:30] Generating top page (month).
[26.04.2025 06:30] Writing top page (month).
[26.04.2025 12:42] Read previous papers.
[26.04.2025 12:42] Get feed.
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17761
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17192
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17502
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17432
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17207
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16511
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17670
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17789
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17040
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16828
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16064
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16921
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17414
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17343
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17069
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15921
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17601
[26.04.2025 12:42] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17788
[26.04.2025 12:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.04.2025 12:42] No deleted papers detected.
[26.04.2025 12:42] Downloading and parsing papers (pdf, html). Total: 18.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17761.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17761.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17761.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17192.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17192.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17192.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17502.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17502.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17502.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17432.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17432.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17432.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17207.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17207.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17207.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16511.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16511.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16511.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17670.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17670.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17670.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17789.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17789.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17789.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17040.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17040.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17040.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16828.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16828.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16828.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16064.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16064.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16064.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.16921.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.16921.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.16921.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17414.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17414.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17414.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17343.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17343.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17343.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17069.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17069.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17069.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.15921.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.15921.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.15921.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17601.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17601.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17601.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Downloading and parsing paper https://huggingface.co/papers/2504.17788.
[26.04.2025 12:42] Extra JSON file exists (./assets/json/2504.17788.json), skip PDF parsing.
[26.04.2025 12:42] Paper image links file exists (./assets/img_data/2504.17788.json), skip HTML parsing.
[26.04.2025 12:42] Success.
[26.04.2025 12:42] Enriching papers with extra data.
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 0. In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a ...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 1. Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific d...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 2. Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability -- ranging from enhanced personalization in image generation to consisten...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 3. The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-tex...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 4. We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for env...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 5. Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approach...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 6. With the advent of large-scale 3D datasets, feed-forward 3D generative models, such as the Large Reconstruction Model (LRM), have gained significant attention and achieved remarkable success. However, we observe that RGB images often lead to conflicting training objectives and lack the necessary cla...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 7. Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training a...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 8. We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embedd...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 9. Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the sol...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 10. Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 11. Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fal...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 12. Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DV-TON, a novel diffusion-based framework for generating high-fidelity and t...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 13. The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Larg...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 14. Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural o...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 15. We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a system to summarise hour long videos with no-supervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are spar...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 16. Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining t...
[26.04.2025 12:42] ********************************************************************************
[26.04.2025 12:42] Abstract 17. Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation. However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation. Furthermore, annotating dynamic Internet videos pr...
[26.04.2025 12:42] Read previous papers.
[26.04.2025 12:42] Generating reviews via LLM API.
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#data", "#diffusion", "#training", "#multimodal", "#cv", "#dataset", "#open_source", "#benchmark"], "emoji": "🖼️", "ru": {"title": "Step1X-Edit: Открытая модель редактирования изображений на уровне лучших закрытых решений", "desc": "Статья представляет новую модель редактирования из
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#agents", "#architecture", "#benchmark", "#dataset", "#open_source", "#science"], "emoji": "🤖", "ru": {"title": "От статьи к коду: автоматизация реализации алгоритмов машинного обучения", "desc": "PaperCoder - это мультиагентная система на основе больших языковых моделей (LLM), кото
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#dataset", "#multimodal"], "emoji": "🖼️", "ru": {"title": "RefVNLI: Новый стандарт оценки генерации изображений по тексту", "desc": "Статья представляет новый метод оценки генерации изображений по текстовому описанию с сохранением визуальной идентичности объекта
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#optimization", "#multimodal"], "emoji": "🔍", "ru": {"title": "UniME: Улучшение мультимодальных представлений через дистилляцию знаний и инструктивную настройку", "desc": "UniME - это новый двухэтапный фреймворк для обучения дискриминативным мульт
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#cv", "#benchmark", "#agents"], "emoji": "🔄", "ru": {"title": "Улучшение пространственного мышления ИИ через абстрактное изменение перспективы", "desc": "Статья представляет фреймворк для перспективно-ориентированных рассуждений в визуально-языковых моде
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#benchmark", "#training", "#data", "#optimization"], "emoji": "🔄", "ru": {"title": "Баланс качества и разнообразия данных для улучшения языковых моделей", "desc": "Эта статья представляет QuaDMix - новый подход к оптимизации данных для обучения больших языковых моделей (LLM). QuaDMi
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#3d", "#optimization", "#dataset"], "emoji": "🧊", "ru": {"title": "Разделяй и властвуй: новый подход к 3D-реконструкции", "desc": "Статья представляет DiMeR - новую модель для реконструкции трехмерных объектов по нескольким изображениям. Основная идея заключается в разделении входны
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#architecture", "#training", "#diffusion", "#cv", "#benchmark", "#optimization", "#multimodal"], "emoji": "🔀", "ru": {"title": "Token-Shuffle: прорыв в высокоэффективной генерации изображений высокого разрешения", "desc": "Статья представляет метод Token-Shuffle для улучшения авторе
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#cv", "#optimization", "#inference", "#multimodal"], "emoji": "🔬", "ru": {"title": "Динамическое сокращение токенов для эффективных VLM без переобучения", "desc": "DyMU - это эффективный фреймворк для снижения вычислительной нагрузки vision-language моделей без дополнительного обуче
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#data", "#reasoning", "#training", "#benchmark"], "emoji": "🧠", "ru": {"title": "ThinkPRM: Эффективная верификация решений с минимальным обучением", "desc": "Статья представляет ThinkPRM - новый подход к созданию процессных моделей вознаграждения (P
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#cv", "#inference", "#optimization", "#training", "#diffusion", "#architecture"], "emoji": "🖼️", "ru": {"title": "Объединение латентных и семантических представлений для улучшенной генерации изображений", "desc": "Статья представляет новый подход к генеративному моделированию изобра
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#open_source", "#multilingual", "#dataset", "#low_resource", "#benchmark"], "emoji": "🌐", "ru": {"title": "IberBench: многоязычная оценка LLM за пределами английского", "desc": "IberBench - это комплексный и расширяемый бенчмарк для оценки производительности крупных языковых моделей
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#video", "#3d", "#leakage", "#diffusion", "#dataset", "#benchmark"], "emoji": "👚", "ru": {"title": "3D-модели для реалистичной виртуальной примерки одежды в видео", "desc": "3DV-TON - это новый подход к виртуальной примерке одежды в видео, основанный на диффузионных моделях. Метод и
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#video", "#long_context", "#dataset", "#games", "#benchmark"], "emoji": "🎥", "ru": {"title": "TimeChat-Online: революция в понимании потокового видео", "desc": "TimeChat-Online - это новая модель VideoLLM для обработки потокового видео в реальном времени. Ключевой компонент - модуль
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training"], "emoji": "🧩", "ru": {"title": "Умная генерация изображений: правильный порядок имеет значение", "desc": "Статья представляет новый подход к авторегрессивной генерации изображений по патчам. Авторы предлагают обучать модель генерировать патчи в пр
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#video", "#multimodal", "#dataset", "#optimization", "#transfer_learning"], "emoji": "🎥", "ru": {"title": "ViSMap: Умное суммирование видео без учителя", "desc": "ViSMap - это система для несупервизорного суммаризации длинных видео, использующая метапромптинг. Она преодолевает разры
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#optimization", "#data", "#interpretability", "#architecture"], "emoji": "🔬", "ru": {"title": "Интерпретируемое нелинейное снижение размерности: мощность и прозрачность", "desc": "Эта статья представляет новый метод снижения размерности данных, сочетающий интерпретируемость линейных
[26.04.2025 12:42] Using data from previous issue: {"categories": ["#dataset", "#data", "#cv"], "emoji": "🎥", "ru": {"title": "DynPose-100K: Масштабный датасет для оценки положения камеры в динамических видео", "desc": "DynPose-100K - это крупномасштабный набор данных динамических видео из интернета с аннотациями положения камеры. Авторы разработали
[26.04.2025 12:42] Trying to get texts in Chinese.
[26.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.
[26.04.2025 12:42] Mistral response. {"id": "4311b5121e1647238b774de4bcbfe024", "object": "chat.completion", "created": 1745671332, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd1\u5e74\u6765\uff0c\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u6700\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578b\u5982GPT-4o\u548cGemini2 Flash\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u6ee1\u8db3\u5927\u591a\u6570\u7528\u6237\u9700\u6c42\uff0c\u4f46\u5f00\u6e90\u7b97\u6cd5\u4e0e\u8fd9\u4e9b\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u4ecd\u6709\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3aStep1X-Edit\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u4f9b\u4e0e\u95ed\u6e90\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528\u591a\u6a21\u6001LLM\u5904\u7406\u53c2\u8003\u56fe\u50cf\u548c\u7528\u6237\u6307\u4ee4\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u56fe\u50cf\u89e3\u7801\u5668\u751f\u6210\u76ee\u6807\u56fe\u50cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cStep1X-Edit\u5728\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u8981\u8d21\u732e\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 323, "total_tokens": 524, "completion_tokens": 201}}
[26.04.2025 12:42] Response: 近年来，图像编辑模型取得了显著进展。最新的多模态模型如GPT-4o和Gemini2 Flash展示了强大的图像编辑能力。这些模型能够满足大多数用户需求，但开源算法与这些闭源模型之间仍有差距。因此，我们发布了一个名为Step1X-Edit的图像编辑模型，旨在提供与闭源模型相媲美的性能。我们使用多模态LLM处理参考图像和用户指令，并结合扩散图像解码器生成目标图像。实验结果显示，Step1X-Edit在图像编辑领域取得了重要贡献。
[26.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

近年来，图像编辑模型取得了显著进展。最新的多模态模型如GPT-4o和Gemini2 Flash展示了强大的图像编辑能力。这些模型能够满足大多数用户需求，但开源算法与这些闭源模型之间仍有差距。因此，我们发布了一个名为Step1X-Edit的图像编辑模型，旨在提供与闭源模型相媲美的性能。我们使用多模态LLM处理参考图像和用户指令，并结合扩散图像解码器生成目标图像。实验结果显示，Step1X-Edit在图像编辑领域取得了重要贡献。
[26.04.2025 12:42] Mistral response. {"id": "9762da44a7614c8492b639dcebcb6a51", "object": "chat.completion", "created": 1745671336, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd1\u5e74\u6765\uff0c\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u6700\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578b\u5982GPT-4o\u548cGemini2 Flash\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u3002\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u6ee1\u8db3\u5927\u591a\u6570\u7528\u6237\u9700\u6c42\uff0c\u4f46\u5f00\u6e90\u7b97\u6cd5\u4e0e\u8fd9\u4e9b\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u4ecd\u6709\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3aStep1X-Edit\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u4f9b\u4e0e\u95ed\u6e90\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528\u591a\u6a21\u6001LLM\u5904\u7406\u53c2\u8003\u56fe\u50cf\u548c\u7528\u6237\u6307\u4ee4\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u56fe\u50cf\u89e3\u7801\u5668\u751f\u6210\u76ee\u6807\u56fe\u50cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cStep1X-Edit\u5728\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u8981\u8d21\u732e\u3002\n\nj\u00ecn ni\u00e1n l\u00e1i, t\u00fa xi\u00e0ng bi\u0101n j\u00ed m\u00f3 x\u00edng q\u01d4 d\u00e9 le xi\u01cen zh\u00f9 j\u00ecn zh\u01cen. zu\u00ec x\u012bn de du\u014d m\u00f3 shu\u00e0i m\u00f3 x\u00edng r\u00fa GPT-4o h\u00e9 Gemini2 Flash zh\u01cen sh\u00ec le qi\u00e1ng d\u00e0 de t\u00fa xi\u00e0ng bi\u0101n j\u00ed n\u00e9ng l\u00ec. zh\u00e8 xi\u0113 m\u00f3 x\u00edng n\u00e9ng g\u00f2u d\u00e0 du\u014d sh\u00f9 y\u00f2ng h\u00f9 x\u016b qi\u00fa, d\u00e0n k\u0101i yu\u00e1n su\u00e0n f\u01ce y\u01d4 zh\u00e8 xi\u0113 b\u00ec yu\u00e1n m\u00f3 x\u00edng zh\u012b ji\u0101n r\u00e9ng y\u01d2u ch\u0101 j\u00f9. y\u012bn c\u01d0, w\u01d2 men f\u0101 b\u00f9 le y\u012b g\u00e8 m\u00edng w\u00e8i Step1X-Edit de t\u00fa xi\u00e0ng bi\u0101n j\u00ed m\u00f3 x\u00edng, zh\u01d0 y\u01d0n t\u00ed g\u014dng y\u01d4 b\u00ec yu\u00e1n m\u00f3 x\u00edng xi\u0101ng b\u01d0 m\u011bi de x\u00ecng n\u00e9ng. w\u01d2 men sh\u01d0 y\u00f2ng du\u014d m\u00f3 shu\u00e0i LLM ch\u01d4 l\u01d0 c\u0101n k\u01ceo t\u00fa xi\u00e0ng h\u00e9 y\u00f2ng h\u00f9 zh\u01d0 l\u01d0ng, b\u00ecng ji\u00e9 h\u00e9 ku\u00f2 s\u00e0n t\u00fa xi\u00e0ng ji\u011b m\u01ce q\u00ec sh\u0113ng ch\u00e9ng m\u00f9 bi\u0101o t\u00fa xi\u00e0ng. sh\u00ed y\u00e0n ji\u00e9 gu\u01d2 xi\u01cen sh\u00ec, Step1X-Edit z\u00e0i t\u00fa xi\u00e0ng bi\u0101n j\u00ed l\u01d0ng y\u00f9 q\u01d4 d\u00e9 le zh\u00f2ng y\u00e0o g\u00f2ng xi\u00e0n."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 215, "total_tokens": 841, "completion_tokens": 626}}
[26.04.2025 12:42] Response: 近年来，图像编辑模型取得了显著进展。最新的多模态模型如GPT-4o和Gemini2 Flash展示了强大的图像编辑能力。这些模型能够满足大多数用户需求，但开源算法与这些闭源模型之间仍有差距。因此，我们发布了一个名为Step1X-Edit的图像编辑模型，旨在提供与闭源模型相媲美的性能。我们使用多模态LLM处理参考图像和用户指令，并结合扩散图像解码器生成目标图像。实验结果显示，Step1X-Edit在图像编辑领域取得了重要贡献。

jìn nián lái, tú xiàng biān jí mó xíng qǔ dé le xiǎn zhù jìn zhǎn. zuì xīn de duō mó shuài mó xíng rú GPT-4o hé Gemini2 Flash zhǎn shì le qiáng dà de tú xiàng biān jí néng lì. zhè xiē mó xíng néng gòu dà duō shù yòng hù xū qiú, dàn kāi yuán suàn fǎ yǔ zhè xiē bì yuán mó xíng zhī jiān réng yǒu chā jù. yīn cǐ, wǒ men fā bù le yī gè míng wèi Step1X-Edit de tú xiàng biān jí mó xíng, zhǐ yǐn tí gōng yǔ bì yuán mó xíng xiāng bǐ měi de xìng néng. wǒ men shǐ yòng duō mó shuài LLM chǔ lǐ cān kǎo tú xiàng hé yòng hù zhǐ lǐng, bìng jié hé kuò sàn tú xiàng jiě mǎ qì shēng chéng mù biāo tú xiàng. shí yàn jié guǒ xiǎn shì, Step1X-Edit zài tú xiàng biān jí lǐng yù qǔ dé le zhòng yào gòng xiàn.
[26.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

近年来，图像编辑模型取得了显著进展。最新的多模态模型如GPT-4o和Gemini2 Flash展示了强大的图像编辑能力。这些模型能够满足大多数用户需求，但开源算法与这些闭源模型之间仍有差距。因此，我们发布了一个名为Step1X-Edit的图像编辑模型，旨在提供与闭源模型相媲美的性能。我们使用多模态LLM处理参考图像和用户指令，并结合扩散图像解码器生成目标图像。实验结果显示，Step1X-Edit在图像编辑领域取得了重要贡献。
[26.04.2025 12:42] Mistral response. {"id": "012281961da245dd98c7e0f8874826aa", "object": "chat.completion", "created": 1745671347, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8fd1\u5e74\u6765\", \"pinyin\": \"j\u00ecn ni\u00e1n l\u00e1i\", \"trans\": \"in recent years\"},\n    {\"word\": \"\u56fe\u50cf\", \"pinyin\": \"t\u00fa xi\u00e0ng\", \"trans\": \"image\"},\n    {\"word\": \"\u7f16\u8f91\", \"pinyin\": \"bi\u0101n j\u00ed\", \"trans\": \"edit\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u53d6\u5f97\", \"pinyin\": \"q\u01d4 d\u00e9\", \"trans\": \"achieve\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u8fdb\u5c55\", \"pinyin\": \"j\u00ecn zh\u01cen\", \"trans\": \"progress\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 t\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u5c55\u793a\", \"pinyin\": \"zh\u01cen sh\u00ec\", \"trans\": \"demonstrate\"},\n    {\"word\": \"\u5f3a\u5927\", \"pinyin\": \"qi\u00e1ng d\u00e0\", \"trans\": \"powerful\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ng l\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u6ee1\u8db3\", \"pinyin\": \"m\u01cen z\u00fa\", \"trans\": \"satisfy\"},\n    {\"word\": \"\u9700\u6c42\", \"pinyin\": \"x\u016b qi\u00fa\", \"trans\": \"demand\"},\n    {\"word\": \"\u5f00\u6e90\", \"pinyin\": \"k\u0101i yu\u00e1n\", \"trans\": \"open-source\"},\n    {\"word\": \"\u7b97\u6cd5\", \"pinyin\": \"su\u00e0n f\u01ce\", \"trans\": \"algorithm\"},\n    {\"word\": \"\u95ed\u6e90\", \"pinyin\": \"b\u00ec yu\u00e1n\", \"trans\": \"closed-source\"},\n    {\"word\": \"\u5dee\u8ddd\", \"pinyin\": \"ch\u0101 j\u00f9\", \"trans\": \"gap\"},\n    {\"word\": \"\u53d1\u5e03\", \"pinyin\": \"f\u0101 b\u00f9\", \"trans\": \"release\"},\n    {\"word\": \"\u540d\u4e3a\", \"pinyin\": \"m\u00edng w\u00e9i\", \"trans\": \"named\"},\n    {\"word\": \"\u65e8\u5728\", \"pinyin\": \"zh\u01d0 z\u00e0i\", \"trans\": \"aim to\"},\n    {\"word\": \"\u63d0\u4f9b\", \"pinyin\": \"t\u00ed g\u014dng\", \"trans\": \"provide\"},\n    {\"word\": \"\u76f8\u5ab2\u7f8e\", \"pinyin\": \"xi\u0101ng p\u00ec m\u011bi\", \"trans\": \"compare favorably with\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecng n\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u4f7f\u7528\", \"pinyin\": \"sh\u01d0 y\u00f2ng\", \"trans\": \"use\"},\n    {\"word\": \"\u5904\u7406\", \"pinyin\": \"ch\u01d4 l\u01d0\", \"trans\": \"process\"},\n    {\"word\": \"\u53c2\u8003\", \"pinyin\": \"c\u0101n k\u01ceo\", \"trans\": \"reference\"},\n    {\"word\": \"\u6307\u4ee4\", \"pinyin\": \"zh\u01d0 l\u00ecng\", \"trans\": \"instruction\"},\n    {\"word\": \"\u7ed3\u5408\", \"pinyin\": \"ji\u00e9 h\u00e9\", \"trans\": \"combine\"},\n    {\"word\": \"\u6269\u6563\", \"pinyin\": \"ku\u00f2 s\u00e0n\", \"trans\": \"diffusion\"},\n    {\"word\": \"\u89e3\u7801\u5668\", \"pinyin\": \"ji\u011b m\u01ce q\u00ec\", \"trans\": \"decoder\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ng ch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u76ee\u6807\", \"pinyin\": \"m\u00f9 bi\u0101o\", \"trans\": \"target\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00ed y\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9 gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u663e\u793a\", \"pinyin\": \"xi\u01cen sh\u00ec\", \"trans\": \"show\"},\n    {\"word\": \"\u9886\u57df\", \"pinyin\": \"l\u01d0ng y\u00f9\", \"trans\": \"field\"},\n    {\"word\": \"\u53d6\u5f97\", \"pinyin\": \"q\u01d4 d\u00e9\", \"trans\": \"achieve\"},\n    {\"word\": \"\u91cd\u8981\", \"pinyin\": \"zh\u00f2ng y\u00e0o\", \"trans\": \"important\"},\n    {\"word\": \"\u8d21\u732e\", \"pinyin\": \"g\u00f2ng xi\u00e0n\", \"trans\": \"contribution\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 247, "total_tokens": 1360, "completion_tokens": 1113}}
[26.04.2025 12:42] Response: [
    {"word": "近年来", "pinyin": "jìn nián lái", "trans": "in recent years"},
    {"word": "图像", "pinyin": "tú xiàng", "trans": "image"},
    {"word": "编辑", "pinyin": "biān jí", "trans": "edit"},
    {"word": "模型", "pinyin": "mó xíng", "trans": "model"},
    {"word": "取得", "pinyin": "qǔ dé", "trans": "achieve"},
    {"word": "显著", "pinyin": "xiǎn zhù", "trans": "significant"},
    {"word": "进展", "pinyin": "jìn zhǎn", "trans": "progress"},
    {"word": "多模态", "pinyin": "duō mó tài", "trans": "multimodal"},
    {"word": "展示", "pinyin": "zhǎn shì", "trans": "demonstrate"},
    {"word": "强大", "pinyin": "qiáng dà", "trans": "powerful"},
    {"word": "能力", "pinyin": "néng lì", "trans": "ability"},
    {"word": "满足", "pinyin": "mǎn zú", "trans": "satisfy"},
    {"word": "需求", "pinyin": "xū qiú", "trans": "demand"},
    {"word": "开源", "pinyin": "kāi yuán", "trans": "open-source"},
    {"word": "算法", "pinyin": "suàn fǎ", "trans": "algorithm"},
    {"word": "闭源", "pinyin": "bì yuán", "trans": "closed-source"},
    {"word": "差距", "pinyin": "chā jù", "trans": "gap"},
    {"word": "发布", "pinyin": "fā bù", "trans": "release"},
    {"word": "名为", "pinyin": "míng wéi", "trans": "named"},
    {"word": "旨在", "pinyin": "zhǐ zài", "trans": "aim to"},
    {"word": "提供", "pinyin": "tí gōng", "trans": "provide"},
    {"word": "相媲美", "pinyin": "xiāng pì měi", "trans": "compare favorably with"},
    {"word": "性能", "pinyin": "xìng néng", "trans": "performance"},
    {"word": "使用", "pinyin": "shǐ yòng", "trans": "use"},
    {"word": "处理", "pinyin": "chǔ lǐ", "trans": "process"},
    {"word": "参考", "pinyin": "cān kǎo", "trans": "reference"},
    {"word": "指令", "pinyin": "zhǐ lìng", "trans": "instruction"},
    {"word": "结合", "pinyin": "jié hé", "trans": "combine"},
    {"word": "扩散", "pinyin": "kuò sàn", "trans": "diffusion"},
    {"word": "解码器", "pinyin": "jiě mǎ qì", "trans": "decoder"},
    {"word": "生成", "pinyin": "shēng chéng", "trans": "generate"},
    {"word": "目标", "pinyin": "mù biāo", "trans": "target"},
    {"word": "实验", "pinyin": "shí yàn", "trans": "experiment"},
    {"word": "结果", "pinyin": "jié guǒ", "trans": "result"},
    {"word": "显示", "pinyin": "xiǎn shì", "trans": "show"},
    {"word": "领域", "pinyin": "lǐng yù", "trans": "field"},
    {"word": "取得", "pinyin": "qǔ dé", "trans": "achieve"},
    {"word": "重要", "pinyin": "zhòng yào", "trans": "important"},
    {"word": "贡献", "pinyin": "gòng xiàn", "trans": "contribution"}
]
[26.04.2025 12:42] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

近年来，图像编辑模型取得了显著进展。最新的多模态模型如GPT-4o和Gemini2 Flash展示了强大的图像编辑能力。这些模型能够满足大多数用户需求，但开源算法与这些闭源模型之间仍有差距。因此，我们发布了一个名为Step1X-Edit的图像编辑模型，旨在提供与闭源模型相媲美的性能。我们使用多模态LLM处理参考图像和用户指令，并结合扩散图像解码器生成目标图像。实验结果显示，Step1X-Edit在图像编辑领域取得了重要贡献。
[26.04.2025 12:42] Mistral response. {"id": "0b2655deee6f4a1bbb59f4553bca2ab8", "object": "chat.completion", "created": 1745671357, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "In recent years, image editing models have made significant progress. The latest multimodal models such as GPT-4o and Gemini2 Flash have demonstrated powerful image editing capabilities. These models can meet the needs of most users, but there is still a gap between open-source algorithms and these closed-source models. Therefore, we have released an image editing model called Step1X-Edit, aiming to provide performance comparable to closed-source models. We use multimodal LLM to process reference images and user instructions, combined with a diffusion image decoder to generate target images. Experimental results show that Step1X-Edit has made important contributions in the field of image editing."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 213, "total_tokens": 357, "completion_tokens": 144}}
[26.04.2025 12:42] Response: In recent years, image editing models have made significant progress. The latest multimodal models such as GPT-4o and Gemini2 Flash have demonstrated powerful image editing capabilities. These models can meet the needs of most users, but there is still a gap between open-source algorithms and these closed-source models. Therefore, we have released an image editing model called Step1X-Edit, aiming to provide performance comparable to closed-source models. We use multimodal LLM to process reference images and user instructions, combined with a diffusion image decoder to generate target images. Experimental results show that Step1X-Edit has made important contributions in the field of image editing.
[26.04.2025 12:42] Renaming data file.
[26.04.2025 12:42] Renaming previous data. hf_papers.json to ./d/2025-04-25.json
[26.04.2025 12:42] Saving new data file.
[26.04.2025 12:42] Generating page.
[26.04.2025 12:42] Renaming previous page.
[26.04.2025 12:42] Renaming previous data. index.html to ./d/2025-04-25.html
[26.04.2025 12:42] [Experimental] Generating Chinese page for reading.
[26.04.2025 12:42] Chinese vocab [{'word': '近年来', 'pinyin': 'jìn nián lái', 'trans': 'in recent years'}, {'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '编辑', 'pinyin': 'biān jí', 'trans': 'edit'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}, {'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '强大', 'pinyin': 'qiáng dà', 'trans': 'powerful'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '满足', 'pinyin': 'mǎn zú', 'trans': 'satisfy'}, {'word': '需求', 'pinyin': 'xū qiú', 'trans': 'demand'}, {'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'}, {'word': '算法', 'pinyin': 'suàn fǎ', 'trans': 'algorithm'}, {'word': '闭源', 'pinyin': 'bì yuán', 'trans': 'closed-source'}, {'word': '差距', 'pinyin': 'chā jù', 'trans': 'gap'}, {'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}, {'word': '名为', 'pinyin': 'míng wéi', 'trans': 'named'}, {'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}, {'word': '相媲美', 'pinyin': 'xiāng pì měi', 'trans': 'compare favorably with'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'}, {'word': '参考', 'pinyin': 'cān kǎo', 'trans': 'reference'}, {'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'instruction'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '解码器', 'pinyin': 'jiě mǎ qì', 'trans': 'decoder'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'target'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'show'}, {'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'field'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '重要', 'pinyin': 'zhòng yào', 'trans': 'important'}, {'word': '贡献', 'pinyin': 'gòng xiàn', 'trans': 'contribution'}]
[26.04.2025 12:42] Renaming previous Chinese page.
[26.04.2025 12:42] Renaming previous data. zh.html to ./d/2025-04-25_zh_reading_task.html
[26.04.2025 12:42] Writing Chinese reading task.
[26.04.2025 12:42] Writing result.
[26.04.2025 12:42] Renaming log file.
[26.04.2025 12:42] Renaming previous data. log.txt to ./logs/2025-04-26_last_log.txt
