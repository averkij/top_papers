[23.01.2025 06:14] Read previous papers.
[23.01.2025 06:14] Generating top page (month).
[23.01.2025 06:14] Writing top page (month).
[23.01.2025 07:10] Read previous papers.
[23.01.2025 07:10] Get feed.
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12948
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12909
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.13074
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.13106
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12599
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12895
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.13007
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.11067
[23.01.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12570
[23.01.2025 07:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.01.2025 07:10] No deleted papers detected.
[23.01.2025 07:10] Downloading and parsing papers (pdf, html). Total: 9.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.12948.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.12948.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.12948.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.12909.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.12909.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.12909.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.13074.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.13074.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.13074.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.13106.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.13106.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.13106.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.12599.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.12599.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.12599.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.12895.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.12895.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.12895.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.13007.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.13007.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.13007.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.11067.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.11067.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.11067.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2501.12570.
[23.01.2025 07:10] Extra JSON file exists (./assets/json/2501.12570.json), skip PDF parsing.
[23.01.2025 07:10] Paper image links file exists (./assets/img_data/2501.12570.json), skip HTML parsing.
[23.01.2025 07:10] Success.
[23.01.2025 07:10] Enriching papers with extra data.
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 0. We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero...
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 1. Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LL...
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 2. Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, l...
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 3. In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: the vision-centric training paradigm and vision-centric framework design. Th...
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 4. Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large languag...
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 5. Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing...
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 6. Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large Language Models (LLMs), relies on reward models to select the best candidate solution from multiple generations. However, traditional reward models often assign arbitrary and inconsistent scores, limiting their effectiveness....
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 7. Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and...
[23.01.2025 07:10] ********************************************************************************
[23.01.2025 07:10] Abstract 8. Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning ...
[23.01.2025 07:10] Read previous papers.
[23.01.2025 07:10] Generating reviews via LLM API.
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#open_source", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–Ω–æ–º—É –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π DeepSeek-R1-Zero –∏ DeepSeek-R1. DeepSeek
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#multimodal", "#story_generation", "#3d", "#open_source", "#agents", "#hallucinations"], "emoji": "üé¨", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –∫–∏–Ω–æ—Å—Ç—É–¥–∏—è: –ò–ò-–∞–≥–µ–Ω—Ç—ã —Å–æ–∑–¥–∞—é—Ç —Ñ–∏–ª—å–º—ã –æ—Ç –∏–¥–µ–∏ –¥–æ –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞", "desc": "FilmAgent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ—Ç–±–æ—Ä —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª—è–º Mixture-of-Experts (MoE) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Autonomy-of-Experts (AoE). –í AoE —ç–∫—Å–ø–µ—Ä—Ç—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#agi", "#games", "#video", "#benchmark"], "emoji": "üé•", "ru": {"title": "VideoLLaMA3: –ó—Ä–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ", "desc": "VideoLLaMA3 - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#training", "#benchmark", "#rl", "#reasoning", "#long_context", "#math"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Kimi 
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#rlhf", "#training", "#alignment", "#inference"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ª–µ—Ç—É: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Test-time Preference Optimization (TPO), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥–Ω—ã–µ
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#dataset", "#math", "#rlhf"], "emoji": "üèÜ", "ru": {"title": "–ü–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è –≤ LLM", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#open_source", "#games", "#optimization", "#graphs", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "IntellAgent: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ –ò–ò", "desc": "IntellAgent - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω
[23.01.2025 07:10] Using data from previous issue: {"categories": ["#reasoning", "#math", "#optimization", "#training", "#benchmark", "#inference"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ OpenAI's O1. –ê–≤—Ç–æ—Ä—ã –ø—Ä
[23.01.2025 07:10] Loading Chinese text from previous data.
[23.01.2025 07:10] Renaming data file.
[23.01.2025 07:10] Renaming previous data. hf_papers.json to ./d/2025-01-23.json
[23.01.2025 07:10] Saving new data file.
[23.01.2025 07:10] Generating page.
[23.01.2025 07:10] Renaming previous page.
[23.01.2025 07:10] Renaming previous data. index.html to ./d/2025-01-23.html
[23.01.2025 07:10] [Experimental] Generating Chinese page for reading.
[23.01.2025 07:10] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«î l«ê', 'trans': 'handle'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n wu', 'trans': 'task'}, {'word': 'ÈáçË¶ÅÊÄß', 'pinyin': 'zh√≤ng y√†o x√¨ng', 'trans': 'importance'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†n y«íu', 'trans': 'existing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Ê®°‰ªø', 'pinyin': 'm√≥ f«éng', 'trans': 'imitate'}, {'word': '‰∏ìÂÆ∂', 'pinyin': 'zhuƒÅn jiƒÅ', 'trans': 'expert'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'Â§±Ë¥•', 'pinyin': 'shƒ´ b√†i', 'trans': 'fail'}, {'word': 'ÊÅ¢Â§ç', 'pinyin': 'huƒ´ f√π', 'trans': 'recover'}, {'word': 'Ëø≠‰ª£', 'pinyin': 'di√© d√†i', 'trans': 'iterate'}, {'word': 'Ëá™ËÆ≠ÁªÉ', 'pinyin': 'z√¨ x√πn li√†n', 'trans': 'self-training'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': '‰ª£ÁêÜ', 'pinyin': 'd√†i l«ê', 'trans': 'agent'}, {'word': 'ÂèçÊÄù', 'pinyin': 'f«én sƒ´', 'trans': 'reflect'}, {'word': 'ËíôÁâπÂç°ÁΩóÊ†ëÊêúÁ¥¢', 'pinyin': 'm√©ng t√® k«é lu√≥ sh√π s≈çu su«í', 'trans': 'Monte Carlo Tree Search'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√π j√¨ng', 'trans': 'path'}, {'word': 'Á∫†Ê≠£', 'pinyin': 'ji≈´ zh√®ng', 'trans': 'correct'}, {'word': 'Ë°å‰∏∫', 'pinyin': 'x√≠ng w√©i', 'trans': 'behavior'}, {'word': '‰∫íÂä®', 'pinyin': 'h√π d√≤ng', 'trans': 'interactive'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°n j√¨ng', 'trans': 'environment'}, {'word': 'Âü∫Á∫ø', 'pinyin': 'jƒ´ xi√†n', 'trans': 'baseline'}]
[23.01.2025 07:10] Renaming previous Chinese page.
[23.01.2025 07:10] Renaming previous data. zh.html to ./d/2025-01-22_zh_reading_task.html
[23.01.2025 07:10] Writing Chinese reading task.
[23.01.2025 07:10] Writing result.
[23.01.2025 07:10] Renaming log file.
[23.01.2025 07:10] Renaming previous data. log.txt to ./logs/2025-01-23_last_log.txt
