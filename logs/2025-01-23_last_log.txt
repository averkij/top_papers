[23.01.2025 02:08] Read previous papers.
[23.01.2025 02:08] Generating top page (month).
[23.01.2025 02:08] Writing top page (month).
[23.01.2025 03:12] Read previous papers.
[23.01.2025 03:12] Get feed.
[23.01.2025 03:12] Extract page data from URL. URL: https://huggingface.co/papers/2501.12570
[23.01.2025 03:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.01.2025 03:12] Downloading and parsing papers (pdf, html). Total: 1.
[23.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.12570.
[23.01.2025 03:12] Downloading paper 2501.12570 from http://arxiv.org/pdf/2501.12570v1...
[23.01.2025 03:12] Extracting affiliations from text.
[23.01.2025 03:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning Haotian Luo 1 Li Shen 1 Haiying He 2 Yibo Wang 3 Shiwei Liu 4 Wei Li 5 Naiqiang Tan 5 Xiaochun Cao 1 Dacheng Tao "
[23.01.2025 03:12] Response: []
[23.01.2025 03:12] Extracting affiliations from text.
[23.01.2025 03:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning Haotian Luo 1 Li Shen 1 Haiying He 2 Yibo Wang 3 Shiwei Liu 4 Wei Li 5 Naiqiang Tan 5 Xiaochun Cao 1 Dacheng TaoRecently, long-thought reasoning LLMs, such as OpenAIs O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the models problem-solving abilities and has achieved promising results. However, longthought reasoning process leads to substantial increase in inference time. pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLMs baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner 5 2 0 2 2 2 ] . [ 1 0 7 5 2 1 . 1 0 5 2 : r 1. Introduction Reasoning represents fundamental capability of large language models (LLMs), serving as cornerstone in the advancement of artificial intelligence research (Huang & Chang, 2023). Recently OpenAIs O1(OpenAI, 2024) 1Shenzhen Campus of Sun Yat-sen University 2China Agriculture University 3Tsinghua University 4University of Oxford 5Didichuxing Co. Ltd 6Nanyang Technological University. Correspondence to: Li Shen <mathshenli@gmail.com>. 1 have introduced long-thought reasoning models that mimic human-like problem-solving processes. In addition to O1, researchers have also developed models that inference with similar long-thought reasoning pattern, such as DeepseekR1 (DeepSeek, 2024), QwQ (Qwen, 2024) and Marcoo1(Zhao et al., 2024). These models leverage long chainof-thought framework, enabling them to tackle complex problems by iteratively identifying and correcting errors, simplifying intricate steps, and exploring alternative strategies when initial approaches prove inadequate. Furthermore, Mulberry (Yao et al., 2024) has demonstrated that O1-Like reasoning can also play significant role in multimodal reasoning. This reasoning paradigm significantly enhances the problem-solving capabilities of large language models (LLMs) by allowing them to approach complex tasks in more systematic and human-like manner, demonstrating an ability to handle problems that would otherwise be challenging or intractable for conventional LLMs. While long-thought reasoning enhances reasoning capabilities and improves accuracy, it is accompanied by longer output sequences, which result in increased computational overhead. critical challenge lies in developing mechanisms that enable LLMs to dynamically adjust the length and complexity of their reasoning processes in accordance with the difficulty of the problems they encounter. In this paper, we first revisit the long-thought reasoning processes. we observe that the reasoning processes in longthought reasoning LLMs often exhibit significant redundancies, which leads to inefficient use of computational resources. This inefficiency not only increases inference costs but also highlights fundamental limitation in the models ability to adapt their reasoning depth to suit the demands of diverse tasks. Building on this analysis, we formulate an optimization objective aimed at minimizing reasoning overhead while maintaining accuracy as constraint. To achieve this objective, we propose RL-based loss function. Our experiments are conducted using opensource long-thought reasoning LLMs, and we compare our approach against several competing methods like SFT and DPO (Rafailov et al., 2024). Through extensive experiments, we demonstrate the efficiency of our proposed methods. Additionally, we perform further studies on the influence of hyperparameters and dataset diffucilty on our approach, in O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning order to gain deeper insights into the characteristics and behavior of this novel framework. extending the length of the solution generated during reasoning can also significantly enhance the models performance. In conclusion, our contributions can be outlined as follows: 2.3. LLM Alignment We identify and experimentally validate the existence of length disharmony in the reasoning process of long thought models, which leads to redundant inference overhead. We formulate an optimization problem aimed at improving model inference efficiency while maintaining accuracy, and based on this, we propose LengthHarmonizing Fine-Tuning (O1-Pruner) approach. Through extensive experiments, we demonstrate the effectiveness of O1-Pruner and conduct in-depth analyses, to provide insights and inspiration for future research in this area. 2. Related Work 2.1. Chain of Thought Chain-of-Thought (CoT) (Wei et al., 2023) prompting has emerged as powerful technique for improving the reasoning capabilities of large language models (LLMs). By encouraging the model to solve complex problems step by step, CoT significantly enhances the accuracy and interpretability of its outputs. CoT is particularly effective for tasks that requiring multiple solving steps, such as mathematical problem-solving and logical reasoning. Beyond the basic CoT paradigm, many innovative frameworks like Tree of Thought (ToT) (Yao et al., 2023) and Graph of Thought (GoT) (Besta et al., 2024) expand upon the CoT architecture by investigating various reasoning trajectories or integrating structures based on networks. Besides, Chain-of-thought reasoning also furnishes discernible traces of logical progression, enabling human to comprehend the models decision-making pathway, thereby rendering the reasoning process both transparent and credible. 2.2. Inference-time Scaling Inference-time scaling refers to the ability of large language models (LLMs) to improve thei"
[23.01.2025 03:12] Mistral response. {"id": "3bebb3a7e7584de4a63f8267d5dc1ade", "object": "chat.completion", "created": 1737601933, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Shenzhen Campus of Sun Yat-sen University\", \"China Agriculture University\", \"Tsinghua University\", \"University of Oxford\", \"Didichuxing Co. Ltd\", \"Nanyang Technological University\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1535, "total_tokens": 1593, "completion_tokens": 58}}
[23.01.2025 03:12] Response: ```python
["Shenzhen Campus of Sun Yat-sen University", "China Agriculture University", "Tsinghua University", "University of Oxford", "Didichuxing Co. Ltd", "Nanyang Technological University"]
```
[23.01.2025 03:12] Deleting PDF ./assets/pdf/2501.12570.pdf.
[23.01.2025 03:12] Success.
[23.01.2025 03:12] Enriching papers with extra data.
[23.01.2025 03:12] ********************************************************************************
[23.01.2025 03:12] Abstract 0. Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning ...
[23.01.2025 03:12] Read previous papers.
[23.01.2025 03:12] Generating reviews via LLM API.
[23.01.2025 03:12] Querying the API.
[23.01.2025 03:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner
[23.01.2025 03:12] Response: {
  "desc": "Статья описывает метод оптимизации работы языковых моделей с длительным рассуждением, таких как OpenAI's O1. Авторы предлагают технику под названием Length-Harmonizing Fine-Tuning (O1-Pruner), которая сокращает время вывода, сохраняя точность модели. Метод использует предварительную выборку для оценки базовой производительности модели, а затем применяет обучение с подкреплением для генерации более коротких процессов рассуждения. Эксперименты на различных бенчмарках математического рассуждения показали, что O1-Pruner значительно снижает вычислительные затраты при сохранении или даже повышении точности.",
  "emoji": "⚡",
  "title": "Ускорение мышления ИИ без потери качества"
}
[23.01.2025 03:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner"

[23.01.2025 03:12] Response: ```python
['INFERENCE', 'TRAINING', 'MATH', 'BENCHMARK']
```
[23.01.2025 03:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner"

[23.01.2025 03:12] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[23.01.2025 03:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses a new approach to improve long-thought reasoning in large language models (LLMs) like OpenAI\'s O1. The authors identify that while these models enhance problem-solving, they also increase inference time due to inefficient token usage. To tackle this, they introduce Length-Harmonizing Fine-Tuning (O1-Pruner), which optimizes the reasoning process by balancing accuracy and efficiency. Their experiments show that O1-Pruner reduces inference overhead and improves accuracy on mathematical reasoning tasks, making it a valuable advancement in LLM performance.","title":"Optimizing Long-Thought Reasoning for Efficient Problem Solving"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc="This paper discusses a new approach to improve long-thought reasoning in large language models (LLMs) like OpenAI's O1. The authors identify that while these models enhance problem-solving, they also increase inference time due to inefficient token usage. To tackle this, they introduce Length-Harmonizing Fine-Tuning (O1-Pruner), which optimizes the reasoning process by balancing accuracy and efficiency. Their experiments show that O1-Pruner reduces inference overhead and improves accuracy on mathematical reasoning tasks, making it a valuable advancement in LLM performance.", title='Optimizing Long-Thought Reasoning for Efficient Problem Solving'))
[23.01.2025 03:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近，长思考推理的语言模型（LLM）如OpenAI的O1，采用了类似人类思考复杂问题的扩展推理过程。这种推理范式显著增强了模型的解决问题能力，并取得了良好的效果。然而，长思考推理过程导致推理时间大幅增加。为了解决这个问题，我们提出了长度协调微调（O1-Pruner），旨在在保持准确性的同时，减少长思考LLM的推理开销。","title":"优化推理效率，提升准确性！"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='最近，长思考推理的语言模型（LLM）如OpenAI的O1，采用了类似人类思考复杂问题的扩展推理过程。这种推理范式显著增强了模型的解决问题能力，并取得了良好的效果。然而，长思考推理过程导致推理时间大幅增加。为了解决这个问题，我们提出了长度协调微调（O1-Pruner），旨在在保持准确性的同时，减少长思考LLM的推理开销。', title='优化推理效率，提升准确性！'))
[23.01.2025 03:12] Loading Chinese text from previous data.
[23.01.2025 03:12] Renaming data file.
[23.01.2025 03:12] Renaming previous data. hf_papers.json to ./d/2025-01-23.json
[23.01.2025 03:12] Saving new data file.
[23.01.2025 03:12] Generating page.
[23.01.2025 03:12] Renaming previous page.
[23.01.2025 03:12] Renaming previous data. index.html to ./d/2025-01-23.html
[23.01.2025 03:12] [Experimental] Generating Chinese page for reading.
[23.01.2025 03:12] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'handle'}, {'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'}, {'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '模仿', 'pinyin': 'mó fǎng', 'trans': 'imitate'}, {'word': '专家', 'pinyin': 'zhuān jiā', 'trans': 'expert'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '失败', 'pinyin': 'shī bài', 'trans': 'fail'}, {'word': '恢复', 'pinyin': 'huī fù', 'trans': 'recover'}, {'word': '迭代', 'pinyin': 'dié dài', 'trans': 'iterate'}, {'word': '自训练', 'pinyin': 'zì xùn liàn', 'trans': 'self-training'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '代理', 'pinyin': 'dài lǐ', 'trans': 'agent'}, {'word': '反思', 'pinyin': 'fǎn sī', 'trans': 'reflect'}, {'word': '蒙特卡罗树搜索', 'pinyin': 'méng tè kǎ luó shù sōu suǒ', 'trans': 'Monte Carlo Tree Search'}, {'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'}, {'word': '纠正', 'pinyin': 'jiū zhèng', 'trans': 'correct'}, {'word': '行为', 'pinyin': 'xíng wéi', 'trans': 'behavior'}, {'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interactive'}, {'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'}, {'word': '基线', 'pinyin': 'jī xiàn', 'trans': 'baseline'}]
[23.01.2025 03:12] Renaming previous Chinese page.
[23.01.2025 03:12] Renaming previous data. zh.html to ./d/2025-01-22_zh_reading_task.html
[23.01.2025 03:12] Writing Chinese reading task.
[23.01.2025 03:12] Writing result.
[23.01.2025 03:12] Renaming log file.
[23.01.2025 03:12] Renaming previous data. log.txt to ./logs/2025-01-23_last_log.txt
