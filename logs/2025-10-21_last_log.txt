[21.10.2025 20:13] Read previous papers.
[21.10.2025 20:13] Generating top page (month).
[21.10.2025 20:13] Writing top page (month).
[21.10.2025 21:11] Read previous papers.
[21.10.2025 21:11] Get feed.
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17681
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16872
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17800
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17354
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17269
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15346
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17715
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16751
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16333
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17509
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16888
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17803
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17795
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17498
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15821
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16720
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15021
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17797
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17790
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17431
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16259
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16258
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14605
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17793
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16641
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16276
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15527
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16727
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16499
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16156
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15768
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16136
[21.10.2025 21:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06471
[21.10.2025 21:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.10.2025 21:11] No deleted papers detected.
[21.10.2025 21:11] Downloading and parsing papers (pdf, html). Total: 33.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17681.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17681.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17681.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16872.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16872.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16872.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17800.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17800.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17800.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17354.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17354.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17354.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17269.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17269.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17269.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.15346.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.15346.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.15346.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17715.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17715.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17715.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16751.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16751.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16751.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16333.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16333.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16333.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17509.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17509.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17509.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16888.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16888.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16888.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17803.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17803.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17803.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17795.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17795.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17795.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17498.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17498.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17498.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.15821.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.15821.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.15821.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16720.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16720.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16720.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.15021.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.15021.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.15021.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17797.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17797.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17797.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17790.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17790.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17790.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17431.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17431.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17431.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16259.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16259.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16259.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16258.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16258.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16258.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.14605.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.14605.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.14605.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.17793.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.17793.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.17793.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16641.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16641.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16641.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16276.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16276.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16276.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.15527.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.15527.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.15527.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16727.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16727.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16727.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16499.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16499.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16499.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16156.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16156.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16156.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.15768.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.15768.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.15768.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.16136.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.16136.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.16136.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Downloading and parsing paper https://huggingface.co/papers/2510.06471.
[21.10.2025 21:11] Extra JSON file exists (./assets/json/2510.06471.json), skip PDF parsing.
[21.10.2025 21:11] Paper image links file exists (./assets/img_data/2510.06471.json), skip HTML parsing.
[21.10.2025 21:11] Success.
[21.10.2025 21:11] Enriching papers with extra data.
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 0. PICABench and PICAEval evaluate physical realism in image editing by assessing eight sub-dimensions and using VLM-as-a-judge with human annotations, highlighting the need for physics-based solutions.  					AI-generated summary 				 Image editing has achieved remarkable progress recently. Modern edit...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 1. DeepAnalyze-8B, an agentic LLM, autonomously completes the data science pipeline from raw data to research reports using curriculum-based training and data-grounded trajectory synthesis.  					AI-generated summary 				 Autonomous data science, from raw data sources to analyst-grade deep research rep...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 2. Glyph compresses long textual inputs into images using vision-language models, achieving significant token compression and improved performance in long-context tasks.  					AI-generated summary 				 Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document un...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 3. Nyx, a unified mixed-modal retriever, enhances vision-language generation by retrieving and reasoning over mixed-modal data, outperforming existing RAG systems in real-world scenarios.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhanci...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 4. FineVision, a large-scale and curated dataset, enhances vision-language models through rigorous data collection, de-duplication, and human oversight, leading to improved performance.  					AI-generated summary 				 The advancement of vision-language models (VLMs) is hampered by a fragmented landscap...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 5. SAFE, a selective ensembling framework for large language models, improves long-form generation by considering tokenization mismatch and consensus in probability distributions, leading to better accuracy and efficiency.  					AI-generated summary 				 Ensembling Large Language Models (LLMs) has gain...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 6. QueST, a framework combining difficulty-aware graph sampling and fine-tuning, generates large-scale synthetic coding problems to enhance the performance of large language models in competitive coding and reasoning tasks.  					AI-generated summary 				 Large Language Models have achieved strong perf...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 7. Beam search in discrete visual autoregressive models enhances text-to-image generation more effectively than search in continuous diffusion models, highlighting architecture's importance over scale.  					AI-generated summary 				 While inference-time scaling through search has revolutionized Large ...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 8. Reinforcement Learning enhances vision encoders in Multimodal Language Models, leading to better visual representations and performance compared to Supervised Fine-tuning.  					AI-generated summary 				 A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is la...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 9. EliCal, a two-stage framework combining self-consistency supervision and minimal correctness annotations, achieves near-optimal honesty alignment in large language models with limited annotation effort.  					AI-generated summary 				 Honesty alignment-the ability of large language models (LLMs) to ...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 10. Edit-R1, a post-training framework using Diffusion Negative-aware Finetuning and a Multimodal Large Language Model, achieves state-of-the-art results in instruction-based image editing by addressing overfitting and lack of a universal reward model.  					AI-generated summary 				 Instruction-based i...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 11. ConsistEdit, a novel attention control method for MM-DiT, enhances image and video editing by ensuring consistency and fine-grained control across all inference steps and attention layers.  					AI-generated summary 				 Recent advances in training-free attention control methods have enabled flexibl...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 12. Executable Knowledge Graphs (xKG) enhance AI research replication by integrating technical insights and code snippets from scientific literature, improving performance in automated replication tasks.  					AI-generated summary 				 Replicating AI research is a crucial yet challenging task for large ...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 13. Deep Self-Evolving Reasoning (DSER) extends the reasoning capabilities of smaller models by iteratively improving solutions through a probabilistic Markov chain, enabling them to solve previously unsolvable problems and surpass larger models in accuracy.  					AI-generated summary 				 Long-form cha...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 14. Chronos-2, a pretrained model with a group attention mechanism, achieves state-of-the-art performance in zero-shot univariate, multivariate, and covariate-informed forecasting tasks.  					AI-generated summary 				 Pretrained time series models have enabled inference-only forecasting systems that pr...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 15. The survey outlines the shift from pipeline-based to model-native agentic AI, emphasizing the role of reinforcement learning in integrating planning, tool use, and memory within large language models across various domains.  					AI-generated summary 				 The rapid evolution of agentic AI marks a ne...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 16. ECHO is a framework that constructs benchmarks for image generation models using real-world social media data, uncovering complex tasks and improving model evaluation.  					AI-generated summary 				 Recent advances in image generation, often driven by proprietary systems like GPT-4o Image Gen, regu...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 17. Enterprise Deep Research (EDR) is a multi-agent system that automates report generation and real-time data analysis by integrating specialized agents and tools, outperforming existing agentic systems on open benchmarks.  					AI-generated summary 				 As information grows exponentially, enterprises ...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 18. UltraCUA integrates GUI actions with programmatic tools to improve computer-use agent performance and efficiency.  					AI-generated summary 				 Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy executio...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 19. Agentic reinforcement learning models trained for search tasks inherit safety mechanisms but are vulnerable to attacks that reduce their refusal and safety rates.  					AI-generated summary 				 Agentic reinforcement learning (RL) trains large language models to autonomously call tools during reason...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 20. Large reasoning models are vulnerable to reasoning distraction, where irrelevant tasks embedded in prompts reduce accuracy, and a combined SFT and RL defense improves robustness.  					AI-generated summary 				 Recent advances in large reasoning models (LRMs) have enabled remarkable performance on c...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 21. Embody 3D is a multimodal dataset featuring extensive 3D motion data with hand tracking, body shape, text annotations, and audio tracks from multiple participants in various scenarios.  					AI-generated summary 				 The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of 500 ind...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 22. A novel three-stage method, Wiki-PRF, enhances knowledge-based visual question answering by improving multimodal query quality and relevance through visual language models and reinforcement learning.  					AI-generated summary 				 Knowledge-based visual question answering (KB-VQA) requires visual l...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 23. FARE, a family of large-scale parameter evaluators, surpasses specialized RL-trained evaluators in both static benchmarks and real-world tasks through data-driven development and iterative rejection-sampling supervised finetuning.  					AI-generated summary 				 Finetuning specialized generative eva...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 24. MultiVerse, a new multi-turn conversation benchmark, evaluates VLMs across diverse tasks and interaction goals, revealing challenges and the importance of in-context learning.  					AI-generated summary 				 Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmar...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 25. A caching framework with speculative execution reduces web environment latency in web-interactive agentic systems without degrading performance.  					AI-generated summary 				 Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated strong reasoning capabilities. To furthe...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 26. A novel balanced multi-task attention mechanism in custom convolutional neural networks improves satellite land use classification accuracy to 97.23% on the EuroSAT dataset without pre-trained models.  					AI-generated summary 				 This work presents a systematic investigation of custom convolution...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 27. Beacon, a benchmark, measures sycophancy in large language models, revealing it as a combination of linguistic and affective biases that can be mitigated through interventions.  					AI-generated summary 				 Large language models internalize a structural trade-off between truthfulness and obsequiou...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 28. A structured, automated framework inspired by the knapsack problem optimizes agentic system composition by considering performance, budget, and compatibility, achieving higher success rates at lower costs.  					AI-generated summary 				 Designing effective agentic systems requires the seamless comp...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 29. AsyncVoice Agent, with its asynchronous architecture, enhances human-AI collaboration by enabling real-time interaction and interruption of the model's reasoning process, significantly reducing latency while maintaining accuracy.  					AI-generated summary 				 Effective human-AI collaboration on co...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 30. Theoretical and experimental evidence suggests that evaluating AI translators for complex languages can be done solely through their outputs, using a segment-by-segment translation and shuffle test to identify hallucinations and assess quality without reference translations.  					AI-generated summa...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 31. A method using pretrained rectified flow models with periodic guidance successfully transfers appearance and geometric details to 3D assets, outperforming baselines and evaluated using a GPT-based system.  					AI-generated summary 				 Transferring appearance to 3D assets using different representa...
[21.10.2025 21:11] ********************************************************************************
[21.10.2025 21:11] Abstract 32. Test-time scaling improves translation quality in domain-specific models and post-editing but offers limited benefits for general-purpose models in direct translation.  					AI-generated summary 				 Test-time scaling (TTS) has enhanced the performance of Reasoning Models (RMs) on various tasks such...
[21.10.2025 21:11] Read previous papers.
[21.10.2025 21:11] Generating reviews via LLM API.
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#cv", "#optimization", "#dataset", "#survey", "#benchmark"], "emoji": "üîÆ", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ PICABench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#science", "#agi", "#training", "#agents", "#data", "#open_source"], "emoji": "üî¨", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π data scientist —Å 8 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", "desc": "DeepAnalyze-8B ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è agentic LLM, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π data science, —Å–ø–æ—Å–æ–±–Ω–∞—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#long_context", "#data", "#dataset", "#training", "#benchmark", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "–¢–µ–∫—Å—Ç –∫–∞–∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞: —Å–∂–∞—Ç–∏–µ –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Glyph - framework, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#rag", "#training", "#optimization", "#dataset", "#multimodal", "#reasoning", "#games", "#open_source"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Nyx ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è retrieval-augmented generation (RA
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#dataset", "#data", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "FineVision: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏–µ-—è–∑—ã–∫", "desc": "FineVision ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –∫—Ä—É–ø–Ω—ã–π –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏—Ö –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 24 –º–∏–ª–ª–∏–æ–Ω–∞ –æ–±—Ä–∞–∑—Ü–æ–≤, 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#benchmark", "#long_context", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å: –∫–æ–≥–¥–∞ –º–Ω–æ–≥–æ –º–æ–¥–µ–ª–µ–π –ª—É—á—à–µ –æ–¥–Ω–æ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ SAFE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#data", "#synthetic", "#rl", "#dataset", "#training", "#reasoning"], "emoji": "üéØ", "ru": {"title": "QueST: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –ø—Ä–æ–∫–∞—á–∫–∏ coding-—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM", "desc": "QueST - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#cv", "#optimization", "#inference", "#architecture", "#benchmark", "#games"], "emoji": "üîç", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –º–∞—Å—à—Ç–∞–±–∞: beam search –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ beam search –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–∞
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#games", "#training", "#rl", "#optimization"], "emoji": "üëÅÔ∏è", "ru": {"title": "Reinforcement Learning –¥–µ–ª–∞–µ—Ç –∑—Ä–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Å—Ç—Ä–µ–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#alignment", "#dataset", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ß–µ—Å—Ç–Ω–æ—Å—Ç—å AI —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π: –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EliCal ‚Äî –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#multimodal", "#diffusion", "#open_source"], "emoji": "üé®", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Edit-R1 ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#multimodal", "#architecture"], "emoji": "üéØ", "ru": {"title": "ConsistEdit: —Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –≤–Ω–∏–º–∞–Ω–∏—è –≤ MM-DiT –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ConsistEdit ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#science", "#open_source", "#dataset", "#graphs", "#agents"], "emoji": "üî¨", "ru": {"title": "–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–º –∫–æ–¥–æ–º –¥–ª—è —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–∏ AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Executable Knowledge Graphs (xKG) ‚Äî –º–æ–¥—É–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#small_models", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ", "desc": "–ú–µ—Ç–æ–¥ Deep Self-Evolving Reasoning (DSER) –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ–±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞—Ç—å —Å–≤–æ–∏ 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#synthetic", "#dataset", "#training", "#optimization", "#benchmark"], "emoji": "‚è∞", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤—â–∏–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "Chronos-2 - —ç—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ zero-s
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#survey", "#reasoning", "#rl", "#agents"], "emoji": "üß†", "ru": {"title": "–û—Ç –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É: –∫–∞–∫ RL –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç LLM –≤ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏—Ö—Å—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä–Ω—ã—Ö AI-—Å–∏—Å—Ç–µ–º –∫ model-native –ø–æ–¥—Ö–æ–¥—É, –≥–¥–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–∞
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#optimization", "#survey", "#cv", "#benchmark", "#dataset"], "emoji": "üì∏", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "desc": "ECHO ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π. –ò
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#dataset", "#agi", "#benchmark", "#agents", "#optimization"], "emoji": "üîç", "ru": {"title": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏", "desc": "Enterprise Deep Research (EDR) ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#training", "#multimodal", "#agents", "#open_source"], "emoji": "üñ±Ô∏è", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤: –∫–ª–∏–∫–∏ –ø–ª—é—Å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UltraCUA ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#alignment", "#agents", "#rl", "#security", "#rlhf"], "emoji": "üîç‚ö†Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤ –ø–æ–¥ —É–≥—Ä–æ–∑–æ–π –ø—Ä–æ—Å—Ç—ã—Ö –∞—Ç–∞–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º reinforcement learning –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#synthetic", "#reasoning", "#alignment", "#security", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "–ù–µ –¥–∞–π —Å–µ–±—è –æ—Ç–≤–ª–µ—á—å: –∑–∞—â–∏—Ç–∞ reasoning-–º–æ–¥–µ–ª–µ–π –æ—Ç –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö reasoning-–º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º \"reasoning dis
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#dataset", "#3d", "#multimodal"], "emoji": "üï∫", "ru": {"title": "Embody 3D: –æ–≥—Ä–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–∂–∏–≤–ª–µ–Ω–∏—è –∞–≤–∞—Ç–∞—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑ Meta –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Embody 3D ‚Äî –º–∞—Å—à—Ç–∞–±–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å 3D-–¥–∞–Ω–Ω—ã–º–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç 500 —á–∞—Å–æ–≤ –∑–∞–ø–∏—Å–µ–π –æ—Ç 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#reasoning", "#benchmark", "#rl", "#optimization"], "emoji": "üîç", "ru": {"title": "–¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Wiki-PRF –¥–ª—è knowledge-based visual question answering, 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#rl", "#dataset", "#training", "#reasoning", "#benchmark", "#data", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "FARE: –º–æ—â–Ω—ã–µ –æ—Ü–µ–Ω—â–∏–∫–∏ —á–µ—Ä–µ–∑ –¥–∞–Ω–Ω—ã–µ, –∞ –Ω–µ —á–µ—Ä–µ–∑ RL", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ FARE ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ AI-—Å–∏—Å—Ç–µ–º
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#long_context", "#reasoning"], "emoji": "üó£Ô∏è", "ru": {"title": "MultiVerse: –∫–æ–≥–¥–∞ AI-–º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–∞–ª–∏–≤–∞—é—Ç –æ–±—ã—á–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MultiVerse ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ vision-language –º–æ–¥–µ–ª–µ–π (VLM) –≤–µ—Å—Ç
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#reasoning", "#data", "#agents", "#optimization", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å –≤–µ–±-—Å—Ä–µ–¥–æ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#cv", "#optimization", "#dataset", "#architecture", "#open_source"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–º —Å–Ω–∏–º–∫–∞–º –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–≤—ë—Ä—Ç–æ—á–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–µ–º
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#ethics", "#rlhf", "#hallucinations", "#alignment", "#benchmark"], "emoji": "üé≠", "ru": {"title": "–õ–µ—Å—Ç—å vs –ø—Ä–∞–≤–¥–∞: –∫–∞–∫ LLM –≤—ã–±–∏—Ä–∞—é—Ç –º–µ–∂–¥—É –∏—Å—Ç–∏–Ω–æ–π –∏ —É–≥–æ–¥–ª–∏–≤–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Beacon ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–∏–∫–æ—Ñ–∞–Ω—Ç–∏–∏ (–ª—å—Å—Ç–∏–≤–æ—Å—Ç–∏) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#optimization"], "emoji": "üéí", "ru": {"title": "–ó–∞–¥–∞—á–∞ –æ —Ä—é–∫–∑–∞–∫–µ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–±–æ—Ä–∫–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–µ–π –æ —Ä—é–∫–∑–∞–∫–µ. –í–º–µ—Å—Ç–æ —Å—Ç–∞—Ç–∏—á–µ
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#alignment", "#training", "#agents"], "emoji": "üéôÔ∏è", "ru": {"title": "–ü—Ä–µ—Ä—ã–≤–∞–π –∏ —É–ø—Ä–∞–≤–ª—è–π: –≥–æ–ª–æ—Å–æ–≤–æ–π AI-–∞–≥–µ–Ω—Ç —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AsyncVoice Agent ‚Äî —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å AI —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#low_resource", "#benchmark", "#hallucinations", "#data", "#machine_translation", "#multilingual"], "emoji": "üêã", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ AI-–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤ –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–æ–≤: –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è –¥–ª—è —ç–∫–∑–æ—Ç–∏—á–µ—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ AI-–ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤ –¥–ª—è 
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#3d", "#cv", "#diffusion", "#transfer_learning", "#games", "#benchmark"], "emoji": "üé®", "ru": {"title": "–£–º–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –Ω–∞ 3D —á–µ—Ä–µ–∑ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ guidance", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–Ω–æ—Å–∞ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –Ω–∞ 3D-–æ–±—ä–µ–∫—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π rect
[21.10.2025 21:11] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#machine_translation", "#training", "#reasoning", "#inference"], "emoji": "üîÑ", "ru": {"title": "Test-time scaling —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–æ–ª—å–∫–æ –≤ —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ
[21.10.2025 21:11] Renaming data file.
[21.10.2025 21:11] Renaming previous data. hf_papers.json to ./d/2025-10-21.json
[21.10.2025 21:11] Saving new data file.
[21.10.2025 21:11] Generating page.
[21.10.2025 21:11] Renaming previous page.
[21.10.2025 21:11] Renaming previous data. index.html to ./d/2025-10-21.html
[21.10.2025 21:11] Writing result.
[21.10.2025 21:11] Renaming log file.
[21.10.2025 21:11] Renaming previous data. log.txt to ./logs/2025-10-21_last_log.txt
