[16.05.2025 16:13] Read previous papers.
[16.05.2025 16:13] Generating top page (month).
[16.05.2025 16:13] Writing top page (month).
[16.05.2025 17:09] Read previous papers.
[16.05.2025 17:09] Get feed.
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10554
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09666
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09723
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10185
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09694
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10562
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07782
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10527
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10475
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10320
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09990
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09738
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06027
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10558
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10565
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08617
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08581
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10167
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09926
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10046
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09265
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09264
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09263
[16.05.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10468
[16.05.2025 17:09] Extract page data from URL. URL: https://huggingface.co/papers/2505.07096
[16.05.2025 17:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.05.2025 17:09] No deleted papers detected.
[16.05.2025 17:09] Downloading and parsing papers (pdf, html). Total: 25.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10554.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10554.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10554.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09666.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09666.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09666.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09723.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09723.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09723.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10185.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10185.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10185.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09694.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09694.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09694.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10562.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10562.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10562.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.07782.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.07782.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.07782.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10527.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10527.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10527.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10475.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10475.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10475.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10320.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10320.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10320.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09990.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09990.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09990.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09738.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09738.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09738.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.06027.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.06027.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.06027.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10558.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10558.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10558.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10565.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10565.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10565.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.08617.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.08617.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.08617.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.08581.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.08581.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.08581.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10167.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10167.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10167.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09926.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09926.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09926.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10046.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10046.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10046.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09265.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09265.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09265.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09264.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09264.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09264.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.09263.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.09263.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.09263.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.10468.
[16.05.2025 17:09] Extra JSON file exists (./assets/json/2505.10468.json), skip PDF parsing.
[16.05.2025 17:09] Paper image links file exists (./assets/img_data/2505.10468.json), skip HTML parsing.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2505.07096.
[16.05.2025 17:09] Downloading paper 2505.07096 from http://arxiv.org/pdf/2505.07096v2...
[16.05.2025 17:09] Extracting affiliations from text.
[16.05.2025 17:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 2 6 9 0 7 0 . 5 0 5 2 : r X-SIM: Cross-Embodiment Learning via Real-to-Sim-to-Real Prithwish Dan Kushal Kedia Angela Chao Edward W. Duan Maximus A. Pace Wei-Chiu Ma Sanjiban Choudhury Abstract: Human videos offer scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-SIM, real-to-sim-to-real framework that uses object motion as dense and transferable signal for learning robot policies. X-SIM starts by reconstructing photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-SIM introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-SIM does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10 less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/. Keywords: Learning from Human Videos, Sim-to-Real, Representation Learning Human videos offer natural and scalable source of demonstrations for robot policy learning. However, recent advances in robot foundation models [1, 2] rely entirely on large-scale datasets of robot embodiments [3, 4]. Collecting such data requires labor-intensive and expensive teleoperation to provide high-quality expert"
[16.05.2025 17:09] Response: ```python
[]
```
[16.05.2025 17:09] Extracting affiliations from text.
[16.05.2025 17:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 2 6 9 0 7 0 . 5 0 5 2 : r X-SIM: Cross-Embodiment Learning via Real-to-Sim-to-Real Prithwish Dan Kushal Kedia Angela Chao Edward W. Duan Maximus A. Pace Wei-Chiu Ma Sanjiban ChoudhuryAbstract: Human videos offer scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-SIM, real-to-sim-to-real framework that uses object motion as dense and transferable signal for learning robot policies. X-SIM starts by reconstructing photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-SIM introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-SIM does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10 less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/. Keywords: Learning from Human Videos, Sim-to-Real, Representation LearningHuman videos offer natural and scalable source of demonstrations for robot policy learning. However, recent advances in robot foundation models [1, 2] rely entirely on large-scale datasets of robot embodiments [3, 4]. Collecting such data requires labor-intensive and expensive teleoperation to provide high-quality expert demonstrations, making it intractable to scale across diverse tasks and environments. In contrast, human videos (e.g. from YouTube) are abundant and capture wide range of tasks in natural environments. Despite their potential, human videos cannot be directly used in widely-adopted imitation learning pipelines [5, 6], as they lack explicit robot action labels. To bridge this gap, prior work attempts to map human trajectories to robot actions, typically assuming visual or kinematic compatibility. Some methods retarget human hand motion to the robots end-effector [7], but this assumes that human movements are feasible for the robot to replicate [8], which is rarely the case in practice. Other methods reduce the human-robot visual gap by overlaying robot arms on human videos [9, 10], but these rely on solving inverse kinematics, which may be ill-posed due to embodiment mismatch. Another line of work directly translates human videos into robot actions [11, 12, 13], but requires paired human-robot demonstrations, which are expensive and difficult to collect at scale. We tackle the problem of generating robot training data from action-less human videos. Our key insight is that, while human actions are unavailable, the object motion they produce provides Equal Contribution 9th Conference on Robot Learning (CoRL 2025), Seoul, Korea. Figure 1: Overview of X-SIM: We introduce X-SIM, real-to-sim-to-real framework that bridges the humanrobot embodiment gap by learning robot policies. Real-to-Sim. We generate photorealistic simulation using object-centric rewards generated from human videos. Training X-Sim. We first train RL policies with privileged state using GPU-parallelized environment. Then, we collect diverse image-action dataset use it to distill behaviors into an image-conditioned policy. Sim-to-Real. Image-based policy is deployed in the real-world. Its observation encoder automatically calibrates itself by aligning real and sim image observations at test-time. dense supervisory signal for training robot policies in simulation. By reconstructing photorealistic simulation [14] of the human video and tracking object trajectories [15], we define object-centric reward functions that guide RL agents to reproduce the effects of human behavior even when the robot must take entirely different actions. This enables distillation into real-world image-conditioned robot policies without any robot teleoperation data. We propose X-SIM, real-to-sim-to-real framework that bridges the human-robot embodiment gap by learning robot policies in simulation on rewards generated from human videos  (Fig. 1)  . XSIM first extracts object states from RGBD human video and transfers them into photorealistic simulation. It defines dense object-centric reward to efficiently train state-based RL policies in simulation. X-SIM generates large synthetic dataset of paired image-action data by rolling out the trained RL policy and rendering the resulting scenes under varied robot poses, object states, viewpoints, and lighting. Using this dataset, it trains an image-conditioned diffusion policy and transfers directly to the real-world without needing any real robot action data. To narrow the sim-toreal gap at deployment, X-SIM utilizes an online domain adaptation technique to align the robots real world and simulation observations. Our contributions are summarized as follows: 1. We propose X-SIM, real-to-sim-to-real framework that learns image-based robot policies from action-less human videos by tracking object states and matching their motion in simulation. 2. We introduce an online domain adaptation technique to continually reduce the sim-to-real gap by aligning real-world observations with simulation at test time, enabling robust sim-to-real transfer. 3. We evaluate X-SIM across 5 manipulation tasks in 2 environments, showing that it (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) enables generalization to test-time environment changes, including novel camera viewpoints.Imitation Learning. Imitation learning, particularly behavior cloning (BC), is the dominant paradigm for training visuomotor robot policies. Recent algorithms like Diffusion Policy [5] and ACT [6] achieve state-of-the-art results by learning from expert demonstrations consisting of imageaction pairs. However, these methods typically require collecting data via human teleoperat"
[16.05.2025 17:09] Mistral response. {"id": "11404c90633448f68a39e35ad2014ab9", "object": "chat.completion", "created": 1747415380, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1569, "total_tokens": 1577, "completion_tokens": 8}}
[16.05.2025 17:09] Response: ```python
[]
```
[16.05.2025 17:09] Deleting PDF ./assets/pdf/2505.07096.pdf.
[16.05.2025 17:09] Success.
[16.05.2025 17:09] Enriching papers with extra data.
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 0. Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referr...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 1. Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimizatio...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 2. Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world mod...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 3. Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such a...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 4. Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from lang...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 5. Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is a...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 6. We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attemp...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 7. Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodi...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 8. It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel compu...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 9. The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this ...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 10. Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 11. Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this oft...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 12. This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like ...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 13. Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 14. This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a co...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 15. While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinde...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 16. Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 17. The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still ...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 18. Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a ...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 19. This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-moda...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 20. Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation m...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 21. Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect rec...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 22. Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 23. This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, charac...
[16.05.2025 17:09] ********************************************************************************
[16.05.2025 17:09] Abstract 24. Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-S...
[16.05.2025 17:09] Read previous papers.
[16.05.2025 17:09] Generating reviews via LLM API.
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#alignment", "#science", "#rl", "#optimization", "#training", "#reasoning", "#benchmark"], "emoji": "🧠", "ru": {"title": "Выравнивание мета-способностей для улучшения рассуждений ИИ", "desc": "Статья описывает новый подход к улучшению способностей больших языковых моделей к рассужде
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#multimodal", "#training", "#optimization", "#transfer_learning"], "emoji": "🧠", "ru": {"title": "Универсальные системные промпты: новый уровень эффективности LLM", "desc": "Статья представляет новый подход к оптимизации системных промптов для больших языковых моделей (LLM). Авторы 
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#agents", "#dataset", "#training", "#optimization", "#open_source", "#robotics", "#video"], "emoji": "🤖", "ru": {"title": "EVAC: виртуальная среда для эффективного обучения и оценки роботов", "desc": "EVAC - это модель мира с условным действием для имитационного обучения роботов. Он
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#reasoning", "#data", "#training", "#interpretability", "#multimodal"], "emoji": "🧠", "ru": {"title": "Расшифровка мышления ИИ: от цепочек к энциклопедии рассуждений", "desc": "Статья представляет новый подход к анализу цепочек рассуждений (CoT) в больших языковых моделях. Авторы пр
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#multimodal", "#games", "#video", "#benchmark"], "emoji": "🤖", "ru": {"title": "EWMBench: Комплексная оценка воплощенных мировых моделей для генерации видео", "desc": "Статья представляет новый бенчмарк EWMBench для оценки воплощенных мировых моделей (Embod
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#alignment", "#cv", "#optimization", "#multimodal", "#training", "#architecture"], "emoji": "🔍", "ru": {"title": "Конец-в-конец настройка визуальных токенизаторов для улучшения мультимодальных моделей", "desc": "Статья предлагает новый подход ETT для оптимизации визуальных токенизат
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#data", "#training", "#architecture", "#benchmark", "#open_source", "#games", "#agents", "#rl"], "emoji": "🤖", "ru": {"title": "MLE-Dojo: Интерактивная среда для обучения автономных LLM-агентов", "desc": "MLE-Dojo - это фреймворк для обучения, оценки и улучшения автономных агентов н
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#data", "#training", "#benchmark", "#optimization", "#dataset", "#rlhf", "#alignment"], "emoji": "🌍", "ru": {"title": "Масштабирование моделей предпочтений: от данных к глобальному пониманию", "desc": "Исследователи обнаружили, что законы масштабирования, аналогичные тем, что наблюд
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#inference", "#low_resource", "#training", "#architecture", "#optimization"], "emoji": "🚀", "ru": {"title": "Эффективное масштабирование языковых моделей через параллельные вычисления", "desc": "Статья представляет новый подход к масштабированию языковых моделей, названный параллель
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#optimization", "#hallucinations", "#rlhf", "#rl"], "emoji": "🧠", "ru": {"title": "J1: Революция в обучении моделей-судей через рассуждения", "desc": "Статья представляет новый подход к обучению моделей-судей с использованием обучения с подкр
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#open_source", "#benchmark", "#robotics", "#dataset"], "emoji": "👆", "ru": {"title": "PointArena: новый стандарт оценки мультимодальных моделей в задачах указания", "desc": "Статья представляет PointArena - комплексную платформу для оценки м
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#optimization", "#multilingual", "#training", "#transfer_learning", "#data"], "emoji": "🔄", "ru": {"title": "TokenAdapt: Эффективная замена токенизатора в языковых моделях", "desc": "Статья представляет новый метод TokenAdapt для замены токенизатора в предобученных языковых моделях 
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#security", "#ethics"], "emoji": "🧠", "ru": {"title": "Unilogit: умное забывание для больших языковых моделей", "desc": "Статья представляет Unilogit - новый метод самодистилляции для машинного забывания в больших языковых моделях. Unilogit реш
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#optimization"], "emoji": "🎨", "ru": {"title": "Генерация стилизованной векторной графики с помощью ИИ", "desc": "Статья представляет новый метод генерации масштабируемой векторной графики (SVG) с возможностью настройки стиля. Авторы предлагают дв
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training"], "emoji": "🔍", "ru": {"title": "Точные карты глубины для любых сцен через объединение измерений и предсказаний", "desc": "Статья представляет Prior Depth Anything - фреймворк для создания точных и детальных карт глубины для любых сцен. Он объединяет не
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#cv", "#rl", "#dataset", "#agents", "#training", "#open_source", "#reasoning"], "emoji": "🔍", "ru": {"title": "OpenThinkIMG: Революция в обучении ИИ мыслить визуально", "desc": "Статья представляет OpenThinkIMG - первую открытую комплексную платформу для обучения мультимодальных язы
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#cv", "#healthcare", "#multimodal"], "emoji": "🔪", "ru": {"title": "ReSurgSAM2: Революция в интерактивной сегментации хирургических сцен", "desc": "Статья представляет ReSurgSAM2 - двухэтапную систему для сегментации хирургических сцен с использованием текстовых запросов. На первом 
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#data", "#inference", "#interpretability", "#architecture"], "emoji": "🔬", "ru": {"title": "QuXAI: Прозрачность квантово-классического ИИ", "desc": "Эта статья представляет QuXAI - фреймворк для объяснения гибридных квантово-классических моделей машинного обучения (HQML). В основе Q
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#transfer_learning", "#healthcare"], "emoji": "🔍", "ru": {"title": "AdaptCLIP: Эффективное обнаружение аномалий с помощью адаптации CLIP", "desc": "Статья представляет новый метод универсального обнаружения визуальных аномалий под названием Adapt
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#multimodal", "#training", "#diffusion", "#survey"], "emoji": "🔬", "ru": {"title": "Глубокий анализ слияния LLM и DiT для генерации изображений по тексту", "desc": "Статья представляет собой эмпирическое исследование генерации изображений по текстовому описанию. Авторы проводят дета
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#synthetic", "#dataset"], "emoji": "👁️", "ru": {"title": "Универсальная сегментация аномалий без языковых подсказок", "desc": "Статья представляет новый подход к сегментации визуальных аномалий, основанный на чисто визуальной модели без использов
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#optimization", "#survey", "#dataset"], "emoji": "🔍", "ru": {"title": "OneNIP: Обнаружение аномалий по одному нормальному изображению", "desc": "Эта статья представляет новый метод OneNIP для обнаружения аномалий в изображениях с использованием всего одного этал
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#data", "#cv", "#training", "#benchmark", "#dataset", "#synthetic", "#diffusion"], "emoji": "🔍", "ru": {"title": "Генерация аномалий по нескольким примерам для улучшения их обнаружения", "desc": "Статья представляет метод AnoGen для обнаружения аномалий в промышленной инспекции с ис
[16.05.2025 17:09] Using data from previous issue: {"categories": ["#healthcare", "#hallucinations", "#agents", "#architecture", "#multimodal", "#reasoning", "#optimization", "#agi"], "emoji": "🤖", "ru": {"title": "От ИИ-агентов к агентному ИИ: эволюция автономных систем", "desc": "Это исследование проводит критическое различие между ИИ-агентами и а
[16.05.2025 17:09] Querying the API.
[16.05.2025 17:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.
[16.05.2025 17:09] Response: {
  "desc": "X-Sim - это новый подход к обучению роботов манипуляциям на основе видео с людьми. Метод использует фотореалистичное моделирование и отслеживание движения объектов для создания наград в симуляции. Затем обученная политика переносится в реальный мир с помощью диффузионной модели и адаптации домена. X-Sim превосходит базовые методы на 30% и не требует данных телеоперации робота.",
  "emoji": "🤖",
  "title": "Роботы учатся у людей без прямого копирования движений"
}
[16.05.2025 17:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/."

[16.05.2025 17:09] Response: ```python
['RL', 'ROBOTICS', 'TRAINING']
```
[16.05.2025 17:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/."

[16.05.2025 17:09] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION', 'SYNTHETIC']
```
[16.05.2025 17:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces X-Sim, a framework designed to train robot manipulation policies using human videos without needing action labels. It reconstructs a realistic simulation from RGBD videos and tracks object movements to create rewards for reinforcement learning. The learned policy is then refined into a diffusion policy that adapts to real-world conditions through online domain adaptation. X-Sim demonstrates significant improvements in task performance and efficiency compared to existing methods, showcasing its ability to generalize across different environments and viewpoints.","title":"X-Sim: Bridging Human Videos to Robot Actions Efficiently"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces X-Sim, a framework designed to train robot manipulation policies using human videos without needing action labels. It reconstructs a realistic simulation from RGBD videos and tracks object movements to create rewards for reinforcement learning. The learned policy is then refined into a diffusion policy that adapts to real-world conditions through online domain adaptation. X-Sim demonstrates significant improvements in task performance and efficiency compared to existing methods, showcasing its ability to generalize across different environments and viewpoints.', title='X-Sim: Bridging Human Videos to Robot Actions Efficiently'))
[16.05.2025 17:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种名为X-Sim的框架，用于通过人类视频训练机器人操作策略。X-Sim利用物体运动作为密集且可转移的信号，首先从RGBD人类视频重建出逼真的仿真环境，并跟踪物体轨迹以定义以物体为中心的奖励。然后，这些奖励用于在仿真中训练强化学习（RL）策略，并通过合成的视角和光照变化渲染的回放将学习到的策略提炼为图像条件扩散策略。最后，X-Sim引入在线领域适应技术，在部署过程中对真实和仿真观察进行对齐，从而实现无机器人遥控数据的有效转移。","title":"X-Sim：从人类视频到机器人操作的智能转移"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种名为X-Sim的框架，用于通过人类视频训练机器人操作策略。X-Sim利用物体运动作为密集且可转移的信号，首先从RGBD人类视频重建出逼真的仿真环境，并跟踪物体轨迹以定义以物体为中心的奖励。然后，这些奖励用于在仿真中训练强化学习（RL）策略，并通过合成的视角和光照变化渲染的回放将学习到的策略提炼为图像条件扩散策略。最后，X-Sim引入在线领域适应技术，在部署过程中对真实和仿真观察进行对齐，从而实现无机器人遥控数据的有效转移。', title='X-Sim：从人类视频到机器人操作的智能转移'))
[16.05.2025 17:09] Loading Chinese text from previous data.
[16.05.2025 17:09] Renaming data file.
[16.05.2025 17:09] Renaming previous data. hf_papers.json to ./d/2025-05-16.json
[16.05.2025 17:09] Saving new data file.
[16.05.2025 17:09] Generating page.
[16.05.2025 17:09] Renaming previous page.
[16.05.2025 17:09] Renaming previous data. index.html to ./d/2025-05-16.html
[16.05.2025 17:09] [Experimental] Generating Chinese page for reading.
[16.05.2025 17:09] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '潜在', 'pinyin': 'qián zài', 'trans': 'potential'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '之前', 'pinyin': 'zhī qián', 'trans': 'before'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '表明', 'pinyin': 'biǎo míng', 'trans': 'indicate'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '强化', 'pinyin': 'qiáng huà', 'trans': 'reinforce'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learning'}, {'word': '偶尔', 'pinyin': 'ǒu ěr', 'trans': 'occasionally'}, {'word': '引发', 'pinyin': 'yǐn fā', 'trans': 'trigger'}, {'word': '高级', 'pinyin': 'gāo jí', 'trans': 'advanced'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '行为', 'pinyin': 'xíng wéi', 'trans': 'behavior'}, {'word': '自我', 'pinyin': 'zì wǒ', 'trans': 'self'}, {'word': '校正', 'pinyin': 'jiào zhèng', 'trans': 'correct'}, {'word': '验证', 'pinyin': 'yàn zhèng', 'trans': 'verify'}, {'word': '现象', 'pinyin': 'xiàn xiàng', 'trans': 'phenomenon'}, {'word': '称为', 'pinyin': 'chēng wéi', 'trans': 'called'}, {'word': '顿悟', 'pinyin': 'dùn wù', 'trans': 'epiphany'}, {'word': '时刻', 'pinyin': 'shí kè', 'trans': 'moment'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '控制', 'pinyin': 'kòng zhì', 'trans': 'control'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '明确', 'pinyin': 'míng què', 'trans': 'explicit'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'align'}, {'word': '元', 'pinyin': 'yuán', 'trans': 'meta'}, {'word': '演绎', 'pinyin': 'yǎn yì', 'trans': 'deduction'}, {'word': '归纳', 'pinyin': 'guī nà', 'trans': 'induction'}, {'word': '溯因', 'pinyin': 'sù yīn', 'trans': 'abduction'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'display'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improvement'}, {'word': '数学', 'pinyin': 'shù xué', 'trans': 'mathematics'}, {'word': '编程', 'pinyin': 'biān chéng', 'trans': 'programming'}, {'word': '科学', 'pinyin': 'kē xué', 'trans': 'science'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'}]
[16.05.2025 17:09] Renaming previous Chinese page.
[16.05.2025 17:09] Renaming previous data. zh.html to ./d/2025-05-15_zh_reading_task.html
[16.05.2025 17:09] Writing Chinese reading task.
[16.05.2025 17:09] Writing result.
[16.05.2025 17:09] Renaming log file.
[16.05.2025 17:09] Renaming previous data. log.txt to ./logs/2025-05-16_last_log.txt
