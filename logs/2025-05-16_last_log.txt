[16.05.2025 21:10] Read previous papers.
[16.05.2025 21:10] Generating top page (month).
[16.05.2025 21:10] Writing top page (month).
[16.05.2025 22:10] Read previous papers.
[16.05.2025 22:10] Get feed.
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10554
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09666
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09723
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10475
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10185
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09694
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10527
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10562
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07782
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06027
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10320
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09990
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10558
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09738
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10565
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08617
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08581
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10167
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10566
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10046
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09926
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09265
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09264
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09263
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10468
[16.05.2025 22:10] Extract page data from URL. URL: https://huggingface.co/papers/2505.09601
[16.05.2025 22:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07096
[16.05.2025 22:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.05.2025 22:10] No deleted papers detected.
[16.05.2025 22:10] Downloading and parsing papers (pdf, html). Total: 27.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10554.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10554.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10554.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09666.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09666.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09666.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09723.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09723.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09723.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10475.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10475.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10475.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10185.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10185.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10185.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09694.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09694.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09694.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10527.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10527.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10527.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10562.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10562.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10562.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.07782.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.07782.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.07782.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.06027.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.06027.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.06027.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10320.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10320.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10320.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09990.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09990.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09990.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10558.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10558.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10558.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09738.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09738.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09738.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10565.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10565.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10565.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.08617.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.08617.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.08617.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.08581.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.08581.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.08581.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10167.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10167.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10167.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10566.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10566.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10566.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10046.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10046.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10046.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09926.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09926.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09926.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09265.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09265.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09265.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09264.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09264.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09264.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09263.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.09263.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.09263.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.10468.
[16.05.2025 22:10] Extra JSON file exists (./assets/json/2505.10468.json), skip PDF parsing.
[16.05.2025 22:10] Paper image links file exists (./assets/img_data/2505.10468.json), skip HTML parsing.
[16.05.2025 22:10] Success.
[16.05.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2505.09601.
[16.05.2025 22:10] Downloading paper 2505.09601 from http://arxiv.org/pdf/2505.09601v1...
[16.05.2025 22:10] Extracting affiliations from text.
[16.05.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 1 0 6 9 0 . 5 0 5 2 : r Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware Justin Yu1,, Max Letian Fu1,, Huang Huang1, Karim El-Refai1, Rares Andrei Ambrus2, Richard Cheng2, Muhammad Zubair Irshad2, Ken Goldberg1 1University of California, Berkeley, 2Toyota Research Institute Abstract: Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigmhuman teleoperationremains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is smartphone-captured scan of one or more objects and single video of human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com. Keywords: Robot Datasets, Imitation Learning, Data Augmentation The great power of general purpose methods . . . [is that they] continue to scale with increased computation. Richard Sutton, The Bitter Lesson (2019) Robotics has long benefited from computational scalabilitymethods l"
[16.05.2025 22:11] Response: ```python
["University of California, Berkeley", "Toyota Research Institute"]
```
[16.05.2025 22:11] Deleting PDF ./assets/pdf/2505.09601.pdf.
[16.05.2025 22:11] Success.
[16.05.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2505.07096.
[16.05.2025 22:11] Extra JSON file exists (./assets/json/2505.07096.json), skip PDF parsing.
[16.05.2025 22:11] Paper image links file exists (./assets/img_data/2505.07096.json), skip HTML parsing.
[16.05.2025 22:11] Success.
[16.05.2025 22:11] Enriching papers with extra data.
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 0. Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referr...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 1. Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimizatio...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 2. Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world mod...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 3. It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel compu...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 4. Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such a...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 5. Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from lang...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 6. Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodi...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 7. Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is a...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 8. We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attemp...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 9. This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like ...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 10. The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this ...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 11. Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 12. Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 13. Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this oft...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 14. This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a co...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 15. While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinde...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 16. Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 17. The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still ...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 18. Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. T...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 19. This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-moda...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 20. Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a ...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 21. Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation m...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 22. Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect rec...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 23. Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 24. This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, charac...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 25. Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without rel...
[16.05.2025 22:11] ********************************************************************************
[16.05.2025 22:11] Abstract 26. Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-S...
[16.05.2025 22:11] Read previous papers.
[16.05.2025 22:11] Generating reviews via LLM API.
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#alignment", "#science", "#rl", "#optimization", "#training", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ—Ç–∞-—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#training", "#optimization", "#transfer_learning"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã 
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#agents", "#dataset", "#training", "#optimization", "#open_source", "#robotics", "#video"], "emoji": "ü§ñ", "ru": {"title": "EVAC: –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "EVAC - —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ —Å —É—Å–ª–æ–≤–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤. –û–Ω
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#inference", "#low_resource", "#training", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#reasoning", "#data", "#training", "#interpretability", "#multimodal"], "emoji": "üß†", "ru": {"title": "–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –º—ã—à–ª–µ–Ω–∏—è –ò–ò: –æ—Ç —Ü–µ–ø–æ—á–µ–∫ –∫ —ç–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#multimodal", "#games", "#video", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "EWMBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ EWMBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Embod
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#data", "#training", "#benchmark", "#optimization", "#dataset", "#rlhf", "#alignment"], "emoji": "üåç", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π: –æ—Ç –¥–∞–Ω–Ω—ã—Ö –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —á—Ç–æ –Ω–∞–±–ª—é–¥
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#alignment", "#cv", "#optimization", "#multimodal", "#training", "#architecture"], "emoji": "üîç", "ru": {"title": "–ö–æ–Ω–µ—Ü-–≤-–∫–æ–Ω–µ—Ü –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ETT –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#data", "#training", "#architecture", "#benchmark", "#open_source", "#games", "#agents", "#rl"], "emoji": "ü§ñ", "ru": {"title": "MLE-Dojo: –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "MLE-Dojo - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#security", "#ethics"], "emoji": "üß†", "ru": {"title": "Unilogit: —É–º–Ω–æ–µ –∑–∞–±—ã–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Unilogit - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. Unilogit —Ä–µ—à
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#optimization", "#hallucinations", "#rlhf", "#rl"], "emoji": "üß†", "ru": {"title": "J1: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π-—Å—É–¥–µ–π —á–µ—Ä–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π-—Å—É–¥–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#training", "#multimodal", "#reasoning", "#open_source", "#benchmark", "#robotics", "#dataset"], "emoji": "üëÜ", "ru": {"title": "PointArena: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —É–∫–∞–∑–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PointArena - –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#optimization"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ (SVG) —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç–∏–ª—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#multilingual", "#training", "#transfer_learning", "#data"], "emoji": "üîÑ", "ru": {"title": "TokenAdapt: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∑–∞–º–µ–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ TokenAdapt –¥–ª—è –∑–∞–º–µ–Ω—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö 
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#dataset", "#training"], "emoji": "üîç", "ru": {"title": "–¢–æ—á–Ω—ã–µ –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã –¥–ª—è –ª—é–±—ã—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑–º–µ—Ä–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Prior Depth Anything - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω—ã—Ö –∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç –≥–ª—É–±–∏–Ω—ã –¥–ª—è –ª—é–±—ã—Ö —Å—Ü–µ–Ω. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#rl", "#dataset", "#agents", "#training", "#open_source", "#reasoning"], "emoji": "üîç", "ru": {"title": "OpenThinkIMG: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –º—ã—Å–ª–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenThinkIMG - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#healthcare", "#multimodal"], "emoji": "üî™", "ru": {"title": "ReSurgSAM2: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ReSurgSAM2 - –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ù–∞ –ø–µ—Ä–≤–æ–º 
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#data", "#inference", "#interpretability", "#architecture"], "emoji": "üî¨", "ru": {"title": "QuXAI: –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–æ-–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QuXAI - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∫–≤–∞–Ω—Ç–æ–≤–æ-–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (HQML). –í –æ—Å–Ω–æ–≤–µ Q
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d", "#video", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "3D-Fixup: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å 3D-–ø—Ä–∏–æ—Ä–∞–º–∏", "desc": "3D-Fixup - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 2D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–∑—É—á–µ–Ω–Ω—ã—Ö 3D-–ø—Ä–∏–æ—Ä–æ–≤. –û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–ª–æ–∂
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#training", "#diffusion", "#survey"], "emoji": "üî¨", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–ª–∏—è–Ω–∏—è LLM –∏ DiT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –¥–µ—Ç–∞
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#transfer_learning", "#healthcare"], "emoji": "üîç", "ru": {"title": "AdaptCLIP: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ CLIP", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Adapt
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#synthetic", "#dataset"], "emoji": "üëÅÔ∏è", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –±–µ–∑ —è–∑—ã–∫–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —á–∏—Å—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#optimization", "#survey", "#dataset"], "emoji": "üîç", "ru": {"title": "OneNIP: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –æ–¥–Ω–æ–º—É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ OneNIP –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ª
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#data", "#cv", "#training", "#benchmark", "#dataset", "#synthetic", "#diffusion"], "emoji": "üîç", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø—Ä–∏–º–µ—Ä–∞–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AnoGen –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–π –∏–Ω—Å–ø–µ–∫—Ü–∏–∏ —Å –∏—Å
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#healthcare", "#hallucinations", "#agents", "#architecture", "#multimodal", "#reasoning", "#optimization", "#agi"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –∞–≥–µ–Ω—Ç–Ω–æ–º—É –ò–ò: —ç–≤–æ–ª—é—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏ –∏ –∞
[16.05.2025 22:11] Querying the API.
[16.05.2025 22:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com
[16.05.2025 22:11] Response: {
  "desc": "R2R2R - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–ª–∏ —Ç–µ–ª–µ—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–º–∞—Ä—Ç—Ñ–æ–Ω –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –æ–¥–Ω–æ –≤–∏–¥–µ–æ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—ã—Å—è—á –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π, –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–æ–±–æ—Ç–∞. R2R2R –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é 3D Gaussian Splatting –¥–ª—è –≥–∏–±–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—Å—Å–µ—Ç–æ–≤ –∏ —Å–∏–Ω—Ç–µ–∑–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∫–∞–∫ –¥–ª—è –∂–µ—Å—Ç–∫–∏—Ö, —Ç–∞–∫ –∏ –¥–ª—è —à–∞—Ä–Ω–∏—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö R2R2R –∏–∑ –æ–¥–Ω–æ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏, –º–æ–≥—É—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å—Å—è –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ 150 –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö —Å —Ç–µ–ª–µ—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º.",
  "emoji": "ü§ñ",
  "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–æ–±–æ—Ç–æ–≤: –æ—Ç –æ–¥–Ω–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∫ —Ç—ã—Å—è—á–∞–º —Å–∏–º—É–ª—è—Ü–∏–π"
}
[16.05.2025 22:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com"

[16.05.2025 22:11] Response: ```python
["DATASET", "3D", "ROBOTICS"]
```
[16.05.2025 22:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com"

[16.05.2025 22:11] Response: ```python
["SYNTHETIC", "OPTIMIZATION"]
```
[16.05.2025 22:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents Real2Render2Real (R2R2R), a new method for generating training data for robots without the need for expensive human teleoperation or complex simulations. It utilizes smartphone scans and a single video of human actions to create thousands of high-quality, robot-agnostic demonstrations. By employing 3D Gaussian Splatting, R2R2R can flexibly generate 3D models and trajectories for various objects, which are then converted into meshes for use in rendering engines. Experiments show that models trained on data from R2R2R can achieve performance comparable to those trained on a much larger dataset of human demonstrations.","title":"Revolutionizing Robot Training with R2R2R"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents Real2Render2Real (R2R2R), a new method for generating training data for robots without the need for expensive human teleoperation or complex simulations. It utilizes smartphone scans and a single video of human actions to create thousands of high-quality, robot-agnostic demonstrations. By employing 3D Gaussian Splatting, R2R2R can flexibly generate 3D models and trajectories for various objects, which are then converted into meshes for use in rendering engines. Experiments show that models trained on data from R2R2R can achieve performance comparable to those trained on a much larger dataset of human demonstrations.', title='Revolutionizing Robot Training with R2R2R'))
[16.05.2025 22:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Real2Render2RealÔºàR2R2RÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÁîüÊàêÊú∫Âô®‰∫∫ËÆ≠ÁªÉÊï∞ÊçÆÔºåËÄåÊó†ÈúÄ‰æùËµñÁâ©‰ΩìÂä®ÂäõÂ≠¶Ê®°ÊãüÊàñ‰∫∫Â∑•ÈÅ•Êéß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊô∫ËÉΩÊâãÊú∫ÊçïÊçâÁöÑÁâ©‰ΩìÊâ´ÊèèÂíå‰∫∫Á±ªÊºîÁ§∫ËßÜÈ¢ëÔºåÈáçÂª∫ËØ¶ÁªÜÁöÑ3DÁâ©‰ΩìÂá†‰ΩïÂΩ¢Áä∂ÂíåÂ§ñËßÇÔºåÁîüÊàêÈ´òËßÜËßâ‰øùÁúüÂ∫¶ÁöÑÊú∫Âô®‰∫∫ÊºîÁ§∫„ÄÇR2R2RÂà©Áî®3DÈ´òÊñØÁÇπ‰∫ëÔºà3DGSÔºâÊäÄÊúØÔºåÁÅµÊ¥ªÁîüÊàêËµÑ‰∫ßÂíåËΩ®ËøπÔºåÈÄÇÁî®‰∫éÂàöÊÄßÂíåÂÖ≥ËäÇÁâ©‰ΩìÔºåÂπ∂Â∞ÜËøô‰∫õË°®Á§∫ËΩ¨Êç¢‰∏∫ÁΩëÊ†ºÔºå‰ª•‰æø‰∏éÂèØÊâ©Â±ïÁöÑÊ∏≤ÊüìÂºïÊìéÂÖºÂÆπ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂü∫‰∫éR2R2RÊï∞ÊçÆËÆ≠ÁªÉÁöÑÊ®°ÂûãÂèØ‰ª•‰∏éÂü∫‰∫é150‰∏™‰∫∫Â∑•ÈÅ•ÊéßÊºîÁ§∫ËÆ≠ÁªÉÁöÑÊ®°ÂûãÊÄßËÉΩÁõ∏ÂåπÈÖç„ÄÇ","title":"Êú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊñ∞ÊñπÊ≥ïÔºöÊó†ÈúÄÈÅ•ÊéßÂç≥ÂèØÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Real2Render2RealÔºàR2R2RÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÁîüÊàêÊú∫Âô®‰∫∫ËÆ≠ÁªÉÊï∞ÊçÆÔºåËÄåÊó†ÈúÄ‰æùËµñÁâ©‰ΩìÂä®ÂäõÂ≠¶Ê®°ÊãüÊàñ‰∫∫Â∑•ÈÅ•Êéß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊô∫ËÉΩÊâãÊú∫ÊçïÊçâÁöÑÁâ©‰ΩìÊâ´ÊèèÂíå‰∫∫Á±ªÊºîÁ§∫ËßÜÈ¢ëÔºåÈáçÂª∫ËØ¶ÁªÜÁöÑ3DÁâ©‰ΩìÂá†‰ΩïÂΩ¢Áä∂ÂíåÂ§ñËßÇÔºåÁîüÊàêÈ´òËßÜËßâ‰øùÁúüÂ∫¶ÁöÑÊú∫Âô®‰∫∫ÊºîÁ§∫„ÄÇR2R2RÂà©Áî®3DÈ´òÊñØÁÇπ‰∫ëÔºà3DGSÔºâÊäÄÊúØÔºåÁÅµÊ¥ªÁîüÊàêËµÑ‰∫ßÂíåËΩ®ËøπÔºåÈÄÇÁî®‰∫éÂàöÊÄßÂíåÂÖ≥ËäÇÁâ©‰ΩìÔºåÂπ∂Â∞ÜËøô‰∫õË°®Á§∫ËΩ¨Êç¢‰∏∫ÁΩëÊ†ºÔºå‰ª•‰æø‰∏éÂèØÊâ©Â±ïÁöÑÊ∏≤ÊüìÂºïÊìéÂÖºÂÆπ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂü∫‰∫éR2R2RÊï∞ÊçÆËÆ≠ÁªÉÁöÑÊ®°ÂûãÂèØ‰ª•‰∏éÂü∫‰∫é150‰∏™‰∫∫Â∑•ÈÅ•ÊéßÊºîÁ§∫ËÆ≠ÁªÉÁöÑÊ®°ÂûãÊÄßËÉΩÁõ∏ÂåπÈÖç„ÄÇ', title='Êú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊñ∞ÊñπÊ≥ïÔºöÊó†ÈúÄÈÅ•ÊéßÂç≥ÂèØÁîüÊàêËÆ≠ÁªÉÊï∞ÊçÆ'))
[16.05.2025 22:11] Using data from previous issue: {"categories": ["#synthetic", "#rl", "#training", "#robotics", "#optimization", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è —É –ª—é–¥–µ–π –±–µ–∑ –ø—Ä—è–º–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π", "desc": "X-Sim - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[16.05.2025 22:11] Loading Chinese text from previous data.
[16.05.2025 22:11] Renaming data file.
[16.05.2025 22:11] Renaming previous data. hf_papers.json to ./d/2025-05-16.json
[16.05.2025 22:11] Saving new data file.
[16.05.2025 22:11] Generating page.
[16.05.2025 22:11] Renaming previous page.
[16.05.2025 22:11] Renaming previous data. index.html to ./d/2025-05-16.html
[16.05.2025 22:11] [Experimental] Generating Chinese page for reading.
[16.05.2025 22:11] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÊΩúÂú®', 'pinyin': 'qi√°n z√†i', 'trans': 'potential'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': '‰πãÂâç', 'pinyin': 'zhƒ´ qi√°n', 'trans': 'before'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´ y√∫', 'trans': 'based on'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'Âº∫Âåñ', 'pinyin': 'qi√°ng hu√†', 'trans': 'reinforce'}, {'word': 'Â≠¶‰π†', 'pinyin': 'xu√© x√≠', 'trans': 'learning'}, {'word': 'ÂÅ∂Â∞î', 'pinyin': '«íu ƒõr', 'trans': 'occasionally'}, {'word': 'ÂºïÂèë', 'pinyin': 'y«ên fƒÅ', 'trans': 'trigger'}, {'word': 'È´òÁ∫ß', 'pinyin': 'gƒÅo j√≠', 'trans': 'advanced'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'Ë°å‰∏∫', 'pinyin': 'x√≠ng w√©i', 'trans': 'behavior'}, {'word': 'Ëá™Êàë', 'pinyin': 'z√¨ w«í', 'trans': 'self'}, {'word': 'Ê†°Ê≠£', 'pinyin': 'ji√†o zh√®ng', 'trans': 'correct'}, {'word': 'È™åËØÅ', 'pinyin': 'y√†n zh√®ng', 'trans': 'verify'}, {'word': 'Áé∞Ë±°', 'pinyin': 'xi√†n xi√†ng', 'trans': 'phenomenon'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìng w√©i', 'trans': 'called'}, {'word': 'È°øÊÇü', 'pinyin': 'd√πn w√π', 'trans': 'epiphany'}, {'word': 'Êó∂Âàª', 'pinyin': 'sh√≠ k√®', 'trans': 'moment'}, {'word': '‰∏ÄËá¥ÊÄß', 'pinyin': 'yƒ´ zh√¨ x√¨ng', 'trans': 'consistency'}, {'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'predict'}, {'word': 'ÊéßÂà∂', 'pinyin': 'k√≤ng zh√¨', 'trans': 'control'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'ÊòéÁ°Æ', 'pinyin': 'm√≠ng qu√®', 'trans': 'explicit'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨ q√≠', 'trans': 'align'}, {'word': 'ÂÖÉ', 'pinyin': 'yu√°n', 'trans': 'meta'}, {'word': 'ÊºîÁªé', 'pinyin': 'y«én y√¨', 'trans': 'deduction'}, {'word': 'ÂΩíÁ∫≥', 'pinyin': 'guƒ´ n√†', 'trans': 'induction'}, {'word': 'Ê∫ØÂõ†', 'pinyin': 's√π yƒ´n', 'trans': 'abduction'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«én sh√¨', 'trans': 'display'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éi j√¨n', 'trans': 'improvement'}, {'word': 'Êï∞Â≠¶', 'pinyin': 'sh√π xu√©', 'trans': 'mathematics'}, {'word': 'ÁºñÁ®ã', 'pinyin': 'biƒÅn ch√©ng', 'trans': 'programming'}, {'word': 'ÁßëÂ≠¶', 'pinyin': 'kƒì xu√©', 'trans': 'science'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÊµãËØï', 'pinyin': 'c√® sh√¨', 'trans': 'test'}]
[16.05.2025 22:11] Renaming previous Chinese page.
[16.05.2025 22:11] Renaming previous data. zh.html to ./d/2025-05-15_zh_reading_task.html
[16.05.2025 22:11] Writing Chinese reading task.
[16.05.2025 22:11] Writing result.
[16.05.2025 22:11] Renaming log file.
[16.05.2025 22:11] Renaming previous data. log.txt to ./logs/2025-05-16_last_log.txt
