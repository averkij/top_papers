[30.05.2025 04:17] Read previous papers.
[30.05.2025 04:17] Generating top page (month).
[30.05.2025 04:17] Writing top page (month).
[30.05.2025 05:12] Read previous papers.
[30.05.2025 05:12] Get feed.
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23762
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22653
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23693
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23747
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23621
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23604
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23716
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23380
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23359
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23606
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23585
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23419
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23660
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23559
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22961
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23735
[30.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.22618
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22421
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20755
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23758
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23754
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23742
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17818
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23745
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23625
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19286
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18087
[30.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.23416
[30.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.23253
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22943
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20282
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19360
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19236
[30.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20199
[30.05.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.05.2025 05:12] No deleted papers detected.
[30.05.2025 05:12] Downloading and parsing papers (pdf, html). Total: 34.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23762.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23762.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23762.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.22653.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.22653.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.22653.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23693.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23693.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23693.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23747.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23747.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23747.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23621.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23621.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23621.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23604.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23604.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23604.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23716.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23716.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23716.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23380.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23380.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23380.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23359.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23359.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23359.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23606.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23606.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23606.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23585.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23585.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23585.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23419.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23419.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23419.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23660.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23660.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23660.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23559.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23559.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23559.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.22961.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.22961.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.22961.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23735.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23735.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23735.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.22618.
[30.05.2025 05:12] Downloading paper 2505.22618 from http://arxiv.org/pdf/2505.22618v1...
[30.05.2025 05:12] Extracting affiliations from text.
[30.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 8 1 6 2 2 . 5 0 5 2 : r 2025-5-29 Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding Chengyue Wu1,2* Hao Zhang2* Shuchen Xue4 Zhijian Liu2 Shizhe Diao2 Ligeng Zhu2 Ping Luo1 Song Han2,3 Enze Xie2 1The University of Hong Kong *Equal contribution. 2NVIDIA 3MIT 4Independent Researcher Abstract: Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce Fast-dLLM, method that incorporates novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, Fast-dLLM also proposes confidence-aware parallel decoding strategy that selectively decodes tokens exceeding confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to 27.6 throughput improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs. Links: Github Code Project Page 1. Introduction Diffusion-based large language models (Diffusion LLMs) have recently attracted increasing attention due to their potential for parallel token generation and the advantages of bidirectional attention mechanisms. Notably, Mercury [13] runs at over 1,000 tokens per second, and Gemini Diffusion [8] by Google DeepMind has demonstrated the ability to generate over 1,400 tok"
[30.05.2025 05:12] Response: ```python
["The University of Hong Kong", "NVIDIA", "MIT", "Independent Researcher"]
```
[30.05.2025 05:12] Deleting PDF ./assets/pdf/2505.22618.pdf.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.22421.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.22421.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.22421.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.20755.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.20755.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.20755.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23758.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23758.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23758.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23754.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23754.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23754.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23742.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23742.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23742.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.17818.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.17818.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.17818.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23745.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23745.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23745.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23625.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.23625.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.23625.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.19286.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.19286.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.19286.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.18087.
[30.05.2025 05:12] Extra JSON file exists (./assets/json/2505.18087.json), skip PDF parsing.
[30.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.18087.json), skip HTML parsing.
[30.05.2025 05:12] Success.
[30.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.23416.
[30.05.2025 05:13] Downloading paper 2505.23416 from http://arxiv.org/pdf/2505.23416v1...
[30.05.2025 05:13] Extracting affiliations from text.
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 6 1 4 3 2 . 5 0 5 2 : r KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction Jang-Hyun Kim1 2, Jinuk Kim1 2, Sangwoo Kwon1, Jae W. Lee1, Sangdoo Yun3, Hyun Oh Song 1 2 1Seoul National University, 2Neural Processing Research Center, 3NAVER AI Lab {janghyun, hyunoh}@mllab.snu.ac.kr https://github.com/snu-mllab/KVzip "
[30.05.2025 05:13] Response: ```python
["Seoul National University", "Neural Processing Research Center", "NAVER AI Lab"]
```
[30.05.2025 05:13] Deleting PDF ./assets/pdf/2505.23416.pdf.
[30.05.2025 05:13] Success.
[30.05.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.23253.
[30.05.2025 05:13] Downloading paper 2505.23253 from http://arxiv.org/pdf/2505.23253v1...
[30.05.2025 05:13] Extracting affiliations from text.
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 3 5 2 3 2 . 5 0 5 2 : r UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes Yixun Liang1,2* Kunming Luo1,2* Xiao Chen2* Rui Chen1 Hongyu Yan1 Weiyu Li1,2 Jiarui Liu1,2 Ping Tan1,2 1HKUST 2Light Illusion Figure 1. UniTEX generates high-quality and complete textures for both artist-created low-polygon mesh (the van shell) and generative high-polygon meshes (robots, toy bear, bust and roadsign). "
[30.05.2025 05:13] Response: ```python
["HKUST", "Light Illusion"]
```
[30.05.2025 05:13] Deleting PDF ./assets/pdf/2505.23253.pdf.
[30.05.2025 05:13] Success.
[30.05.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.22943.
[30.05.2025 05:13] Extra JSON file exists (./assets/json/2505.22943.json), skip PDF parsing.
[30.05.2025 05:13] Paper image links file exists (./assets/img_data/2505.22943.json), skip HTML parsing.
[30.05.2025 05:13] Success.
[30.05.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.20282.
[30.05.2025 05:13] Extra JSON file exists (./assets/json/2505.20282.json), skip PDF parsing.
[30.05.2025 05:13] Paper image links file exists (./assets/img_data/2505.20282.json), skip HTML parsing.
[30.05.2025 05:13] Success.
[30.05.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.19360.
[30.05.2025 05:13] Extra JSON file exists (./assets/json/2505.19360.json), skip PDF parsing.
[30.05.2025 05:13] Paper image links file exists (./assets/img_data/2505.19360.json), skip HTML parsing.
[30.05.2025 05:13] Success.
[30.05.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.19236.
[30.05.2025 05:13] Extra JSON file exists (./assets/json/2505.19236.json), skip PDF parsing.
[30.05.2025 05:13] Paper image links file exists (./assets/img_data/2505.19236.json), skip HTML parsing.
[30.05.2025 05:13] Success.
[30.05.2025 05:13] Downloading and parsing paper https://huggingface.co/papers/2505.20199.
[30.05.2025 05:13] Extra JSON file exists (./assets/json/2505.20199.json), skip PDF parsing.
[30.05.2025 05:13] Paper image links file exists (./assets/img_data/2505.20199.json), skip HTML parsing.
[30.05.2025 05:13] Success.
[30.05.2025 05:13] Enriching papers with extra data.
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 0. ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.  					AI-generated summary 				 The rapid advancement of large Vision-Language Models (VLMs) has propelled the develo...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 1. LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.  					AI-generated summary 				 Recent studies on post-training large language models (LLMs) for reasoning through reinforcement le...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 2. A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.  					AI-generated summary 				 MLLMs have been widely studied for vid...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 3. Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting the...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 4. Two post-training strategies, distillation and RLVR, enable inference-time scaling in table reasoning tasks, resulting in a model (Table-R1-Zero) that matches GPT-4.1's performance using fewer parameters and shows strong generalization.  					AI-generated summary 				 In this work, we present the fi...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 5. EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.  					AI-generated summary 				 Language models (LMs) perform well on standardized coding benchmarks ...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 6. AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.  					AI-generated summary 				 We introduce AnySplat, a feed forward network for novel view synthe...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 7. UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.  					AI-generated summary 				 Unified multimodal large language models such as Show-o an...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 8. A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.  					AI-generated summary 				 Recent studies have shown that long chain-of-thought (CoT) reasoning can s...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 9. Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.  					AI-generated summary 				 Unified generation models aim to handle diverse tasks across modalitie...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 10. Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational ineffici...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 11. The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 12. Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.  					AI-generated summary 				 This paper presents Diffusion v...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 13. SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.  					AI-generated summary 				 Recent advancements in large language model (LLM) agents have significantly accelerated sc...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 14. ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.  					AI-generated summary 				 Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 15. Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has moti...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 16. A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.  					AI-generated summary 				 Diffusion-based large language models (Diffusion LLMs) have shown promise for...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 17. GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.  					AI-generated summary 				 Recent advancements in world models have revolutionized dynamic environment simulat...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 18. Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.  					AI-generated summary 				 In this paper, we unify more than ...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 19. LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.  					AI-generated summary 				 We...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 20. DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.  					AI-generated summary 				 Theorem proving serves as a major testbed for evaluating complex rea...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 21. Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation q...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 22. PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.  					AI-generated summary 				 Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doc...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 23. TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.  					AI-generated summary 				 Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual a...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 24. ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.  					AI-generated summary 				 Audio source separation is fundamental for machines to understand complex acous...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 25. The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.  					AI-generated summary 				 Large language models have been extensively studied...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 26. CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.  					AI-generated summary 				 Recent progress in Large Vision-Language Models (LVLMs) has enabled promis...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 27. Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabli...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 28. UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.  					AI-generated summary 				 We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, cons...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 29. A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.  					AI-generated summary 				 While pre-trained multimodal repre...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 30. Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.  					AI-generated summary 				 We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimi...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 31. ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.  					AI-generated summary 				 The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, th...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 32. A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.  					AI-generated summary 				 Creativity evaluation remains a challenging frontier for large language m...
[30.05.2025 05:13] ********************************************************************************
[30.05.2025 05:13] Abstract 33. Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.  					AI-generated summary 				 Classifier-Free Guidance (CFG) signific...
[30.05.2025 05:13] Read previous papers.
[30.05.2025 05:13] Generating reviews via LLM API.
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#games", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –ì–ü–ò-–∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞", "desc": "ZeroGUI - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–¥–∞—á –∏ –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, —á—Ç–æ —É–ª
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#rlhf"], "emoji": "üß†", "ru": {"title": "LLM —É—Å—Ç–æ–π—á–∏–≤—ã –∫ —à—É–º—É: –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å –≤–∞–∂–Ω–µ–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#video", "#games", "#interpretability", "#benchmark", "#reasoning", "#alignment", "#rlhf"], "emoji": "üé•", "ru": {"title": "VF-Eval: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ò–ò-–≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VF-Eval –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM)
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#3d", "#dataset", "#multimodal", "#architecture", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏–∑ 2D –Ω–∞–±–ª—é–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Spatial-MLLM - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—å–∫–æ 2D –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. 
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#inference", "#rl", "#reasoning", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–∞–º–∏", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–∞–º
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#small_models", "#rl", "#optimization", "#open_source", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "EvoScale - —ç—Ç–æ –º–µ—Ç–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#cv", "#3d"], "emoji": "üé•", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –±–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∫–∞–º–µ—Ä", "desc": "AnySplat - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç 3D –≥–∞—É—Å—Å–æ–≤—ã –ø—Ä–∏–º–∏—Ç–∏–≤—ã, –∫–æ–¥–∏—Ä—É—é—â–∏–µ –≥–µ
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#multimodal", "#training", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "UniRL - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#reasoning", "#video", "#multimodal", "#benchmark"], "emoji": "üé•", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "VideoReasonBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. –û–Ω —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π —Ç–æ
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#architecture"], "emoji": "üîÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "Muddit - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã —Å –ª–µ–≥
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#math", "#reasoning", "#alignment", "#rlhf"], "emoji": "üß†", "ru": {"title": "OPO: –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º OPO (On-Poli
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#survey"], "emoji": "üîÑ", "ru": {"title": "SWE-bench-Live: –î–∏–Ω–∞–º–∏—á–Ω—ã–π —ç—Ç–∞–ª–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SWE-bench-Live - –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#open_source", "#multimodal", "#diffusion", "#cv", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "D-AR: –î–∏—Ñ—Ñ—É–∑–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diffusion via Autoregres
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#agents", "#healthcare", "#ethics", "#science", "#benchmark", "#open_source", "#security"], "emoji": "üî¨", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ò–ò-—É—á–µ–Ω—ã–π: —ç—Ç–∏—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤", "desc": "SafeScientist - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –Ω–∞
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#agents", "#rl", "#training", "#reasoning", "#alignment"], "emoji": "üß†", "ru": {"title": "ToMAP: –ò–ò-—É–±–µ–∂–¥–∞—é—â–∏–π —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ToMAP - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –±–æ–ª–µ–µ –≥–∏–±–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤-—É–±–µ–∂–¥–∞—é—â–∏—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥—É–ª–µ–π —Ç–µ–æ—Ä–∏–∏ —Ä–∞–∑—É–º–∞. ToMAP —É–ª—É—á—à–∞–µ—Ç
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#long_context", "#benchmark"], "emoji": "üß†", "ru": {"title": "ATLAS: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ATLAS - –Ω–æ–≤—ã–π –º–æ–¥—É–ª—å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. ATLAS –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö 
[30.05.2025 05:13] Querying the API.
[30.05.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.  					AI-generated summary 				 Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to 27.6times throughput improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.
[30.05.2025 05:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –±–ª–æ—á–Ω—ã–π –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π KV-–∫—ç—à, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —É—á–µ—Ç–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–±–æ—Ä–æ—á–Ω–æ –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–µ –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ —Ç–æ—á–Ω–æ—Å—Ç–∏.",
  "emoji": "üöÄ",
  "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –Ø–ú –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞"
}
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.  					AI-generated summary 				 Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to 27.6times throughput improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs."

[30.05.2025 05:13] Response: ```python
['INFERENCE', 'TRAINING']
```
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.  					AI-generated summary 				 Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to 27.6times throughput improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs."

[30.05.2025 05:13] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[30.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method to enhance the speed of diffusion-based large language models (Diffusion LLMs) during text generation. It introduces a block-wise approximate Key-Value (KV) Cache that allows for efficient reuse of cached information, which helps maintain performance while speeding up inference. Additionally, the authors propose a confidence-aware parallel decoding strategy that addresses quality issues caused by token dependency disruptions during simultaneous decoding. The results show significant improvements in processing speed with minimal loss in accuracy, making Diffusion LLMs more competitive with traditional autoregressive models.","title":"Boosting Speed and Quality in Diffusion LLMs!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method to enhance the speed of diffusion-based large language models (Diffusion LLMs) during text generation. It introduces a block-wise approximate Key-Value (KV) Cache that allows for efficient reuse of cached information, which helps maintain performance while speeding up inference. Additionally, the authors propose a confidence-aware parallel decoding strategy that addresses quality issues caused by token dependency disruptions during simultaneous decoding. The results show significant improvements in processing speed with minimal loss in accuracy, making Diffusion LLMs more competitive with traditional autoregressive models.', title='Boosting Speed and Quality in Diffusion LLMs!'))
[30.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂùóÁä∂Ëøë‰ººKVÁºìÂ≠òÊú∫Âà∂ÂíåÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÂπ∂Ë°åËß£Á†ÅÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òÊâ©Êï£ÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶ÔºåËÄå‰∏ç‰ºöÊòæËëóÈôç‰ΩéË¥®Èáè„ÄÇÊâ©Êï£ÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÈùûËá™ÂõûÂΩíÊñáÊú¨ÁîüÊàê‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁî±‰∫éÁº∫‰πèKVÁºìÂ≠òÂíåÂπ∂Ë°åËß£Á†ÅÊó∂ÁöÑË¥®Èáè‰∏ãÈôçÔºåÂÆûÈôÖÊé®ÁêÜÈÄüÂ∫¶Â∏∏Â∏∏ËêΩÂêé‰∫éËá™ÂõûÂΩíÊ®°Âûã„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•ÂùóÁä∂Ëøë‰ººKVÁºìÂ≠òÊú∫Âà∂ÔºåÂÖÅËÆ∏ÁºìÂ≠òÈáçÁî®ÔºåÂá†‰πé‰∏çÂΩ±ÂìçÊÄßËÉΩ„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÂπ∂Ë°åËß£Á†ÅÁ≠ñÁï•ÂèØ‰ª•ÈÄâÊã©ÊÄßÂú∞Ëß£Á†ÅË∂ÖËøáÁΩÆ‰ø°Â∫¶ÈòàÂÄºÁöÑÊ†áËÆ∞Ôºå‰ªéËÄåÂáèËΩª‰æùËµñÊÄßËøùÂèçÔºå‰øùÊåÅÁîüÊàêË¥®Èáè„ÄÇ","title":"ÊèêÂçáÊâ©Êï£ÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶ÁöÑÂàõÊñ∞Á≠ñÁï•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂùóÁä∂Ëøë‰ººKVÁºìÂ≠òÊú∫Âà∂ÂíåÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÂπ∂Ë°åËß£Á†ÅÁ≠ñÁï•Ôºå‰ª•ÊèêÈ´òÊâ©Êï£ÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÈÄüÂ∫¶ÔºåËÄå‰∏ç‰ºöÊòæËëóÈôç‰ΩéË¥®Èáè„ÄÇÊâ©Êï£ÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÈùûËá™ÂõûÂΩíÊñáÊú¨ÁîüÊàê‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÁî±‰∫éÁº∫‰πèKVÁºìÂ≠òÂíåÂπ∂Ë°åËß£Á†ÅÊó∂ÁöÑË¥®Èáè‰∏ãÈôçÔºåÂÆûÈôÖÊé®ÁêÜÈÄüÂ∫¶Â∏∏Â∏∏ËêΩÂêé‰∫éËá™ÂõûÂΩíÊ®°Âûã„ÄÇÊàë‰ª¨ÈÄöËøáÂºïÂÖ•ÂùóÁä∂Ëøë‰ººKVÁºìÂ≠òÊú∫Âà∂ÔºåÂÖÅËÆ∏ÁºìÂ≠òÈáçÁî®ÔºåÂá†‰πé‰∏çÂΩ±ÂìçÊÄßËÉΩ„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÂπ∂Ë°åËß£Á†ÅÁ≠ñÁï•ÂèØ‰ª•ÈÄâÊã©ÊÄßÂú∞Ëß£Á†ÅË∂ÖËøáÁΩÆ‰ø°Â∫¶ÈòàÂÄºÁöÑÊ†áËÆ∞Ôºå‰ªéËÄåÂáèËΩª‰æùËµñÊÄßËøùÂèçÔºå‰øùÊåÅÁîüÊàêË¥®Èáè„ÄÇ', title='ÊèêÂçáÊâ©Êï£ÂûãÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶ÁöÑÂàõÊñ∞Á≠ñÁï•'))
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#games", "#agents", "#3d", "#training"], "emoji": "üöó", "ru": {"title": "GeoDrive: 3D-–≥–µ–æ–º–µ—Ç—Ä–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "GeoDrive - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é 3D-–≥–µ–æ–º–µ—Ç—Ä–∏—é –¥–ª—è —É–ª
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#3d", "#transfer_learning", "#cv", "#diffusion", "#dataset", "#math", "#optimization", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "Uni-Instruct: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Uni-Instruct - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Å—Ç–∏–ª
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#story_generation", "#diffusion"], "emoji": "üé®", "ru": {"title": "–£–º–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "LoRAShop - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π LoRA. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#math", "#rl", "#training", "#reasoning", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ —Ç–µ–æ—Ä–µ–º —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "desc": "DeepTheorem - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#video", "#diffusion", "#benchmark", "#open_source"], "emoji": "üé¨", "ru": {"title": "MAGREF: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MAGREF - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#open_source", "#training", "#healthcare", "#dataset", "#science"], "emoji": "ü©∫", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω–µ", "desc": "PatientSim - —ç—Ç–æ —Å–∏–º—É–ª—è—Ç–æ—Ä, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–π —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#interpretability", "#benchmark", "#architecture", "#security"], "emoji": "üîç", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "TrustVLM - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#transfer_learning", "#audio", "#benchmark", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫", "desc": "ZeroSep - —ç—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∞—É–¥–∏–æ–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π —Ç–µ–∫—Å—Ç–æ–º. –û–Ω–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#data", "#interpretability", "#graphs", "#architecture", "#dataset", "#reasoning", "#benchmark", "#training"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–Ω–∞–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#healthcare", "#dataset", "#science", "#multimodal", "#cv", "#benchmark", "#interpretability", "#reasoning"], "emoji": "ü©ª", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ", "desc": "CheXStruct –∏ CXReasonBench - —ç—Ç–æ –Ω–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[30.05.2025 05:13] Querying the API.
[30.05.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4times and FlashAttention decoding latency by approximately 2times, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.
[30.05.2025 05:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç KVzip - –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ (KV) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. KVzip –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø–∞—Ä KV, –∏—Å–ø–æ–ª—å–∑—É—è —Å–∞–º—É –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏ —É–¥–∞–ª—è–µ—Ç –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–µ –ø–∞—Ä—ã. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ KVzip —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä KV-–∫—ç—à–∞ –≤ 3-4 —Ä–∞–∑–∞ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è FlashAttention –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 2 —Ä–∞–∑–∞ –ø—Ä–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Å–∂–∞—Ç–∏—é KV-–∫—ç—à–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏.",
  "emoji": "üóúÔ∏è",
  "title": "KVzip: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫—ç—à–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4times and FlashAttention decoding latency by approximately 2times, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios."

[30.05.2025 05:13] Response: ```python
["INFERENCE", "TRAINING"]
```
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4times and FlashAttention decoding latency by approximately 2times, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios."

[30.05.2025 05:13] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION", "REASONING"]
```
[30.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents KVzip, a novel method for managing key-value (KV) caches in transformer-based large language models (LLMs) during inference. As the context length increases, traditional KV caches become large and slow, but KVzip allows for efficient reuse of compressed caches across different queries. It evaluates the importance of KV pairs using the LLM\'s ability to reconstruct contexts, allowing less important pairs to be evicted. The results show that KVzip can reduce cache size significantly while improving decoding speed and maintaining performance across various tasks and models.","title":"Efficient KV Cache Management with KVzip"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents KVzip, a novel method for managing key-value (KV) caches in transformer-based large language models (LLMs) during inference. As the context length increases, traditional KV caches become large and slow, but KVzip allows for efficient reuse of compressed caches across different queries. It evaluates the importance of KV pairs using the LLM's ability to reconstruct contexts, allowing less important pairs to be evicted. The results show that KVzip can reduce cache size significantly while improving decoding speed and maintaining performance across various tasks and models.", title='Efficient KV Cache Management with KVzip'))
[30.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫KVzipÁöÑÁºìÂ≠òÈ©±ÈÄêÊñπÊ≥ïÔºåÊó®Âú®‰ºòÂåñÂü∫‰∫éTransformerÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÁöÑÈîÆÂÄºÔºàKVÔºâÁºìÂ≠ò„ÄÇÈöèÁùÄ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÂ¢ûÂä†ÔºåKVÁºìÂ≠òÁöÑÂ§ßÂ∞è‰πüÈöè‰πãÊâ©Â§ßÔºåÂØºËá¥ÂÜÖÂ≠òÂºÄÈîÄÂíåÊ≥®ÊÑèÂäõÂª∂ËøüÊòæËëóÂ¢ûÂä†„ÄÇKVzipÈÄöËøáÈáèÂåñKVÂØπÁöÑÈáçË¶ÅÊÄßÔºåËÉΩÂ§üÊúâÊïàÂú∞ÈáçÁî®ÂéãÁº©ÁöÑKVÁºìÂ≠òÔºå‰ªéËÄåÂú®Â§öÁßçÊü•ËØ¢‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÁºìÂ≠òÁÆ°ÁêÜ„ÄÇÂÆûÈ™åËØÅÊòéÔºåKVzipÂèØ‰ª•Â∞ÜKVÁºìÂ≠òÂ§ßÂ∞èÂáèÂ∞ë3-4ÂÄçÔºåÂπ∂Â∞ÜFlashAttentionËß£Á†ÅÂª∂ËøüÈôç‰ΩéÁ∫¶2ÂÄçÔºåÂêåÊó∂Âú®ÈóÆÁ≠î„ÄÅÊ£ÄÁ¥¢„ÄÅÊé®ÁêÜÂíå‰ª£Á†ÅÁêÜËß£‰ªªÂä°‰∏≠Âá†‰πéÊ≤°ÊúâÊÄßËÉΩÊçüÂ§±„ÄÇ","title":"KVzipÔºöÈ´òÊïàÁöÑKVÁºìÂ≠òÁÆ°ÁêÜÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫KVzipÁöÑÁºìÂ≠òÈ©±ÈÄêÊñπÊ≥ïÔºåÊó®Âú®‰ºòÂåñÂü∫‰∫éTransformerÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÁöÑÈîÆÂÄºÔºàKVÔºâÁºìÂ≠ò„ÄÇÈöèÁùÄ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÂ¢ûÂä†ÔºåKVÁºìÂ≠òÁöÑÂ§ßÂ∞è‰πüÈöè‰πãÊâ©Â§ßÔºåÂØºËá¥ÂÜÖÂ≠òÂºÄÈîÄÂíåÊ≥®ÊÑèÂäõÂª∂ËøüÊòæËëóÂ¢ûÂä†„ÄÇKVzipÈÄöËøáÈáèÂåñKVÂØπÁöÑÈáçË¶ÅÊÄßÔºåËÉΩÂ§üÊúâÊïàÂú∞ÈáçÁî®ÂéãÁº©ÁöÑKVÁºìÂ≠òÔºå‰ªéËÄåÂú®Â§öÁßçÊü•ËØ¢‰∏≠ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÁºìÂ≠òÁÆ°ÁêÜ„ÄÇÂÆûÈ™åËØÅÊòéÔºåKVzipÂèØ‰ª•Â∞ÜKVÁºìÂ≠òÂ§ßÂ∞èÂáèÂ∞ë3-4ÂÄçÔºåÂπ∂Â∞ÜFlashAttentionËß£Á†ÅÂª∂ËøüÈôç‰ΩéÁ∫¶2ÂÄçÔºåÂêåÊó∂Âú®ÈóÆÁ≠î„ÄÅÊ£ÄÁ¥¢„ÄÅÊé®ÁêÜÂíå‰ª£Á†ÅÁêÜËß£‰ªªÂä°‰∏≠Âá†‰πéÊ≤°ÊúâÊÄßËÉΩÊçüÂ§±„ÄÇ', title='KVzipÔºöÈ´òÊïàÁöÑKVÁºìÂ≠òÁÆ°ÁêÜÊñπÊ≥ï'))
[30.05.2025 05:13] Querying the API.
[30.05.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.  					AI-generated summary 				 We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.
[30.05.2025 05:13] Response: {
  "desc": "UniTEX - —ç—Ç–æ –Ω–æ–≤–∞—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö 3D-—Ç–µ–∫—Å—Ç—É—Ä. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¢–µ–∫—Å—Ç—É—Ä–Ω—ã–µ –§—É–Ω–∫—Ü–∏–∏ (TF) –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∏–∑–±–µ–≥–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π UV-–º–∞–ø–ø–∏–Ω–≥–∞. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –ë–æ–ª—å—à—É—é –ú–æ–¥–µ–ª—å –¢–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è (LTM) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è TF –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏. UniTEX —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã (DiT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç—É—Ä –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ.",
  "emoji": "üé®",
  "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ç–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏: UniTEX –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"
}
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.  					AI-generated summary 				 We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX."

[30.05.2025 05:13] Response: ```python
["3D"]
```
[30.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.  					AI-generated summary 				 We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX."

[30.05.2025 05:13] Response: ```python
["DIFFUSION", "OPEN_SOURCE"]
```
[30.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UniTEX is a new framework designed for generating high-quality 3D textures without the need for UV mapping. It uses Texture Functions to create a continuous representation of textures based on 3D spatial data, allowing for better consistency and quality. The framework employs a transformer-based model to predict these textures directly from images and geometry, enhancing the process of texture synthesis. By avoiding traditional UV-based methods, UniTEX addresses common challenges in 3D texture generation, resulting in superior visual quality and integrity.","title":"Revolutionizing 3D Texture Generation with UniTEX"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UniTEX is a new framework designed for generating high-quality 3D textures without the need for UV mapping. It uses Texture Functions to create a continuous representation of textures based on 3D spatial data, allowing for better consistency and quality. The framework employs a transformer-based model to predict these textures directly from images and geometry, enhancing the process of texture synthesis. By avoiding traditional UV-based methods, UniTEX addresses common challenges in 3D texture generation, resulting in superior visual quality and integrity.', title='Revolutionizing 3D Texture Generation with UniTEX'))
[30.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UniTEXÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏âÁª¥Á∫πÁêÜÁîüÊàêÊ°ÜÊû∂ÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÂõæÂÉèÂíåÂá†‰ΩïÂΩ¢Áä∂ÁîüÊàêÈ´òË¥®Èáè„ÄÅ‰∏ÄËá¥ÁöÑ‰∏âÁª¥Á∫πÁêÜÔºåËÄåÊó†ÈúÄ‰ΩøÁî®UVÊò†Â∞Ñ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁ∫πÁêÜÂáΩÊï∞ÔºàTexture FunctionsÔºâÂ∞ÜÁ∫πÁêÜÁîüÊàêÊèêÂçáÂà∞‰∏âÁª¥Á©∫Èó¥ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüUVÊò†Â∞ÑÂ∏¶Êù•ÁöÑÊãìÊâëÊ®°Á≥äÈóÆÈ¢ò„ÄÇUniTEXÂà©Áî®Âü∫‰∫éÂèòÊç¢Âô®ÁöÑÂ§ßÂûãÁ∫πÁêÜÊ®°ÂûãÔºàLarge Texturing ModelÔºâÁõ¥Êé•È¢ÑÊµãÁ∫πÁêÜÂáΩÊï∞Ôºå‰ªéËÄåÊèêÈ´òÁ∫πÁêÜË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniTEXÂú®ËßÜËßâË¥®ÈáèÂíåÁ∫πÁêÜÂÆåÊï¥ÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑËá™Âä®Âåñ‰∏âÁª¥Á∫πÁêÜÁîüÊàêËß£ÂÜ≥ÊñπÊ°à„ÄÇ","title":"UniTEXÔºöÊó†UVÊò†Â∞ÑÁöÑÈ´òË¥®Èáè‰∏âÁª¥Á∫πÁêÜÁîüÊàê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UniTEXÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏âÁª¥Á∫πÁêÜÁîüÊàêÊ°ÜÊû∂ÔºåËÉΩÂ§üÁõ¥Êé•‰ªéÂõæÂÉèÂíåÂá†‰ΩïÂΩ¢Áä∂ÁîüÊàêÈ´òË¥®Èáè„ÄÅ‰∏ÄËá¥ÁöÑ‰∏âÁª¥Á∫πÁêÜÔºåËÄåÊó†ÈúÄ‰ΩøÁî®UVÊò†Â∞Ñ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁ∫πÁêÜÂáΩÊï∞ÔºàTexture FunctionsÔºâÂ∞ÜÁ∫πÁêÜÁîüÊàêÊèêÂçáÂà∞‰∏âÁª¥Á©∫Èó¥ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüUVÊò†Â∞ÑÂ∏¶Êù•ÁöÑÊãìÊâëÊ®°Á≥äÈóÆÈ¢ò„ÄÇUniTEXÂà©Áî®Âü∫‰∫éÂèòÊç¢Âô®ÁöÑÂ§ßÂûãÁ∫πÁêÜÊ®°ÂûãÔºàLarge Texturing ModelÔºâÁõ¥Êé•È¢ÑÊµãÁ∫πÁêÜÂáΩÊï∞Ôºå‰ªéËÄåÊèêÈ´òÁ∫πÁêÜË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUniTEXÂú®ËßÜËßâË¥®ÈáèÂíåÁ∫πÁêÜÂÆåÊï¥ÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑËá™Âä®Âåñ‰∏âÁª¥Á∫πÁêÜÁîüÊàêËß£ÂÜ≥ÊñπÊ°à„ÄÇ', title='UniTEXÔºöÊó†UVÊò†Â∞ÑÁöÑÈ´òË¥®Èáè‰∏âÁª¥Á∫πÁêÜÁîüÊàê'))
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#hallucinations", "#multimodal", "#benchmark", "#security", "#training"], "emoji": "üé≠", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–ª–∞–±—ã—Ö –º–µ—Å—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±–º–∞–Ω—á–∏–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Multimodal Adversarial Compositionality (MAC) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training"], "emoji": "üöÄ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ –æ–¥–Ω–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#synthetic", "#multimodal", "#hallucinations", "#cv", "#benchmark"], "emoji": "üìä", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é ChartLens", "desc": "ChartLens - —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#creativity", "#dataset", "#open_source", "#data", "#benchmark"], "emoji": "üé®", "ru": {"title": "CrEval: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–æ–ø–∞—Ä–Ω–æ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å
[30.05.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#benchmark", "#training"], "emoji": "üé≠", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ë–µ—Å–∫–ª–∞—Å—Å–æ–≤–æ–µ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ (A-CFG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞
[30.05.2025 05:13] Loading Chinese text from previous data.
[30.05.2025 05:13] Renaming data file.
[30.05.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-05-30.json
[30.05.2025 05:13] Saving new data file.
[30.05.2025 05:13] Generating page.
[30.05.2025 05:13] Renaming previous page.
[30.05.2025 05:13] Renaming previous data. index.html to ./d/2025-05-30.html
[30.05.2025 05:13] [Experimental] Generating Chinese page for reading.
[30.05.2025 05:13] Chinese vocab [{'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng hu√† xu√© x√≠', 'trans': 'reinforcement learning'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«î y√†o', 'trans': 'major'}, {'word': 'ÈöúÁ¢ç', 'pinyin': 'zh√†ng √†i', 'trans': 'obstacle'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'ÁÜµ', 'pinyin': 'shƒÅng', 'trans': 'entropy'}, {'word': 'Â¥©Ê∫É', 'pinyin': 'bƒìng ku√¨', 'trans': 'collapse'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'discover'}, {'word': 'Âπ≤È¢Ñ', 'pinyin': 'gƒÅn y√π', 'trans': 'intervention'}, {'word': 'ÊÉÖÂÜµ', 'pinyin': 'q√≠ng ku√†ng', 'trans': 'situation'}, {'word': 'Èò∂ÊÆµ', 'pinyin': 'jiƒì du√†n', 'trans': 'stage'}, {'word': 'ÊÄ•Ââß', 'pinyin': 'j√≠ j√π', 'trans': 'drastic'}, {'word': '‰∏ãÈôç', 'pinyin': 'xi√† ji√†ng', 'trans': 'decline'}, {'word': 'ÂØºËá¥', 'pinyin': 'd«éo zh√¨', 'trans': 'lead to'}, {'word': 'Êé¢Á¥¢', 'pinyin': 't√†n su«í', 'trans': 'exploration'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ÂáèÂº±', 'pinyin': 'ji«én ru√≤', 'trans': 'weaken'}, {'word': 'ÂÅúÊªû', 'pinyin': 't√≠ng zh√¨', 'trans': 'stagnate'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ËΩ¨Êç¢', 'pinyin': 'zhu«én hu√†n', 'trans': 'conversion'}, {'word': 'ÊñπÁ®ã', 'pinyin': 'fƒÅng ch√©ng', 'trans': 'equation'}, {'word': '‰∏ãÊ∏∏', 'pinyin': 'xi√† y√≥u', 'trans': 'downstream'}, {'word': 'ÁêÜËÆ∫', 'pinyin': 'l«ê l√πn', 'trans': 'theory'}, {'word': 'ÂÆûËØÅ', 'pinyin': 'sh√≠ zh√®ng', 'trans': 'empirical'}, {'word': 'ÂàÜÊûê', 'pinyin': 'fƒìn xƒ´', 'trans': 'analysis'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ng t√†i', 'trans': 'dynamics'}, {'word': 'ÊúÄÁªà', 'pinyin': 'zu√¨ zh≈çng', 'trans': 'ultimately'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨ sh√π', 'trans': 'technique'}, {'word': 'ÊéßÂà∂', 'pinyin': 'k√≤ng zh√¨', 'trans': 'control'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': '‰øÉËøõ', 'pinyin': 'c√π j√¨n', 'trans': 'promote'}, {'word': 'ÈÅøÂÖç', 'pinyin': 'b√¨ mi«én', 'trans': 'avoid'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}]
[30.05.2025 05:13] Renaming previous Chinese page.
[30.05.2025 05:13] Renaming previous data. zh.html to ./d/2025-05-29_zh_reading_task.html
[30.05.2025 05:13] Writing Chinese reading task.
[30.05.2025 05:13] Writing result.
[30.05.2025 05:13] Renaming log file.
[30.05.2025 05:13] Renaming previous data. log.txt to ./logs/2025-05-30_last_log.txt
