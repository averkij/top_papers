[30.05.2025 08:17] Read previous papers.
[30.05.2025 08:17] Generating top page (month).
[30.05.2025 08:17] Writing top page (month).
[30.05.2025 09:12] Read previous papers.
[30.05.2025 09:12] Get feed.
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22653
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23762
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23747
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23621
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23693
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23604
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23716
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23660
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23646
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23359
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20088
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23380
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22914
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23419
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23606
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23585
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22618
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22421
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23416
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23758
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23735
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23559
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22961
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20755
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23742
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23754
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23745
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23253
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21114
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17818
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14321
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23761
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23625
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22918
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22765
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20282
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19360
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19286
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18087
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23183
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22943
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22810
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22126
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20199
[30.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19236
[30.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.05.2025 09:12] No deleted papers detected.
[30.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 45.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22653.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22653.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22653.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23762.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23762.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23762.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23747.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23747.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23747.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23621.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23621.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23621.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23693.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23693.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23693.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23604.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23604.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23604.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23716.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23716.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23716.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23660.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23660.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23660.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23646.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23646.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23646.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23359.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23359.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23359.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.20088.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.20088.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.20088.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23380.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23380.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23380.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22914.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22914.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22914.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23419.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23419.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23419.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23606.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23606.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23606.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23585.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23585.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23585.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22618.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22618.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22618.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22421.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22421.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22421.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23416.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23416.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23416.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23758.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23758.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23758.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23735.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23735.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23735.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23559.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23559.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23559.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22961.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22961.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22961.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.20755.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.20755.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.20755.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23742.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23742.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23742.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23754.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23754.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23754.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23745.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23745.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23745.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23253.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23253.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23253.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.21114.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.21114.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.21114.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.17818.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.17818.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.17818.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.14321.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.14321.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.14321.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23761.
[30.05.2025 09:12] Downloading paper 2505.23761 from http://arxiv.org/pdf/2505.23761v1...
[30.05.2025 09:12] Failed to download and parse paper https://huggingface.co/papers/2505.23761: 'LTChar' object is not iterable
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23625.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23625.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23625.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22918.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22918.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22918.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22765.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22765.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22765.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.20282.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.20282.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.20282.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.19360.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.19360.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.19360.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.19286.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.19286.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.19286.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.18087.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.18087.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.18087.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.23183.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.23183.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.23183.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22943.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22943.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22943.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22810.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22810.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22810.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.22126.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.22126.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.22126.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.20199.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.20199.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.20199.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.19236.
[30.05.2025 09:12] Extra JSON file exists (./assets/json/2505.19236.json), skip PDF parsing.
[30.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.19236.json), skip HTML parsing.
[30.05.2025 09:12] Success.
[30.05.2025 09:12] Enriching papers with extra data.
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 0. LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.  					AI-generated summary 				 Recent studies on post-training large language models (LLMs) for reasoning through reinforcement le...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 1. ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.  					AI-generated summary 				 The rapid advancement of large Vision-Language Models (VLMs) has propelled the develo...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 2. Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting the...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 3. Two post-training strategies, distillation and RLVR, enable inference-time scaling in table reasoning tasks, resulting in a model (Table-R1-Zero) that matches GPT-4.1's performance using fewer parameters and shows strong generalization.  					AI-generated summary 				 In this work, we present the fi...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 4. A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.  					AI-generated summary 				 MLLMs have been widely studied for vid...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 5. EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.  					AI-generated summary 				 Language models (LMs) perform well on standardized coding benchmarks ...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 6. AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.  					AI-generated summary 				 We introduce AnySplat, a feed forward network for novel view synthe...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 7. Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.  					AI-generated summary 				 This paper presents Diffusion v...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 8. Large reasoning models exhibit varying susceptibility to hallucination depending on post-training pipelines, revealing critical cognitive behaviors and uncertainty misalignment as contributing factors.  					AI-generated summary 				 Recently evolved large reasoning models (LRMs) show powerful perfo...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 9. A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.  					AI-generated summary 				 Recent studies have shown that long chain-of-thought (CoT) reasoning can s...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 10. A new automated method using concept-based vectors and a Hierarchical Multi-Domain Regression model improves preference explanations and predictions for large language models.  					AI-generated summary 				 Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, a...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 11. UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.  					AI-generated summary 				 Unified multimodal large language models such as Show-o an...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 12. A multi-modal CAD reconstruction model leveraging vision-language models and reinforcement learning achieves state-of-the-art performance across various datasets, including real-world ones.  					AI-generated summary 				 Computer-Aided Design (CAD) plays a central role in engineering and manufactur...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 13. The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 14. Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.  					AI-generated summary 				 Unified generation models aim to handle diverse tasks across modalitie...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 15. Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational ineffici...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 16. A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.  					AI-generated summary 				 Diffusion-based large language models (Diffusion LLMs) have shown promise for...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 17. GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.  					AI-generated summary 				 Recent advancements in world models have revolutionized dynamic environment simulat...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 18. Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabli...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 19. LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.  					AI-generated summary 				 We...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 20. Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has moti...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 21. SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.  					AI-generated summary 				 Recent advancements in large language model (LLM) agents have significantly accelerated sc...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 22. ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.  					AI-generated summary 				 Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 23. Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.  					AI-generated summary 				 In this paper, we unify more than ...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 24. Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation q...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 25. DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.  					AI-generated summary 				 Theorem proving serves as a major testbed for evaluating complex rea...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 26. TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.  					AI-generated summary 				 Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual a...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 27. UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.  					AI-generated summary 				 We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, cons...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 28. Researchers propose a novel differentiable solver search algorithm that optimizes the computational efficiency and quality of diffusion models for image generation tasks.  					AI-generated summary 				 Diffusion models have demonstrated remarkable generation quality but at the cost of numerous func...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 29. PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.  					AI-generated summary 				 Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doc...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 30. Existing video understanding benchmarks often conflate knowledge-based and purely image-based questions, rather than clearly isolating a model's temporal reasoning ability, which is the key aspect that distinguishes video understanding from other modalities. We identify two major limitations that ob...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 31. Theoretical analysis of Direct Preference Optimization (DPO) reveals that log-ratio reward parameterization is optimal for learning target policy via preference optimization, linked to log-margin ordered policies, and explains policy reinforcement and smoothing based on differential information entr...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 32. ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.  					AI-generated summary 				 Audio source separation is fundamental for machines to understand complex acous...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 33. Re-ttention uses temporal redundancy in diffusion models to enable high sparse attention in visual generation, maintaining quality with minimal computational overhead.  					AI-generated summary 				 Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual conte...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 34. A StressTest benchmark and synthetic Stress17k dataset are introduced to improve speech-aware language models' ability to interpret sentence stress in spoken language.  					AI-generated summary 				 Sentence stress refers to emphasis, placed on specific words within a spoken utterance to highlight ...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 35. Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.  					AI-generated summary 				 We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimi...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 36. ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.  					AI-generated summary 				 The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, th...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 37. The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.  					AI-generated summary 				 Large language models have been extensively studied...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 38. CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.  					AI-generated summary 				 Recent progress in Large Vision-Language Models (LVLMs) has enabled promis...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 39. Evaluation of word-level quality estimation techniques leverages model interpretability and uncertainty quantification to identify translation errors with a focus on the impact of label variation and the performance of supervised versus unsupervised metrics.  					AI-generated summary 				 Word-leve...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 40. A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.  					AI-generated summary 				 While pre-trained multimodal repre...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 41. VidText is a new benchmark that evaluates video text understanding across various tasks, covering global summarization and local retrieval, and highlights challenges for current multimodal models.  					AI-generated summary 				 Visual texts embedded in videos carry rich semantic information, which ...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 42. The introduction of SridBench, a benchmark for scientific figure generation, reveals that current top-tier models, such as GPT-4o-image, fall short in semantic and structural accuracy compared to human performance, underscoring the need for more advanced multimodal reasoning-driven visual generation...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 43. Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.  					AI-generated summary 				 Classifier-Free Guidance (CFG) signific...
[30.05.2025 09:12] ********************************************************************************
[30.05.2025 09:12] Abstract 44. A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.  					AI-generated summary 				 Creativity evaluation remains a challenging frontier for large language m...
[30.05.2025 09:12] Read previous papers.
[30.05.2025 09:12] Generating reviews via LLM API.
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#rlhf"], "emoji": "üß†", "ru": {"title": "LLM —É—Å—Ç–æ–π—á–∏–≤—ã –∫ —à—É–º—É: –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å –≤–∞–∂–Ω–µ–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#agents", "#rl", "#optimization", "#games", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –ì–ü–ò-–∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞", "desc": "ZeroGUI - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–¥–∞—á –∏ –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, —á—Ç–æ —É–ª
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#3d", "#dataset", "#multimodal", "#architecture", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏–∑ 2D –Ω–∞–±–ª—é–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Spatial-MLLM - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—å–∫–æ 2D –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. 
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#inference", "#rl", "#reasoning", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–∞–º–∏", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞–¥ —Ç–∞–±–ª–∏—Ü–∞–º
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#video", "#games", "#interpretability", "#benchmark", "#reasoning", "#alignment", "#rlhf"], "emoji": "üé•", "ru": {"title": "VF-Eval: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ò–ò-–≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VF-Eval –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM)
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#small_models", "#rl", "#optimization", "#open_source", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "EvoScale - —ç—Ç–æ –º–µ—Ç–æ–¥, —Å–æ—á–µ—Ç–∞—é—â–∏–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#3d"], "emoji": "üé•", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –±–µ–∑ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∫–∞–º–µ—Ä", "desc": "AnySplat - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç 3D –≥–∞—É—Å—Å–æ–≤—ã –ø—Ä–∏–º–∏—Ç–∏–≤—ã, –∫–æ–¥–∏—Ä—É—é—â–∏–µ –≥–µ
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#open_source", "#multimodal", "#diffusion", "#cv", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "D-AR: –î–∏—Ñ—Ñ—É–∑–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Diffusion via Autoregres
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#hallucinations"], "emoji": "ü§ñ", "ru": {"title": "–ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –ø—Ä–∏—á–∏–Ω—ã –∏ —Ä–µ—à–µ–Ω–∏—è", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º —É –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#video", "#multimodal", "#benchmark"], "emoji": "üé•", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "VideoReasonBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ. –û–Ω —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π —Ç–æ
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#dataset", "#data", "#interpretability", "#alignment", "#rlhf", "#training"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–≠—Ç–æ—Ç –Ω–∞—É—á–Ω—ã–π —Ç—Ä—É–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#multimodal", "#training", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "UniRL - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#3d", "#games", "#multimodal", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è CAD: –æ–±—ä–µ–¥–∏–Ω—è—è –∑—Ä–µ–Ω–∏–µ, —è–∑—ã–∫ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ C
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#survey"], "emoji": "üîÑ", "ru": {"title": "SWE-bench-Live: –î–∏–Ω–∞–º–∏—á–Ω—ã–π —ç—Ç–∞–ª–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SWE-bench-Live - –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#multimodal", "#architecture"], "emoji": "üîÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "Muddit - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã —Å –ª–µ–≥
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#math", "#reasoning", "#alignment", "#rlhf"], "emoji": "üß†", "ru": {"title": "OPO: –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º OPO (On-Poli
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization", "#diffusion"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –Ø–ú –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –±–ª–æ—á–Ω—ã–π –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π KV-–∫
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#games", "#agents", "#3d", "#training"], "emoji": "üöó", "ru": {"title": "GeoDrive: 3D-–≥–µ–æ–º–µ—Ç—Ä–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "GeoDrive - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é 3D-–≥–µ–æ–º–µ—Ç—Ä–∏—é –¥–ª—è —É–ª
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#training", "#inference", "#long_context", "#optimization", "#reasoning"], "emoji": "üóúÔ∏è", "ru": {"title": "KVzip: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫—ç—à–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç KVzip - –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ (KV) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#story_generation", "#diffusion"], "emoji": "üé®", "ru": {"title": "–£–º–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "LoRAShop - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–µ–π LoRA. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#long_context", "#benchmark"], "emoji": "üß†", "ru": {"title": "ATLAS: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ATLAS - –Ω–æ–≤—ã–π –º–æ–¥—É–ª—å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. ATLAS –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö 
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#agents", "#healthcare", "#ethics", "#science", "#benchmark", "#open_source", "#security"], "emoji": "üî¨", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ò–ò-—É—á–µ–Ω—ã–π: —ç—Ç–∏—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤", "desc": "SafeScientist - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –Ω–∞
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#agents", "#rl", "#training", "#reasoning", "#alignment"], "emoji": "üß†", "ru": {"title": "ToMAP: –ò–ò-—É–±–µ–∂–¥–∞—é—â–∏–π —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ToMAP - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –±–æ–ª–µ–µ –≥–∏–±–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤-—É–±–µ–∂–¥–∞—é—â–∏—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥—É–ª–µ–π —Ç–µ–æ—Ä–∏–∏ —Ä–∞–∑—É–º–∞. ToMAP —É–ª—É—á—à–∞–µ—Ç
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#3d", "#transfer_learning", "#cv", "#diffusion", "#dataset", "#math", "#optimization", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "Uni-Instruct: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Uni-Instruct - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–¥–Ω–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Å—Ç–∏–ª
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#video", "#diffusion", "#benchmark", "#open_source"], "emoji": "üé¨", "ru": {"title": "MAGREF: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MAGREF - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#math", "#rl", "#training", "#reasoning", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ —Ç–µ–æ—Ä–µ–º —Å –ø–æ–º–æ—â—å—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "desc": "DeepTheorem - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#interpretability", "#benchmark", "#architecture", "#security"], "emoji": "üîç", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "TrustVLM - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#3d", "#open_source", "#diffusion"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ç–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏: UniTEX –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "desc": "UniTEX - —ç—Ç–æ –Ω–æ–≤–∞—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö 3D-—Ç–µ–∫—Å—Ç—É—Ä. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#diffusion", "#data", "#cv", "#architecture"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ—à–∞—Ç–µ–ª—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ä–µ—à–∞—Ç–µ–ª—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#training", "#healthcare", "#dataset", "#science"], "emoji": "ü©∫", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω–µ", "desc": "PatientSim - —ç—Ç–æ —Å–∏–º—É–ª—è—Ç–æ—Ä, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–π —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#video", "#benchmark"], "emoji": "üé•", "ru": {"title": "–î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VBenchComp - –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#interpretability", "#optimization", "#training", "#alignment", "#synthetic", "#rlhf"], "emoji": "üß†", "ru": {"title": "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ DPO —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–∞ Direct Preference Optimizat
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#transfer_learning", "#audio", "#benchmark", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫", "desc": "ZeroSep - —ç—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∞—É–¥–∏–æ–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π —Ç–µ–∫—Å—Ç–æ–º. –û–Ω–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training", "#video", "#diffusion", "#cv"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Re-ttention –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#audio", "#optimization", "#training", "#synthetic", "#benchmark", "#dataset"], "emoji": "üó£Ô∏è", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ñ—Ä–∞–∑–æ–≤–æ–≥–æ —É–¥–∞—Ä–µ–Ω–∏—è –≤ —Ä–µ—á–µ–≤—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ StressTest –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Stress17k –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training"], "emoji": "üöÄ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ –æ–¥–Ω–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#synthetic", "#multimodal", "#hallucinations", "#cv", "#benchmark"], "emoji": "üìä", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é ChartLens", "desc": "ChartLens - —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#data", "#interpretability", "#graphs", "#architecture", "#dataset", "#reasoning", "#benchmark", "#training"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì—Ä–∞—Ñ–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–Ω–∞–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#healthcare", "#dataset", "#science", "#multimodal", "#cv", "#benchmark", "#interpretability", "#reasoning"], "emoji": "ü©ª", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ", "desc": "CheXStruct –∏ CXReasonBench - —ç—Ç–æ –Ω–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#multilingual", "#translation", "#data", "#benchmark", "#interpretability"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: –æ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∫ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#hallucinations", "#multimodal", "#benchmark", "#security", "#training"], "emoji": "üé≠", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–ª–∞–±—ã—Ö –º–µ—Å—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±–º–∞–Ω—á–∏–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Multimodal Adversarial Compositionality (MAC) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#reasoning", "#benchmark"], "emoji": "üé•", "ru": {"title": "VidText: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–∏–¥–µ–æ", "desc": "VidText - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–∏–¥–µ–æ, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –æ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –æ–±–æ–±—â–µ–Ω–∏—è –¥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#science", "#interpretability", "#multimodal", "#reasoning"], "emoji": "üî¨", "ru": {"title": "SridBench: –≤—ã–∑–æ–≤ –ò–ò –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–∞—É—á–Ω—ã—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π", "desc": "SridBench - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. –û–Ω –≤–∫–ª—é—á–∞
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#benchmark", "#training"], "emoji": "üé≠", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ë–µ—Å–∫–ª–∞—Å—Å–æ–≤–æ–µ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ (A-CFG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞
[30.05.2025 09:12] Using data from previous issue: {"categories": ["#creativity", "#dataset", "#open_source", "#data", "#benchmark"], "emoji": "üé®", "ru": {"title": "CrEval: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–æ–ø–∞—Ä–Ω–æ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å
[30.05.2025 09:12] Trying to get texts in Chinese.
[30.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.  					AI-generated summary 				 Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.
[30.05.2025 09:13] Mistral response. {"id": "8e627f00898a42b2947e765641c1bcb8", "object": "chat.completion", "created": 1748596364, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u5956\u52b1\u566a\u97f3\uff08reward noise\uff09\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002\u5373\u4f7f\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u624b\u52a8\u7ffb\u8f6c40%\u7684\u5956\u52b1\u51fd\u6570\u8f93\u51fa\uff0c\u6a21\u578b\u4ecd\u80fd\u5feb\u901f\u6536\u655b\uff0c\u63d0\u9ad8\u5176\u6570\u5b66\u4efb\u52a1\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5956\u52b1\u5173\u952e\u63a8\u7406\u77ed\u8bed\uff08RPR\uff09\u7684\u51fa\u73b0\uff0c\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e5f\u5f97\u5230\u4e86\u63d0\u5347\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u6539\u8fdb\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u57fa\u7840\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 440, "total_tokens": 618, "completion_tokens": 178}}
[30.05.2025 09:13] Response: ËøôÁØáÊñáÁ´†Á†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåLLMsÂú®Â•ñÂä±Âô™Èü≥Ôºàreward noiseÔºâÂ≠òÂú®ÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂç≥‰ΩøÂú®Êï∞Â≠¶‰ªªÂä°‰∏≠ÊâãÂä®ÁøªËΩ¨40%ÁöÑÂ•ñÂä±ÂáΩÊï∞ËæìÂá∫ÔºåÊ®°Âûã‰ªçËÉΩÂø´ÈÄüÊî∂ÊïõÔºåÊèêÈ´òÂÖ∂Êï∞Â≠¶‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂ•ñÂä±ÂÖ≥ÈîÆÊé®ÁêÜÁü≠ËØ≠ÔºàRPRÔºâÁöÑÂá∫Áé∞ÔºåÊ®°ÂûãÂú®ÂºÄÊîæÂºè‰ªªÂä°‰∏≠ÁöÑË°®Áé∞‰πüÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜÊîπËøõÊ®°ÂûãÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂü∫Á°ÄËÉΩÂäõÁöÑÈáçË¶ÅÊÄß„ÄÇ
[30.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†Á†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåLLMsÂú®Â•ñÂä±Âô™Èü≥Ôºàreward noiseÔºâÂ≠òÂú®ÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂç≥‰ΩøÂú®Êï∞Â≠¶‰ªªÂä°‰∏≠ÊâãÂä®ÁøªËΩ¨40%ÁöÑÂ•ñÂä±ÂáΩÊï∞ËæìÂá∫ÔºåÊ®°Âûã‰ªçËÉΩÂø´ÈÄüÊî∂ÊïõÔºåÊèêÈ´òÂÖ∂Êï∞Â≠¶‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂ•ñÂä±ÂÖ≥ÈîÆÊé®ÁêÜÁü≠ËØ≠ÔºàRPRÔºâÁöÑÂá∫Áé∞ÔºåÊ®°ÂûãÂú®ÂºÄÊîæÂºè‰ªªÂä°‰∏≠ÁöÑË°®Áé∞‰πüÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜÊîπËøõÊ®°ÂûãÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂü∫Á°ÄËÉΩÂäõÁöÑÈáçË¶ÅÊÄß„ÄÇ
[30.05.2025 09:13] Mistral response. {"id": "166f4bde8d034922be401f8cdf679826", "object": "chat.completion", "created": 1748596382, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u5956\u52b1\u566a\u97f3\uff08reward noise\uff09\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002\u5373\u4f7f\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u624b\u52a8\u7ffb\u8f6c40%\u7684\u5956\u52b1\u51fd\u6570\u8f93\u51fa\uff0c\u6a21\u578b\u4ecd\u80fd\u5feb\u901f\u6536\u655b\uff0c\u63d0\u9ad8\u5176\u6570\u5b66\u4efb\u52a1\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5956\u52b1\u5173\u952e\u63a8\u7406\u77ed\u8bed\uff08RPR\uff09\u7684\u51fa\u73b0\uff0c\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e5f\u5f97\u5230\u4e86\u63d0\u5347\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u6539\u8fdb\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u57fa\u7840\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng y\u00e1nji\u016b le d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) z\u00e0i qi\u00e1ngzh\u00f9 xu\u00e9x\u00ed zh\u014dng de bi\u01ceoxi\u00e0n. Y\u00e1nji\u016b f\u0101xi\u00e0n, LLMs z\u00e0i ji\u01cengl\u00ec z\u00e0oy\u012bn (reward noise) c\u00fanz\u00e0i de q\u00edngku\u00e0ng xi\u00e0 r\u00e9ng n\u00e9ng bi\u01ceoxi\u00e0n ch\u016b qi\u00e1ngd\u00e0 de l\u01d4c\u01d4x\u00ecng. J\u00edsh\u01d0 z\u00e0i sh\u00f9xu\u00e9 r\u00e8nw\u00f9 zh\u014dng sh\u01d2ud\u00f2ng f\u0101nzhu\u01cen 40% de ji\u01cengl\u00ec h\u00e1nsh\u00f9 sh\u016bch\u016b, m\u00f3x\u00edng r\u00e9ng n\u00e9ng ku\u00e0is\u00f9 sh\u014duli\u01cen, t\u00edg\u0101o q\u00ed sh\u00f9xu\u00e9 r\u00e8nw\u00f9 de bi\u01ceoxi\u00e0n. T\u014dnggu\u00f2 ji\u01cengl\u00ec gu\u01cenji\u00e0n tu\u012bl\u01d0 du\u01ceny\u01d4 (RPR) de ch\u016bxi\u00e0n, m\u00f3x\u00edng z\u00e0i k\u0101if\u00e0ngsh\u00ec r\u00e8nw\u00f9 zh\u014dng de bi\u01ceoxi\u00e0n y\u011b d\u00e9d\u00e0o le t\u00edsh\u0113ng. Zh\u00e8xi\u0113 f\u0101xi\u00e0n qi\u00e1ngdi\u00e0o le g\u01ceij\u00ecn m\u00f3x\u00edng z\u00e0i y\u00f9x\u00f9n ji\u0113du\u00e0n de j\u012bch\u01d4 n\u00e9ngl\u00ec de zh\u00f2ngy\u00e0ox\u00ecng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 192, "total_tokens": 738, "completion_tokens": 546}}
[30.05.2025 09:13] Response: ËøôÁØáÊñáÁ´†Á†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåLLMsÂú®Â•ñÂä±Âô™Èü≥Ôºàreward noiseÔºâÂ≠òÂú®ÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂç≥‰ΩøÂú®Êï∞Â≠¶‰ªªÂä°‰∏≠ÊâãÂä®ÁøªËΩ¨40%ÁöÑÂ•ñÂä±ÂáΩÊï∞ËæìÂá∫ÔºåÊ®°Âûã‰ªçËÉΩÂø´ÈÄüÊî∂ÊïõÔºåÊèêÈ´òÂÖ∂Êï∞Â≠¶‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂ•ñÂä±ÂÖ≥ÈîÆÊé®ÁêÜÁü≠ËØ≠ÔºàRPRÔºâÁöÑÂá∫Áé∞ÔºåÊ®°ÂûãÂú®ÂºÄÊîæÂºè‰ªªÂä°‰∏≠ÁöÑË°®Áé∞‰πüÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜÊîπËøõÊ®°ÂûãÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂü∫Á°ÄËÉΩÂäõÁöÑÈáçË¶ÅÊÄß„ÄÇ

Zh√® piƒÅn w√©nzhƒÅng y√°nji≈´ le d√† y«îy√°n m√≥x√≠ng (LLMs) z√†i qi√°ngzh√π xu√©x√≠ zh≈çng de bi«éoxi√†n. Y√°nji≈´ fƒÅxi√†n, LLMs z√†i ji«éngl√¨ z√†oyƒ´n (reward noise) c√∫nz√†i de q√≠ngku√†ng xi√† r√©ng n√©ng bi«éoxi√†n ch≈´ qi√°ngd√† de l«îc«îx√¨ng. J√≠sh«ê z√†i sh√πxu√© r√®nw√π zh≈çng sh«íud√≤ng fƒÅnzhu«én 40% de ji«éngl√¨ h√°nsh√π sh≈´ch≈´, m√≥x√≠ng r√©ng n√©ng ku√†is√π sh≈çuli«én, t√≠gƒÅo q√≠ sh√πxu√© r√®nw√π de bi«éoxi√†n. T≈çnggu√≤ ji«éngl√¨ gu«énji√†n tuƒ´l«ê du«ény«î (RPR) de ch≈´xi√†n, m√≥x√≠ng z√†i kƒÅif√†ngsh√¨ r√®nw√π zh≈çng de bi«éoxi√†n yƒõ d√©d√†o le t√≠shƒìng. Zh√®xiƒì fƒÅxi√†n qi√°ngdi√†o le g«éij√¨n m√≥x√≠ng z√†i y√πx√πn jiƒìdu√†n de jƒ´ch«î n√©ngl√¨ de zh√≤ngy√†ox√¨ng.
[30.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†Á†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåLLMsÂú®Â•ñÂä±Âô™Èü≥Ôºàreward noiseÔºâÂ≠òÂú®ÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂç≥‰ΩøÂú®Êï∞Â≠¶‰ªªÂä°‰∏≠ÊâãÂä®ÁøªËΩ¨40%ÁöÑÂ•ñÂä±ÂáΩÊï∞ËæìÂá∫ÔºåÊ®°Âûã‰ªçËÉΩÂø´ÈÄüÊî∂ÊïõÔºåÊèêÈ´òÂÖ∂Êï∞Â≠¶‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂ•ñÂä±ÂÖ≥ÈîÆÊé®ÁêÜÁü≠ËØ≠ÔºàRPRÔºâÁöÑÂá∫Áé∞ÔºåÊ®°ÂûãÂú®ÂºÄÊîæÂºè‰ªªÂä°‰∏≠ÁöÑË°®Áé∞‰πüÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜÊîπËøõÊ®°ÂûãÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂü∫Á°ÄËÉΩÂäõÁöÑÈáçË¶ÅÊÄß„ÄÇ
[30.05.2025 09:13] Mistral response. {"id": "065238715f044557bfbbb2fe830fd161", "object": "chat.completion", "created": 1748596406, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u5927\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng\", \"trans\": \"large language model\"},\n    {\"word\": \"\u5f3a\u5316\u5b66\u4e60\", \"pinyin\": \"qi\u00e1ng hu\u00e0 xu\u00e9x\u00ed\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u5956\u52b1\u566a\u97f3\", \"pinyin\": \"ji\u01ceng l\u00ec z\u00e0o y\u012bn\", \"trans\": \"reward noise\"},\n    {\"word\": \"\u9c81\u68d2\u6027\", \"pinyin\": \"l\u01d4 b\u0101ng x\u00ecng\", \"trans\": \"robustness\"},\n    {\"word\": \"\u624b\u52a8\u7ffb\u8f6c\", \"pinyin\": \"sh\u01d2u d\u00f2ng f\u0101n zhu\u01cen\", \"trans\": \"manually flip\"},\n    {\"word\": \"\u5956\u52b1\u51fd\u6570\", \"pinyin\": \"ji\u01ceng l\u00ec h\u00e1n sh\u00f9\", \"trans\": \"reward function\"},\n    {\"word\": \"\u8f93\u51fa\", \"pinyin\": \"sh\u016b ch\u016b\", \"trans\": \"output\"},\n    {\"word\": \"\u5feb\u901f\u6536\u655b\", \"pinyin\": \"ku\u00e0i s\u00f9 sh\u014du li\u01cen\", \"trans\": \"rapid convergence\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00ed g\u0101o\", \"trans\": \"improve\"},\n    {\"word\": \"\u5956\u52b1\u5173\u952e\u63a8\u7406\u77ed\u8bed\", \"pinyin\": \"ji\u01ceng l\u00ec gu\u01cen ji\u00e0n tu\u012b l\u01d0 du\u01cen y\u01d4\", \"trans\": \"reward key reasoning phrases\"},\n    {\"word\": \"\u5f00\u653e\u5f0f\u4efb\u52a1\", \"pinyin\": \"k\u0101i f\u00e0ng sh\u00ec r\u00e8n w\u00f9\", \"trans\": \"open-ended tasks\"},\n    {\"word\": \"\u53d1\u73b0\", \"pinyin\": \"f\u0101 xi\u00e0n\", \"trans\": \"findings\"},\n    {\"word\": \"\u5f3a\u8c03\", \"pinyin\": \"qi\u00e1ng di\u00e0o\", \"trans\": \"emphasize\"},\n    {\"word\": \"\u6539\u8fdb\", \"pinyin\": \"g\u01cei j\u00ecn\", \"trans\": \"improve\"},\n    {\"word\": \"\u9884\u8bad\u7ec3\u9636\u6bb5\", \"pinyin\": \"y\u00f9 x\u00f9n li\u00e0n ji\u0113 du\u00e0n\", \"trans\": \"pre-training stage\"},\n    {\"word\": \"\u57fa\u7840\u80fd\u529b\", \"pinyin\": \"j\u012b ch\u01d4 n\u00e9ng l\u00ec\", \"trans\": \"foundational capabilities\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 224, "total_tokens": 836, "completion_tokens": 612}}
[30.05.2025 09:13] Response: [
    {"word": "Â§ßËØ≠Ë®ÄÊ®°Âûã", "pinyin": "d√† y«îy√°n m√≥x√≠ng", "trans": "large language model"},
    {"word": "Âº∫ÂåñÂ≠¶‰π†", "pinyin": "qi√°ng hu√† xu√©x√≠", "trans": "reinforcement learning"},
    {"word": "Ë°®Áé∞", "pinyin": "bi«éo xi√†n", "trans": "performance"},
    {"word": "Â•ñÂä±Âô™Èü≥", "pinyin": "ji«éng l√¨ z√†o yƒ´n", "trans": "reward noise"},
    {"word": "È≤ÅÊ£íÊÄß", "pinyin": "l«î bƒÅng x√¨ng", "trans": "robustness"},
    {"word": "ÊâãÂä®ÁøªËΩ¨", "pinyin": "sh«íu d√≤ng fƒÅn zhu«én", "trans": "manually flip"},
    {"word": "Â•ñÂä±ÂáΩÊï∞", "pinyin": "ji«éng l√¨ h√°n sh√π", "trans": "reward function"},
    {"word": "ËæìÂá∫", "pinyin": "sh≈´ ch≈´", "trans": "output"},
    {"word": "Âø´ÈÄüÊî∂Êïõ", "pinyin": "ku√†i s√π sh≈çu li«én", "trans": "rapid convergence"},
    {"word": "ÊèêÈ´ò", "pinyin": "t√≠ gƒÅo", "trans": "improve"},
    {"word": "Â•ñÂä±ÂÖ≥ÈîÆÊé®ÁêÜÁü≠ËØ≠", "pinyin": "ji«éng l√¨ gu«én ji√†n tuƒ´ l«ê du«én y«î", "trans": "reward key reasoning phrases"},
    {"word": "ÂºÄÊîæÂºè‰ªªÂä°", "pinyin": "kƒÅi f√†ng sh√¨ r√®n w√π", "trans": "open-ended tasks"},
    {"word": "ÂèëÁé∞", "pinyin": "fƒÅ xi√†n", "trans": "findings"},
    {"word": "Âº∫Ë∞É", "pinyin": "qi√°ng di√†o", "trans": "emphasize"},
    {"word": "ÊîπËøõ", "pinyin": "g«éi j√¨n", "trans": "improve"},
    {"word": "È¢ÑËÆ≠ÁªÉÈò∂ÊÆµ", "pinyin": "y√π x√πn li√†n jiƒì du√†n", "trans": "pre-training stage"},
    {"word": "Âü∫Á°ÄËÉΩÂäõ", "pinyin": "jƒ´ ch«î n√©ng l√¨", "trans": "foundational capabilities"}
]
[30.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†Á†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåLLMsÂú®Â•ñÂä±Âô™Èü≥Ôºàreward noiseÔºâÂ≠òÂú®ÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂç≥‰ΩøÂú®Êï∞Â≠¶‰ªªÂä°‰∏≠ÊâãÂä®ÁøªËΩ¨40%ÁöÑÂ•ñÂä±ÂáΩÊï∞ËæìÂá∫ÔºåÊ®°Âûã‰ªçËÉΩÂø´ÈÄüÊî∂ÊïõÔºåÊèêÈ´òÂÖ∂Êï∞Â≠¶‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂ•ñÂä±ÂÖ≥ÈîÆÊé®ÁêÜÁü≠ËØ≠ÔºàRPRÔºâÁöÑÂá∫Áé∞ÔºåÊ®°ÂûãÂú®ÂºÄÊîæÂºè‰ªªÂä°‰∏≠ÁöÑË°®Áé∞‰πüÂæóÂà∞‰∫ÜÊèêÂçá„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞É‰∫ÜÊîπËøõÊ®°ÂûãÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂü∫Á°ÄËÉΩÂäõÁöÑÈáçË¶ÅÊÄß„ÄÇ
[30.05.2025 09:13] Mistral response. {"id": "8d53310a769847319f34e648564b2568", "object": "chat.completion", "created": 1748596428, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article investigates the performance of large language models (LLMs) in reinforcement learning. The study found that LLMs exhibit strong robustness even in the presence of reward noise. Even when manually flipping 40% of the reward function outputs in mathematical tasks, the model can still converge quickly and improve its performance on these tasks. By rewarding the appearance of key reasoning phrases (RPR), the model's performance on open-ended tasks is also enhanced. These findings underscore the importance of improving the model's foundational capabilities during the pre-training phase."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 190, "total_tokens": 310, "completion_tokens": 120}}
[30.05.2025 09:13] Response: This article investigates the performance of large language models (LLMs) in reinforcement learning. The study found that LLMs exhibit strong robustness even in the presence of reward noise. Even when manually flipping 40% of the reward function outputs in mathematical tasks, the model can still converge quickly and improve its performance on these tasks. By rewarding the appearance of key reasoning phrases (RPR), the model's performance on open-ended tasks is also enhanced. These findings underscore the importance of improving the model's foundational capabilities during the pre-training phase.
[30.05.2025 09:13] Renaming data file.
[30.05.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-05-30.json
[30.05.2025 09:13] Saving new data file.
[30.05.2025 09:13] Generating page.
[30.05.2025 09:13] Renaming previous page.
[30.05.2025 09:13] Renaming previous data. index.html to ./d/2025-05-30.html
[30.05.2025 09:13] [Experimental] Generating Chinese page for reading.
[30.05.2025 09:13] Chinese vocab [{'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«îy√°n m√≥x√≠ng', 'trans': 'large language model'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng hu√† xu√©x√≠', 'trans': 'reinforcement learning'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Â•ñÂä±Âô™Èü≥', 'pinyin': 'ji«éng l√¨ z√†o yƒ´n', 'trans': 'reward noise'}, {'word': 'È≤ÅÊ£íÊÄß', 'pinyin': 'l«î bƒÅng x√¨ng', 'trans': 'robustness'}, {'word': 'ÊâãÂä®ÁøªËΩ¨', 'pinyin': 'sh«íu d√≤ng fƒÅn zhu«én', 'trans': 'manually flip'}, {'word': 'Â•ñÂä±ÂáΩÊï∞', 'pinyin': 'ji«éng l√¨ h√°n sh√π', 'trans': 'reward function'}, {'word': 'ËæìÂá∫', 'pinyin': 'sh≈´ ch≈´', 'trans': 'output'}, {'word': 'Âø´ÈÄüÊî∂Êïõ', 'pinyin': 'ku√†i s√π sh≈çu li«én', 'trans': 'rapid convergence'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'Â•ñÂä±ÂÖ≥ÈîÆÊé®ÁêÜÁü≠ËØ≠', 'pinyin': 'ji«éng l√¨ gu«én ji√†n tuƒ´ l«ê du«én y«î', 'trans': 'reward key reasoning phrases'}, {'word': 'ÂºÄÊîæÂºè‰ªªÂä°', 'pinyin': 'kƒÅi f√†ng sh√¨ r√®n w√π', 'trans': 'open-ended tasks'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'findings'}, {'word': 'Âº∫Ë∞É', 'pinyin': 'qi√°ng di√†o', 'trans': 'emphasize'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éi j√¨n', 'trans': 'improve'}, {'word': 'È¢ÑËÆ≠ÁªÉÈò∂ÊÆµ', 'pinyin': 'y√π x√πn li√†n jiƒì du√†n', 'trans': 'pre-training stage'}, {'word': 'Âü∫Á°ÄËÉΩÂäõ', 'pinyin': 'jƒ´ ch«î n√©ng l√¨', 'trans': 'foundational capabilities'}]
[30.05.2025 09:13] Renaming previous Chinese page.
[30.05.2025 09:13] Renaming previous data. zh.html to ./d/2025-05-29_zh_reading_task.html
[30.05.2025 09:13] Writing Chinese reading task.
[30.05.2025 09:13] Writing result.
[30.05.2025 09:13] Renaming log file.
[30.05.2025 09:13] Renaming previous data. log.txt to ./logs/2025-05-30_last_log.txt
