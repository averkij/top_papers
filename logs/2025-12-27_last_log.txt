[27.12.2025 02:05] Read previous papers.
[27.12.2025 02:05] Generating top page (month).
[27.12.2025 02:05] Writing top page (month).
[27.12.2025 06:40] Read previous papers.
[27.12.2025 06:40] Get feed.
[27.12.2025 06:40] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21218
[27.12.2025 06:40] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20605
[27.12.2025 06:40] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15716
[27.12.2025 06:40] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19995
[27.12.2025 06:40] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19949
[27.12.2025 06:40] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19680
[27.12.2025 06:40] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13043
[27.12.2025 06:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.12.2025 06:40] No deleted papers detected.
[27.12.2025 06:40] Downloading and parsing papers (pdf, html). Total: 7.
[27.12.2025 06:40] Downloading and parsing paper https://huggingface.co/papers/2512.21218.
[27.12.2025 06:40] Extra JSON file exists (./assets/json/2512.21218.json), skip PDF parsing.
[27.12.2025 06:40] Paper image links file exists (./assets/img_data/2512.21218.json), skip HTML parsing.
[27.12.2025 06:40] Success.
[27.12.2025 06:40] Downloading and parsing paper https://huggingface.co/papers/2512.20605.
[27.12.2025 06:40] Extra JSON file exists (./assets/json/2512.20605.json), skip PDF parsing.
[27.12.2025 06:40] Paper image links file exists (./assets/img_data/2512.20605.json), skip HTML parsing.
[27.12.2025 06:40] Success.
[27.12.2025 06:40] Downloading and parsing paper https://huggingface.co/papers/2512.15716.
[27.12.2025 06:40] Extra JSON file exists (./assets/json/2512.15716.json), skip PDF parsing.
[27.12.2025 06:40] Paper image links file exists (./assets/img_data/2512.15716.json), skip HTML parsing.
[27.12.2025 06:40] Success.
[27.12.2025 06:40] Downloading and parsing paper https://huggingface.co/papers/2512.19995.
[27.12.2025 06:40] Extra JSON file exists (./assets/json/2512.19995.json), skip PDF parsing.
[27.12.2025 06:40] Paper image links file exists (./assets/img_data/2512.19995.json), skip HTML parsing.
[27.12.2025 06:40] Success.
[27.12.2025 06:40] Downloading and parsing paper https://huggingface.co/papers/2512.19949.
[27.12.2025 06:40] Extra JSON file exists (./assets/json/2512.19949.json), skip PDF parsing.
[27.12.2025 06:40] Paper image links file exists (./assets/img_data/2512.19949.json), skip HTML parsing.
[27.12.2025 06:40] Success.
[27.12.2025 06:40] Downloading and parsing paper https://huggingface.co/papers/2512.19680.
[27.12.2025 06:40] Extra JSON file exists (./assets/json/2512.19680.json), skip PDF parsing.
[27.12.2025 06:40] Paper image links file exists (./assets/img_data/2512.19680.json), skip HTML parsing.
[27.12.2025 06:40] Success.
[27.12.2025 06:40] Downloading and parsing paper https://huggingface.co/papers/2512.13043.
[27.12.2025 06:40] Extra JSON file exists (./assets/json/2512.13043.json), skip PDF parsing.
[27.12.2025 06:40] Paper image links file exists (./assets/img_data/2512.13043.json), skip HTML parsing.
[27.12.2025 06:40] Success.
[27.12.2025 06:40] Enriching papers with extra data.
[27.12.2025 06:40] ********************************************************************************
[27.12.2025 06:40] Abstract 0. While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to addres...
[27.12.2025 06:40] ********************************************************************************
[27.12.2025 06:40] Abstract 1. Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token c...
[27.12.2025 06:40] ********************************************************************************
[27.12.2025 06:40] Abstract 2. Spatia, a spatial memory-aware video generation framework, maintains long-term spatial and temporal consistency by preserving and updating a 3D scene point cloud, enabling realistic video generation and interactive editing.  					AI-generated summary 				 Existing video generation models struggle to...
[27.12.2025 06:40] ********************************************************************************
[27.12.2025 06:40] Abstract 3. Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Re...
[27.12.2025 06:40] ********************************************************************************
[27.12.2025 06:40] Abstract 4. Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic fram...
[27.12.2025 06:40] ********************************************************************************
[27.12.2025 06:40] Abstract 5. VA-$\pi$ optimizes autoregressive visual generators using a pixel-space objective to improve image quality and performance without retraining tokenizers or using external rewards.  					AI-generated summary 				 Autoregressive (AR) visual generation relies on tokenizers to map images to and from dis...
[27.12.2025 06:40] ********************************************************************************
[27.12.2025 06:40] Abstract 6. Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR)...
[27.12.2025 06:40] Read previous papers.
[27.12.2025 06:40] Generating reviews via LLM API.
[27.12.2025 06:40] Using data from previous issue: {"categories": [], "emoji": "üëÅÔ∏è", "ru": {"title": "–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Ç–µ–∫—Å—Ç–æ—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä
[27.12.2025 06:40] Using data from previous issue: {"categories": ["#architecture", "#training", "#rl", "#reasoning", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è: RL –≤–Ω—É—Ç—Ä–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∫ –±–æ–ª—å—à–∏–º –∞–≤—Ç–æ
[27.12.2025 06:40] Using data from previous issue: {"categories": ["#video", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "Spatia ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è —è–≤–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤
[27.12.2025 06:40] Using data from previous issue: {"categories": ["#training", "#interpretability", "#reasoning", "#math", "#architecture"], "emoji": "üß†", "ru": {"title": "–ê–Ω–∞—Ç–æ–º–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —ç—Ç–∞–ø–æ–≤ –º—ã—à–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ThinkARM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[27.12.2025 06:40] Using data from previous issue: {"categories": ["#benchmark", "#video", "#3d"], "emoji": "üé¨", "ru": {"title": "–°–∫—Ä—ã—Ç–æ–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ –≤–∏–¥–µ–æ-—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç, —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –ª–∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—è—Ö (VidFMs), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–∞—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö. –ê
[27.12.2025 06:40] Using data from previous issue: {"categories": ["#cv", "#training", "#rl"], "emoji": "üé®", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —á–µ—Ä–µ–∑ –ø–∏–∫—Å–µ–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è VA-œÄ ‚Äî –ª—ë–≥–∫–∏–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ—Ä–µgress–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –≤ –ø–∏–∫—Å–µ–ª—å
[27.12.2025 06:40] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#training", "#agents"], "emoji": "üöÄ", "ru": {"title": "–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π —É—á–∏—Ç–µ–ª—å –∏–∑ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è GTR-Turbo ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö
[27.12.2025 06:40] Renaming data file.
[27.12.2025 06:40] Renaming previous data. hf_papers.json to ./d/2025-12-26.json
[27.12.2025 06:40] Saving new data file.
[27.12.2025 06:40] Generating page.
[27.12.2025 06:40] Renaming previous page.
[27.12.2025 06:40] Renaming previous data. index.html to ./d/2025-12-26.html
[27.12.2025 06:40] Writing result.
[27.12.2025 06:40] Renaming log file.
[27.12.2025 06:40] Renaming previous data. log.txt to ./logs/2025-12-27_last_log.txt
