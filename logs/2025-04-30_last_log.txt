[30.04.2025 02:27] Read previous papers.
[30.04.2025 02:27] Generating top page (month).
[30.04.2025 02:27] Writing top page (month).
[30.04.2025 03:33] Read previous papers.
[30.04.2025 03:33] Get feed.
[30.04.2025 03:33] Extract page data from URL. URL: https://huggingface.co/papers/2504.20595
[30.04.2025 03:33] Extract page data from URL. URL: https://huggingface.co/papers/2504.20734
[30.04.2025 03:33] Get page data from previous paper. URL: https://huggingface.co/papers/2504.20571
[30.04.2025 03:33] Get page data from previous paper. URL: https://huggingface.co/papers/2504.20998
[30.04.2025 03:33] Extract page data from URL. URL: https://huggingface.co/papers/2504.20995
[30.04.2025 03:33] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.04.2025 03:33] No deleted papers detected.
[30.04.2025 03:33] Downloading and parsing papers (pdf, html). Total: 5.
[30.04.2025 03:33] Downloading and parsing paper https://huggingface.co/papers/2504.20595.
[30.04.2025 03:33] Downloading paper 2504.20595 from http://arxiv.org/pdf/2504.20595v1...
[30.04.2025 03:33] Extracting affiliations from text.
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ReasonIR: Training Retrievers for Reasoning Tasks ,, Rulin Shao,,, Rui Qiao,,, Varsha Kishore,, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus Bryan Kian Hsiang Low,, Sewon Min, Wen-tau Yih, Pang Wei Koh,, Luke Zettlemoyer, FAIR at Meta, University of Washington, National University of Singapore, Singapore-MIT Alliance for Research and Technology, Allen Institute for Artificial Intelligence, Stanford University, Joint first author Massachusetts Institute of Technology, University of California, Berkeley 5 2 0 2 9 2 ] A . [ 1 5 9 5 0 2 . 4 0 5 2 : r We present REASONIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop synthetic data generation pipeline that, for each document, our pipeline creates challenging and relevant query, along with plausibly related but ultimately unhelpful hard negative. By training on mixture of our synthetic data and existing public data, REASONIR-8B achieves new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, REASONIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, REASONIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model. Date: April 30, 2025 Correspondence: rulin@meta.com, rui.qiao@smart.mit.edu Code & Data: https://github.com/facebookresearch/Rea"
[30.04.2025 03:33] Response: ```python
[
    "FAIR at Meta",
    "University of Washington",
    "National University of Singapore",
    "Singapore-MIT Alliance for Research and Technology",
    "Allen Institute for Artificial Intelligence",
    "Stanford University",
    "Massachusetts Institute of Technology",
    "University of California, Berkeley"
]
```
[30.04.2025 03:33] Deleting PDF ./assets/pdf/2504.20595.pdf.
[30.04.2025 03:33] Success.
[30.04.2025 03:33] Downloading and parsing paper https://huggingface.co/papers/2504.20734.
[30.04.2025 03:33] Downloading paper 2504.20734 from http://arxiv.org/pdf/2504.20734v1...
[30.04.2025 03:33] Extracting affiliations from text.
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 4 3 7 0 2 . 4 0 5 2 : r UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities Woongyeong Yeo1 Kangsan Kim1 Soyeong Jeong1 Jinheon Baek1 Sung Ju Hwang1,2 KAIST1 DeepAuto.ai2 {wgcyeo, kksan07, starsuzi, jinheon.baek, sungju.hwang}@kaist.ac.kr "
[30.04.2025 03:33] Response: ```python
["KAIST", "DeepAuto.ai"]
```
[30.04.2025 03:33] Deleting PDF ./assets/pdf/2504.20734.pdf.
[30.04.2025 03:33] Success.
[30.04.2025 03:33] Downloading and parsing paper https://huggingface.co/papers/2504.20571.
[30.04.2025 03:33] Extra JSON file exists (./assets/json/2504.20571.json), skip PDF parsing.
[30.04.2025 03:33] Paper image links file exists (./assets/img_data/2504.20571.json), skip HTML parsing.
[30.04.2025 03:33] Success.
[30.04.2025 03:33] Downloading and parsing paper https://huggingface.co/papers/2504.20998.
[30.04.2025 03:33] Extra JSON file exists (./assets/json/2504.20998.json), skip PDF parsing.
[30.04.2025 03:33] Paper image links file exists (./assets/img_data/2504.20998.json), skip HTML parsing.
[30.04.2025 03:33] Success.
[30.04.2025 03:33] Downloading and parsing paper https://huggingface.co/papers/2504.20995.
[30.04.2025 03:33] Downloading paper 2504.20995 from http://arxiv.org/pdf/2504.20995v1...
[30.04.2025 03:33] Extracting affiliations from text.
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 5 9 9 0 2 . 4 0 5 2 : r TesserAct: Learning 4D Embodied World Models Haoyu Zhen1 Qiao Sun1 Hongxin Zhang1 Junyan Li1 Siyuan Zhou2 Yilun Du3 Chuang Gan1 1UMass Amherst 2HKUST 3Harvard University https://TesserActWorld.github.io Figure 1. We propose TesserAct, the 4D Embodied World Model, which takes an input image and text instruction to generate RGB, depth, and normal videos, reconstructing 4D scene and predicting actions. Our model not only achieves strong performance on in-domain data (right) but also generalizes effectively to unseen scenes, novel objects (top left), and cross-domain scenarios (bottom left). "
[30.04.2025 03:33] Response: ```python
["UMass Amherst", "HKUST", "Harvard University"]
```
[30.04.2025 03:33] Deleting PDF ./assets/pdf/2504.20995.pdf.
[30.04.2025 03:33] Success.
[30.04.2025 03:33] Enriching papers with extra data.
[30.04.2025 03:33] ********************************************************************************
[30.04.2025 03:33] Abstract 0. We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a ...
[30.04.2025 03:33] ********************************************************************************
[30.04.2025 03:33] Abstract 1. Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other moda...
[30.04.2025 03:33] ********************************************************************************
[30.04.2025 03:33] Abstract 2. We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model perfo...
[30.04.2025 03:33] ********************************************************************************
[30.04.2025 03:33] Abstract 3. Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how ...
[30.04.2025 03:33] ********************************************************************************
[30.04.2025 03:33] Abstract 4. This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (R...
[30.04.2025 03:33] Read previous papers.
[30.04.2025 03:33] Generating reviews via LLM API.
[30.04.2025 03:33] Querying the API.
[30.04.2025 03:33] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.
[30.04.2025 03:33] Response: {
  "desc": "ReasonIR-8B - это первая модель для поиска информации, специально обученная для задач рассуждения. Авторы разработали конвейер генерации синтетических данных, создающий сложные и релевантные запросы для каждого документа. Модель, обученная на смеси синтетических и существующих публичных данных, достигает нового уровня производительности на бенчмарке BRIGHT. При применении к задачам RAG, ReasonIR-8B значительно улучшает результаты на MMLU и GPQA по сравнению с baseline без доступа к внешней информации.",
  "emoji": "🧠",
  "title": "ReasonIR-8B: Прорыв в информационном поиске для задач рассуждения"
}
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model."

[30.04.2025 03:33] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'RAG', 'TRAINING']
```
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model."

[30.04.2025 03:33] Response: ```python
['REASONING', 'SYNTHETIC', 'OPEN_SOURCE']
```
[30.04.2025 03:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReasonIR-8B is a novel information retrieval model designed specifically for reasoning tasks, addressing the limitations of existing retrievers that focus on simple factual queries. It utilizes a synthetic data generation pipeline to create complex queries and challenging hard negatives, enhancing the training process. By combining this synthetic data with existing datasets, ReasonIR-8B achieves impressive performance metrics on the BRIGHT benchmark, surpassing previous models. Additionally, it demonstrates improved efficiency during test-time by leveraging longer, more informative queries, making it a valuable tool for reasoning-intensive applications.","title":"Revolutionizing Reasoning with ReasonIR-8B"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ReasonIR-8B is a novel information retrieval model designed specifically for reasoning tasks, addressing the limitations of existing retrievers that focus on simple factual queries. It utilizes a synthetic data generation pipeline to create complex queries and challenging hard negatives, enhancing the training process. By combining this synthetic data with existing datasets, ReasonIR-8B achieves impressive performance metrics on the BRIGHT benchmark, surpassing previous models. Additionally, it demonstrates improved efficiency during test-time by leveraging longer, more informative queries, making it a valuable tool for reasoning-intensive applications.', title='Revolutionizing Reasoning with ReasonIR-8B'))
[30.04.2025 03:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们提出了ReasonIR-8B，这是第一个专门为一般推理任务训练的检索器。现有的检索器在推理任务上的表现有限，部分原因是现有的训练数据集主要集中在与文档直接相关的短小事实查询上。我们开发了一种合成数据生成管道，为每个文档创建具有挑战性和相关性的查询，以及一个看似相关但实际上无用的困难负样本。通过在合成数据和现有公共数据的混合上进行训练，ReasonIR-8B在BRIGHT基准上达到了29.9的nDCG@10（不使用重排序器）和36.9的nDCG@10（使用重排序器），并在RAG任务中显著提高了MMLU和GPQA的性能。","title":"推理任务的专属检索器：ReasonIR-8B"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们提出了ReasonIR-8B，这是第一个专门为一般推理任务训练的检索器。现有的检索器在推理任务上的表现有限，部分原因是现有的训练数据集主要集中在与文档直接相关的短小事实查询上。我们开发了一种合成数据生成管道，为每个文档创建具有挑战性和相关性的查询，以及一个看似相关但实际上无用的困难负样本。通过在合成数据和现有公共数据的混合上进行训练，ReasonIR-8B在BRIGHT基准上达到了29.9的nDCG@10（不使用重排序器）和36.9的nDCG@10（使用重排序器），并在RAG任务中显著提高了MMLU和GPQA的性能。', title='推理任务的专属检索器：ReasonIR-8B'))
[30.04.2025 03:33] Querying the API.
[30.04.2025 03:33] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.
[30.04.2025 03:33] Response: {
  "desc": "UniversalRAG - это новая система извлечения и интеграции знаний из разнородных источников с различными модальностями и уровнями детализации. В отличие от существующих подходов RAG, ограниченных одномодальными корпусами, UniversalRAG использует механизм маршрутизации с учетом модальности для динамического выбора наиболее подходящего корпуса. Система также организует каждую модальность на несколько уровней детализации, позволяя точно настраивать извлечение информации в соответствии со сложностью и объемом запроса. Эксперименты на 8 тестовых наборах данных показали превосходство UniversalRAG над одномодальными и унифицированными базовыми моделями.",
  "emoji": "🌐",
  "title": "UniversalRAG: Универсальное извлечение знаний из разнородных источников"
}
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines."

[30.04.2025 03:33] Response: ```python
['RAG', 'MULTIMODAL', 'BENCHMARK']
```
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines."

[30.04.2025 03:33] Response: ```python
["REASONING", "INTERPRETABILITY"]
```
[30.04.2025 03:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces UniversalRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge from various sources and modalities. Unlike traditional RAG methods that rely on a single type of corpus, UniversalRAG addresses the diverse nature of real-world queries by employing a modality-aware routing mechanism. This mechanism allows the model to dynamically select the most relevant corpus based on the query\'s modality and granularity. The effectiveness of UniversalRAG is demonstrated through extensive testing on multiple benchmarks, outperforming existing models that focus on either single modalities or unified representations.","title":"UniversalRAG: Bridging Knowledge Across Modalities for Enhanced Retrieval"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces UniversalRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge from various sources and modalities. Unlike traditional RAG methods that rely on a single type of corpus, UniversalRAG addresses the diverse nature of real-world queries by employing a modality-aware routing mechanism. This mechanism allows the model to dynamically select the most relevant corpus based on the query's modality and granularity. The effectiveness of UniversalRAG is demonstrated through extensive testing on multiple benchmarks, outperforming existing models that focus on either single modalities or unified representations.", title='UniversalRAG: Bridging Knowledge Across Modalities for Enhanced Retrieval'))
[30.04.2025 03:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种新的检索增强生成框架，称为UniversalRAG，旨在从多种异构知识源中检索和整合信息，以提高模型的事实准确性。现有的检索增强生成方法通常仅限于文本数据，而UniversalRAG能够处理多种模态的信息，如图像和视频。该框架采用了一种模态感知的路由机制，能够动态识别最合适的模态特定语料库，从而进行针对性的检索。此外，UniversalRAG还将每种模态组织为多个粒度级别，以便根据查询的复杂性和范围进行精细化检索。","title":"多模态知识整合的全新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种新的检索增强生成框架，称为UniversalRAG，旨在从多种异构知识源中检索和整合信息，以提高模型的事实准确性。现有的检索增强生成方法通常仅限于文本数据，而UniversalRAG能够处理多种模态的信息，如图像和视频。该框架采用了一种模态感知的路由机制，能够动态识别最合适的模态特定语料库，从而进行针对性的检索。此外，UniversalRAG还将每种模态组织为多个粒度级别，以便根据查询的复杂性和范围进行精细化检索。', title='多模态知识整合的全新框架'))
[30.04.2025 03:33] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#math", "#open_source", "#reasoning"], "emoji": "🧠", "ru": {"title": "Один пример - огромный скачок в математических способностях ИИ", "desc": "Исследование показывает эффективность обучения с подкреплением с верифицируемым вознаграждением (RLVR)
[30.04.2025 03:33] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#multimodal"], "emoji": "🦎", "ru": {"title": "Персонализация мультимодальных ИИ-моделей на основе нескольких примеров", "desc": "Статья представляет Yo'Chameleon - первую попытку персонализации больших мультимодальных моделей. Система использует 
[30.04.2025 03:33] Querying the API.
[30.04.2025 03:33] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.
[30.04.2025 03:33] Response: {
  "desc": "В статье представлен эффективный подход к обучению новых 4D-моделей воплощенного мира, которые предсказывают динамическую эволюцию 3D-сцен во времени в ответ на действия воплощенного агента. Авторы предлагают обучать 4D-модель мира на видео RGB-DN (RGB, глубина и нормали), что превосходит традиционные 2D-модели. Метод включает дополнение существующих наборов данных видео роботизированных манипуляций информацией о глубине и нормалях, а также тонкую настройку модели генерации видео. Предложенный алгоритм позволяет напрямую преобразовывать сгенерированные RGB-DN видео в высококачественную 4D-сцену мира.",
  "emoji": "🤖",
  "title": "4D-модели мира: новый уровень предсказания динамики воплощенных агентов"
}
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models."

[30.04.2025 03:33] Response: ```python
['3D', 'VIDEO', 'ROBOTICS']
```
[30.04.2025 03:33] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models."

[30.04.2025 03:33] Response: ```python
[]
```
[30.04.2025 03:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a novel method for creating 4D world models that can predict how 3D scenes change over time based on the actions of an embodied agent. By using RGB-DN videos, which include color, depth, and normal information, the approach improves upon traditional 2D models by capturing detailed spatial and temporal dynamics. The authors enhance existing robotic manipulation datasets with depth and normal data, then fine-tune a video generation model to produce accurate RGB-DN predictions for each frame. This results in high-quality 4D scene representations that maintain coherence over time and space, enabling better policy learning and novel view synthesis in dynamic environments.","title":"Revolutionizing 4D Scene Prediction for Embodied Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a novel method for creating 4D world models that can predict how 3D scenes change over time based on the actions of an embodied agent. By using RGB-DN videos, which include color, depth, and normal information, the approach improves upon traditional 2D models by capturing detailed spatial and temporal dynamics. The authors enhance existing robotic manipulation datasets with depth and normal data, then fine-tune a video generation model to produce accurate RGB-DN predictions for each frame. This results in high-quality 4D scene representations that maintain coherence over time and space, enabling better policy learning and novel view synthesis in dynamic environments.', title='Revolutionizing 4D Scene Prediction for Embodied Agents'))
[30.04.2025 03:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种有效的方法，用于学习新颖的四维具身世界模型，该模型能够预测三维场景在具身智能体动作下的动态演变，确保时空一致性。我们通过训练RGB-DN（RGB、深度和法线）视频来学习四维世界模型，这种方法超越了传统的二维模型，能够将详细的形状、配置和时间变化纳入预测中。具体而言，我们首先利用现成模型扩展现有的机器人操作视频数据集，加入深度和法线信息。接着，我们在这个标注数据集上微调视频生成模型，联合预测每一帧的RGB-DN，最终将生成的RGB、深度和法线视频直接转换为高质量的四维场景。","title":"学习四维世界模型，提升具身智能的预测能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种有效的方法，用于学习新颖的四维具身世界模型，该模型能够预测三维场景在具身智能体动作下的动态演变，确保时空一致性。我们通过训练RGB-DN（RGB、深度和法线）视频来学习四维世界模型，这种方法超越了传统的二维模型，能够将详细的形状、配置和时间变化纳入预测中。具体而言，我们首先利用现成模型扩展现有的机器人操作视频数据集，加入深度和法线信息。接着，我们在这个标注数据集上微调视频生成模型，联合预测每一帧的RGB-DN，最终将生成的RGB、深度和法线视频直接转换为高质量的四维场景。', title='学习四维世界模型，提升具身智能的预测能力'))
[30.04.2025 03:34] Loading Chinese text from previous data.
[30.04.2025 03:34] Renaming data file.
[30.04.2025 03:34] Renaming previous data. hf_papers.json to ./d/2025-04-30.json
[30.04.2025 03:34] Saving new data file.
[30.04.2025 03:34] Generating page.
[30.04.2025 03:34] Renaming previous page.
[30.04.2025 03:34] Renaming previous data. index.html to ./d/2025-04-30.html
[30.04.2025 03:34] [Experimental] Generating Chinese page for reading.
[30.04.2025 03:34] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '局限性', 'pinyin': 'jú xiàn xìng', 'trans': 'limitations'}, {'word': '特别', 'pinyin': 'tè bié', 'trans': 'especially'}, {'word': '非拉丁字母', 'pinyin': 'fēi lā dīng zì mǔ', 'trans': 'non-Latin characters'}, {'word': '精确', 'pinyin': 'jīng què', 'trans': 'precise'}, {'word': '灵活', 'pinyin': 'líng huó', 'trans': 'flexible'}, {'word': '排版', 'pinyin': 'pái bǎn', 'trans': 'typesetting'}, {'word': '元素', 'pinyin': 'yuán sù', 'trans': 'elements'}, {'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': 'RepText', 'pinyin': 'RepText', 'trans': 'RepText'}, {'word': '准确', 'pinyin': 'zhǔn què', 'trans': 'accurate'}, {'word': '渲染', 'pinyin': 'xuàn rán', 'trans': 'render'}, {'word': '多语言', 'pinyin': 'duō yǔ yán', 'trans': 'multilingual'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '无需', 'pinyin': 'wú xū', 'trans': 'without needing'}, {'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understand'}, {'word': '采用', 'pinyin': 'cǎi yòng', 'trans': 'adopt'}, {'word': 'ControlNet', 'pinyin': 'ControlNet', 'trans': 'ControlNet'}, {'word': '设置', 'pinyin': 'shè zhì', 'trans': 'setting'}, {'word': '额外', 'pinyin': 'é wài', 'trans': 'additional'}, {'word': '整合', 'pinyin': 'zhěng hé', 'trans': 'integrate'}, {'word': '语言无关', 'pinyin': 'yǔ yán wú guān', 'trans': 'language-agnostic'}, {'word': '字形', 'pinyin': 'zì xíng', 'trans': 'glyph'}, {'word': '位置', 'pinyin': 'wèi zhì', 'trans': 'position'}, {'word': '使用户', 'pinyin': 'shǐ yòng hù', 'trans': 'enable users'}, {'word': '根据', 'pinyin': 'gēn jù', 'trans': 'according to'}, {'word': '需要', 'pinyin': 'xū yào', 'trans': 'need'}, {'word': '自定义', 'pinyin': 'zì dìng yì', 'trans': 'customize'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '字体', 'pinyin': 'zì tǐ', 'trans': 'font'}, {'word': '通过', 'pinyin': 'tōng guò', 'trans': 'through'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '文本感知', 'pinyin': 'wén běn gǎn zhī', 'trans': 'text-aware'}, {'word': '损失', 'pinyin': 'sǔn shī', 'trans': 'loss'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'}, {'word': '噪声', 'pinyin': 'zào shēng', 'trans': 'noise'}, {'word': '潜在', 'pinyin': 'qián zài', 'trans': 'latent'}, {'word': '初始化', 'pinyin': 'chū shǐ huà', 'trans': 'initialization'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '掩码', 'pinyin': 'yǎn mǎ', 'trans': 'mask'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '稳定性', 'pinyin': 'wěn dìng xìng', 'trans': 'stability'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '表明', 'pinyin': 'biǎo míng', 'trans': 'indicate'}, {'word': '开源', 'pinyin': 'kāi yuán', 'trans': 'open-source'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '封闭源', 'pinyin': 'fēng bì yuán', 'trans': 'closed-source'}, {'word': '相媲美', 'pinyin': 'xiāng pì měi', 'trans': 'compare favorably'}, {'word': '文章', 'pinyin': 'wén zhāng', 'trans': 'article'}, {'word': '最后', 'pinyin': 'zuì hòu', 'trans': 'finally'}, {'word': '还', 'pinyin': 'hái', 'trans': 'still'}]
[30.04.2025 03:34] Renaming previous Chinese page.
[30.04.2025 03:34] Renaming previous data. zh.html to ./d/2025-04-29_zh_reading_task.html
[30.04.2025 03:34] Writing Chinese reading task.
[30.04.2025 03:34] Writing result.
[30.04.2025 03:34] Renaming log file.
[30.04.2025 03:34] Renaming previous data. log.txt to ./logs/2025-04-30_last_log.txt
