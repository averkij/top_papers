[28.04.2025 19:09] Read previous papers.
[28.04.2025 19:09] Generating top page (month).
[28.04.2025 19:09] Writing top page (month).
[28.04.2025 20:12] Read previous papers.
[28.04.2025 20:12] Get feed.
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15376
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16656
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.18415
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17821
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16427
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17768
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17816
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.18425
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15716
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.12080
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17025
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.18225
[28.04.2025 20:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.04.2025 20:12] No deleted papers detected.
[28.04.2025 20:12] Downloading and parsing papers (pdf, html). Total: 12.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.15376.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.15376.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.15376.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16656.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16656.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16656.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.18415.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.18415.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.18415.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17821.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17821.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17821.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16427.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16427.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16427.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17768.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17768.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17768.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17816.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17816.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17816.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.18425.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.18425.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.18425.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.15716.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.15716.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.15716.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.12080.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.12080.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.12080.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17025.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17025.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17025.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.18225.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.18225.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.18225.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Enriching papers with extra data.
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 0. We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of cam...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 1. We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standi...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 2. Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward net...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 3. Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In ...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 4. Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 5. Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention method...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 6. We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, whi...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 7. We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 8. Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to addr...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 9. Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and i...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 10. The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not o...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 11. We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support...
[28.04.2025 20:12] Read previous papers.
[28.04.2025 20:12] Generating reviews via LLM API.
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#cv", "#multimodal"], "emoji": "🎥", "ru": {"title": "Новый взгляд на понимание движения камеры в видео", "desc": "CameraBench - это новый крупномасштабный датасет и бенчмарк для оценки и улучшения понимания движения камеры в видео. Датасет содержит около 30
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#training", "#agents", "#optimization", "#reasoning", "#multimodal", "#open_source", "#rl"], "emoji": "🧠", "ru": {"title": "Skywork R1V2: Прорыв в мультимодальных рассуждениях с помощью гибридного обучения с подкреплением", "desc": "Skywork R1V2 - эт
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#inference", "#training"], "emoji": "🧠", "ru": {"title": "Эффективное сжатие больших языковых моделей без потери качества", "desc": "BitNet v2 - это новая технология для эффективного развертывания 1-битных больших языковых моделей (LLM). Она решает 
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#benchmark", "#open_source", "#reasoning", "#video", "#dataset", "#science"], "emoji": "🌏", "ru": {"title": "Преодоление культурных и языковых барьеров в понимании видео искусственным интеллектом", "desc": "VideoVista-CulturalLingo - это новый бенчм
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#open_source", "#multimodal"], "emoji": "🗣️", "ru": {"title": "MMLA: новый бенчмарк для оценки понимания мультимодальной семантики языковыми моделями", "desc": "Статья представляет MMLA - новый набор данных для оценки способности мультимодальных языковых мо
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#long_context", "#architecture", "#training", "#optimization"], "emoji": "🕸️", "ru": {"title": "Разреженное внимание: ключ к обработке длинных последовательностей в трансформерах", "desc": "Это исследование фокусируется на применении разреженного внимания в трансформерных моделях дл
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#video", "#dataset", "#training"], "emoji": "🎬", "ru": {"title": "Эффективная персонализация видео без дополнительной настройки", "desc": "Исследователи предлагают новый подход к созданию персонализированных видео с использованием машинного обучения. Их метод разделяет обучение спец
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#inference", "#training", "#benchmark", "#dataset", "#open_source", "#audio", "#data"], "emoji": "🎵", "ru": {"title": "Kimi-Audio: универсальная модель для работы со звуком", "desc": "Kimi-Audio - это открытая аудио-модель, которая превосходит аналоги в понимании, генерации и обрабо
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#training", "#dataset", "#reasoning"], "emoji": "💹", "ru": {"title": "Усиление финансовых рассуждений ИИ через структурированное обучение", "desc": "DianJin-R1 - это фреймворк для улучшения рассуждений больших языковых моделей в финансовой сфере. Он использует о
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#dataset", "#transfer_learning", "#benchmark", "#cv", "#games"], "emoji": "🎭", "ru": {"title": "DC-SAM: Продвинутая сегментация в контексте для изображений и видео", "desc": "Эта статья представляет метод Dual Consistency SAM (DC-SAM) для адаптации моделей SAM и SAM2 к задаче сегмен
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#small_models", "#multilingual", "#low_resource", "#transfer_learning", "#training"], "emoji": "🇮🇹", "ru": {"title": "Оптимизация англоязычных LLM для итальянского языка", "desc": "Эта статья посвящена оптимизации больших языковых моделей (LLM) для итальянского языка. Авторы сравнив
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#reasoning", "#rag", "#benchmark", "#synthetic", "#small_models"], "emoji": "🧠", "ru": {"title": "Малые модели - большие возможности для RAG", "desc": "Представлены новые малые модели для рассуждений в задачах RAG, поиска и суммаризации источников - Plei
[28.04.2025 20:12] Loading Chinese text from previous data.
[28.04.2025 20:12] Renaming data file.
[28.04.2025 20:12] Renaming previous data. hf_papers.json to ./d/2025-04-28.json
[28.04.2025 20:12] Saving new data file.
[28.04.2025 20:12] Generating page.
[28.04.2025 20:12] Renaming previous page.
[28.04.2025 20:12] Renaming previous data. index.html to ./d/2025-04-28.html
[28.04.2025 20:12] [Experimental] Generating Chinese page for reading.
[28.04.2025 20:12] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'}, {'word': '摄像机', 'pinyin': 'shè xiàng jī', 'trans': 'camera'}, {'word': '运动', 'pinyin': 'yùn dòng', 'trans': 'motion'}, {'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understand'}, {'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diverse'}, {'word': '网络视频', 'pinyin': 'wǎng luò shì pín', 'trans': 'online video'}, {'word': '专家', 'pinyin': 'zhuān jiā', 'trans': 'expert'}, {'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'}, {'word': '多阶段', 'pinyin': 'duō jiē duàn', 'trans': 'multi-stage'}, {'word': '质量控制', 'pinyin': 'zhì liàng kòng zhì', 'trans': 'quality control'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '标注', 'pinyin': 'biāo zhù', 'trans': 'annotate'}, {'word': '摄影师', 'pinyin': 'shè yǐng shī', 'trans': 'photographer'}, {'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'}, {'word': '基元', 'pinyin': 'jī yuán', 'trans': 'primitive'}, {'word': '分类法', 'pinyin': 'fēn lèi fǎ', 'trans': 'classification method'}, {'word': '例如', 'pinyin': 'lì rú', 'trans': 'for example'}, {'word': '某些', 'pinyin': 'mǒu xiē', 'trans': 'some'}, {'word': '跟随', 'pinyin': 'gēn suí', 'trans': 'follow'}, {'word': '需要', 'pinyin': 'xū yào', 'trans': 'need'}, {'word': '场景', 'pinyin': 'chǎng jǐng', 'trans': 'scene'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '主体', 'pinyin': 'zhǔ tǐ', 'trans': 'subject'}, {'word': '移动', 'pinyin': 'yí dòng', 'trans': 'move'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'study'}, {'word': '量化', 'pinyin': 'liàng huà', 'trans': 'quantify'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'field'}, {'word': '专业知识', 'pinyin': 'zhuān yè zhī shi', 'trans': 'professional knowledge'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': '教程', 'pinyin': 'jiào chéng', 'trans': 'tutorial'}, {'word': '培训', 'pinyin': 'péi xùn', 'trans': 'training'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '结构从运动', 'pinyin': 'jié gòu cóng yùn dòng', 'trans': 'Structure from Motion (SfM)'}, {'word': '视频语言模型', 'pinyin': 'shì pín yǔ yán mó xíng', 'trans': 'Video Language Model (VLM)'}, {'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'}, {'word': '依赖', 'pinyin': 'yī lài', 'trans': 'depend on'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '几何', 'pinyin': 'jǐ hé', 'trans': 'geometric'}, {'word': '轨迹', 'pinyin': 'guǐ jī', 'trans': 'trajectory'}, {'word': '估计', 'pinyin': 'gū jì', 'trans': 'estimate'}, {'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '优势', 'pinyin': 'yōu shì', 'trans': 'advantage'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'}, {'word': '包括', 'pinyin': 'bāo kuò', 'trans': 'include'}, {'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'}, {'word': '字幕', 'pinyin': 'zì mù', 'trans': 'subtitle'}, {'word': '问答', 'pinyin': 'wèn dá', 'trans': 'question and answer'}, {'word': '文本检索', 'pinyin': 'wén běn jiǎn suǒ', 'trans': 'text retrieval'}, {'word': '希望', 'pinyin': 'xī wàng', 'trans': 'hope'}, {'word': '推动', 'pinyin': 'tuī dòng', 'trans': 'promote'}, {'word': '未来', 'pinyin': 'wèi lái', 'trans': 'future'}, {'word': '努力', 'pinyin': 'nǔ lì', 'trans': 'effort'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '最终', 'pinyin': 'zuì zhōng', 'trans': 'ultimate'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'goal'}]
[28.04.2025 20:12] Renaming previous Chinese page.
[28.04.2025 20:12] Renaming previous data. zh.html to ./d/2025-04-27_zh_reading_task.html
[28.04.2025 20:12] Writing Chinese reading task.
[28.04.2025 20:12] Writing result.
[28.04.2025 20:12] Renaming log file.
[28.04.2025 20:12] Renaming previous data. log.txt to ./logs/2025-04-28_last_log.txt
