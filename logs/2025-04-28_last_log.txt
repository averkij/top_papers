[28.04.2025 02:30] Read previous papers.
[28.04.2025 02:30] Generating top page (month).
[28.04.2025 02:30] Writing top page (month).
[28.04.2025 03:36] Read previous papers.
[28.04.2025 03:36] Get feed.
[28.04.2025 03:36] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15376
[28.04.2025 03:36] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16427
[28.04.2025 03:36] Extract page data from URL. URL: https://huggingface.co/papers/2504.18415
[28.04.2025 03:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.04.2025 03:36] No deleted papers detected.
[28.04.2025 03:36] Downloading and parsing papers (pdf, html). Total: 3.
[28.04.2025 03:36] Downloading and parsing paper https://huggingface.co/papers/2504.15376.
[28.04.2025 03:36] Extra JSON file exists (./assets/json/2504.15376.json), skip PDF parsing.
[28.04.2025 03:36] Paper image links file exists (./assets/img_data/2504.15376.json), skip HTML parsing.
[28.04.2025 03:36] Success.
[28.04.2025 03:36] Downloading and parsing paper https://huggingface.co/papers/2504.16427.
[28.04.2025 03:36] Extra JSON file exists (./assets/json/2504.16427.json), skip PDF parsing.
[28.04.2025 03:36] Paper image links file exists (./assets/img_data/2504.16427.json), skip HTML parsing.
[28.04.2025 03:36] Success.
[28.04.2025 03:36] Downloading and parsing paper https://huggingface.co/papers/2504.18415.
[28.04.2025 03:36] Downloading paper 2504.18415 from http://arxiv.org/pdf/2504.18415v1...
[28.04.2025 03:37] Extracting affiliations from text.
[28.04.2025 03:37] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 2 ] . [ 1 5 1 4 8 1 . 4 0 5 2 : r BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs Hongyu Wang Shuming Ma Furu Wei https://aka.ms/GeneralAI "
[28.04.2025 03:37] Response: []
[28.04.2025 03:37] Extracting affiliations from text.
[28.04.2025 03:37] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 2 ] . [ 1 5 1 4 8 1 . 4 0 5 2 : r BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs Hongyu Wang Shuming Ma Furu Wei https://aka.ms/GeneralAIEfficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference. Theres Plenty of Room at the Bottom. Richard Feynman (a) Wo of BitNet b1.58 (b) Wdown of BitNet b1.58 (c) Wo of BitNet v2 (d) Wdown of BitNet Figure 1: Top: Overview of BitNet v2 and H-BitLinear. Bottom: The distribution of the activation of output projection Wo in attention and down projection Wdown in FFN. BitNet v2 utilizes H-BitLinear to eliminate the large amount of outlier channels in the intermediate states. The Hadamard transformation reshapes the original sharp distribution into more Gaussian-like form. Equal contribution. Corresponding author. S. Ma and F. Wei are with Microsoft Research. H. Wang is with University of Chinese Academy of Sciences.The field of deep learning is rapidly embracing quantization-aware training and low-bit inference, driven by hardware advancements like next-generation GPUs (e.g., GB200) offering native support for 4-bit computations. This promises significant efficiency gains for large-scale models. Pioneering work like BitNet b1.58 [MWM+24] demonstrated that 1.58-bit LLMs can match full-precision performance while drastically reducing inference costs (latency, memory, throughput, energy) [WZS+25]. However, while BitNet b1.58 quantizes weights to 1.58 bits, alleviating memory bandwidth bottlenecks, it retains 8-bit activations. This reliance on 8-bit precision prevents these models from fully leveraging the 4-bit computational capabilities of emerging hardware, shifting the efficiency bottleneck towards computation itself. Achieving lower bit-width activations is crucial for maximizing hardware utilization, particularly for efficient kernel design in batched inference scenarios. Research [WMW24, LPC+24] highlights key challenge: the non-uniform distribution of activations within LLMs. While inputs to attention and feed-forward network (FFN) layers often exhibit Gaussian-like distributions amenable to quantization, their intermediate states (outputs before final projection) contain significant outliers, hindering aggressive low-bit quantization. BitNet a4.8 [WMW24] attempted to address this by selectively using 4-bit quantization for inputs and 8-bit sparsification for intermediate states. While achieving minimal performance loss compared to 8-bit activations, sparsification is less suited for maximizing throughput in batched inference, where dense computations are often preferred for hardware efficiency. To bridge this gap and unlock the full potential of 4-bit computation for 1.58-bit LLMs, we introduce BitNet v2. Our framework enables native 4-bit activations across the model. The core innovation is H-BitLinear, novel linear layer replacing the standard output projections in attention and down projections in FFNs. H-BitLinear applies an online Hadamard transformation before activation quantization. This strategically reshapes the sharp, outlier-prone distributions of intermediate states into more manageable, Gaussian-like forms, significantly reducing the impact of outliers in 1.58-bit models. We train BitNet v2 from scratch using 8-bit activations, achieving negligible performance loss compared to BitNet b1.58 [MWM+24]. Subsequently, the model can be efficiently fine-tuned with small amount of data to operate with native 4-bit activations. Extensive experiments demonstrate that our 4-bit BitNet v2 variant achieves performance comparable to BitNet a4.8 while offering superior computational efficiency for batched inference scenarios.We illustrate the architecture of BitNet v2 in Figure 1. We implement BitNet v2 using LLaMAlike components, including RMS normalization [ZS19], SwishGLU [Sha20] and removing all bias. Compared to BitNet [WMD+23], we use H-BitLinear for Wo in attention and Wdown in FFN layers to deal with outlier channels of intermediate states. BitNet v2 is trained with 1.58-bit weights and INT8 activations from scratch, then continue-trained with INT4 activations for all linear layers except input/output embedding. 2.1 H-BitLinear Following [WMD+23, MWM+24], as for weight quantization, we use per-tensor absmean function to quantize the weights into ternary values, i.e., {-1, 0, 1}: Qw(W) = αRoundClip( α + ϵ , 1, 1), α = mean(W) RoundClip(X, a, b) = min(max(round(X), a), b) (1) (2) For low-bit activations, previous works [WMW24, LPC+24] have shown that the distributions of the inputs to the attention and feed-forward-network layers (i.e., the activations of Wqkv and Wup,gate) tend to exhibit Gaussian-like shape, while the intermediates states (i.e., the activations of Wo and Wdown) have more outlier channels and massive amount of entries around zero. 2 (a) Wqkv of BitNet b1. (b) Wo of BitNet b1.58 (c) Wup,gate of BitNet b1.58 (d) Wdown of BitNet b1.58 (e) Wqkv of BitNet v2 (f) Wo of BitNet v2 (g) Wup,gate of BitNet v2 (h) Wdown of BitNet Figure 2: The activation distribution of BitNet b1.58 and BitNet v2 with 8-bit activations. Therefore, we introduce H-BitLinear for Wo in attention and Wdown in FFN layers. H-BitLinear employs Hadamard transformation before activation quantization to first reduce the number of outlier channels. The Hadamard transformation satisfies that: Hadamard(X) = HmX Hm = 1 (cid:18) Hm1 Hm1 Hm1 Hm1 (cid:19) , H0 = (1) (3) (4) where Hm is 2m 2m matrix and Rn, = 2m. We use fast-hadamard-transform2 to perform the matrix multiplication, which has O(n log n) computational complexity. As shown in Figure 2 and Figure 3, with Hadamard transformation, the distribution of the intermediate states becomes close"
[28.04.2025 03:37] Mistral response. {"id": "0231a056e2c44d0b9f6856a52cef56fd", "object": "chat.completion", "created": 1745811455, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Microsoft Research\", \"University of Chinese Academy of Sciences\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1815, "total_tokens": 1835, "completion_tokens": 20}}
[28.04.2025 03:37] Response: ```python
["Microsoft Research", "University of Chinese Academy of Sciences"]
```
[28.04.2025 03:37] Deleting PDF ./assets/pdf/2504.18415.pdf.
[28.04.2025 03:37] Success.
[28.04.2025 03:37] Enriching papers with extra data.
[28.04.2025 03:37] ********************************************************************************
[28.04.2025 03:37] Abstract 0. We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of cam...
[28.04.2025 03:37] ********************************************************************************
[28.04.2025 03:37] Abstract 1. Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (...
[28.04.2025 03:37] ********************************************************************************
[28.04.2025 03:37] Abstract 2. Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward net...
[28.04.2025 03:37] Read previous papers.
[28.04.2025 03:37] Generating reviews via LLM API.
[28.04.2025 03:37] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#cv", "#multimodal"], "emoji": "🎥", "ru": {"title": "Новый взгляд на понимание движения камеры в видео", "desc": "CameraBench - это новый крупномасштабный датасет и бенчмарк для оценки и улучшения понимания движения камеры в видео. Датасет содержит около 30
[28.04.2025 03:37] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#open_source", "#multimodal"], "emoji": "🗣️", "ru": {"title": "MMLA: новый бенчмарк для оценки понимания мультимодальной семантики языковыми моделями", "desc": "Статья представляет MMLA - новый набор данных для оценки способности мультимодальных языковых мо
[28.04.2025 03:37] Querying the API.
[28.04.2025 03:37] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.
[28.04.2025 03:37] Response: {
  "desc": "BitNet v2 - это новая технология для эффективного развертывания 1-битных больших языковых моделей (LLM). Она решает проблему выбросов в активациях, которые затрудняют квантование до малого числа бит. Ключевой компонент - модуль H-BitLinear, применяющий онлайн-преобразование Адамара перед квантованием активаций. Эксперименты показывают, что BitNet v2 с 4-битными активациями достигает производительности BitNet b1.58, значительно снижая потребление памяти и вычислительные затраты.",
  "emoji": "🧠",
  "title": "Эффективное сжатие больших языковых моделей без потери качества"
}
[28.04.2025 03:37] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference."

[28.04.2025 03:37] Response: ```python
["INFERENCE", "TRAINING", "SMALL_MODELS"]
```
[28.04.2025 03:37] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference."

[28.04.2025 03:37] Response: ```python
["OPTIMIZATION"]
```
[28.04.2025 03:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents BitNet v2, a new framework designed to improve the deployment of 1-bit Large Language Models (LLMs) by enabling efficient 4-bit activation quantization. The challenge of activation outliers, which complicate the quantization process, is addressed through a novel module called H-BitLinear that applies an online Hadamard transformation. This transformation helps to convert sharp activation distributions into smoother, Gaussian-like forms, making them more suitable for low-bit representation. The results demonstrate that BitNet v2 can be trained with 4-bit activations with minimal performance loss, significantly lowering memory usage and computational costs during inference.","title":"Revolutionizing 1-bit LLMs with Efficient 4-bit Quantization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents BitNet v2, a new framework designed to improve the deployment of 1-bit Large Language Models (LLMs) by enabling efficient 4-bit activation quantization. The challenge of activation outliers, which complicate the quantization process, is addressed through a novel module called H-BitLinear that applies an online Hadamard transformation. This transformation helps to convert sharp activation distributions into smoother, Gaussian-like forms, making them more suitable for low-bit representation. The results demonstrate that BitNet v2 can be trained with 4-bit activations with minimal performance loss, significantly lowering memory usage and computational costs during inference.', title='Revolutionizing 1-bit LLMs with Efficient 4-bit Quantization'))
[28.04.2025 03:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种名为BitNet v2的新框架，旨在高效部署1位大型语言模型（LLMs）。该框架通过引入H-BitLinear模块，解决了在注意力和前馈网络激活中出现的异常值问题，从而实现原生4位激活量化。H-BitLinear模块通过在线Hadamard变换，将激活分布平滑为更接近高斯分布的形式，适合低位表示。实验表明，使用原生4位激活训练的BitNet v2在性能上几乎没有下降，同时显著减少了内存占用和计算成本。","title":"高效量化，轻松部署1位语言模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种名为BitNet v2的新框架，旨在高效部署1位大型语言模型（LLMs）。该框架通过引入H-BitLinear模块，解决了在注意力和前馈网络激活中出现的异常值问题，从而实现原生4位激活量化。H-BitLinear模块通过在线Hadamard变换，将激活分布平滑为更接近高斯分布的形式，适合低位表示。实验表明，使用原生4位激活训练的BitNet v2在性能上几乎没有下降，同时显著减少了内存占用和计算成本。', title='高效量化，轻松部署1位语言模型'))
[28.04.2025 03:37] Loading Chinese text from previous data.
[28.04.2025 03:37] Renaming data file.
[28.04.2025 03:37] Renaming previous data. hf_papers.json to ./d/2025-04-28.json
[28.04.2025 03:37] Saving new data file.
[28.04.2025 03:37] Generating page.
[28.04.2025 03:37] Renaming previous page.
[28.04.2025 03:37] Renaming previous data. index.html to ./d/2025-04-28.html
[28.04.2025 03:37] [Experimental] Generating Chinese page for reading.
[28.04.2025 03:37] Chinese vocab [{'word': '尽管', 'pinyin': 'jǐn guǎn', 'trans': 'although'}, {'word': '迅速', 'pinyin': 'xùn sù', 'trans': 'rapidly'}, {'word': '缺失', 'pinyin': 'quē shī', 'trans': 'lack'}, {'word': '复现', 'pinyin': 'fù xiàn', 'trans': 'reproduce'}, {'word': '擅长', 'pinyin': 'shàn cháng', 'trans': 'be good at'}, {'word': '启发', 'pinyin': 'qǐ fā', 'trans': 'inspire'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': '转化', 'pinyin': 'zhuǎn huà', 'trans': 'transform'}, {'word': '功能', 'pinyin': 'gōng néng', 'trans': 'functional'}, {'word': '代理', 'pinyin': 'dài lǐ', 'trans': 'agent'}, {'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'}, {'word': '协作', 'pinyin': 'xié zuò', 'trans': 'collaborate'}, {'word': '忠实', 'pinyin': 'zhōng shí', 'trans': 'faithful'}]
[28.04.2025 03:37] Renaming previous Chinese page.
[28.04.2025 03:37] Renaming previous data. zh.html to ./d/2025-04-27_zh_reading_task.html
[28.04.2025 03:37] Writing Chinese reading task.
[28.04.2025 03:37] Writing result.
[28.04.2025 03:37] Renaming log file.
[28.04.2025 03:37] Renaming previous data. log.txt to ./logs/2025-04-28_last_log.txt
