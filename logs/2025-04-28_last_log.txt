[28.04.2025 19:09] Read previous papers.
[28.04.2025 19:09] Generating top page (month).
[28.04.2025 19:09] Writing top page (month).
[28.04.2025 20:12] Read previous papers.
[28.04.2025 20:12] Get feed.
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15376
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16656
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.18415
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17821
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16427
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17768
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17816
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.18425
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15716
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.12080
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17025
[28.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.18225
[28.04.2025 20:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.04.2025 20:12] No deleted papers detected.
[28.04.2025 20:12] Downloading and parsing papers (pdf, html). Total: 12.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.15376.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.15376.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.15376.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16656.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16656.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16656.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.18415.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.18415.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.18415.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17821.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17821.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17821.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.16427.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.16427.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.16427.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17768.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17768.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17768.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17816.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17816.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17816.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.18425.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.18425.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.18425.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.15716.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.15716.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.15716.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.12080.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.12080.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.12080.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.17025.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.17025.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.17025.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.18225.
[28.04.2025 20:12] Extra JSON file exists (./assets/json/2504.18225.json), skip PDF parsing.
[28.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.18225.json), skip HTML parsing.
[28.04.2025 20:12] Success.
[28.04.2025 20:12] Enriching papers with extra data.
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 0. We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of cam...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 1. We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standi...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 2. Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward net...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 3. Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In ...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 4. Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 5. Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention method...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 6. We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, whi...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 7. We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 8. Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to addr...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 9. Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and i...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 10. The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not o...
[28.04.2025 20:12] ********************************************************************************
[28.04.2025 20:12] Abstract 11. We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support...
[28.04.2025 20:12] Read previous papers.
[28.04.2025 20:12] Generating reviews via LLM API.
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#cv", "#multimodal"], "emoji": "üé•", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≤–∏–¥–µ–æ", "desc": "CameraBench - —ç—Ç–æ –Ω–æ–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ –≤–∏–¥–µ–æ. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–∫–æ–ª–æ 30
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#training", "#agents", "#optimization", "#reasoning", "#multimodal", "#open_source", "#rl"], "emoji": "üß†", "ru": {"title": "Skywork R1V2: –ü—Ä–æ—Ä—ã–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å –ø–æ–º–æ—â—å—é –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "Skywork R1V2 - —ç—Ç
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "BitNet v2 - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è 1-–±–∏—Ç–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ —Ä–µ—à–∞–µ—Ç 
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#benchmark", "#open_source", "#reasoning", "#video", "#dataset", "#science"], "emoji": "üåè", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∏ —è–∑—ã–∫–æ–≤—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "VideoVista-CulturalLingo - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#open_source", "#multimodal"], "emoji": "üó£Ô∏è", "ru": {"title": "MMLA: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MMLA - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#long_context", "#architecture", "#training", "#optimization"], "emoji": "üï∏Ô∏è", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –∫–ª—é—á –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#video", "#dataset", "#training"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ò—Ö –º–µ—Ç–æ–¥ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å–ø–µ—Ü
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#inference", "#training", "#benchmark", "#dataset", "#open_source", "#audio", "#data"], "emoji": "üéµ", "ru": {"title": "Kimi-Audio: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ –∑–≤—É–∫–æ–º", "desc": "Kimi-Audio - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –∞—É–¥–∏–æ-–º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#training", "#dataset", "#reasoning"], "emoji": "üíπ", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "DianJin-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#dataset", "#transfer_learning", "#benchmark", "#cv", "#games"], "emoji": "üé≠", "ru": {"title": "DC-SAM: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Dual Consistency SAM (DC-SAM) –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π SAM –∏ SAM2 –∫ –∑–∞–¥–∞—á–µ —Å–µ–≥–º–µ–Ω
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#small_models", "#multilingual", "#low_resource", "#transfer_learning", "#training"], "emoji": "üáÆüáπ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã—Ö LLM –¥–ª—è –∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –∏—Ç–∞–ª—å—è–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤
[28.04.2025 20:12] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#reasoning", "#rag", "#benchmark", "#synthetic", "#small_models"], "emoji": "üß†", "ru": {"title": "–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ - –±–æ–ª—å—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è RAG", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö RAG, –ø–æ–∏—Å–∫–∞ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ - Plei
[28.04.2025 20:12] Loading Chinese text from previous data.
[28.04.2025 20:12] Renaming data file.
[28.04.2025 20:12] Renaming previous data. hf_papers.json to ./d/2025-04-28.json
[28.04.2025 20:12] Saving new data file.
[28.04.2025 20:12] Generating page.
[28.04.2025 20:12] Renaming previous page.
[28.04.2025 20:12] Renaming previous data. index.html to ./d/2025-04-28.html
[28.04.2025 20:12] [Experimental] Generating Chinese page for reading.
[28.04.2025 20:12] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éi j√¨n', 'trans': 'improve'}, {'word': 'ÊëÑÂÉèÊú∫', 'pinyin': 'sh√® xi√†ng jƒ´', 'trans': 'camera'}, {'word': 'ËøêÂä®', 'pinyin': 'y√πn d√≤ng', 'trans': 'motion'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understand'}, {'word': 'Â§öÊ†∑Âåñ', 'pinyin': 'du≈ç y√†ng hu√†', 'trans': 'diverse'}, {'word': 'ÁΩëÁªúËßÜÈ¢ë', 'pinyin': 'w«éng lu√≤ sh√¨ p√≠n', 'trans': 'online video'}, {'word': '‰∏ìÂÆ∂', 'pinyin': 'zhuƒÅn jiƒÅ', 'trans': 'expert'}, {'word': '‰∏•Ê†º', 'pinyin': 'y√°n g√©', 'trans': 'strict'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç jiƒì du√†n', 'trans': 'multi-stage'}, {'word': 'Ë¥®ÈáèÊéßÂà∂', 'pinyin': 'zh√¨ li√†ng k√≤ng zh√¨', 'trans': 'quality control'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ ch√©ng', 'trans': 'process'}, {'word': 'Ê†áÊ≥®', 'pinyin': 'biƒÅo zh√π', 'trans': 'annotate'}, {'word': 'ÊëÑÂΩ±Â∏à', 'pinyin': 'sh√® y«êng shƒ´', 'trans': 'photographer'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√® j√¨', 'trans': 'design'}, {'word': 'Âü∫ÂÖÉ', 'pinyin': 'jƒ´ yu√°n', 'trans': 'primitive'}, {'word': 'ÂàÜÁ±ªÊ≥ï', 'pinyin': 'fƒìn l√®i f«é', 'trans': 'classification method'}, {'word': '‰æãÂ¶Ç', 'pinyin': 'l√¨ r√∫', 'trans': 'for example'}, {'word': 'Êüê‰∫õ', 'pinyin': 'm«íu xiƒì', 'trans': 'some'}, {'word': 'Ë∑üÈöè', 'pinyin': 'gƒìn su√≠', 'trans': 'follow'}, {'word': 'ÈúÄË¶Å', 'pinyin': 'x≈´ y√†o', 'trans': 'need'}, {'word': 'Âú∫ÊôØ', 'pinyin': 'ch«éng j«êng', 'trans': 'scene'}, {'word': 'ÂÜÖÂÆπ', 'pinyin': 'n√®i r√≥ng', 'trans': 'content'}, {'word': '‰∏ª‰Ωì', 'pinyin': 'zh«î t«ê', 'trans': 'subject'}, {'word': 'ÁßªÂä®', 'pinyin': 'y√≠ d√≤ng', 'trans': 'move'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'study'}, {'word': 'ÈáèÂåñ', 'pinyin': 'li√†ng hu√†', 'trans': 'quantify'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êng y√π', 'trans': 'field'}, {'word': '‰∏ì‰∏öÁü•ËØÜ', 'pinyin': 'zhuƒÅn y√® zhƒ´ shi', 'trans': 'professional knowledge'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´ y√∫', 'trans': 'based on'}, {'word': 'ÊïôÁ®ã', 'pinyin': 'ji√†o ch√©ng', 'trans': 'tutorial'}, {'word': 'ÂüπËÆ≠', 'pinyin': 'p√©i x√πn', 'trans': 'training'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«în qu√® x√¨ng', 'trans': 'accuracy'}, {'word': 'ÁªìÊûÑ‰ªéËøêÂä®', 'pinyin': 'ji√© g√≤u c√≥ng y√πn d√≤ng', 'trans': 'Structure from Motion (SfM)'}, {'word': 'ËßÜÈ¢ëËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ p√≠n y«î y√°n m√≥ x√≠ng', 'trans': 'Video Language Model (VLM)'}, {'word': 'ÊçïÊçâ', 'pinyin': 'b«î zhu≈ç', 'trans': 'capture'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´ l√†i', 'trans': 'depend on'}, {'word': 'ËØ≠‰πâ', 'pinyin': 'y«î y√¨', 'trans': 'semantic'}, {'word': 'Âá†‰Ωï', 'pinyin': 'j«ê h√©', 'trans': 'geometric'}, {'word': 'ËΩ®Ëøπ', 'pinyin': 'gu«ê jƒ´', 'trans': 'trajectory'}, {'word': '‰º∞ËÆ°', 'pinyin': 'g≈´ j√¨', 'trans': 'estimate'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': '‰ºòÂäø', 'pinyin': 'y≈çu sh√¨', 'trans': 'advantage'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√© h√©', 'trans': 'combine'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«én sh√¨', 'trans': 'demonstrate'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'application'}, {'word': 'ÂåÖÊã¨', 'pinyin': 'bƒÅo ku√≤', 'trans': 'include'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìng qi√°ng', 'trans': 'enhance'}, {'word': 'Â≠óÂπï', 'pinyin': 'z√¨ m√π', 'trans': 'subtitle'}, {'word': 'ÈóÆÁ≠î', 'pinyin': 'w√®n d√°', 'trans': 'question and answer'}, {'word': 'ÊñáÊú¨Ê£ÄÁ¥¢', 'pinyin': 'w√©n bƒõn ji«én su«í', 'trans': 'text retrieval'}, {'word': 'Â∏åÊúõ', 'pinyin': 'xƒ´ w√†ng', 'trans': 'hope'}, {'word': 'Êé®Âä®', 'pinyin': 'tuƒ´ d√≤ng', 'trans': 'promote'}, {'word': 'Êú™Êù•', 'pinyin': 'w√®i l√°i', 'trans': 'future'}, {'word': 'Âä™Âäõ', 'pinyin': 'n«î l√¨', 'trans': 'effort'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'ÊúÄÁªà', 'pinyin': 'zu√¨ zh≈çng', 'trans': 'ultimate'}, {'word': 'ÁõÆÊ†á', 'pinyin': 'm√π biƒÅo', 'trans': 'goal'}]
[28.04.2025 20:12] Renaming previous Chinese page.
[28.04.2025 20:12] Renaming previous data. zh.html to ./d/2025-04-27_zh_reading_task.html
[28.04.2025 20:12] Writing Chinese reading task.
[28.04.2025 20:12] Writing result.
[28.04.2025 20:12] Renaming log file.
[28.04.2025 20:12] Renaming previous data. log.txt to ./logs/2025-04-28_last_log.txt
