[28.04.2025 11:32] Read previous papers.
[28.04.2025 11:32] Generating top page (month).
[28.04.2025 11:32] Writing top page (month).
[28.04.2025 12:20] Read previous papers.
[28.04.2025 12:20] Get feed.
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15376
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16656
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.18415
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17821
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16427
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17768
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.17816
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15716
[28.04.2025 12:20] Get page data from previous paper. URL: https://huggingface.co/papers/2504.12080
[28.04.2025 12:20] Extract page data from URL. URL: https://huggingface.co/papers/2504.17025
[28.04.2025 12:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.04.2025 12:20] No deleted papers detected.
[28.04.2025 12:20] Downloading and parsing papers (pdf, html). Total: 10.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.15376.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.15376.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.15376.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.16656.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.16656.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.16656.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.18415.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.18415.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.18415.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.17821.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.17821.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.17821.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.16427.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.16427.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.16427.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.17768.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.17768.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.17768.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.17816.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.17816.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.17816.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.15716.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.15716.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.15716.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.12080.
[28.04.2025 12:20] Extra JSON file exists (./assets/json/2504.12080.json), skip PDF parsing.
[28.04.2025 12:20] Paper image links file exists (./assets/img_data/2504.12080.json), skip HTML parsing.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Downloading and parsing paper https://huggingface.co/papers/2504.17025.
[28.04.2025 12:20] Downloading paper 2504.17025 from http://arxiv.org/pdf/2504.17025v1...
[28.04.2025 12:20] Extracting affiliations from text.
[28.04.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation Luca Moroni1*, Giovanni Puccetti2, Pere-Lluis Huguet Cabot1, Andrei Stefan Bejgu4 Edoardo Barba1, Alessio Miaschi3 Felice DellOrletta3, Andrea Esuli2, Roberto Navigli1 1Sapienza University of Rome {surname}@diag.uniroma1.it 2ISTI-CNR {name.surname}@isti.cnr.it 3ILC-CNR {name.surname}@ilc.cnr.it 4Babelscape {surname}@babelscape.com 5 2 0 2 3 2 ] . [ 1 5 2 0 7 1 . 4 0 5 2 : r a "
[28.04.2025 12:20] Response: ```python
[
    "Sapienza University of Rome",
    "ISTI-CNR",
    "ILC-CNR",
    "Babelscape"
]
```
[28.04.2025 12:20] Deleting PDF ./assets/pdf/2504.17025.pdf.
[28.04.2025 12:20] Success.
[28.04.2025 12:20] Enriching papers with extra data.
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 0. We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of cam...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 1. We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standi...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 2. Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward net...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 3. Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In ...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 4. Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 5. Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention method...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 6. We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, whi...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 7. Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to addr...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 8. Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and i...
[28.04.2025 12:20] ********************************************************************************
[28.04.2025 12:20] Abstract 9. The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not o...
[28.04.2025 12:20] Read previous papers.
[28.04.2025 12:20] Generating reviews via LLM API.
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#cv", "#multimodal"], "emoji": "🎥", "ru": {"title": "Новый взгляд на понимание движения камеры в видео", "desc": "CameraBench - это новый крупномасштабный датасет и бенчмарк для оценки и улучшения понимания движения камеры в видео. Датасет содержит около 30
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#training", "#agents", "#optimization", "#reasoning", "#multimodal", "#open_source", "#rl"], "emoji": "🧠", "ru": {"title": "Skywork R1V2: Прорыв в мультимодальных рассуждениях с помощью гибридного обучения с подкреплением", "desc": "Skywork R1V2 - эт
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#inference", "#training"], "emoji": "🧠", "ru": {"title": "Эффективное сжатие больших языковых моделей без потери качества", "desc": "BitNet v2 - это новая технология для эффективного развертывания 1-битных больших языковых моделей (LLM). Она решает 
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#benchmark", "#open_source", "#reasoning", "#video", "#dataset", "#science"], "emoji": "🌏", "ru": {"title": "Преодоление культурных и языковых барьеров в понимании видео искусственным интеллектом", "desc": "VideoVista-CulturalLingo - это новый бенчм
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#open_source", "#multimodal"], "emoji": "🗣️", "ru": {"title": "MMLA: новый бенчмарк для оценки понимания мультимодальной семантики языковыми моделями", "desc": "Статья представляет MMLA - новый набор данных для оценки способности мультимодальных языковых мо
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#long_context", "#architecture", "#training", "#optimization"], "emoji": "🕸️", "ru": {"title": "Разреженное внимание: ключ к обработке длинных последовательностей в трансформерах", "desc": "Это исследование фокусируется на применении разреженного внимания в трансформерных моделях дл
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#video", "#dataset", "#training"], "emoji": "🎬", "ru": {"title": "Эффективная персонализация видео без дополнительной настройки", "desc": "Исследователи предлагают новый подход к созданию персонализированных видео с использованием машинного обучения. Их метод разделяет обучение спец
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#training", "#dataset", "#reasoning"], "emoji": "💹", "ru": {"title": "Усиление финансовых рассуждений ИИ через структурированное обучение", "desc": "DianJin-R1 - это фреймворк для улучшения рассуждений больших языковых моделей в финансовой сфере. Он использует о
[28.04.2025 12:20] Using data from previous issue: {"categories": ["#dataset", "#transfer_learning", "#benchmark", "#cv", "#games"], "emoji": "🎭", "ru": {"title": "DC-SAM: Продвинутая сегментация в контексте для изображений и видео", "desc": "Эта статья представляет метод Dual Consistency SAM (DC-SAM) для адаптации моделей SAM и SAM2 к задаче сегмен
[28.04.2025 12:20] Querying the API.
[28.04.2025 12:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.
[28.04.2025 12:20] Response: {
  "desc": "Эта статья посвящена оптимизации больших языковых моделей (LLM) для итальянского языка. Авторы сравнивают различные методы адаптации словаря и предлагают новый метод SAVA, использующий нейронное отображение для замены словаря. Они адаптировали модели Mistral-7b-v0.1 и Llama-3.1-8B, уменьшив токенную избыточность и оптимизировав словарь. Исследователи показали, что после адаптации словаря модели могут восстановить свою производительность с помощью ограниченного дообучения на целевом языке.",
  "emoji": "🇮🇹",
  "title": "Оптимизация англоязычных LLM для итальянского языка"
}
[28.04.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks."

[28.04.2025 12:20] Response: ```python
['MULTILINGUAL', 'TRAINING', 'SMALL_MODELS']
```
[28.04.2025 12:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks."

[28.04.2025 12:20] Response: ```python
["LOW_RESOURCE", "TRANSFER_LEARNING"]
```
[28.04.2025 12:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of adapting Large Language Models (LLMs) for non-English languages, specifically Italian, which often suffer from inefficiencies due to language contamination. The authors introduce a new method called Semantic Alignment Vocabulary Adaptation (SAVA), which uses neural mapping to optimize vocabulary for better performance. By applying SAVA to two LLMs, they demonstrate a significant reduction in token fertility and a decrease in model parameters while maintaining competitive performance on various tasks. The results indicate that with proper vocabulary adaptation, LLMs can effectively learn and perform in target languages with minimal additional training.","title":"Optimizing LLMs for Italian: SAVA Unleashed!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of adapting Large Language Models (LLMs) for non-English languages, specifically Italian, which often suffer from inefficiencies due to language contamination. The authors introduce a new method called Semantic Alignment Vocabulary Adaptation (SAVA), which uses neural mapping to optimize vocabulary for better performance. By applying SAVA to two LLMs, they demonstrate a significant reduction in token fertility and a decrease in model parameters while maintaining competitive performance on various tasks. The results indicate that with proper vocabulary adaptation, LLMs can effectively learn and perform in target languages with minimal additional training.', title='Optimizing LLMs for Italian: SAVA Unleashed!'))
[28.04.2025 12:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"随着预训练大型语言模型（LLMs）的数量不断增加，大多数模型主要针对英语设计。虽然最先进的LLMs可以处理其他语言，但由于语言污染或多语言预训练数据的影响，它们在非英语语言上的表现并不理想，导致编码效率低下和推理速度缓慢。本文比较了多种词汇适应技术，以优化英语LLMs在意大利语上的表现，并提出了一种新方法——语义对齐词汇适应（SAVA），利用神经映射进行词汇替换。通过对词汇的适应，这些模型在目标语言上经过有限的持续训练后，能够恢复其性能。","title":"优化英语模型，提升意大利语表现！"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='随着预训练大型语言模型（LLMs）的数量不断增加，大多数模型主要针对英语设计。虽然最先进的LLMs可以处理其他语言，但由于语言污染或多语言预训练数据的影响，它们在非英语语言上的表现并不理想，导致编码效率低下和推理速度缓慢。本文比较了多种词汇适应技术，以优化英语LLMs在意大利语上的表现，并提出了一种新方法——语义对齐词汇适应（SAVA），利用神经映射进行词汇替换。通过对词汇的适应，这些模型在目标语言上经过有限的持续训练后，能够恢复其性能。', title='优化英语模型，提升意大利语表现！'))
[28.04.2025 12:21] Loading Chinese text from previous data.
[28.04.2025 12:21] Renaming data file.
[28.04.2025 12:21] Renaming previous data. hf_papers.json to ./d/2025-04-28.json
[28.04.2025 12:21] Saving new data file.
[28.04.2025 12:21] Generating page.
[28.04.2025 12:21] Renaming previous page.
[28.04.2025 12:21] Renaming previous data. index.html to ./d/2025-04-28.html
[28.04.2025 12:21] [Experimental] Generating Chinese page for reading.
[28.04.2025 12:21] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'}, {'word': '摄像机', 'pinyin': 'shè xiàng jī', 'trans': 'camera'}, {'word': '运动', 'pinyin': 'yùn dòng', 'trans': 'motion'}, {'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understand'}, {'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diverse'}, {'word': '网络视频', 'pinyin': 'wǎng luò shì pín', 'trans': 'online video'}, {'word': '专家', 'pinyin': 'zhuān jiā', 'trans': 'expert'}, {'word': '严格', 'pinyin': 'yán gé', 'trans': 'strict'}, {'word': '多阶段', 'pinyin': 'duō jiē duàn', 'trans': 'multi-stage'}, {'word': '质量控制', 'pinyin': 'zhì liàng kòng zhì', 'trans': 'quality control'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '标注', 'pinyin': 'biāo zhù', 'trans': 'annotate'}, {'word': '摄影师', 'pinyin': 'shè yǐng shī', 'trans': 'photographer'}, {'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'}, {'word': '基元', 'pinyin': 'jī yuán', 'trans': 'primitive'}, {'word': '分类法', 'pinyin': 'fēn lèi fǎ', 'trans': 'classification method'}, {'word': '例如', 'pinyin': 'lì rú', 'trans': 'for example'}, {'word': '某些', 'pinyin': 'mǒu xiē', 'trans': 'some'}, {'word': '跟随', 'pinyin': 'gēn suí', 'trans': 'follow'}, {'word': '需要', 'pinyin': 'xū yào', 'trans': 'need'}, {'word': '场景', 'pinyin': 'chǎng jǐng', 'trans': 'scene'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '主体', 'pinyin': 'zhǔ tǐ', 'trans': 'subject'}, {'word': '移动', 'pinyin': 'yí dòng', 'trans': 'move'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'study'}, {'word': '量化', 'pinyin': 'liàng huà', 'trans': 'quantify'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '领域', 'pinyin': 'lǐng yù', 'trans': 'field'}, {'word': '专业知识', 'pinyin': 'zhuān yè zhī shi', 'trans': 'professional knowledge'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': '教程', 'pinyin': 'jiào chéng', 'trans': 'tutorial'}, {'word': '培训', 'pinyin': 'péi xùn', 'trans': 'training'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '结构从运动', 'pinyin': 'jié gòu cóng yùn dòng', 'trans': 'Structure from Motion (SfM)'}, {'word': '视频语言模型', 'pinyin': 'shì pín yǔ yán mó xíng', 'trans': 'Video Language Model (VLM)'}, {'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'}, {'word': '依赖', 'pinyin': 'yī lài', 'trans': 'depend on'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '几何', 'pinyin': 'jǐ hé', 'trans': 'geometric'}, {'word': '轨迹', 'pinyin': 'guǐ jī', 'trans': 'trajectory'}, {'word': '估计', 'pinyin': 'gū jì', 'trans': 'estimate'}, {'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '优势', 'pinyin': 'yōu shì', 'trans': 'advantage'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'}, {'word': '包括', 'pinyin': 'bāo kuò', 'trans': 'include'}, {'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'}, {'word': '字幕', 'pinyin': 'zì mù', 'trans': 'subtitle'}, {'word': '问答', 'pinyin': 'wèn dá', 'trans': 'question and answer'}, {'word': '文本检索', 'pinyin': 'wén běn jiǎn suǒ', 'trans': 'text retrieval'}, {'word': '希望', 'pinyin': 'xī wàng', 'trans': 'hope'}, {'word': '推动', 'pinyin': 'tuī dòng', 'trans': 'promote'}, {'word': '未来', 'pinyin': 'wèi lái', 'trans': 'future'}, {'word': '努力', 'pinyin': 'nǔ lì', 'trans': 'effort'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '最终', 'pinyin': 'zuì zhōng', 'trans': 'ultimate'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'goal'}]
[28.04.2025 12:21] Renaming previous Chinese page.
[28.04.2025 12:21] Renaming previous data. zh.html to ./d/2025-04-27_zh_reading_task.html
[28.04.2025 12:21] Writing Chinese reading task.
[28.04.2025 12:21] Writing result.
[28.04.2025 12:21] Renaming log file.
[28.04.2025 12:21] Renaming previous data. log.txt to ./logs/2025-04-28_last_log.txt
