[05.01.2026 21:22] Read previous papers.
[05.01.2026 21:22] Generating top page (month).
[05.01.2026 21:22] Writing top page (month).
[05.01.2026 22:23] Read previous papers.
[05.01.2026 22:23] Get feed.
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00393
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24615
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00664
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24330
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24271
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00796
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2512.24695
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00417
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00747
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22955
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00671
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00575
[05.01.2026 22:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.00204
[05.01.2026 22:23] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.01.2026 22:23] No deleted papers detected.
[05.01.2026 22:23] Downloading and parsing papers (pdf, html). Total: 13.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00393.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00393.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00393.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2512.24615.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2512.24615.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2512.24615.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00664.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00664.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00664.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2512.24330.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2512.24330.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2512.24330.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2512.24271.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2512.24271.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2512.24271.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00796.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00796.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00796.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2512.24695.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2512.24695.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2512.24695.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00417.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00417.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00417.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00747.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00747.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00747.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2512.22955.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2512.22955.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2512.22955.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00671.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00671.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00671.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00575.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00575.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00575.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Downloading and parsing paper https://huggingface.co/papers/2601.00204.
[05.01.2026 22:23] Extra JSON file exists (./assets/json/2601.00204.json), skip PDF parsing.
[05.01.2026 22:23] Paper image links file exists (./assets/img_data/2601.00204.json), skip HTML parsing.
[05.01.2026 22:23] Success.
[05.01.2026 22:23] Enriching papers with extra data.
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 0. In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and speciali...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 1. Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic ...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 2. A framework called Avatar Forcing uses diffusion forcing and direct preference optimization with synthetic losing samples to enable real-time, expressive multimodal interactions in talking head avatars without labeled data.  					AI-generated summary 				 Talking head generation creates lifelike ava...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 3. While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulat...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 4. Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy commo...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 5. Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce ener...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 6. Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Lear...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 7. The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to mode...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 8. Large language model training methods that optimize for correctness can cause reasoning path diversity collapse, but a new variational framework provides principled solutions to maintain both accuracy and creativity.  					AI-generated summary 				 State-of-the-art large language model (LLM) pipelin...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 9. Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution....
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 10. FwPKM introduces a dynamic, fast-weight episodic memory mechanism for sequence modeling that balances storage capacity and efficiency, achieving strong performance on long-context tasks like Needle in a Haystack evaluations.  					AI-generated summary 				 Sequence modeling layers in modern language...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 11. InfoSynth generates novel, diverse coding benchmarks for large language models using information-theoretic metrics and genetic algorithms, enabling scalable and self-verifying evaluation.  					AI-generated summary 				 Large language models (LLMs) have demonstrated significant advancements in reaso...
[05.01.2026 22:23] ********************************************************************************
[05.01.2026 22:23] Abstract 12. 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key...
[05.01.2026 22:23] Read previous papers.
[05.01.2026 22:23] Generating reviews via LLM API.
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#benchmark", "#video", "#3d"], "emoji": "üé¨", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è 4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–∑ –æ–¥–Ω–æ–π –∫–∞–º–µ—Ä—ã", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ NeoVerse ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å 4D-–º–∏—Ä–∞, —Å–ø–æ—Å–æ–±–Ω–∞—è –≤—ã–ø–æ–ª–Ω—è—Ç—å 4D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ —Å –Ω–æ–≤—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ä–∞–∑
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#benchmark", "#optimization", "#training", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ —ç–≤–æ–ª—é—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä—É—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Youtu-Agent ‚Äî –º–æ–¥—É–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#video", "#optimization", "#training", "#multimodal", "#synthetic", "#diffusion", "#rlhf"], "emoji": "üé≠", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –∞–≤–∞—Ç–∞—Ä—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ –ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ", "desc": "Avatar Forcing ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å –≥–æ–≤–æ—Ä—è—â
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#benchmark", "#dataset", "#cv", "#open_source", "#optimization", "#training", "#multimodal", "#agents"], "emoji": "üîç", "ru": {"title": "–ê–≥–µ–Ω—Ç—Å–∫–æ–µ –≤–∏–¥–µ–Ω–∏–µ: –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#rl", "#dataset", "#video", "#open_source", "#training", "#multimodal", "#synthetic", "#diffusion", "#hallucinations"], "emoji": "üé¨", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ì–∞–±–æ—Ä–∞ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ AdaGaR –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D —Å—Ü–µ–Ω –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–¥–æ–ª–µ
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#reasoning", "#training", "#architecture", "#long_context", "#optimization"], "emoji": "üéØ", "ru": {"title": "–í–ª–æ–∂–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏–µ—Å—è –º–æ–¥–µ–ª–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Nested Learning (–≤–ª–æ–∂–µ–Ω–Ω–æ
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#architecture"], "emoji": "üîÑ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–≤—è–∑–∏ —á–µ—Ä–µ–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Deep Delta Learning (DDL) ‚Äî –æ–±–æ–±—â–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—è—Ü–∏—é —Ç–æ–∂–¥–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç–æ–±—Ä
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#interpretability", "#training", "#rlhf", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#rl"], "emoji": "üîç", "ru": {"title": "–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–ª–∏—è–µ—Ç –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#long_context"], "emoji": "üìö", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ FwPKM, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–¥—É–ª—å P
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#open_source", "#synthetic", "#benchmark", "#plp"], "emoji": "üß¨", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è LLM —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é —Ç–µ–æ—Ä–∏—é", "desc": "InfoSynth ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü
[05.01.2026 22:23] Using data from previous issue: {"categories": ["#3d", "#architecture"], "emoji": "üîÑ", "ru": {"title": "–ü–ª–∞–≤–Ω—ã–µ 3D —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Å–µ—Ç—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MorphAny3D, –º–µ—Ç–æ–¥ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–≤–Ω—ã—Ö 3D –º–æ—Ä—Ñ–∏—Ä–æ–≤–∞–Ω–∏–π –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π. –ö
[05.01.2026 22:23] Renaming data file.
[05.01.2026 22:23] Renaming previous data. hf_papers.json to ./d/2026-01-05.json
[05.01.2026 22:23] Saving new data file.
[05.01.2026 22:23] Generating page.
[05.01.2026 22:23] Renaming previous page.
[05.01.2026 22:23] Renaming previous data. index.html to ./d/2026-01-05.html
[05.01.2026 22:23] Writing result.
[05.01.2026 22:23] Renaming log file.
[05.01.2026 22:23] Renaming previous data. log.txt to ./logs/2026-01-05_last_log.txt
