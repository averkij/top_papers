[21.01.2026 23:28] Read previous papers.
[21.01.2026 23:28] Generating top page (month).
[21.01.2026 23:28] Writing top page (month).
[22.01.2026 01:53] Read previous papers.
[22.01.2026 01:53] Get feed.
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12993
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11655
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13029
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14250
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14192
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13836
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11969
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14004
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11522
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12294
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13247
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11888
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13288
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14232
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14251
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14046
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13976
[22.01.2026 01:53] Extract page data from URL. URL: https://huggingface.co/papers/2601.13761
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14249
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14209
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13697
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12937
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10237
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13591
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13253
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13251
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13075
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12910
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10700
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13677
[22.01.2026 01:53] Extract page data from URL. URL: https://huggingface.co/papers/2601.13481
[22.01.2026 01:53] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11898
[22.01.2026 01:53] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.01.2026 01:53] No deleted papers detected.
[22.01.2026 01:53] Downloading and parsing papers (pdf, html). Total: 32.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.12993.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.12993.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.12993.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.11655.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.11655.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.11655.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13029.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13029.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13029.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14250.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14250.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14250.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14192.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14192.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14192.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13836.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13836.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13836.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.11969.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.11969.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.11969.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14004.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14004.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14004.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.11522.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.11522.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.11522.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.12294.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.12294.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.12294.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13247.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13247.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13247.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.11888.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.11888.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.11888.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13288.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13288.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13288.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14232.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14232.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14232.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14251.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14251.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14251.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14046.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14046.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14046.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13976.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13976.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13976.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13761.
[22.01.2026 01:53] Downloading paper 2601.13761 from https://arxiv.org/pdf/2601.13761v1...
[22.01.2026 01:53] Extracting affiliations from text.
[22.01.2026 01:53] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution Shengda Fan1*, Xuyan Ye1, Yankai Lin1 1 Gaoling School of Artificial Intelligence, Renmin University of China {fanshengda, yexvyan0923, yankailin}@ruc.edu.cn 6 2 0 2 0 2 ] . [ 1 1 6 7 3 1 . 1 0 6 2 : r a "
[22.01.2026 01:53] Response: ```python
["Gaoling School of Artificial Intelligence, Renmin University of China"]
```
[22.01.2026 01:53] Deleting PDF ./assets/pdf/2601.13761.pdf.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14249.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14249.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14249.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.14209.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.14209.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.14209.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13697.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13697.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13697.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.12937.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.12937.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.12937.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.10237.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.10237.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.10237.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13591.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13591.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13591.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13253.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13253.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13253.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13251.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13251.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13251.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13075.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13075.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13075.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.12910.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.12910.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.12910.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.10700.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.10700.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.10700.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13677.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.13677.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.13677.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.13481.
[22.01.2026 01:53] Downloading paper 2601.13481 from https://arxiv.org/pdf/2601.13481v1...
[22.01.2026 01:53] Extracting affiliations from text.
[22.01.2026 01:53] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1731 Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu, Qika Lin, Member, IEEE, and Jun Liu, Senior Member, IEEE AbstractLinguistic expressions of emotions, including depression, anxiety, and trauma-related states, are widespread in clinical notes, counseling dialogues, and online mental health communities. Accurate recognition of these emotions is crucial for clinical triage, risk assessment, and timely intervention in mental health related applications. Despite recent advances showing that large language models (LLMs) can generalize well to various emotion analysis tasks, their diagnostic reliability in high-stakes and context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing approaches are challenged by two major issues: emotional comorbidity, where multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these issues, we propose APOLO (Automated Prompt Optimization for Linguistic emOtion diagnosis), framework that systematically explores broader and finer-grained prompt space to enhance diagnostic efficiency and robustness. APOLO models instruction refinement as Partially Observable Markov Decision Process (POMDP) and introduces multi-agent collaboration mechanism comprising the PlannerTeacherCriticStudentTarget roles. This closed-loop design enables continuous optimization of prompt generation, evaluation, and evolution. After the Planner agent formulates high-level optimization trajectory within the POMDP framework, the TeacherCriticStudent agents collaboratively refine the prompts along this trajectory, iteratively enhancing the stability and effectiveness of the reasoning process. Finally, the Target agent determines whether to continue optimization or terminate t"
[22.01.2026 01:53] Response: ```python
[]
```
[22.01.2026 01:53] Extracting affiliations from text.
[22.01.2026 01:53] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1731 Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu, Qika Lin, Member, IEEE, and Jun Liu, Senior Member, IEEE AbstractLinguistic expressions of emotions, including depression, anxiety, and trauma-related states, are widespread in clinical notes, counseling dialogues, and online mental health communities. Accurate recognition of these emotions is crucial for clinical triage, risk assessment, and timely intervention in mental health related applications. Despite recent advances showing that large language models (LLMs) can generalize well to various emotion analysis tasks, their diagnostic reliability in high-stakes and context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing approaches are challenged by two major issues: emotional comorbidity, where multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these issues, we propose APOLO (Automated Prompt Optimization for Linguistic emOtion diagnosis), framework that systematically explores broader and finer-grained prompt space to enhance diagnostic efficiency and robustness. APOLO models instruction refinement as Partially Observable Markov Decision Process (POMDP) and introduces multi-agent collaboration mechanism comprising the PlannerTeacherCriticStudentTarget roles. This closed-loop design enables continuous optimization of prompt generation, evaluation, and evolution. After the Planner agent formulates high-level optimization trajectory within the POMDP framework, the TeacherCriticStudent agents collaboratively refine the prompts along this trajectory, iteratively enhancing the stability and effectiveness of the reasoning process. Finally, the Target agent determines whether to continue optimization or terminate the search based on performance evaluation. Experimental results demonstrate that APOLO improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, providing generalizable and scalable paradigm for trustworthy LLM applications in mental healthcare. Index TermsLinguistic Emotion Diagnosis; Emotional Comorbidity; Inefficient Exploration; Automated Prompt Optimization; Multi-Agent Collaboration; Medical Language Processing; Trustworthy Artificial IntelligenceEmotion diagnosis for mental health plays an essential role in understanding mental health conditions and tracking the progression of related disorders [1]. Emotions associated with illnesses, including depression, anxiety, and posttraumatic stress, frequently appear in clinical notes, counseling conversations, and online mental-health communities [2], [3]. These texts capture patients cognitive and affective states and play vital role in clinical triage, risk assessment, and intervention decisions [4]. Traditional emotion recognition methods, often based on shallow features or lexicons, perform reasonably well on generic sentiment tasks but struggle in medical contexts due to semantic ambiguity, implicit emotional expressions, and domain variability [5], [6], [7]. Recently, large language models (LLMs) have demonstrated remarkable capabilities in affective and psycholinguistic reasoning, achieving strong few-shot or zero-shot performance on mental healthrelated text analysis [8], * Corresponding author: Haiping Zhu and Qika Lin. Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu and Jun Liu are with the School of Computer Science and Technology, Xian Jiaotong University, Shaanxi, China, 710049. E-mail: {zhangjian062422, Asteria wzq, wang zy, fuweiping, heyucs}@stu.xjtu.edu.cn, {zhuhaiping, liukeen}@xjtu.edu.cn. Qika Lin is with Saw Swee Hock School of Public Health, National University of Singapore, Singapore 119077. E-mail: qikalin@foxmail.com. Fig. 1. Examples of disease-related emotion diagnosis under three prompting strategies: zero-shot, CoT, and APOLO. [9], [10]. However, in high-stakes clinical scenarios, LLM outputs are highly sensitive to the phrasing and structure of prompts. Minor variations in instruction design or reasoning path can lead to drastically different diagnostic outcomes [11]. For instance, cant sleep lately and dont want to see anyone implies both anxiety and depression, yet poorly designed prompt may capture only one. Handcrafted prompts, although guided by expert intuition, fail to systematically cover diverse semantic cues and implicit intentions, resulting in limited consistency and transferabil6 2 0 2 0 2 ] A . [ 1 1 8 4 3 1 . 1 0 6 2 : r IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023 1732 ity [11], [12]. Automated Prompt Optimization (APO) provides principled way to overcome these limitations by systematically exploring broader and finer-grained prompt space [13]. Through iterative search and evaluation, APO enables adaptive discovery of optimal task-specific prompts [11]. Nevertheless, generic APO methods remain inadequate for emotion diagnosis, where complex high-context semantics and safety constraints prevail [14]. As shown in Figure 1, we compare three prompting strategies for disease-related emotion recognition. The zero-shot prompt identifies only one emotion, while the Chain-of-Thought (CoT) prompt captures two, but both yield incomplete emotion diagnoses. This observation highlights that conventional prompting strategies fail to uncover the full spectrum of disease-related emotions, motivating the need for more systematic and adaptive approach to prompt optimization [15]. Such incompleteness arises from two fundamental issues: emotional comorbidity and inefficient exploration. Emotional comorbidity. Disease-related emotions often appear in intertwined and co-occurring forms, such as anxiety with depression, trauma with guilt, or anhedonia with self-blame. single utterance may convey multiple affective states that interact hierarchically or causally. Fixedtemplate prompts or single-label formulations fail to capture such dependencies [1]. For instance, the sentence feel guilty and afraid that might break down simultaneously expresses guilt and fear; without hierarchical reasoning, the model tends to detect only the dominant emotion. Existing prompt designs typically rely on flat labels or keyword cues, lacking mechanisms to model emotional co-occurrence and semantic ov"
[22.01.2026 01:53] Mistral response. {"id": "e0c02d942f1a417888419f419c48e71c", "created": 1769046831, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1363, "total_tokens": 1427, "completion_tokens": 64}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"School of Computer Science and Technology, Xi'an Jiaotong University, Shaanxi, China, 710049\",\n    \"Saw Swee Hock School of Public Health, National University of Singapore, Singapore 119077\"\n]\n```"}}]}
[22.01.2026 01:53] Response: ```python
[
    "School of Computer Science and Technology, Xi'an Jiaotong University, Shaanxi, China, 710049",
    "Saw Swee Hock School of Public Health, National University of Singapore, Singapore 119077"
]
```
[22.01.2026 01:53] Deleting PDF ./assets/pdf/2601.13481.pdf.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Downloading and parsing paper https://huggingface.co/papers/2601.11898.
[22.01.2026 01:53] Extra JSON file exists (./assets/json/2601.11898.json), skip PDF parsing.
[22.01.2026 01:53] Paper image links file exists (./assets/img_data/2601.11898.json), skip HTML parsing.
[22.01.2026 01:53] Success.
[22.01.2026 01:53] Enriching papers with extra data.
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 0. Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.  					AI-generated summary 				 We introduce Being-H0.5, a foundational Vision-Language-Act...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 1. Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.  					AI-generated summary 				 Issue resolution, a complex Software Engineering (SWE) task integral t...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 2. Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.  					AI-generated summary 				 Understanding and reasoning about the physical worl...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 3. OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.  					AI-generated summary 				 Videos convey richer information than images or text, capturing both...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 4. Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.  					AI-generated summary 				 Recent years have witnessed increasing interest ...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 5. FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.  					AI-generated summary 				 Although Multimodal Large Language Mod...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 6. A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.  					AI-generated summary 				 Existing works increasingly adopt memory-centric mec...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 7. Mechanistic interpretability is presented as an actionable framework for understanding and optimizing large language models through systematic localization, steering, and improvement methods.  					AI-generated summary 				 Mechanistic Interpretability (MI) has emerged as a vital approach to demysti...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 8. UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.  					AI-generated summary 				 Despite recent progress, medical foundation models still...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 9. ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.  					AI-generated summary 				 Reward-guided search methods have demonstrated strong potential in enhan...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 10. WorldMind addresses the modal disconnect in LLMs by autonomously building a symbolic world knowledge repository that enhances physical feasibility and task optimality through experience-based learning.  					AI-generated summary 				 Current Large Language Models (LLMs) exhibit a critical modal disc...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 11. A novel retriever training framework for agentic search that uses both local relevance and global answer correctness metrics with iterative optimization between the search agent and retriever.  					AI-generated summary 				 Agentic search has recently emerged as a powerful paradigm, where an agent ...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 12. Lightweight probes trained on hidden states of LLMs enable efficient classification tasks without additional computational overhead, improving safety and sentiment analysis performance.  					AI-generated summary 				 Production LLM systems often rely on separate models for safety and other classifi...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 13. KAGE-Env is a JAX-native 2D platformer environment that isolates visual shifts from underlying control problems, enabling systematic analysis of visual generalization in reinforcement learning.  					AI-generated summary 				 Pixel-based reinforcement learning agents often fail under purely visual d...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 14. LightOnOCR-2-1B is a compact 1B-parameter vision-language model that performs end-to-end document image-to-text conversion with improved localization and robustness through specialized training techniques.  					AI-generated summary 				 We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilin...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 15. PRiSM benchmark evaluates phonetic perception in speech models through standardized transcription-based metrics and downstream applications across clinical, educational, and multilingual domains.  					AI-generated summary 				 Phone recognition (PR) serves as the atomic interface for language-agnos...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 16. FantasyVLN presents a unified implicit reasoning framework for vision-and-language navigation that enhances reasoning capabilities without explicit token overhead, achieving real-time performance with improved accuracy.  					AI-generated summary 				 Achieving human-level performance in Vision-and-...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 17. A two-stage framework called DARC stabilizes self-play with large language models by decoupling question generation and using asymmetric self-distillation with document-augmented teachers to improve reasoning performance.  					AI-generated summary 				 Self-play with large language models has emerg...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 18. Researchers introduce a novel metric called Rank-Surprisal Ratio (RSR) to better assess the suitability of reasoning trajectories for distilling knowledge from large language models, demonstrating superior performance compared to existing methods.  					AI-generated summary 				 Long chain-of-though...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 19. Intervention Training improves large language model reasoning by enabling fine-grained credit assignment through targeted corrections that localize errors and enhance reinforcement learning performance.  					AI-generated summary 				 Outcome-reward reinforcement learning (RL) has proven effective a...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 20. GRADFILTERING is an uncertainty-aware data selection framework for instruction tuning that uses gradient signal-to-noise ratio to improve LLM adaptation efficiency and performance.  					AI-generated summary 				 Instruction tuning is a standard paradigm for adapting large language models (LLMs), bu...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 21. Membership inference attacks fail to reliably detect copyrighted text usage in large language models when training data is paraphrased using structure-aware methods that preserve semantic content.  					AI-generated summary 				 As large language models (LLMs) are trained on increasingly opaque corp...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 22. Differentially private stochastic gradient descent with shuffled sampling faces fundamental privacy-utility trade-offs that require substantial noise for meaningful privacy protection, limiting practical performance.  					AI-generated summary 				 Differentially Private Stochastic Gradient Descent ...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 23. A comprehensive benchmark for evaluating LLM-based data agents across diverse data science tasks demonstrates superior performance for multimodal agents while highlighting persistent challenges in unstructured data domains.  					AI-generated summary 				 Recent LLM-based data agents aim to automate...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 24. A hybrid methodology combining FastText embeddings, clustering, and AI classification generates a large-scale Turkish semantic relations dataset with high accuracy validation.  					AI-generated summary 				 We present a hybrid methodology for generating large-scale semantic relationship datasets in...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 25. A large-scale semantic clustering system addresses the limitation of neural embeddings in distinguishing synonyms from antonyms through a specialized three-way discriminator and novel clustering algorithm.  					AI-generated summary 				 Neural embeddings have a notorious blind spot: they can't reli...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 26. AI mentor METIS outperforms GPT-5 and Claude Sonnet 4.5 in supporting undergraduate research writing across multiple stages, with higher student scores and improved document-grounded outputs, though challenges remain in tool routing and stage classification.  					AI-generated summary 				 Many stud...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 27. SciCoQA is a dataset for identifying mismatches between scientific publications and code implementations, containing 611 discrepancies across multiple disciplines and demonstrating the challenge of detecting such issues even for advanced language models.  					AI-generated summary 				 We present Sc...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 28. A framework for generating structured counterfactual pairs using LLMs and SCMs enables improved evaluation and analysis of concept-based explanations in high-stakes domains.  					AI-generated summary 				 Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influ...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 29. Class-stratified Scheduled Power Predictive Entropy (ClaSP PE) is a novel active learning strategy that improves 3D biomedical image segmentation by addressing class imbalance and selection redundancy through stratified querying and power noising with decay scheduling.  					AI-generated summary 			...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 30. APOLO framework uses automated prompt optimization through multi-agent collaboration to improve emotion diagnosis accuracy and robustness in mental healthcare applications.  					AI-generated summary 				 Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are p...
[22.01.2026 01:53] ********************************************************************************
[22.01.2026 01:53] Abstract 31. RemoteVAR is a visual autoregressive framework for remote sensing change detection that improves upon existing methods through multi-resolution feature fusion and autoregressive training tailored for change map prediction.  					AI-generated summary 				 Remote sensing change detection aims to local...
[22.01.2026 01:53] Read previous papers.
[22.01.2026 01:53] Generating reviews via LLM API.
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#multimodal", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –≤—Å–µ—Ö —Ä–æ–±–æ—Ç–æ–≤: universal VLA —Å –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–º –æ–±–æ–±—â–µ–Ω–∏–µ–º", "desc": "Being-H0.5 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å Vision-Language-Action (VLA), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#dataset", "#agents", "#data", "#training", "#open_source", "#plp", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –æ—à–∏–±–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#open_source", "#reasoning"], "emoji": "üßä", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Think3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –∏–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ 3
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ", "desc": "OmniTransfer –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–æ—Å
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#rl", "#optimization", "#long_context", "#agents", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–∏–∑–∏—Ä
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#audio", "#video", "#benchmark", "#dataset", "#training", "#multimodal"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: —É—á–∏–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å –∑–∞–≤—Ç—Ä–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ FutureOmni ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#long_context", "#survey", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MemoryRewardBench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#alignment", "#open_source", "#survey", "#interpretability"], "emoji": "üîç", "ru": {"title": "–û—Ç –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∫ –¥–µ–π—Å—Ç–≤–∏—é: –ª–æ–∫–∞–ª–∏–∑—É–π, —É–ø—Ä–∞–≤–ª—è–π, —É–ª—É—á—à–∞–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–∞–∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã 
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#diffusion", "#training", "#data", "#architecture", "#open_source", "#multimodal", "#healthcare"], "emoji": "ü´Ä", "ru": {"title": "–°–∏–Ω–µ—Ä–≥–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏—è: —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "UniX ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å-—Ñ—É–Ω–¥–∞
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset", "#rl"], "emoji": "üîß", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ToolPRMBench ‚Äî –º–∞—Å—à—Ç–∞–±–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#transfer_learning", "#alignment", "#hallucinations", "#reasoning"], "emoji": "üåç", "ru": {"title": "–ì—Äounded –º–∏—Ä –¥–ª—è LLM: –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å—É—â–µ—Å—Ç–≤–∏–º—ã–º –ø–ª–∞–Ω–∞–º", "desc": "WorldMind —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–∞–ª—å–Ω–æ–π —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#training", "#rag"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–∏—Å–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—èÊ°ÜÊû∂ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è retriever'–∞, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–æ–≤. 
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#training", "#inference", "#small_models"], "emoji": "üîç", "ru": {"title": "–õ—ë–≥–∫–∏–µ –∑–æ–Ω–¥—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ª—ë–≥–∫–∏—Ö –∑–æ–Ω–¥–æ–≤ (probes) –Ω–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#optimization", "#cv", "#games", "#benchmark", "#rl"], "emoji": "üéÆ", "ru": {"title": "–ß–∏—Å—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞–±–ª—é–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ KAGE-Env ‚Äî —Å—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ JAX, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#training", "#multimodal", "#small_models", "#dataset", "#cv", "#multilingual", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–õ—ë–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å –µ–¥–∏–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LightOnOCR-2-1B ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å —Å 1 –º
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#open_source", "#audio", "#dataset", "#low_resource", "#multilingual", "#benchmark"], "emoji": "üî§", "ru": {"title": "–ì–ª—É–±–æ–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ PRiSM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Ñ–æ–Ω–µ—Ç–∏–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–∞—Å–ø–æ
[22.01.2026 01:53] Using data from previous issue: {"categories": ["#training", "#agents", "#interpretability", "#multimodal", "#reasoning", "#architecture"], "emoji": "üß≠", "ru": {"title": "–°–∫—Ä—ã—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "FantasyVLN –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑—Ä–µ–Ω–∏—è
[22.01.2026 01:53] Querying the API.
[22.01.2026 01:53] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A two-stage framework called DARC stabilizes self-play with large language models by decoupling question generation and using asymmetric self-distillation with document-augmented teachers to improve reasoning performance.  					AI-generated summary 				 Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.
[22.01.2026 01:53] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ DARC –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—É—é —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é, –≥–¥–µ —É—á–∏—Ç–µ–ª—å —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Å–µ–≤–¥–æ–ª–µ–π–±–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—á–µ–Ω–∏–∫–∞ –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –≠—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –æ—à–∏–±–æ–∫ —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞ 10.9 –ø—É–Ω–∫—Ç–∞ –ø–æ –¥–µ–≤—è—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.",
  "emoji": "üéì",
  "title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –¥–µ–∫—É–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ"
}
```
[22.01.2026 01:53] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A two-stage framework called DARC stabilizes self-play with large language models by decoupling question generation and using asymmetric self-distillation with document-augmented teachers to improve reasoning performance.  					AI-generated summary 				 Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC."

[22.01.2026 01:53] Response: ```python
['TRAINING', 'RAG', 'BENCHMARK']
```

**Justification:**

- **TRAINING**: The paper focuses on improving model training methods through a two-stage framework (DARC) that addresses optimization instability and introduces asymmetric self-distillation mechanisms for better fine-tuning.

- **RAG**: The approach uses document-augmented teachers and external corpora to generate pseudo-labels and improve reasoning, which is a form of retrieval-augmented generation.

- **BENCHMARK**: The paper evaluates performance across nine reasoning benchmarks and three backbone models, demonstrating empirical results on established evaluation frameworks.
[22.01.2026 01:53] Error. Failed to parse JSON from LLM. ["TRAINING", "RAG", "BENCHMARK"]


**Justification:**

- **TRAINING**: The paper focuses on improving model training methods through a two-stage framework (DARC) that addresses optimization instability and introduces asymmetric self-distillation mechanisms for better fine-tuning.

- **RAG**: The approach uses document-augmented teachers and external corpora to generate pseudo-labels and improve reasoning, which is a form of retrieval-augmented generation.

- **BENCHMARK**: The paper evaluates performance across nine reasoning benchmarks and three backbone models, demonstrating empirical results on established evaluation frameworks.
[22.01.2026 01:53] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A two-stage framework called DARC stabilizes self-play with large language models by decoupling question generation and using asymmetric self-distillation with document-augmented teachers to improve reasoning performance.  					AI-generated summary 				 Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC."

[22.01.2026 01:53] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[22.01.2026 01:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents DARC, a two-stage framework designed to enhance the stability of self-play in large language models. It addresses issues of optimization instability caused by non-stationary objectives and bootstrapping errors in self-generated labels. DARC first trains a Questioner to create questions of varying difficulty, using external resources for calibration. Then, it employs asymmetric self-distillation, where a document-augmented teacher provides high-quality labels to guide the Solver, leading to significant improvements in reasoning performance across multiple benchmarks.","title":"Stabilizing Self-Play with DARC: A New Era for Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents DARC, a two-stage framework designed to enhance the stability of self-play in large language models. It addresses issues of optimization instability caused by non-stationary objectives and bootstrapping errors in self-generated labels. DARC first trains a Questioner to create questions of varying difficulty, using external resources for calibration. Then, it employs asymmetric self-distillation, where a document-augmented teacher provides high-quality labels to guide the Solver, leading to significant improvements in reasoning performance across multiple benchmarks.', title='Stabilizing Self-Play with DARC: A New Era for Language Models'))
[22.01.2026 01:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DARCÔºàËß£ËÄ¶‰∏çÂØπÁß∞Êé®ÁêÜËØæÁ®ãÔºâÊòØ‰∏Ä‰∏™‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËß£ËÄ¶ÈóÆÈ¢òÁîüÊàêÊù•Á®≥ÂÆöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËá™ÊàëÂØπÂºà„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàËÆ≠ÁªÉÊèêÈóÆËÄÖÁîüÊàêÈöæÂ∫¶Ê†°ÂáÜÁöÑÈóÆÈ¢òÔºåÁÑ∂Âêé‰ΩøÁî®‰∏çÂØπÁß∞Ëá™Ëí∏È¶èÊú∫Âà∂ËÆ≠ÁªÉÊ±ÇËß£ËÄÖÔºå‰ª•ÊèêÈ´òÊé®ÁêÜÊÄßËÉΩ„ÄÇÈÄöËøáÂºïÂÖ•ÊñáÊ°£Â¢ûÂº∫ÊïôÂ∏àÔºåDARCËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑ‰º™Ê†áÁ≠æÊù•ÊåáÂØºÊ±ÇËß£ËÄÖ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDARCÂú®‰πù‰∏™Êé®ÁêÜÂü∫ÂáÜÂíå‰∏â‰∏™Âü∫Á°ÄÊ®°Âûã‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü10.9ÂàÜÔºå‰∏îÂú®‰∏ç‰æùËµñ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊÄßËÉΩÊé•ËøëÂÆåÂÖ®ÁõëÁù£Ê®°Âûã„ÄÇ","title":"DARCÔºöÁ®≥ÂÆöËá™ÊàëÂØπÂºàÁöÑÂàõÊñ∞Ê°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DARCÔºàËß£ËÄ¶‰∏çÂØπÁß∞Êé®ÁêÜËØæÁ®ãÔºâÊòØ‰∏Ä‰∏™‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËß£ËÄ¶ÈóÆÈ¢òÁîüÊàêÊù•Á®≥ÂÆöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËá™ÊàëÂØπÂºà„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàËÆ≠ÁªÉÊèêÈóÆËÄÖÁîüÊàêÈöæÂ∫¶Ê†°ÂáÜÁöÑÈóÆÈ¢òÔºåÁÑ∂Âêé‰ΩøÁî®‰∏çÂØπÁß∞Ëá™Ëí∏È¶èÊú∫Âà∂ËÆ≠ÁªÉÊ±ÇËß£ËÄÖÔºå‰ª•ÊèêÈ´òÊé®ÁêÜÊÄßËÉΩ„ÄÇÈÄöËøáÂºïÂÖ•ÊñáÊ°£Â¢ûÂº∫ÊïôÂ∏àÔºåDARCËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑ‰º™Ê†áÁ≠æÊù•ÊåáÂØºÊ±ÇËß£ËÄÖ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDARCÂú®‰πù‰∏™Êé®ÁêÜÂü∫ÂáÜÂíå‰∏â‰∏™Âü∫Á°ÄÊ®°Âûã‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü10.9ÂàÜÔºå‰∏îÂú®‰∏ç‰æùËµñ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊÄßËÉΩÊé•ËøëÂÆåÂÖ®ÁõëÁù£Ê®°Âûã„ÄÇ', title='DARCÔºöÁ®≥ÂÆöËá™ÊàëÂØπÂºàÁöÑÂàõÊñ∞Ê°ÜÊû∂'))
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#data", "#training", "#reasoning", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å–∏–≥–Ω–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Rank-Surprisal Ratio (RSR) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–µ–∫—Ç–æ
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#small_models", "#training"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞ –æ—à–∏–±–∫–∏ –≤ —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç Intervention Training (InT) ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#data", "#training", "#small_models"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM", "desc": "GRADFILTERING ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#training", "#security", "#leakage"], "emoji": "üîì", "ru": {"title": "–•—Ä—É–ø–∫–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ —Å–µ–º–∞–Ω—Ç–∏–∫–æ-—Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∞—Ç–∞–∫ –≤—ã–≤–æ–¥–∞ —á–ª–µ–Ω—Å—Ç–≤–∞ (MIA) –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞—â–∏—â—ë–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä—Å–∫–∏–º –ø—Ä–∞–≤–æ–º —Ç–µ–∫—Å—Ç–∞ –ø
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#security", "#optimization", "#training", "#math"], "emoji": "üîê", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é –≤ DP-SGD", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–∏–≤–∞—Ç–Ω—ã–π —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (DP-SGD) –≤ —Ä–∞–º–∫–∞—Ö f-–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –ø—Ä–∏
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning", "#agents", "#survey", "#multimodal"], "emoji": "üìä", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –≤ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –±–æ—Ä—é—Ç—Å—è —Å –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DSAEval ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#multilingual", "#data", "#low_resource", "#dataset", "#open_source", "#synthetic"], "emoji": "üáπüá∑", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Ç—É—Ä–µ—Ü–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª
[22.01.2026 01:54] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–†–∞–∑–ª–∏—á–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–Ω—Ç–æ–Ω–∏–º–æ–≤: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥—Ä–µ–π—Ñ–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–Ω—Ç–æ–Ω–∏–º—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#benchmark", "#training", "#agents", "#science"], "emoji": "üìö", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ò–ò-–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –ø–∏—Å—å–º–∞ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ METIS ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#long_context", "#hallucinations", "#synthetic", "#science"], "emoji": "üîç", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –æ—à–∏–±–æ–∫: –∫–æ–≥–¥–∞ –∫–æ–¥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–µ", "desc": "SciCoQA ‚Äî —ç—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏—è—Ö –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#healthcare", "#interpretability", "#ethics", "#benchmark", "#dataset", "#synthetic"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç—ã", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç LIBERTy ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–∏
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#data", "#cv", "#benchmark", "#healthcare"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä: –∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è 3D —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ClaSP PE –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã
[22.01.2026 01:54] Querying the API.
[22.01.2026 01:54] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

APOLO framework uses automated prompt optimization through multi-agent collaboration to improve emotion diagnosis accuracy and robustness in mental healthcare applications.  					AI-generated summary 				 Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.
[22.01.2026 01:54] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ APOLO –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ (prompts) –∫ –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Å —Ü–µ–ª—å—é —É–ª—É—á—à–µ–Ω–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã-–ø–ª–∞–Ω–∏ÃÅ—Ä–æ–≤—â–∏–∫, —É—á–∏—Ç–µ–ª—å, –∫—Ä–∏—Ç–∏–∫, —Å—Ç—É–¥–µ–Ω—Ç –∏ —Ü–µ–ª–µ–≤–æ–π –∞–≥–µ–Ω—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏, —Ñ–æ—Ä–º—É–ª–∏—Ä—É—è –ø—Ä–æ—Ü–µ—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ —á–∞—Å—Ç–∏—á–Ω–æ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–π –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. APOLO —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–º–æ—Ä–±–∏–¥–Ω–æ—Å—Ç—å (–∫–æ–≥–¥–∞ —Å–æ—Å—É—â–µ—Å—Ç–≤—É—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π) –∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å—Ç–∞–±–∏–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö.",
  "emoji": "üß†",
  "title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π"
}
```
[22.01.2026 01:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"APOLO framework uses automated prompt optimization through multi-agent collaboration to improve emotion diagnosis accuracy and robustness in mental healthcare applications.  					AI-generated summary 				 Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare."

[22.01.2026 01:54] Response: ```python
['AGENTS', 'HEALTHCARE', 'TRAINING']
```
[22.01.2026 01:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"APOLO framework uses automated prompt optimization through multi-agent collaboration to improve emotion diagnosis accuracy and robustness in mental healthcare applications.  					AI-generated summary 				 Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare."

[22.01.2026 01:54] Response: ```python
['OPTIMIZATION', 'REASONING', 'SCIENCE']
```
[22.01.2026 01:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The APOLO framework enhances emotion diagnosis in mental healthcare by using automated prompt optimization through a multi-agent system. It addresses the challenges of emotional comorbidity and the need for efficient exploration of relevant cues in clinical settings. By treating prompt refinement as a Partially Observable Markov Decision Process, it employs roles like Planner, Teacher, Critic, and Student to iteratively improve prompts. Experimental results indicate that APOLO significantly boosts diagnostic accuracy and robustness, making it a promising tool for reliable applications of large language models in mental health.","title":"Optimizing Prompts for Better Emotion Diagnosis in Mental Health"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The APOLO framework enhances emotion diagnosis in mental healthcare by using automated prompt optimization through a multi-agent system. It addresses the challenges of emotional comorbidity and the need for efficient exploration of relevant cues in clinical settings. By treating prompt refinement as a Partially Observable Markov Decision Process, it employs roles like Planner, Teacher, Critic, and Student to iteratively improve prompts. Experimental results indicate that APOLO significantly boosts diagnostic accuracy and robustness, making it a promising tool for reliable applications of large language models in mental health.', title='Optimizing Prompts for Better Emotion Diagnosis in Mental Health'))
[22.01.2026 01:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"APOLOÊ°ÜÊû∂ÈÄöËøáÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÂÆûÁé∞Ëá™Âä®ÂåñÊèêÁ§∫‰ºòÂåñÔºå‰ª•ÊèêÈ´òÂøÉÁêÜÂÅ•Â∫∑Â∫îÁî®‰∏≠ÁöÑÊÉÖÊÑüËØäÊñ≠ÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÊÉÖÊÑüÂÖ±ÁóÖÂíå‰∏¥Â∫äÁõ∏ÂÖ≥Á∫øÁ¥¢Êé¢Á¥¢ÊïàÁéá‰Ωé‰∏ãÁöÑÊåëÊàò„ÄÇAPOLOÂ∞ÜÊåá‰ª§‰ºòÂåñËßÜ‰∏∫ÈÉ®ÂàÜÂèØËßÇÂØüÁöÑÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÂπ∂ÈááÁî®Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊú∫Âà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAPOLOÂú®ÁâπÂÆöÈ¢ÜÂüüÂíåÂàÜÂ±ÇÂü∫ÂáÜÊµãËØï‰∏≠ÊåÅÁª≠ÊèêÈ´ò‰∫ÜËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ","title":"APOLOÔºöÊèêÂçáÂøÉÁêÜÂÅ•Â∫∑ÊÉÖÊÑüËØäÊñ≠ÁöÑÊô∫ËÉΩÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='APOLOÊ°ÜÊû∂ÈÄöËøáÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÂÆûÁé∞Ëá™Âä®ÂåñÊèêÁ§∫‰ºòÂåñÔºå‰ª•ÊèêÈ´òÂøÉÁêÜÂÅ•Â∫∑Â∫îÁî®‰∏≠ÁöÑÊÉÖÊÑüËØäÊñ≠ÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÊÉÖÊÑüÂÖ±ÁóÖÂíå‰∏¥Â∫äÁõ∏ÂÖ≥Á∫øÁ¥¢Êé¢Á¥¢ÊïàÁéá‰Ωé‰∏ãÁöÑÊåëÊàò„ÄÇAPOLOÂ∞ÜÊåá‰ª§‰ºòÂåñËßÜ‰∏∫ÈÉ®ÂàÜÂèØËßÇÂØüÁöÑÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºåÂπ∂ÈááÁî®Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊú∫Âà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAPOLOÂú®ÁâπÂÆöÈ¢ÜÂüüÂíåÂàÜÂ±ÇÂü∫ÂáÜÊµãËØï‰∏≠ÊåÅÁª≠ÊèêÈ´ò‰∫ÜËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ', title='APOLOÔºöÊèêÂçáÂøÉÁêÜÂÅ•Â∫∑ÊÉÖÊÑüËØäÊñ≠ÁöÑÊô∫ËÉΩÊ°ÜÊû∂'))
[22.01.2026 01:54] Using data from previous issue: {"categories": ["#open_source", "#optimization"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–∞—Ö", "desc": "RemoteVAR ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —Å–Ω–∏–º–∫–∞—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–≥–æ 
[22.01.2026 01:54] Renaming data file.
[22.01.2026 01:54] Renaming previous data. hf_papers.json to ./d/2026-01-22.json
[22.01.2026 01:54] Saving new data file.
[22.01.2026 01:54] Generating page.
[22.01.2026 01:54] Renaming previous page.
[22.01.2026 01:54] Renaming previous data. index.html to ./d/2026-01-22.html
[22.01.2026 01:54] Writing result.
[22.01.2026 01:54] Renaming log file.
[22.01.2026 01:54] Renaming previous data. log.txt to ./logs/2026-01-22_last_log.txt
