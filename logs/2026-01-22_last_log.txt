[22.01.2026 22:25] Read previous papers.
[22.01.2026 22:25] Generating top page (month).
[22.01.2026 22:25] Writing top page (month).
[22.01.2026 23:23] Read previous papers.
[22.01.2026 23:23] Get feed.
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12538
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12346
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15282
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14171
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13572
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14750
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14490
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14722
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13044
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14027
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11141
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07853
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14245
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15220
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14352
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14417
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14256
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13918
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14681
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14152
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15059
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15100
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12029
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11387
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14253
[22.01.2026 23:23] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13262
[22.01.2026 23:23] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.01.2026 23:23] No deleted papers detected.
[22.01.2026 23:23] Downloading and parsing papers (pdf, html). Total: 26.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.12538.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.12538.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.12538.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.12346.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.12346.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.12346.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.15282.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.15282.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.15282.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14171.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14171.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14171.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.13572.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.13572.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.13572.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14750.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14750.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14750.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14490.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14490.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14490.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14722.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14722.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14722.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.13044.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.13044.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.13044.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14027.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14027.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14027.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.11141.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.11141.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.11141.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.07853.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.07853.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.07853.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14245.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14245.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14245.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.15220.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.15220.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.15220.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14352.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14352.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14352.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14417.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14417.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14417.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14256.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14256.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14256.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.13918.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.13918.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.13918.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14681.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14681.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14681.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14152.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14152.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14152.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.15059.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.15059.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.15059.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.15100.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.15100.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.15100.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.12029.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.12029.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.12029.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.11387.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.11387.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.11387.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.14253.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.14253.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.14253.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Downloading and parsing paper https://huggingface.co/papers/2601.13262.
[22.01.2026 23:23] Extra JSON file exists (./assets/json/2601.13262.json), skip PDF parsing.
[22.01.2026 23:23] Paper image links file exists (./assets/img_data/2601.13262.json), skip HTML parsing.
[22.01.2026 23:23] Success.
[22.01.2026 23:23] Enriching papers with extra data.
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 0. Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.  					AI-generated summary 				 Reasoning is a fundamental cognitive process underly...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 1. MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.  					AI-generated summary 				 Deep Research Agents (DRAs) generate citation-rich reports via multi-step search a...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 2. A comprehensive robotics benchmark evaluates video generation models across multiple task domains and embodiments, revealing deficiencies in physical realism and introducing a large-scale dataset to address training data shortages.  					AI-generated summary 				 Video generation models have signifi...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 3. RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.  					AI-generated summary 				 Writing effective rebuttals is a high-stakes task that demands more than...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 4. Reinforced Agent Merging (RAM) addresses the limitations of traditional merging methods for reinforcement learning-trained agents by distinguishing shared and task-specific parameters to preserve critical behaviors during model integration.  					AI-generated summary 				 Reinforcement learning (RL)...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 5. Render-of-Thought framework converts textual reasoning steps into images using vision-language models to improve reasoning traceability and efficiency while maintaining competitive performance.  					AI-generated summary 				 Chain-of-Thought (CoT) prompting has achieved remarkable success in unlock...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 6. GutenOCR enhances vision-language models for document understanding by enabling unified reading, detection, and grounding through prompt-based interfaces trained on diverse document types.  					AI-generated summary 				 GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 7. Thai-focused vision-language model for document extraction combining OCR, layout reconstruction, and structural consistency with reduced computational requirements.  					AI-generated summary 				 Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 8. A 115M-parameter FastConformer-Transducer model achieves low-latency Thai speech recognition with reduced computational cost through text normalization and curriculum learning, accompanied by a benchmark dataset for standardized evaluation.  					AI-generated summary 				 Large encoder-decoder model...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 9. A general coding agent paradigm enables flexible formal theorem proving by directly interfacing with proof assistants and retrieving relevant theorems without task-specific training.  					AI-generated summary 				 Agentic systems have recently become the dominant paradigm for formal theorem proving...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 10. Chroma 1.0 enables real-time spoken dialogue with personalized voice cloning through discrete speech representations and interleaved text-audio token scheduling.  					AI-generated summary 				 Recent end-to-end spoken dialogue systems leverage speech tokenizers and neural audio codecs to enable LLM...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 11. FinVault presents the first execution-grounded security benchmark for financial agents, revealing significant vulnerabilities in current defense mechanisms when applied to real-world financial workflows.  					AI-generated summary 				 Financial agents powered by large language models (LLMs) are inc...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 12. A multi-agent framework for compositional image retrieval that uses specialized agents for generation, filtering, and verification to improve semantic and visual query matching.  					AI-generated summary 				 Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventi...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 13. Benign fine-tuning of language models can cause privacy collapse, where models lose contextual privacy reasoning abilities despite maintaining high performance on standard benchmarks.  					AI-generated summary 				 We identify a novel phenomenon in language models: benign fine-tuning of frontier mo...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 14. RoboBrain 2.5 enhances embodied AI through improved 3D spatial reasoning and temporal value estimation for more precise manipulation tasks.  					AI-generated summary 				 We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, ...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 15. Research investigates the relationship between speaker embeddings and phonological rules in accent control for text-to-speech systems, introducing a metric to measure rule preservation versus embedding influence.  					AI-generated summary 				 Many spoken languages, including English, exhibit wide ...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 16. A unified model learns image representations useful for both recognition and generation by using a hyper-network for implicit neural representation with knowledge distillation, achieving state-of-the-art results while enabling generative capabilities through compressed embeddings.  					AI-generated...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 17. AgentEHR presents a benchmark for autonomous EHR navigation requiring complex decision-making, while RetroSum framework improves performance through retrospective summarization and evolving experience strategies.  					AI-generated summary 				 Large Language Models have demonstrated profound utilit...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 18. FARE is a hierarchical exploration framework that combines large language model reasoning with reinforcement learning control to enable efficient autonomous robot navigation in complex environments.  					AI-generated summary 				 This work advances autonomous robot exploration by integrating agent-...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 19. Research reveals that causal attention in language models creates information bottlenecks when question-answer options follow context, leading to performance drops of over 14 percentage points compared to reversed prompt ordering.  					AI-generated summary 				 Large language models exhibit surpris...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 20. Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully und...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 21. WebSeek is a mixed-initiative browser extension that enables interactive web data extraction and analysis with AI-assisted guidance and automation.  					AI-generated summary 				 Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely ...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 22. The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using ...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 23. Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 24. Motion 3-to-4 synthesizes high-quality 4D dynamic objects from monocular video and 3D mesh by decomposing into static shape generation and motion reconstruction with canonical mesh and transformer-based frame processing.  					AI-generated summary 				 We present Motion 3-to-4, a feed-forward framew...
[22.01.2026 23:23] ********************************************************************************
[22.01.2026 23:23] Abstract 25. A multilingual medical reasoning framework using curriculum-informed reinforcement learning achieves high language consistency and logical correctness across thirteen languages including underrepresented ones.  					AI-generated summary 				 While large language models (LLMs) have shown to perform w...
[22.01.2026 23:23] Read previous papers.
[22.01.2026 23:23] Generating reviews via LLM API.
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#robotics", "#healthcare", "#training", "#survey", "#math", "#benchmark", "#science", "#rl", "#reasoning", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫ –¥–µ–π—Å—Ç–≤–∏—é: LLM –∫–∞–∫ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º –º–∏—Ä–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –∏—Å–ø–æ–ª—å–∑–æ
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents", "#science", "#interpretability", "#multimodal", "#rag"], "emoji": "üìä", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º, —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ MMDeepResearch
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#robotics", "#video", "#benchmark", "#dataset", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –æ—Ü–µ–Ω–∫–∏ –∫ –¥–∞–Ω–Ω—ã–º: –±–µ–Ω—á–º–∞—Ä–∫ –∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RBench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ —Ä–æ–±–æ
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#open_source", "#hallucinations", "#reasoning", "#agents"], "emoji": "üîç", "ru": {"title": "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π", "desc": "RebuttalAgent ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–µ
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#training", "#rl", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ: —Ä–∞–∑–ª–∏—á–∞–π –∏ —Å–æ—Ö—Ä–∞–Ω—è–π –≤ RL-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Reinforced Agent Merging (RAM) –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤ –µ–¥–∏–Ω—É—é
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#reasoning", "#interpretability", "#open_source"], "emoji": "üé®", "ru": {"title": "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –æ–±—Ä–∞–∑–∞–º –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Render-of-Thought (RoT) ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#cv", "#dataset", "#science", "#open_source", "#training", "#synthetic", "#multimodal", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è, –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–∏–≤—è–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö", "desc": "GutenOCR ‚Äî —ç—Ç–æ —Å–µ–º—å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#dataset", "#inference", "#open_source", "#synthetic", "#low_resource", "#cv", "#multilingual", "#small_models", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–∞–π—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –±–æ–ª—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤", "desc": "–†–∞–±–æ—Ç–∞
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#dataset", "#training", "#open_source", "#audio", "#low_resource", "#benchmark", "#multilingual", "#small_models", "#optimization"], "emoji": "üé§", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–∞–π—Å–∫–æ–π —Ä–µ—á–∏ —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –∏ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "–ê–≤—Ç–æ—Ä—ã –ø
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#reasoning", "#science", "#agents", "#benchmark", "#plp", "#math", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#open_source"], "emoji": "üó£Ô∏è", "ru": {"title": "–ì–æ–ª–æ—Å —Å –¥—É—à–æ–π ‚Äî –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "Chroma 1.0 ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —É—Å—Ç–Ω–æ–π –¥–∏–∞–ª–æ–≥–æ–≤–æ–π —Ä–µ—á–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–µ—á–∏ –∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥
[22.01.2026 23:23] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "FinVault –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø—Ä–æ–≤–µ—Ä—è—è –∏—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#rag", "#multimodal", "#agents"], "emoji": "üîç", "ru": {"title": "–ö–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ XR –¥–ª—è —Å–æ—Å—Ç–∞–≤–Ω–æ–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Å–∏—Å—Ç–µ–º—É —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#agents", "#ethics", "#alignment", "#security", "#benchmark", "#training"], "emoji": "üîê", "ru": {"title": "–ú–æ–ª—á–∞–ª–∏–≤—ã–π –≤—Ä–∞–≥: –∫–∞–∫ fine-tuning –∫—Ä—É—à–∏—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –≤–æ –≤—Ä–µ–º—è –æ–±—É
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#architecture", "#3d", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è —Ç–æ—á–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboBrain 2.5 ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å-–æ—Å–Ω–æ–≤–∞ –¥–ª—è –≤–æ–ø–ª–æ—â—ë–Ω–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–≤–∏–≤–∞–µ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#audio", "#multimodal"], "emoji": "üéôÔ∏è", "ru": {"title": "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ü–µ–Ω—Ç–∞ –∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏: —Ñ–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –≥–æ–≤–æ—Ä—è—â–∏—Ö –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ñ–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Å–∏
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#training", "#cv", "#architecture", "#open_source", "#optimization"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≥–∏–ø–µ—Ä—Å–µ—Ç—å –Ω–µ—è–≤–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#benchmark", "#healthcare", "#training", "#agents"], "emoji": "üè•", "ru": {"title": "–†–µ—Ç—Ä–æ—Å–ø–µ–∫—Ç–∏–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ LLM –ø–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º –¥–∞–Ω–Ω—ã–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ AgentEHR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ 
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#robotics", "#graphs", "#training", "#rl", "#reasoning", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞: LLM –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, RL –¥–ª—è –¥–µ–π—Å—Ç–≤–∏—è", "desc": "FARE ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–æ–π —è
[22.01.2026 23:23] Using data from previous issue: {"categories": [], "emoji": "üîó", "ru": {"title": "–ü–æ—Ä—è–¥–æ–∫ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —É–∑–∫–∏–µ –º–µ—Å—Ç–∞ –≤ –ø—Ä–∏—á–∏–Ω–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏—á–∏–Ω–Ω–∞—è –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å–æ–∑–¥–∞—ë—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —É–∑–∫–∏–µ –º–µ—Å—Ç–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤ 
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#agents"], "emoji": "‚öñÔ∏è", "ru": {"title": "–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ–Ω–∏–º–∞–Ω–∏—è: —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –≤ CI/CD", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ ¬´–≤–∞–∫—É—É–º–∞ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏¬ª –≤ CI/CD –∫–æ–Ω–≤–µ–π–µ—Ä–∞—Ö, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∫–æ–¥ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ª—é–¥–∏ –º–æ–≥—É—Ç –µ–≥–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å. –ê–≤
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#multimodal", "#agents"], "emoji": "üï∑Ô∏è", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–µ–±-—Å–∫—Ä–µ–ø–∏–Ω–≥ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AI", "desc": "WebSeek ‚Äî —ç—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞, –∫–æ—Ç–æ—Ä–æ–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ-–º–∞—à–∏–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#science", "#math", "#open_source"], "emoji": "üåä", "ru": {"title": "–ß–∏—Å–ª–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–ª–∏—Ç–æ–Ω–æ–≤ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç sangkuriang ‚Äî –æ—Ç–∫—Ä—ã—Ç—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –Ω–∞ Python –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏—è –ö–æ—Ä—Ç–µ–≤–µ–≥–∞-–¥–µ –§—Ä–∏–∑–∞, –∫–æ—Ç–æ—Ä–æ–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω
[22.01.2026 23:23] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–†–æ–ª—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤ –æ—Ü–µ–Ω–∫–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "–í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–∑—É—á–∞–µ—Ç—Å—è —Ä–æ–ª—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –º–æ–¥–µ–ª—è–º–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π —ç–∫
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#video", "#3d"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ –∫ —á–µ—Ç–≤—ë—Ä—Ç–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é: —Å–∏–Ω—Ç–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —Ñ–æ—Ä–º—É –∏ –¥–≤–∏–∂–µ–Ω–∏–µ", "desc": "Motion 3-to-4 ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ 
[22.01.2026 23:23] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#rlhf", "#multilingual", "#rl", "#science", "#training", "#benchmark", "#optimization", "#healthcare", "#open_source", "#low_resource"], "emoji": "üåç", "ru": {"title": "–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ LLM —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc
[22.01.2026 23:23] Renaming data file.
[22.01.2026 23:23] Renaming previous data. hf_papers.json to ./d/2026-01-22.json
[22.01.2026 23:23] Saving new data file.
[22.01.2026 23:23] Generating page.
[22.01.2026 23:23] Renaming previous page.
[22.01.2026 23:23] Renaming previous data. index.html to ./d/2026-01-22.html
[22.01.2026 23:23] Writing result.
[22.01.2026 23:23] Renaming log file.
[22.01.2026 23:23] Renaming previous data. log.txt to ./logs/2026-01-22_last_log.txt
