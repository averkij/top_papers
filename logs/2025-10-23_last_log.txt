[23.10.2025 03:33] Read previous papers.
[23.10.2025 03:33] Generating top page (month).
[23.10.2025 03:33] Writing top page (month).
[23.10.2025 04:14] Read previous papers.
[23.10.2025 04:14] Get feed.
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19363
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19338
[23.10.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2510.18927
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19336
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19307
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19386
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19286
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19817
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19808
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19457
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19316
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18313
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16844
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19488
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18940
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18428
[23.10.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2510.18941
[23.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18917
[23.10.2025 04:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.10.2025 04:14] No deleted papers detected.
[23.10.2025 04:14] Downloading and parsing papers (pdf, html). Total: 18.
[23.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.19363.
[23.10.2025 04:14] Extra JSON file exists (./assets/json/2510.19363.json), skip PDF parsing.
[23.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.19363.json), skip HTML parsing.
[23.10.2025 04:14] Success.
[23.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.19338.
[23.10.2025 04:14] Extra JSON file exists (./assets/json/2510.19338.json), skip PDF parsing.
[23.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.19338.json), skip HTML parsing.
[23.10.2025 04:14] Success.
[23.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.18927.
[23.10.2025 04:14] Downloading paper 2510.18927 from http://arxiv.org/pdf/2510.18927v1...
[23.10.2025 04:15] Extracting affiliations from text.
[23.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-10-23 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Zhiheng Xi1, Xin Guo1, Yang Nan1, Enyu Zhou1, Junrui Shen1, Wenxiang Chen1, Jiaqi Liu1, Jixuan Huang1, Zhihao Zhang1, Honglin Guo1, Xun Deng2, Zhikai Lei2, Miao Zheng2, Guoteng Wang2, Shuo Zhang2, Peng Sun2, Rui Zheng2, Hang Yan2, Tao Gui1,3, Qi Zhang1, Xuanjing Huang1 1Fudan University 2Shanghai Qiji Zhifeng Co., Ltd. 3Shanghai Innovation Institute zhxi22@m.fudan.edu.cn, {tgui,qz}@fudan.edu.cn Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settingswhere stale data from past policies are used for trainingimproves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenariosincluding sample replay and partial rolloutBAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale "
[23.10.2025 04:15] Response: ```python
["Fudan University", "Shanghai Qiji Zhifeng Co., Ltd.", "Shanghai Innovation Institute"]
```
[23.10.2025 04:15] Deleting PDF ./assets/pdf/2510.18927.pdf.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19336.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19336.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19336.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19307.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19307.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19307.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19386.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19386.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19386.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19286.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19286.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19286.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19817.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19817.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19817.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19808.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19808.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19808.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19457.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19457.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19457.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19316.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19316.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19316.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18313.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18313.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18313.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.16844.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.16844.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.16844.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.19488.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.19488.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.19488.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18940.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18940.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18940.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18428.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18428.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18428.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18941.
[23.10.2025 04:15] Downloading paper 2510.18941 from http://arxiv.org/pdf/2510.18941v1...
[23.10.2025 04:15] Extracting affiliations from text.
[23.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 1 4 9 8 1 . 0 1 5 2 : r Preprint. Under Review. PROFBENCH: MULTI-DOMAIN RUBRICS REQUIRING PROFESSIONAL KNOWLEDGE TO ANSWER AND JUDGE Zhilin Wang, Jaehun Jung, Ximing Lu, Shizhe Diao, Ellie Evans, Jiaqi Zeng, Pavlo Molchanov, Yejin Choi, Jan Kautz, Yi Dong NVIDIA {zhilinw, yidong}@nvidia.com "
[23.10.2025 04:15] Response: ```python
["NVIDIA"]
```
[23.10.2025 04:15] Deleting PDF ./assets/pdf/2510.18941.pdf.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.18917.
[23.10.2025 04:15] Extra JSON file exists (./assets/json/2510.18917.json), skip PDF parsing.
[23.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.18917.json), skip HTML parsing.
[23.10.2025 04:15] Success.
[23.10.2025 04:15] Enriching papers with extra data.
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 0. LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.  					AI-generated summary 				 Reasoning over long contexts is essential for large lan...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 1. The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.  					AI-generated summary 				 In this technical report, we present the Ring-linear mo...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 2. BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  					AI-generated summary 				 Reinforcement learning (RL)...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 3. DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.  					AI-generated summary 				 Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability a...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 4. A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them i...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 5. ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.  					AI-generated summary 				 With the advancements in hardware, software, and large language...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 6. TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.  					AI-generated summary 				 Since the introduction of the Model Context Proto...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 7. olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.  					AI-generated summary 				 We present olmOCR 2, the latest in our f...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 8. Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.  					AI-generated summary 				 Recent advances in mu...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 9. Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' abil...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 10. KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.  					AI-generated summary 				 Large Multimodal Models encode extensive factual knowledge i...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 11. OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.  					AI-generated summary 				 Autonomous driving ...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 12. FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.  					AI-generated summary 				 Generating professional financial repo...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 13. VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.  					AI-generated summary 				 Training computer-use agents requires massive amounts of GUI interact...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 14. NeuroAda is a parameter-efficient fine-tuning method that combines selective adaptation with bypass connections to achieve high performance with minimal trainable parameters and reduced memory usage.  					AI-generated summary 				 Existing parameter-efficient fine-tuning (PEFT) methods primarily fa...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 15. AlphaOPT is a self-improving library that enables an LLM to learn from limited demonstrations and solver feedback, improving optimization modeling across industries without costly retraining.  					AI-generated summary 				 Optimization modeling enables critical decisions across industries but remai...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 16. ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  					AI-generated summary 				 Evaluating progress in large language models (LLMs) is often constrained by the ...
[23.10.2025 04:15] ********************************************************************************
[23.10.2025 04:15] Abstract 17. RIR-Mega is a large dataset of simulated room impulse responses with tools for validation and a baseline model for predicting RT60 from waveforms.  					AI-generated summary 				 Room impulse responses are a core resource for dereverberation, robust speech recognition, source localization, and room ...
[23.10.2025 04:15] Read previous papers.
[23.10.2025 04:15] Generating reviews via LLM API.
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#training", "#rl", "#long_context", "#reasoning"], "emoji": "🔗", "ru": {"title": "Обучение LLM длинному рассуждению через цепочки UUID", "desc": "LoongRL — это метод reinforcement learning для улучшения рассуждений над длинными контекстами в больших языковых моделях. Ключевая идея —
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#long_context", "#benchmark", "#inference", "#optimization"], "emoji": "💍", "ru": {"title": "Гибридное внимание для эффективного вывода на длинных контекстах", "desc": "Серия моделей Ring-linear использует гибридную архитектуру, комбинирующую линейное и
[23.10.2025 04:15] Querying the API.
[23.10.2025 04:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  					AI-generated summary 				 Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking.
[23.10.2025 04:15] Response: ```json
{
  "desc": "Статья представляет метод BAPO для обучения больших языковых моделей с помощью reinforcement learning в off-policy режиме. Авторы выявили две ключевые проблемы: дисбаланс в оптимизации из-за доминирования отрицательных градиентов и фиксированный clipping механизм в PPO, который подавляет entropy и ограничивает exploration. BAPO решает эти проблемы через динамическую адаптацию clipping bounds, что балансирует положительные и отрицательные вклады в обновление политики. Метод показал впечатляющие результаты на бенчмарках AIME 2024/2025, превзойдя как open-source модели, так и проприетарные системы вроде o3-mini.",
  "emoji": "⚖️",
  "title": "Балансировка градиентов для стабильного обучения LLM через RL"
}
```
[23.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  					AI-generated summary 				 Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking."

[23.10.2025 04:15] Response: ```python
['RL', 'RLHF', 'TRAINING']
```
[23.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  					AI-generated summary 				 Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking."

[23.10.2025 04:15] Response: ```python
["OPTIMIZATION"]
```
[23.10.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BAlanced Policy Optimization with Adaptive Clipping (BAPO) enhances off-policy reinforcement learning by dynamically modifying clipping bounds to improve training efficiency and stability. The paper identifies issues like negative-advantage samples dominating the policy gradient, which can lead to poor performance and instability. It introduces the Entropy-Clip Rule, highlighting how fixed clipping mechanisms can hinder exploration by favoring over-exploitation. BAPO effectively addresses these challenges, resulting in faster and more efficient training across various scenarios, outperforming existing models on benchmark tests.","title":"Dynamic Clipping for Enhanced Reinforcement Learning Stability"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BAlanced Policy Optimization with Adaptive Clipping (BAPO) enhances off-policy reinforcement learning by dynamically modifying clipping bounds to improve training efficiency and stability. The paper identifies issues like negative-advantage samples dominating the policy gradient, which can lead to poor performance and instability. It introduces the Entropy-Clip Rule, highlighting how fixed clipping mechanisms can hinder exploration by favoring over-exploitation. BAPO effectively addresses these challenges, resulting in faster and more efficient training across various scenarios, outperforming existing models on benchmark tests.', title='Dynamic Clipping for Enhanced Reinforcement Learning Stability'))
[23.10.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BAPO（平衡策略优化与自适应裁剪）解决了离线强化学习中的挑战，通过动态调整裁剪边界来提高样本效率、稳定性和性能。该方法识别出优化中的不平衡现象，负优势样本主导策略梯度，抑制有用行为并可能导致梯度爆炸。同时，BAPO引入了熵裁剪规则，揭示了固定裁剪机制如何阻碍熵增加的更新，导致策略过度开发而忽视探索。通过在多种离线场景中应用，BAPO实现了快速、稳定和数据高效的训练，超越了多个开源和商业模型。","title":"动态裁剪，平衡优化，提升强化学习效率"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BAPO（平衡策略优化与自适应裁剪）解决了离线强化学习中的挑战，通过动态调整裁剪边界来提高样本效率、稳定性和性能。该方法识别出优化中的不平衡现象，负优势样本主导策略梯度，抑制有用行为并可能导致梯度爆炸。同时，BAPO引入了熵裁剪规则，揭示了固定裁剪机制如何阻碍熵增加的更新，导致策略过度开发而忽视探索。通过在多种离线场景中应用，BAPO实现了快速、稳定和数据高效的训练，超越了多个开源和商业模型。', title='动态裁剪，平衡优化，提升强化学习效率'))
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#survey", "#training", "#architecture", "#dataset", "#data", "#benchmark", "#multimodal", "#optimization"], "emoji": "📱", "ru": {"title": "DaMo: умная оптимизация данных для AI-агентов на смартфонах", "desc": "DaMo - это обучаемая нейросеть, которая оптимизирует состав обучающих дан
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#games", "#optimization", "#rlhf", "#cv", "#training", "#rl"], "emoji": "🎓", "ru": {"title": "Компактные VLM учатся у гигантов через reinforcement и imitation learning", "desc": "Статья представляет алгоритм Unified Reinforcement and Imitation Learning (RIL) для создания эффективных
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#rl", "#games", "#benchmark", "#security", "#agents", "#open_source", "#optimization"], "emoji": "🤖", "ru": {"title": "ColorAgent: умный OS-агент с обучением через взаимодействие", "desc": "ColorAgent — это AI-агент для операционной системы Android, способный выполнять сложные много
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning"], "emoji": "🔧", "ru": {"title": "Тысячи инструментов — испытание для AI-агентов", "desc": "Исследователи представили TheMCPCompany — бенчмарк для оценки AI-агентов, использующих инструменты через REST API для взаимодействия с реальными сервисами.
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#rl", "#dataset", "#benchmark", "#open_source", "#synthetic", "#optimization", "#cv"], "emoji": "📄", "ru": {"title": "OCR нового поколения через обучение с подкреплением на unit-тестах", "desc": "Представлена модель olmOCR 2 на базе vision language model с 7 миллиардами параметров д
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#synthetic", "#alignment", "#reasoning"], "emoji": "🍌", "ru": {"title": "Pico-Banana-400K: масштабный датасет для обучения редактированию изображений по текстовым инструкциям", "desc": "Статья представляет Pico-Banana-400K — датасет из 400 ты
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#reasoning"], "emoji": "⏰", "ru": {"title": "MINED: учим мультимодальные модели понимать время", "desc": "Статья представляет MINED — новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) понимать знания, чувствитель
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "KORE: инъекция знаний без забывания старого", "desc": "KORE — это метод для внедрения новых знаний в большие мультимодальные модели с сохранением ранее изученной информации. Метод испол
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#training", "#video", "#games", "#agents", "#3d", "#optimization"], "emoji": "🚗", "ru": {"title": "Всевидящая world model для автономного вождения с панорамным видео и 3D-наградами", "desc": "OmniNWM — это универсальная world model для автономного вождения, которая одновременно гене
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#architecture"], "emoji": "📊", "ru": {"title": "AI-аналитик создаёт финансовые отчёты профессионального уровня", "desc": "FinSight — это мультиагентный фреймворк для автоматической генерации профессиональных финансовых отчётов с графиками и аналитикой. Сист
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#training", "#dataset", "#video", "#data", "#agents", "#optimization", "#transfer_learning"], "emoji": "🎥", "ru": {"title": "Обучение AI-агентов на YouTube видео вместо ручной разметки", "desc": "Исследователи создали VideoAgentTrek — систему, которая автоматически извлекает данные 
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#open_source", "#small_models", "#training", "#optimization"], "emoji": "🔀", "ru": {"title": "NeuroAda: тонкая настройка нейросетей через обходные пути для важных параметров", "desc": "NeuroAda — это метод эффективной тонкой настройки (PEFT), который объединяет селективную адаптацию
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#interpretability", "#dataset", "#transfer_learning", "#optimization", "#rlhf", "#training"], "emoji": "📚", "ru": {"title": "Библиотека опыта для самообучения LLM в оптимизационном моделировании", "desc": "AlphaOPT — это самообучающаяся библиотека опыта, которая позволяет LLM автома
[23.10.2025 04:15] Querying the API.
[23.10.2025 04:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  					AI-generated summary 				 Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench
[23.10.2025 04:15] Response: ```json
{
  "desc": "Исследователи представили ProfBench - бенчмарк для оценки больших языковых моделей в профессиональных областях, содержащий более 7000 пар ответов и критериев оценки от экспертов с PhD по физике и химии, MBA в финансах и консалтинге. Для доступной оценки созданы специальные LLM-судьи, которые снижают стоимость тестирования в сотни раз и устраняют проблему завышения моделями собственных оценок. Результаты показывают, что даже лучшие модели вроде GPT-5-high достигают только 65.9% производительности на этом сложном бенчмарке. Обнаружен значительный разрыв между проприетарными и открытыми моделями в решении комплексных профессиональных задач.",
  "emoji": "🎓",
  "title": "ProfBench: даже лучшие LLM справляются только на 66% с профессиональными задачами"
}
```
[23.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  					AI-generated summary 				 Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench"

[23.10.2025 04:15] Response: ```python
['BENCHMARK', 'DATASET', 'DATA']
```
[23.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  					AI-generated summary 				 Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench"

[23.10.2025 04:15] Response: ```python
['OPEN_SOURCE', 'SCIENCE']
```
[23.10.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ProfBench is a benchmark designed to evaluate large language models (LLMs) specifically in professional domains using criteria set by human experts. It includes over 7000 response-criterion pairs evaluated by professionals in fields like Physics, Chemistry, Finance, and Consulting. The study highlights the limitations of current LLMs, revealing that even the best models struggle with complex tasks, achieving only 65.9% performance on average. Additionally, it uncovers significant performance differences between proprietary models and those with open weights, emphasizing the importance of extended reasoning in professional applications.","title":"ProfBench: Bridging the Gap in LLM Evaluation for Professional Domains"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ProfBench is a benchmark designed to evaluate large language models (LLMs) specifically in professional domains using criteria set by human experts. It includes over 7000 response-criterion pairs evaluated by professionals in fields like Physics, Chemistry, Finance, and Consulting. The study highlights the limitations of current LLMs, revealing that even the best models struggle with complex tasks, achieving only 65.9% performance on average. Additionally, it uncovers significant performance differences between proprietary models and those with open weights, emphasizing the importance of extended reasoning in professional applications.', title='ProfBench: Bridging the Gap in LLM Evaluation for Professional Domains'))
[23.10.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ProfBench是一个评估大型语言模型在专业领域表现的工具，使用人类专家的标准进行评估。它包含超过7000对响应标准，由物理、化学、金融和咨询等领域的专家进行评估。研究发现，即使是最先进的模型，如GPT-5-high，在整体表现上也仅达到65.9%。此外，研究还揭示了专有模型与开放权重模型之间的显著性能差异，以及扩展思维在处理复杂专业任务中的重要性。","title":"ProfBench：评估专业领域大型语言模型的挑战与差异"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ProfBench是一个评估大型语言模型在专业领域表现的工具，使用人类专家的标准进行评估。它包含超过7000对响应标准，由物理、化学、金融和咨询等领域的专家进行评估。研究发现，即使是最先进的模型，如GPT-5-high，在整体表现上也仅达到65.9%。此外，研究还揭示了专有模型与开放权重模型之间的显著性能差异，以及扩展思维在处理复杂专业任务中的重要性。', title='ProfBench：评估专业领域大型语言模型的挑战与差异'))
[23.10.2025 04:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#open_source", "#science"], "emoji": "🔊", "ru": {"title": "Мега-датасет импульсных откликов помещений для акустических исследований", "desc": "Представлен RIR-Mega — крупный датасет симулированных импульсных откликов помещений (room impulse responses), который и
[23.10.2025 04:15] Renaming data file.
[23.10.2025 04:15] Renaming previous data. hf_papers.json to ./d/2025-10-23.json
[23.10.2025 04:15] Saving new data file.
[23.10.2025 04:15] Generating page.
[23.10.2025 04:15] Renaming previous page.
[23.10.2025 04:15] Renaming previous data. index.html to ./d/2025-10-23.html
[23.10.2025 04:15] Writing result.
[23.10.2025 04:15] Renaming log file.
[23.10.2025 04:15] Renaming previous data. log.txt to ./logs/2025-10-23_last_log.txt
