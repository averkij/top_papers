[23.10.2025 05:14] Read previous papers.
[23.10.2025 05:14] Generating top page (month).
[23.10.2025 05:14] Writing top page (month).
[23.10.2025 06:18] Read previous papers.
[23.10.2025 06:18] Get feed.
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19338
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18927
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19363
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19430
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19336
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19488
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19307
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19808
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16844
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19386
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19286
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19028
[23.10.2025 06:18] Extract page data from URL. URL: https://huggingface.co/papers/2510.17932
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19817
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19457
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19316
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18313
[23.10.2025 06:18] Extract page data from URL. URL: https://huggingface.co/papers/2510.15731
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18940
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18428
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18909
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19753
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19492
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18941
[23.10.2025 06:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18917
[23.10.2025 06:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.10.2025 06:18] No deleted papers detected.
[23.10.2025 06:18] Downloading and parsing papers (pdf, html). Total: 25.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19338.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19338.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19338.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.18927.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.18927.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.18927.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19363.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19363.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19363.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19430.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19430.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19430.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19336.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19336.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19336.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19488.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19488.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19488.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19307.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19307.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19307.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19808.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19808.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19808.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.16844.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.16844.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.16844.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19386.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19386.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19386.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19286.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19286.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19286.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19028.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19028.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19028.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.17932.
[23.10.2025 06:18] Downloading paper 2510.17932 from http://arxiv.org/pdf/2510.17932v1...
[23.10.2025 06:18] Extracting affiliations from text.
[23.10.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 2 3 9 7 1 . 0 1 5 2 : r FROM CHARTS TO CODE: HIERARCHICAL BENCHMARK FOR MULTIMODAL MODELS Jiahao Tang1 Henry Hengyuan Zhao2 Lijian Wu1 Yifei Tao3 Dongxing Mao1 Yang Wan1 Jingru Tan1 Min Zeng1 Min Li1 Alex Jinpeng Wang 1 1CSU-JPG, Central South University 2National University of Singapore 3Nanyang Technological University "
[23.10.2025 06:18] Response: ```python
["CSU-JPG, Central South University", "National University of Singapore", "Nanyang Technological University"]
```
[23.10.2025 06:18] Deleting PDF ./assets/pdf/2510.17932.pdf.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19817.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19817.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19817.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19457.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19457.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19457.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19316.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19316.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19316.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.18313.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.18313.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.18313.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.15731.
[23.10.2025 06:18] Downloading paper 2510.15731 from http://arxiv.org/pdf/2510.15731v1...
[23.10.2025 06:18] Extracting affiliations from text.
[23.10.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Maximo Eduardo Rulli Simone Petruzzi Edoardo Michielon Fabrizio Silvestri Simone Scardapane Alessio Devoto Sapienza University of Rome Fastweb 5 2 0 2 7 1 ] . [ 1 1 3 7 5 1 . 0 1 5 2 : r a "
[23.10.2025 06:18] Response: ```python
["Sapienza University of Rome", "Fastweb"]
```
[23.10.2025 06:18] Deleting PDF ./assets/pdf/2510.15731.pdf.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.18940.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.18940.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.18940.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.18428.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.18428.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.18428.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.18909.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.18909.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.18909.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19753.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19753.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19753.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.19492.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.19492.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.19492.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.18941.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.18941.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.18941.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2510.18917.
[23.10.2025 06:18] Extra JSON file exists (./assets/json/2510.18917.json), skip PDF parsing.
[23.10.2025 06:18] Paper image links file exists (./assets/img_data/2510.18917.json), skip HTML parsing.
[23.10.2025 06:18] Success.
[23.10.2025 06:18] Enriching papers with extra data.
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 0. The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.  					AI-generated summary 				 In this technical report, we present the Ring-linear mo...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 1. BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  					AI-generated summary 				 Reinforcement learning (RL)...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 2. LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.  					AI-generated summary 				 Reasoning over long contexts is essential for large lan...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 3. GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks.  					AI-generated summary 				 Training Vision-Language-Action (VLA) models for generalist robots typicall...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 4. DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.  					AI-generated summary 				 Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability a...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 5. VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.  					AI-generated summary 				 Training computer-use agents requires massive amounts of GUI interact...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 6. A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them i...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 7. Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.  					AI-generated summary 				 Recent advances in mu...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 8. FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.  					AI-generated summary 				 Generating professional financial repo...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 9. ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.  					AI-generated summary 				 With the advancements in hardware, software, and large language...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 10. TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.  					AI-generated summary 				 Since the introduction of the Model Context Proto...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 11. Current large language models exhibit significant limitations in social reasoning, particularly in inferring interpersonal relationships across different languages, and thinking models or chain-of-thought prompting offer minimal improvement.  					AI-generated summary 				 As large language models (...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 12. Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.  					AI-generated summary 				 We introduce Chart2Code, a new benchmark for ev...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 13. olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.  					AI-generated summary 				 We present olmOCR 2, the latest in our f...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 14. Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' abil...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 15. KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.  					AI-generated summary 				 Large Multimodal Models encode extensive factual knowledge i...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 16. OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.  					AI-generated summary 				 Autonomous driving ...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 17. Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).  					AI-generated summary 				 Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional ...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 18. NeuroAda is a parameter-efficient fine-tuning method that combines selective adaptation with bypass connections to achieve high performance with minimal trainable parameters and reduced memory usage.  					AI-generated summary 				 Existing parameter-efficient fine-tuning (PEFT) methods primarily fa...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 19. AlphaOPT is a self-improving library that enables an LLM to learn from limited demonstrations and solver feedback, improving optimization modeling across industries without costly retraining.  					AI-generated summary 				 Optimization modeling enables critical decisions across industries but remai...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 20. The Orthogonal Diversity-Aware Selection (ODiS) algorithm enhances large language model performance by ensuring both quality and diversity in training data through orthogonal decomposition of evaluation dimensions.  					AI-generated summary 				 High-quality pre-training data is crutial for large l...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 21. Transformers struggle with generalizable algorithms, preferring heuristics; a disentangled Transformer can learn graph algorithms within its capacity but resorts to heuristics otherwise.  					AI-generated summary 				 Transformers often fail to learn generalizable algorithms, instead relying on bri...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 22. Theoretical and empirical investigation shows strong transferability between membership inference attacks and machine-generated text detection, highlighting the need for cross-task collaboration and introducing MINT for unified evaluation.  					AI-generated summary 				 Although membership inferenc...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 23. ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  					AI-generated summary 				 Evaluating progress in large language models (LLMs) is often constrained by the ...
[23.10.2025 06:18] ********************************************************************************
[23.10.2025 06:18] Abstract 24. RIR-Mega is a large dataset of simulated room impulse responses with tools for validation and a baseline model for predicting RT60 from waveforms.  					AI-generated summary 				 Room impulse responses are a core resource for dereverberation, robust speech recognition, source localization, and room ...
[23.10.2025 06:18] Read previous papers.
[23.10.2025 06:18] Generating reviews via LLM API.
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#architecture", "#training", "#long_context", "#benchmark", "#inference", "#optimization"], "emoji": "💍", "ru": {"title": "Гибридное внимание для эффективного вывода на длинных контекстах", "desc": "Серия моделей Ring-linear использует гибридную архитектуру, комбинирующую линейное и
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#rlhf"], "emoji": "⚖️", "ru": {"title": "Балансировка градиентов для стабильного обучения LLM через RL", "desc": "Статья представляет метод BAPO для обучения больших языковых моделей с помощью reinforcement learning в off-policy режиме. Авторы вы
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#training", "#rl", "#long_context", "#reasoning"], "emoji": "🔗", "ru": {"title": "Обучение LLM длинному рассуждению через цепочки UUID", "desc": "LoongRL — это метод reinforcement learning для улучшения рассуждений над длинными контекстами в больших языковых моделях. Ключевая идея —
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#small_models", "#agi", "#transfer_learning", "#agents", "#optimization", "#3d", "#training", "#reasoning"], "emoji": "🤖", "ru": {"title": "Обучение роботов на синтетических данных из world model", "desc": "GigaBrain-0 - это VLA-модель (Vision-Language-Action) для обучения роботов, 
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#survey", "#training", "#architecture", "#dataset", "#data", "#benchmark", "#multimodal", "#optimization"], "emoji": "📱", "ru": {"title": "DaMo: умная оптимизация данных для AI-агентов на смартфонах", "desc": "DaMo - это обучаемая нейросеть, которая оптимизирует состав обучающих дан
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#training", "#dataset", "#video", "#data", "#agents", "#optimization", "#transfer_learning"], "emoji": "🎥", "ru": {"title": "Обучение AI-агентов на YouTube видео вместо ручной разметки", "desc": "Исследователи создали VideoAgentTrek — систему, которая автоматически извлекает данные 
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#games", "#optimization", "#rlhf", "#cv", "#training", "#rl"], "emoji": "🎓", "ru": {"title": "Компактные VLM учатся у гигантов через reinforcement и imitation learning", "desc": "Статья представляет алгоритм Unified Reinforcement and Imitation Learning (RIL) для создания эффективных
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#synthetic", "#alignment", "#reasoning"], "emoji": "🍌", "ru": {"title": "Pico-Banana-400K: масштабный датасет для обучения редактированию изображений по текстовым инструкциям", "desc": "Статья представляет Pico-Banana-400K — датасет из 400 ты
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#architecture"], "emoji": "📊", "ru": {"title": "AI-аналитик создаёт финансовые отчёты профессионального уровня", "desc": "FinSight — это мультиагентный фреймворк для автоматической генерации профессиональных финансовых отчётов с графиками и аналитикой. Сист
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#rl", "#games", "#benchmark", "#security", "#agents", "#open_source", "#optimization"], "emoji": "🤖", "ru": {"title": "ColorAgent: умный OS-агент с обучением через взаимодействие", "desc": "ColorAgent — это AI-агент для операционной системы Android, способный выполнять сложные много
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning"], "emoji": "🔧", "ru": {"title": "Тысячи инструментов — испытание для AI-агентов", "desc": "Исследователи представили TheMCPCompany — бенчмарк для оценки AI-агентов, использующих инструменты через REST API для взаимодействия с реальными сервисами.
[23.10.2025 06:18] Using data from previous issue: {"categories": ["#low_resource", "#alignment", "#dataset", "#multilingual", "#reasoning"], "emoji": "👥", "ru": {"title": "LLM не понимают человеческие отношения так хорошо, как мы думали", "desc": "Исследователи представили датасет SCRIPTS для оценки способности языковых моделей определять межличнос
[23.10.2025 06:18] Querying the API.
[23.10.2025 06:18] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.  					AI-generated summary 				 We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.
[23.10.2025 06:18] Response: ```json
{
  "desc": "Chart2Code — это иерархический бенчмарк для оценки способностей больших мультимодальных моделей понимать графики и генерировать код. Бенчмарк состоит из трёх уровней возрастающей сложности: воспроизведение графиков, их редактирование и создание графиков из больших таблиц. В тестировании участвовали 25 современных LLM, включая GPT-5, но даже лучшие модели показали невысокие результаты — GPT-5 набрал в среднем 0.57 по качеству кода и 0.22 по качеству визуализации. Бенчмарк содержит 2023 задачи по 22 типам графиков и призван стимулировать развитие мультимодальных моделей для практических задач работы с визуализацией данных.",
  "emoji": "📊",
  "title": "Научить AI создавать графики — сложнее, чем кажется"
}
```
[23.10.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.  					AI-generated summary 				 We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code."

[23.10.2025 06:18] Response: ```python
['BENCHMARK', 'MULTIMODAL']
```
[23.10.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.  					AI-generated summary 				 We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code."

[23.10.2025 06:19] Response: ```python
["GAMES", "INTERPRETABILITY", "REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```
[23.10.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Chart2Code is a new benchmark designed to evaluate how well large multimodal models (LMMs) can understand charts and generate corresponding code. It features three levels of tasks that increase in complexity, starting from simple chart reproduction to more complex tasks like editing charts and generating charts from detailed tables. The benchmark includes 2,023 tasks across 22 different chart types, with metrics to assess both the accuracy of the generated code and the quality of the charts produced. Initial tests on state-of-the-art models, including GPT-5, show that these tasks are quite challenging, highlighting the need for further advancements in multimodal reasoning capabilities.","title":"Chart2Code: Elevating Chart Understanding in AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Chart2Code is a new benchmark designed to evaluate how well large multimodal models (LMMs) can understand charts and generate corresponding code. It features three levels of tasks that increase in complexity, starting from simple chart reproduction to more complex tasks like editing charts and generating charts from detailed tables. The benchmark includes 2,023 tasks across 22 different chart types, with metrics to assess both the accuracy of the generated code and the quality of the charts produced. Initial tests on state-of-the-art models, including GPT-5, show that these tasks are quite challenging, highlighting the need for further advancements in multimodal reasoning capabilities.', title='Chart2Code: Elevating Chart Understanding in AI'))
[23.10.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Chart2Code是一个用于评估大型多模态模型（LMM）在图表理解和代码生成能力的分层基准。它设计了三个复杂度逐渐增加的任务，涵盖了多种真实场景。第一级任务是从参考图形和用户查询中重现图表；第二级任务涉及复杂的修改，如更改图表类型或添加元素；第三级任务则要求模型将信息密集的长表格转换为符合用户指令的图表。通过对25个最先进的LMM进行基准测试，结果显示即使是最先进的模型GPT-5在任务中的表现也相对较低，表明Chart2Code的挑战性。","title":"Chart2Code：图表理解与代码生成的新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Chart2Code是一个用于评估大型多模态模型（LMM）在图表理解和代码生成能力的分层基准。它设计了三个复杂度逐渐增加的任务，涵盖了多种真实场景。第一级任务是从参考图形和用户查询中重现图表；第二级任务涉及复杂的修改，如更改图表类型或添加元素；第三级任务则要求模型将信息密集的长表格转换为符合用户指令的图表。通过对25个最先进的LMM进行基准测试，结果显示即使是最先进的模型GPT-5在任务中的表现也相对较低，表明Chart2Code的挑战性。', title='Chart2Code：图表理解与代码生成的新基准'))
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#rl", "#dataset", "#benchmark", "#open_source", "#synthetic", "#optimization", "#cv"], "emoji": "📄", "ru": {"title": "OCR нового поколения через обучение с подкреплением на unit-тестах", "desc": "Представлена модель olmOCR 2 на базе vision language model с 7 миллиардами параметров д
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#reasoning"], "emoji": "⏰", "ru": {"title": "MINED: учим мультимодальные модели понимать время", "desc": "Статья представляет MINED — новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) понимать знания, чувствитель
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "KORE: инъекция знаний без забывания старого", "desc": "KORE — это метод для внедрения новых знаний в большие мультимодальные модели с сохранением ранее изученной информации. Метод испол
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#training", "#video", "#games", "#agents", "#3d", "#optimization"], "emoji": "🚗", "ru": {"title": "Всевидящая world model для автономного вождения с панорамным видео и 3D-наградами", "desc": "OmniNWM — это универсальная world model для автономного вождения, которая одновременно гене
[23.10.2025 06:19] Querying the API.
[23.10.2025 06:19] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).  					AI-generated summary 				 Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models.
[23.10.2025 06:19] Response: ```json
{
  "title": "Динамические паттерны внимания в диффузионных языковых моделях",
  "emoji": "🌊",
  "desc": "Исследователи изучили механизмы внимания в Masked Diffusion Language Models (DLM) — новом классе языковых моделей, которые генерируют токены параллельно, в отличие от традиционных autoregressive моделей. Оказалось, что DLM также демонстрируют эффект attention sinking (концентрации внимания на определённых позициях), но с уникальными особенностями: позиции «стоков» внимания динамически меняются в процессе генерации. В отличие от autoregressive моделей, DLM остаются устойчивыми при удалении этих attention sinks — производительность падает незначительно. Эти результаты раскрывают фундаментальные различия в том, как диффузионные и autoregressive модели распределяют и используют механизмы внимания."
}
```
[23.10.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).  					AI-generated summary 				 Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models."

[23.10.2025 06:19] Response: ```python
['ARCHITECTURE']
```
[23.10.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).  					AI-generated summary 				 Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models."

[23.10.2025 06:19] Response: ```python
["DIFFUSION", "INTERPRETABILITY"]
```
[23.10.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates Masked Diffusion Language Models (DLMs) and their attention mechanisms compared to Autoregressive Models (ARMs). It identifies a phenomenon called attention sinking, where certain tokens receive less attention during generation. The study finds that DLMs exhibit dynamic attention sinks that shift over time, unlike the static sinks in ARMs. Additionally, DLMs show robustness to the removal of these sinks, indicating a more resilient attention allocation strategy.","title":"Unveiling the Dynamic Attention of Masked Diffusion Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates Masked Diffusion Language Models (DLMs) and their attention mechanisms compared to Autoregressive Models (ARMs). It identifies a phenomenon called attention sinking, where certain tokens receive less attention during generation. The study finds that DLMs exhibit dynamic attention sinks that shift over time, unlike the static sinks in ARMs. Additionally, DLMs show robustness to the removal of these sinks, indicating a more resilient attention allocation strategy.', title='Unveiling the Dynamic Attention of Masked Diffusion Models'))
[23.10.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究分析了掩蔽扩散语言模型（DLMs）与自回归模型（ARMs）在注意力机制上的差异。DLMs使用双向注意力的变换器编码器，能够并行生成标记，并且在性能上与ARMs相当。我们发现DLMs存在注意力沉没现象，但其特征与ARMs不同，DLMs的沉没位置在生成过程中会动态变化。尽管ARMs对注意力沉没的去除非常敏感，但DLMs表现出较强的鲁棒性，掩蔽沉没对性能的影响较小。","title":"掩蔽扩散模型的注意力机制新发现"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究分析了掩蔽扩散语言模型（DLMs）与自回归模型（ARMs）在注意力机制上的差异。DLMs使用双向注意力的变换器编码器，能够并行生成标记，并且在性能上与ARMs相当。我们发现DLMs存在注意力沉没现象，但其特征与ARMs不同，DLMs的沉没位置在生成过程中会动态变化。尽管ARMs对注意力沉没的去除非常敏感，但DLMs表现出较强的鲁棒性，掩蔽沉没对性能的影响较小。', title='掩蔽扩散模型的注意力机制新发现'))
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#open_source", "#small_models", "#training", "#optimization"], "emoji": "🔀", "ru": {"title": "NeuroAda: тонкая настройка нейросетей через обходные пути для важных параметров", "desc": "NeuroAda — это метод эффективной тонкой настройки (PEFT), который объединяет селективную адаптацию
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#interpretability", "#dataset", "#transfer_learning", "#optimization", "#rlhf", "#training"], "emoji": "📚", "ru": {"title": "Библиотека опыта для самообучения LLM в оптимизационном моделировании", "desc": "AlphaOPT — это самообучающаяся библиотека опыта, которая позволяет LLM автома
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#optimization", "#dataset", "#data", "#training"], "emoji": "⊥", "ru": {"title": "Ортогональность измерений для разнообразия обучающих данных LLM", "desc": "Алгоритм ODiS улучшает качество обучающих данных для больших языковых моделей путём одновр
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#architecture", "#graphs", "#optimization", "#training"], "emoji": "🔗", "ru": {"title": "Трансформеры учат алгоритмы только в пределах своих возможностей", "desc": "Исследование показывает, что Transformer-модели часто не могут выучить обобщаемые алгоритмы и вместо этого полагаются 
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#benchmark", "#multimodal"], "emoji": "🔄", "ru": {"title": "Два в одном: атаки на приватность и детекция AI-текста используют одни методы", "desc": "Исследование показывает сильную взаимосвязь между атаками на определение членства в обучающей выборке (
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#data", "#science", "#open_source", "#dataset", "#benchmark"], "emoji": "🎓", "ru": {"title": "ProfBench: даже лучшие LLM справляются только на 66% с профессиональными задачами", "desc": "Исследователи представили ProfBench - бенчмарк для оценки больших языковых моделей в профессиона
[23.10.2025 06:19] Using data from previous issue: {"categories": ["#dataset", "#data", "#open_source", "#science"], "emoji": "🔊", "ru": {"title": "Мега-датасет импульсных откликов помещений для акустических исследований", "desc": "Представлен RIR-Mega — крупный датасет симулированных импульсных откликов помещений (room impulse responses), который и
[23.10.2025 06:19] Renaming data file.
[23.10.2025 06:19] Renaming previous data. hf_papers.json to ./d/2025-10-23.json
[23.10.2025 06:19] Saving new data file.
[23.10.2025 06:19] Generating page.
[23.10.2025 06:19] Renaming previous page.
[23.10.2025 06:19] Renaming previous data. index.html to ./d/2025-10-23.html
[23.10.2025 06:19] Writing result.
[23.10.2025 06:19] Renaming log file.
[23.10.2025 06:19] Renaming previous data. log.txt to ./logs/2025-10-23_last_log.txt
