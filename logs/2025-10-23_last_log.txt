[23.10.2025 10:14] Read previous papers.
[23.10.2025 10:14] Generating top page (month).
[23.10.2025 10:14] Writing top page (month).
[23.10.2025 11:10] Read previous papers.
[23.10.2025 11:10] Get feed.
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19338
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18927
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19363
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19430
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19488
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19336
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19808
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19307
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16844
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19028
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15731
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19817
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18313
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17932
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19386
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19316
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19286
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19457
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18428
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18909
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15050
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18941
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18940
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19753
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19492
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18917
[23.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18840
[23.10.2025 11:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.10.2025 11:10] No deleted papers detected.
[23.10.2025 11:10] Downloading and parsing papers (pdf, html). Total: 27.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19338.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19338.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19338.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18927.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18927.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18927.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19363.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19363.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19363.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19430.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19430.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19430.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19488.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19488.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19488.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19336.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19336.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19336.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19808.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19808.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19808.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19307.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19307.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19307.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.16844.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.16844.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.16844.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19028.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19028.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19028.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.15731.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.15731.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.15731.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19817.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19817.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19817.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18313.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18313.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18313.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.17932.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.17932.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.17932.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19386.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19386.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19386.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19316.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19316.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19316.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19286.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19286.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19286.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19457.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19457.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19457.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18428.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18428.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18428.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18909.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18909.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18909.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.15050.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.15050.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.15050.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18941.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18941.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18941.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18940.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18940.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18940.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19753.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19753.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19753.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19492.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.19492.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.19492.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18917.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18917.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18917.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.18840.
[23.10.2025 11:10] Extra JSON file exists (./assets/json/2510.18840.json), skip PDF parsing.
[23.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.18840.json), skip HTML parsing.
[23.10.2025 11:10] Success.
[23.10.2025 11:10] Enriching papers with extra data.
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 0. The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.  					AI-generated summary 				 In this technical report, we present the Ring-linear mo...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 1. BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  					AI-generated summary 				 Reinforcement learning (RL)...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 2. LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.  					AI-generated summary 				 Reasoning over long contexts is essential for large lan...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 3. GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks.  					AI-generated summary 				 Training Vision-Language-Action (VLA) models for generalist robots typicall...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 4. VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.  					AI-generated summary 				 Training computer-use agents requires massive amounts of GUI interact...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 5. DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.  					AI-generated summary 				 Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability a...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 6. Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.  					AI-generated summary 				 Recent advances in mu...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 7. A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them i...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 8. FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.  					AI-generated summary 				 Generating professional financial repo...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 9. Current large language models exhibit significant limitations in social reasoning, particularly in inferring interpersonal relationships across different languages, and thinking models or chain-of-thought prompting offer minimal improvement.  					AI-generated summary 				 As large language models (...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 10. Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).  					AI-generated summary 				 Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional ...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 11. olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.  					AI-generated summary 				 We present olmOCR 2, the latest in our f...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 12. OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.  					AI-generated summary 				 Autonomous driving ...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 13. Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.  					AI-generated summary 				 We introduce Chart2Code, a new benchmark for ev...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 14. ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.  					AI-generated summary 				 With the advancements in hardware, software, and large language...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 15. KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.  					AI-generated summary 				 Large Multimodal Models encode extensive factual knowledge i...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 16. TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.  					AI-generated summary 				 Since the introduction of the Model Context Proto...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 17. Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' abil...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 18. AlphaOPT is a self-improving library that enables an LLM to learn from limited demonstrations and solver feedback, improving optimization modeling across industries without costly retraining.  					AI-generated summary 				 Optimization modeling enables critical decisions across industries but remai...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 19. The Orthogonal Diversity-Aware Selection (ODiS) algorithm enhances large language model performance by ensuring both quality and diversity in training data through orthogonal decomposition of evaluation dimensions.  					AI-generated summary 				 High-quality pre-training data is crutial for large l...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 20. DRIFT, a lightweight method, enhances multimodal large language models' reasoning ability by transferring knowledge in gradient space, outperforming naive merging and supervised fine-tuning with reduced computational cost.  					AI-generated summary 				 Multimodal large language models (MLLMs) are ...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 21. ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  					AI-generated summary 				 Evaluating progress in large language models (LLMs) is often constrained by the ...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 22. NeuroAda is a parameter-efficient fine-tuning method that combines selective adaptation with bypass connections to achieve high performance with minimal trainable parameters and reduced memory usage.  					AI-generated summary 				 Existing parameter-efficient fine-tuning (PEFT) methods primarily fa...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 23. Transformers struggle with generalizable algorithms, preferring heuristics; a disentangled Transformer can learn graph algorithms within its capacity but resorts to heuristics otherwise.  					AI-generated summary 				 Transformers often fail to learn generalizable algorithms, instead relying on bri...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 24. Theoretical and empirical investigation shows strong transferability between membership inference attacks and machine-generated text detection, highlighting the need for cross-task collaboration and introducing MINT for unified evaluation.  					AI-generated summary 				 Although membership inferenc...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 25. RIR-Mega is a large dataset of simulated room impulse responses with tools for validation and a baseline model for predicting RT60 from waveforms.  					AI-generated summary 				 Room impulse responses are a core resource for dereverberation, robust speech recognition, source localization, and room ...
[23.10.2025 11:10] ********************************************************************************
[23.10.2025 11:10] Abstract 26. SeeTok, a vision-centric method, renders text as images and uses pretrained multimodal LLMs to interpret them, offering efficiency and robustness improvements over traditional subword tokenization.  					AI-generated summary 				 People see text. Humans read by recognizing words as visual objects, i...
[23.10.2025 11:10] Read previous papers.
[23.10.2025 11:10] Generating reviews via LLM API.
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#long_context", "#benchmark", "#inference", "#optimization"], "emoji": "üíç", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö", "desc": "–°–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Ring-linear –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â—É—é –ª–∏–Ω–µ–π–Ω–æ–µ –∏
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#rlhf"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ RL", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ BAPO –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é reinforcement learning –≤ off-policy —Ä–µ–∂–∏–º–µ. –ê–≤—Ç–æ—Ä—ã –≤—ã
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#training", "#rl", "#long_context", "#reasoning"], "emoji": "üîó", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ LLM –¥–ª–∏–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫–∏ UUID", "desc": "LoongRL ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ reinforcement learning –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞–¥ –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è ‚Äî
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#small_models", "#agi", "#transfer_learning", "#agents", "#optimization", "#3d", "#training", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ world model", "desc": "GigaBrain-0 - —ç—Ç–æ VLA-–º–æ–¥–µ–ª—å (Vision-Language-Action) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤, 
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#training", "#dataset", "#video", "#data", "#agents", "#optimization", "#transfer_learning"], "emoji": "üé•", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ YouTube –≤–∏–¥–µ–æ –≤–º–µ—Å—Ç–æ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ VideoAgentTrek ‚Äî —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ 
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#survey", "#training", "#architecture", "#dataset", "#data", "#benchmark", "#multimodal", "#optimization"], "emoji": "üì±", "ru": {"title": "DaMo: —É–º–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∞—Ö", "desc": "DaMo - —ç—Ç–æ –æ–±—É—á–∞–µ–º–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–∞–≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#synthetic", "#alignment", "#reasoning"], "emoji": "üçå", "ru": {"title": "Pico-Banana-400K: –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Pico-Banana-400K ‚Äî –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 400 —Ç—ã
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#games", "#optimization", "#rlhf", "#cv", "#training", "#rl"], "emoji": "üéì", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ VLM —É—á–∞—Ç—Å—è —É –≥–∏–≥–∞–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ reinforcement –∏ imitation learning", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Unified Reinforcement and Imitation Learning (RIL) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#architecture"], "emoji": "üìä", "ru": {"title": "AI-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å–æ–∑–¥–∞—ë—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –æ—Ç—á—ë—Ç—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è", "desc": "FinSight ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –æ—Ç—á—ë—Ç–æ–≤ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π. –°–∏—Å—Ç
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#low_resource", "#alignment", "#dataset", "#multilingual", "#reasoning"], "emoji": "üë•", "ru": {"title": "LLM –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–∞–∫ —Ö–æ—Ä–æ—à–æ, –∫–∞–∫ –º—ã –¥—É–º–∞–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç SCRIPTS –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –º–µ–∂–ª–∏—á–Ω–æ—Å
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#interpretability"], "emoji": "üåä", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤ Masked Diffusion Language Models (DLM) ‚Äî –Ω–æ–≤–æ–º –∫–ª–∞—Å—Å–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#rl", "#dataset", "#benchmark", "#open_source", "#synthetic", "#optimization", "#cv"], "emoji": "üìÑ", "ru": {"title": "OCR –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ unit-—Ç–µ—Å—Ç–∞—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å olmOCR 2 –Ω–∞ –±–∞–∑–µ vision language model —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#training", "#video", "#games", "#agents", "#3d", "#optimization"], "emoji": "üöó", "ru": {"title": "–í—Å–µ–≤–∏–¥—è—â–∞—è world model –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è —Å –ø–∞–Ω–æ—Ä–∞–º–Ω—ã–º –≤–∏–¥–µ–æ –∏ 3D-–Ω–∞–≥—Ä–∞–¥–∞–º–∏", "desc": "OmniNWM ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è world model –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#multimodal", "#interpretability", "#optimization", "#benchmark", "#games"], "emoji": "üìä", "ru": {"title": "–ù–∞—É—á–∏—Ç—å AI —Å–æ–∑–¥–∞–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ ‚Äî —Å–ª–æ–∂–Ω–µ–µ, —á–µ–º –∫–∞–∂–µ—Ç—Å—è", "desc": "Chart2Code ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#rl", "#games", "#benchmark", "#security", "#agents", "#open_source", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "ColorAgent: —É–º–Ω—ã–π OS-–∞–≥–µ–Ω—Ç —Å –æ–±—É—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "ColorAgent ‚Äî —ç—Ç–æ AI-–∞–≥–µ–Ω—Ç –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã Android, —Å–ø–æ—Å–æ–±–Ω—ã–π –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–Ω–æ–≥–æ
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "KORE: –∏–Ω—ä–µ–∫—Ü–∏—è –∑–Ω–∞–Ω–∏–π –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è —Å—Ç–∞—Ä–æ–≥–æ", "desc": "KORE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞–Ω–µ–µ –∏–∑—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning"], "emoji": "üîß", "ru": {"title": "–¢—ã—Å—è—á–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ ‚Äî –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ TheMCPCompany ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ REST API –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏.
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#reasoning"], "emoji": "‚è∞", "ru": {"title": "MINED: —É—á–∏–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –≤—Ä–µ–º—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MINED ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –ø–æ–Ω–∏–º–∞—Ç—å –∑–Ω–∞–Ω–∏—è, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#interpretability", "#dataset", "#transfer_learning", "#optimization", "#rlhf", "#training"], "emoji": "üìö", "ru": {"title": "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ–ø—ã—Ç–∞ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è LLM –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "AlphaOPT ‚Äî —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ–ø—ã—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –∞–≤—Ç–æ–º–∞
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#optimization", "#dataset", "#data", "#training"], "emoji": "‚ä•", "ru": {"title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏–π –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö LLM", "desc": "–ê–ª–≥–æ—Ä–∏—Ç–º ODiS —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –æ–¥–Ω–æ–≤—Ä
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#benchmark", "#reasoning", "#multimodal", "#transfer_learning"], "emoji": "üß≠", "ru": {"title": "–ü–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "DRIFT ‚Äî —ç—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —É –º—É–ª—å—Ç–∏
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#data", "#science", "#open_source", "#dataset", "#benchmark"], "emoji": "üéì", "ru": {"title": "ProfBench: –¥–∞–∂–µ –ª—É—á—à–∏–µ LLM —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ 66% —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ ProfBench - –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#open_source", "#small_models", "#training", "#optimization"], "emoji": "üîÄ", "ru": {"title": "NeuroAda: —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —á–µ—Ä–µ–∑ –æ–±—Ö–æ–¥–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", "desc": "NeuroAda ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (PEFT), –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#architecture", "#graphs", "#optimization", "#training"], "emoji": "üîó", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —É—á–∞—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ç–æ–ª—å–∫–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å–≤–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ Transformer-–º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –Ω–µ –º–æ–≥—É—Ç –≤—ã—É—á–∏—Ç—å –æ–±–æ–±—â–∞–µ–º—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –ø–æ–ª–∞–≥–∞—é—Ç—Å—è 
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#benchmark", "#multimodal"], "emoji": "üîÑ", "ru": {"title": "–î–≤–∞ –≤ –æ–¥–Ω–æ–º: –∞—Ç–∞–∫–∏ –Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –∏ –¥–µ—Ç–µ–∫—Ü–∏—è AI-—Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–Ω–∏ –º–µ—Ç–æ–¥—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—É—é –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ç–∞–∫–∞–º–∏ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–ª–µ–Ω—Å—Ç–≤–∞ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ (
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#data", "#open_source", "#science"], "emoji": "üîä", "ru": {"title": "–ú–µ–≥–∞-–¥–∞—Ç–∞—Å–µ—Ç –∏–º–ø—É–ª—å—Å–Ω—ã—Ö –æ—Ç–∫–ª–∏–∫–æ–≤ –ø–æ–º–µ—â–µ–Ω–∏–π –¥–ª—è –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω RIR-Mega ‚Äî –∫—Ä—É–ø–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–º–ø—É–ª—å—Å–Ω—ã—Ö –æ—Ç–∫–ª–∏–∫–æ–≤ –ø–æ–º–µ—â–µ–Ω–∏–π (room impulse responses), –∫–æ—Ç–æ—Ä—ã–π –∏
[23.10.2025 11:10] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#optimization", "#low_resource", "#data"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–∑–∞–º–∏: –∫–æ–≥–¥–∞ LLM –≤–∏–¥–∏—Ç —Å–ª–æ–≤–∞ –∫–∞–∫ –∫–∞—Ä—Ç–∏–Ω–∫–∏", "desc": "SeeTok –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è —Ç–µ–∫—Å—Ç –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏
[23.10.2025 11:10] Renaming data file.
[23.10.2025 11:10] Renaming previous data. hf_papers.json to ./d/2025-10-23.json
[23.10.2025 11:10] Saving new data file.
[23.10.2025 11:10] Generating page.
[23.10.2025 11:10] Renaming previous page.
[23.10.2025 11:10] Renaming previous data. index.html to ./d/2025-10-23.html
[23.10.2025 11:10] Writing result.
[23.10.2025 11:10] Renaming log file.
[23.10.2025 11:10] Renaming previous data. log.txt to ./logs/2025-10-23_last_log.txt
