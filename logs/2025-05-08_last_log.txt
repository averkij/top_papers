[08.05.2025 08:15] Read previous papers.
[08.05.2025 08:15] Generating top page (month).
[08.05.2025 08:15] Writing top page (month).
[08.05.2025 09:12] Read previous papers.
[08.05.2025 09:12] Get feed.
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04588
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04512
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04622
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04364
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04528
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03912
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03418
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.00358
[08.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.03570
[08.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02393
[08.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.05.2025 09:12] No deleted papers detected.
[08.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 10.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04588.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04588.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04588.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04512.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04512.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04512.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04622.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04622.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04622.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04364.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04364.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04364.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04528.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04528.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04528.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.03912.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.03912.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.03912.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.03418.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.03418.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.03418.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.00358.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.00358.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.00358.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.03570.
[08.05.2025 09:12] Downloading paper 2505.03570 from http://arxiv.org/pdf/2505.03570v1...
[08.05.2025 09:12] Extracting affiliations from text.
[08.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 0 7 5 3 0 . 5 0 5 2 : r OSUNIVERSE: BENCHMARK FOR MULTIMODAL GUI-NAVIGATION AI AGENTS Mariya Davydova*, Daniel Jeffries*, Patrick Barker, Arturo MÃ¡rquez Flores, SinÃ©ad Ryan Kentauros AI Inc. research@kentauros.ai "
[08.05.2025 09:12] Response: ```python
["Kentauros AI Inc."]
```
[08.05.2025 09:12] Deleting PDF ./assets/pdf/2505.03570.pdf.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.02393.
[08.05.2025 09:12] Extra JSON file exists (./assets/json/2505.02393.json), skip PDF parsing.
[08.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.02393.json), skip HTML parsing.
[08.05.2025 09:12] Success.
[08.05.2025 09:12] Enriching papers with extra data.
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 0. Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 1. Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framew...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 2. Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 3. Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, partic...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 4. As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiabil...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 5. Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural design...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 6. Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs c...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 7. Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performa...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 8. In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, ...
[08.05.2025 09:12] ********************************************************************************
[08.05.2025 09:12] Abstract 9. Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that...
[08.05.2025 09:12] Read previous papers.
[08.05.2025 09:12] Generating reviews via LLM API.
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#rlhf", "#optimization", "#training", "#reasoning", "#rl"], "emoji": "ğŸ”", "ru": {"title": "ZeroSearch: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ZeroSearch - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#video", "#multimodal"], "emoji": "ğŸ¬", "ru": {"title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸", "desc": "HunyuanCustom - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#3d", "#cv", "#games"], "emoji": "ğŸ§Š", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PrimitiveAnything - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#benchmark", "#open_source", "#multimodal"], "emoji": "ğŸ", "ru": {"title": "SwarmBench: Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¾ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SwarmBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ¾ĞµĞ²Ğ¾
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#math", "#training", "#benchmark", "#alignment", "#interpretability", "#reasoning", "#agents"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²Ñ
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#agents", "#open_source", "#multimodal"], "emoji": "ğŸ¤–", "ru": {"title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… VLA Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ VLA (Vision-Language-Action) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚Ğµ
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#rl", "#survey", "#math", "#training", "#reasoning", "#science", "#data"], "emoji": "ğŸ§ ", "ru": {"title": "LLM: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#data", "#multimodal"], "emoji": "ğŸ”€", "ru": {"title": "R&B: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº R&B Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹Ğº
[08.05.2025 09:12] Querying the API.
[08.05.2025 09:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.
[08.05.2025 09:12] Response: {
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OSUniverse - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 50% ÑƒÑĞ¿ĞµÑ…Ğ°, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ„Ğ¸ÑĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ³Ñ€ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ½ĞµĞµ 2%.",
  "emoji": "ğŸ–¥ï¸",
  "title": "OSUniverse: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ"
}
[08.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse."

[08.05.2025 09:12] Response: ```python
['BENCHMARK', 'AGENTS', 'MULTIMODAL']
```
[08.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse."

[08.05.2025 09:12] Response: ```python
["GAMES", "OPTIMIZATION", "OPEN_SOURCE"]
```
[08.05.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation.","title":"OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation.', title='OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks'))
[08.05.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†OSUniverseï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜çº§GUIå¯¼èˆªAIä»£ç†çš„å¤æ‚å¤šæ¨¡æ€æ¡Œé¢ä»»åŠ¡åŸºå‡†ï¼Œæ—¨åœ¨æ˜“ç”¨æ€§ã€å¯æ‰©å±•æ€§ã€å…¨é¢è¦†ç›–æµ‹è¯•æ¡ˆä¾‹å’Œè‡ªåŠ¨éªŒè¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬å°†ä»»åŠ¡åˆ†ä¸ºä¸åŒå¤æ‚åº¦çš„çº§åˆ«ï¼Œä»åŸºæœ¬çš„ç²¾ç¡®ç‚¹å‡»åˆ°éœ€è¦çµæ´»æ€§ã€ç²¾ç¡®æ€§å’Œæ¸…æ™°æ€ç»´çš„å¤šæ­¥éª¤ã€å¤šåº”ç”¨ç¨‹åºæµ‹è¯•ã€‚åœ¨åŸºå‡†çš„ç¬¬ä¸€ç‰ˆä¸­ï¼Œæˆ‘ä»¬è°ƒæ•´äº†æµ‹è¯•æ¡ˆä¾‹çš„å¤æ‚æ€§ï¼Œä»¥ç¡®ä¿å½“æ—¶çš„æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ä»£ç†çš„ç»“æœä¸è¶…è¿‡50%ï¼Œè€Œæ™®é€šç™½é¢†å·¥äººå¯ä»¥å®Œç¾å®Œæˆæ‰€æœ‰è¿™äº›ä»»åŠ¡ã€‚è¯¥åŸºå‡†å¯ä»¥æ‰‹åŠ¨è¯„åˆ†ï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¹³å‡é”™è¯¯ç‡ä½äº2%çš„è‡ªåŠ¨éªŒè¯æœºåˆ¶ï¼Œä¸ºå…¨é¢è‡ªåŠ¨åŒ–æµ‹é‡GUIå¯¼èˆªAIä»£ç†çš„è¿›å±•ã€èƒ½åŠ›å’Œæœ‰æ•ˆæ€§æä¾›äº†åšå®åŸºç¡€ã€‚","title":"OSUniverseï¼šGUIå¯¼èˆªAIçš„å…¨æ–°åŸºå‡†"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†OSUniverseï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜çº§GUIå¯¼èˆªAIä»£ç†çš„å¤æ‚å¤šæ¨¡æ€æ¡Œé¢ä»»åŠ¡åŸºå‡†ï¼Œæ—¨åœ¨æ˜“ç”¨æ€§ã€å¯æ‰©å±•æ€§ã€å…¨é¢è¦†ç›–æµ‹è¯•æ¡ˆä¾‹å’Œè‡ªåŠ¨éªŒè¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬å°†ä»»åŠ¡åˆ†ä¸ºä¸åŒå¤æ‚åº¦çš„çº§åˆ«ï¼Œä»åŸºæœ¬çš„ç²¾ç¡®ç‚¹å‡»åˆ°éœ€è¦çµæ´»æ€§ã€ç²¾ç¡®æ€§å’Œæ¸…æ™°æ€ç»´çš„å¤šæ­¥éª¤ã€å¤šåº”ç”¨ç¨‹åºæµ‹è¯•ã€‚åœ¨åŸºå‡†çš„ç¬¬ä¸€ç‰ˆä¸­ï¼Œæˆ‘ä»¬è°ƒæ•´äº†æµ‹è¯•æ¡ˆä¾‹çš„å¤æ‚æ€§ï¼Œä»¥ç¡®ä¿å½“æ—¶çš„æœ€å…ˆè¿›ï¼ˆSOTAï¼‰ä»£ç†çš„ç»“æœä¸è¶…è¿‡50%ï¼Œè€Œæ™®é€šç™½é¢†å·¥äººå¯ä»¥å®Œç¾å®Œæˆæ‰€æœ‰è¿™äº›ä»»åŠ¡ã€‚è¯¥åŸºå‡†å¯ä»¥æ‰‹åŠ¨è¯„åˆ†ï¼ŒåŒæ—¶æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¹³å‡é”™è¯¯ç‡ä½äº2%çš„è‡ªåŠ¨éªŒè¯æœºåˆ¶ï¼Œä¸ºå…¨é¢è‡ªåŠ¨åŒ–æµ‹é‡GUIå¯¼èˆªAIä»£ç†çš„è¿›å±•ã€èƒ½åŠ›å’Œæœ‰æ•ˆæ€§æä¾›äº†åšå®åŸºç¡€ã€‚', title='OSUniverseï¼šGUIå¯¼èˆªAIçš„å…¨æ–°åŸºå‡†'))
[08.05.2025 09:12] Using data from previous issue: {"categories": ["#video", "#benchmark", "#multimodal", "#synthetic"], "emoji": "ğŸ•µï¸", "ru": {"title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸Ğ· RGB Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ IEF-VAD Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ RGB-ĞºĞ°Ğ´Ñ€Ñ‹ Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚
[08.05.2025 09:12] Trying to get texts in Chinese.
[08.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.
[08.05.2025 09:12] Mistral response. {"id": "73ce310fef644335b895554a48f7f454", "object": "chat.completion", "created": 1746695565, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u641c\u7d22\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002\u6700\u8fd1\u7684\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u5b9e\u65f6\u641c\u7d22\u5f15\u64ce\u4e92\u52a8\u6765\u6539\u8fdbLLMs\u7684\u641c\u7d22\u80fd\u529b\uff0c\u4f46\u9762\u4e34\u6587\u6863\u8d28\u91cf\u4e0d\u53ef\u63a7\u548cAPI\u8d39\u7528\u9ad8\u6602\u7684\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86ZeroSearch\uff0c\u4e00\u79cd\u4e0d\u9700\u8981\u4e0e\u5b9e\u9645\u641c\u7d22\u5f15\u64ce\u4e92\u52a8\u7684RL\u6846\u67b6\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8e\u8bfe\u7a0b\u7684\u6eda\u52a8\u7b56\u7565\uff0cZeroSearch\u80fd\u591f\u6709\u6548\u63d0\u5347LLMs\u7684\u641c\u7d22\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u826f\u597d\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 384, "total_tokens": 572, "completion_tokens": 188}}
[08.05.2025 09:12] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœç´¢èƒ½åŠ›çš„é‡è¦æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å®æ—¶æœç´¢å¼•æ“äº’åŠ¨æ¥æ”¹è¿›LLMsçš„æœç´¢èƒ½åŠ›ï¼Œä½†é¢ä¸´æ–‡æ¡£è´¨é‡ä¸å¯æ§å’ŒAPIè´¹ç”¨é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ZeroSearchï¼Œä¸€ç§ä¸éœ€è¦ä¸å®é™…æœç´¢å¼•æ“äº’åŠ¨çš„RLæ¡†æ¶ã€‚é€šè¿‡è½»é‡çº§çš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºè¯¾ç¨‹çš„æ»šåŠ¨ç­–ç•¥ï¼ŒZeroSearchèƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„æœç´¢èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚
[08.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœç´¢èƒ½åŠ›çš„é‡è¦æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å®æ—¶æœç´¢å¼•æ“äº’åŠ¨æ¥æ”¹è¿›LLMsçš„æœç´¢èƒ½åŠ›ï¼Œä½†é¢ä¸´æ–‡æ¡£è´¨é‡ä¸å¯æ§å’ŒAPIè´¹ç”¨é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ZeroSearchï¼Œä¸€ç§ä¸éœ€è¦ä¸å®é™…æœç´¢å¼•æ“äº’åŠ¨çš„RLæ¡†æ¶ã€‚é€šè¿‡è½»é‡çº§çš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºè¯¾ç¨‹çš„æ»šåŠ¨ç­–ç•¥ï¼ŒZeroSearchèƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„æœç´¢èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚
[08.05.2025 09:12] Mistral response. {"id": "766f8b42e3164f9b96a5d4b9e116eff9", "object": "chat.completion", "created": 1746695567, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Zh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le t\u00edsh\u0113ng d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) s\u014dusu\u01d2 n\u00e9ngl\u00ec de zh\u00f2ngy\u00e0ox\u00ecng. Zu\u00ecj\u00ecn de y\u00e1nji\u016b sh\u01d0y\u00f2ng qi\u00e1ngzh\u00ec xu\u00e9x\u00ed (RL) y\u01d4 sh\u00edsh\u00ed s\u014dusu\u01d2 y\u01d0nq\u00edng h\u00f9d\u00f2ng l\u00e1i g\u01ceij\u00ecn LLMs de s\u014dusu\u01d2 n\u00e9ngl\u00ec, d\u00e0n mi\u00e0nl\u00edn w\u00e9nji\u00e0n zh\u00ecli\u00e0ng b\u00f9 k\u011b k\u00f2ng h\u00e9 API f\u00e8iy\u00f2ng g\u0101o'\u00e1ng de ti\u01ceozh\u00e0n. W\u00e8i ji\u011bju\u00e9 zh\u00e8xi\u0113 w\u00e8nt\u00ed, zu\u00f2zh\u011b t\u00edch\u016b le ZeroSearch, y\u012bzh\u01d2ng b\u00f9 x\u016by\u00e0o y\u01d4 sh\u00edj\u00ec s\u014dusu\u01d2 y\u01d0nq\u00edng h\u00f9d\u00f2ng de RL ku\u00e0ngji\u00e0. T\u014dnggu\u00f2 q\u012bngli\u00e0ngj\u00ed de ji\u00e0nd\u016b w\u0113iti\u00e1o h\u00e9 j\u012by\u00fa k\u00e8ch\u00e9ng de g\u01d4nd\u00f2ng c\u00e8l\u00fc\u00e8, ZeroSearch n\u00e9ngg\u00f2u y\u01d2uxi\u00e0o t\u00edsh\u0113ng LLMs de s\u014dusu\u01d2 n\u00e9ngl\u00ec, b\u00ecngqi\u011b z\u00e0i b\u00f9t\u00f3ng c\u0101nsh\u00f9 gu\u012bm\u00f3 de m\u00f3x\u00edng sh\u00e0ng bi\u01ceoxi\u00e0n li\u00e1ngh\u01ceo."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 202, "total_tokens": 574, "completion_tokens": 372}}
[08.05.2025 09:12] Response: ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le tÃ­shÄ“ng dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) sÅusuÇ’ nÃ©nglÃ¬ de zhÃ²ngyÃ oxÃ¬ng. ZuÃ¬jÃ¬n de yÃ¡njiÅ« shÇyÃ²ng qiÃ¡ngzhÃ¬ xuÃ©xÃ­ (RL) yÇ” shÃ­shÃ­ sÅusuÇ’ yÇnqÃ­ng hÃ¹dÃ²ng lÃ¡i gÇijÃ¬n LLMs de sÅusuÇ’ nÃ©nglÃ¬, dÃ n miÃ nlÃ­n wÃ©njiÃ n zhÃ¬liÃ ng bÃ¹ kÄ› kÃ²ng hÃ© API fÃ¨iyÃ²ng gÄo'Ã¡ng de tiÇozhÃ n. WÃ¨i jiÄ›juÃ© zhÃ¨xiÄ“ wÃ¨ntÃ­, zuÃ²zhÄ› tÃ­chÅ« le ZeroSearch, yÄ«zhÇ’ng bÃ¹ xÅ«yÃ o yÇ” shÃ­jÃ¬ sÅusuÇ’ yÇnqÃ­ng hÃ¹dÃ²ng de RL kuÃ ngjiÃ . TÅngguÃ² qÄ«ngliÃ ngjÃ­ de jiÃ ndÅ« wÄ“itiÃ¡o hÃ© jÄ«yÃº kÃ¨chÃ©ng de gÇ”ndÃ²ng cÃ¨lÃ¼Ã¨, ZeroSearch nÃ©nggÃ²u yÇ’uxiÃ o tÃ­shÄ“ng LLMs de sÅusuÇ’ nÃ©nglÃ¬, bÃ¬ngqiÄ› zÃ i bÃ¹tÃ³ng cÄnshÃ¹ guÄ«mÃ³ de mÃ³xÃ­ng shÃ ng biÇoxiÃ n liÃ¡nghÇo.
[08.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœç´¢èƒ½åŠ›çš„é‡è¦æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å®æ—¶æœç´¢å¼•æ“äº’åŠ¨æ¥æ”¹è¿›LLMsçš„æœç´¢èƒ½åŠ›ï¼Œä½†é¢ä¸´æ–‡æ¡£è´¨é‡ä¸å¯æ§å’ŒAPIè´¹ç”¨é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ZeroSearchï¼Œä¸€ç§ä¸éœ€è¦ä¸å®é™…æœç´¢å¼•æ“äº’åŠ¨çš„RLæ¡†æ¶ã€‚é€šè¿‡è½»é‡çº§çš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºè¯¾ç¨‹çš„æ»šåŠ¨ç­–ç•¥ï¼ŒZeroSearchèƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„æœç´¢èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚
[08.05.2025 09:13] Mistral response. {"id": "8fcb5c0aae90412d9a19d06568965c33", "object": "chat.completion", "created": 1746695578, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u8ba8\u8bba', 'pinyin': 't\u01ceo l\u00f9n', 'trans': 'discuss'},\n{'word': '\u63d0\u5347', 'pinyin': 't\u00ed sh\u0113ng', 'trans': 'improve'},\n{'word': '\u5927\u578b', 'pinyin': 'd\u00e0 x\u00edng', 'trans': 'large-scale'},\n{'word': '\u8bed\u8a00\u6a21\u578b', 'pinyin': 'y\u01d4 y\u00e1n m\u00f3 x\u00edng', 'trans': 'language model'},\n{'word': '\u641c\u7d22', 'pinyin': 's\u014du su\u01d2', 'trans': 'search'},\n{'word': '\u80fd\u529b', 'pinyin': 'n\u00e9ng l\u00ec', 'trans': 'ability'},\n{'word': '\u91cd\u8981\u6027', 'pinyin': 'zh\u00f2ng y\u00e0o x\u00ecng', 'trans': 'importance'},\n{'word': '\u5f3a\u5316\u5b66\u4e60', 'pinyin': 'qi\u00e1ng hu\u00e0 xu\u00e9 x\u00ed', 'trans': 'reinforcement learning'},\n{'word': '\u4e92\u52a8', 'pinyin': 'h\u00f9 d\u00f2ng', 'trans': 'interact'},\n{'word': '\u6539\u8fdb', 'pinyin': 'g\u01cei j\u00ecn', 'trans': 'improve'},\n{'word': '\u6587\u6863', 'pinyin': 'w\u00e9n d\u00e0ng', 'trans': 'document'},\n{'word': '\u8d28\u91cf', 'pinyin': 'zh\u00ec li\u00e0ng', 'trans': 'quality'},\n{'word': '\u4e0d\u53ef\u63a7', 'pinyin': 'b\u00f9 k\u011b k\u00f2ng', 'trans': 'uncontrollable'},\n{'word': 'API', 'pinyin': 'API', 'trans': 'API'},\n{'word': '\u8d39\u7528', 'pinyin': 'f\u00e8i y\u00f2ng', 'trans': 'cost'},\n{'word': '\u9ad8\u6602', 'pinyin': 'g\u0101o \u00e1ng', 'trans': 'high'},\n{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceo zh\u00e0n', 'trans': 'challenge'},\n{'word': '\u89e3\u51b3', 'pinyin': 'ji\u011b ju\u00e9', 'trans': 'solve'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': 'ZeroSearch', 'pinyin': 'ZeroSearch', 'trans': 'ZeroSearch'},\n{'word': '\u6846\u67b6', 'pinyin': 'ku\u00e0ng ji\u00e0', 'trans': 'framework'},\n{'word': '\u8f7b\u91cf\u7ea7', 'pinyin': 'q\u012bng li\u00e0ng j\u00ed', 'trans': 'lightweight'},\n{'word': '\u76d1\u7763', 'pinyin': 'ji\u00e0n d\u016b', 'trans': 'supervised'},\n{'word': '\u5fae\u8c03', 'pinyin': 'w\u0113i ti\u00e1o', 'trans': 'fine-tune'},\n{'word': '\u57fa\u4e8e', 'pinyin': 'j\u012b y\u00fa', 'trans': 'based on'},\n{'word': '\u8bfe\u7a0b', 'pinyin': 'k\u00e8 ch\u00e9ng', 'trans': 'course'},\n{'word': '\u6eda\u52a8', 'pinyin': 'g\u01d4n d\u00f2ng', 'trans': 'rolling'},\n{'word': '\u7b56\u7565', 'pinyin': 'c\u00e8 l\u00fc\u00e8', 'trans': 'strategy'},\n{'word': '\u6709\u6548', 'pinyin': 'y\u01d2u xi\u00e0o', 'trans': 'effective'},\n{'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'performance'},\n{'word': '\u826f\u597d', 'pinyin': 'li\u00e1ng h\u01ceo', 'trans': 'good'},\n{'word': '\u53c2\u6570', 'pinyin': 'c\u0101n sh\u01d4', 'trans': 'parameter'},\n{'word': '\u89c4\u6a21', 'pinyin': 'gu\u012b m\u00f3', 'trans': 'scale'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 234, "total_tokens": 1159, "completion_tokens": 925}}
[08.05.2025 09:13] Response: [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},
{'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'improve'},
{'word': 'å¤§å‹', 'pinyin': 'dÃ  xÃ­ng', 'trans': 'large-scale'},
{'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'language model'},
{'word': 'æœç´¢', 'pinyin': 'sÅu suÇ’', 'trans': 'search'},
{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},
{'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'},
{'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ© xÃ­', 'trans': 'reinforcement learning'},
{'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interact'},
{'word': 'æ”¹è¿›', 'pinyin': 'gÇi jÃ¬n', 'trans': 'improve'},
{'word': 'æ–‡æ¡£', 'pinyin': 'wÃ©n dÃ ng', 'trans': 'document'},
{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'},
{'word': 'ä¸å¯æ§', 'pinyin': 'bÃ¹ kÄ› kÃ²ng', 'trans': 'uncontrollable'},
{'word': 'API', 'pinyin': 'API', 'trans': 'API'},
{'word': 'è´¹ç”¨', 'pinyin': 'fÃ¨i yÃ²ng', 'trans': 'cost'},
{'word': 'é«˜æ˜‚', 'pinyin': 'gÄo Ã¡ng', 'trans': 'high'},
{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},
{'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'},
{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},
{'word': 'ZeroSearch', 'pinyin': 'ZeroSearch', 'trans': 'ZeroSearch'},
{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},
{'word': 'è½»é‡çº§', 'pinyin': 'qÄ«ng liÃ ng jÃ­', 'trans': 'lightweight'},
{'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervised'},
{'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'},
{'word': 'åŸºäº', 'pinyin': 'jÄ« yÃº', 'trans': 'based on'},
{'word': 'è¯¾ç¨‹', 'pinyin': 'kÃ¨ chÃ©ng', 'trans': 'course'},
{'word': 'æ»šåŠ¨', 'pinyin': 'gÇ”n dÃ²ng', 'trans': 'rolling'},
{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},
{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'},
{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},
{'word': 'è‰¯å¥½', 'pinyin': 'liÃ¡ng hÇo', 'trans': 'good'},
{'word': 'å‚æ•°', 'pinyin': 'cÄn shÇ”', 'trans': 'parameter'},
{'word': 'è§„æ¨¡', 'pinyin': 'guÄ« mÃ³', 'trans': 'scale'}]
[08.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœç´¢èƒ½åŠ›çš„é‡è¦æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å®æ—¶æœç´¢å¼•æ“äº’åŠ¨æ¥æ”¹è¿›LLMsçš„æœç´¢èƒ½åŠ›ï¼Œä½†é¢ä¸´æ–‡æ¡£è´¨é‡ä¸å¯æ§å’ŒAPIè´¹ç”¨é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ZeroSearchï¼Œä¸€ç§ä¸éœ€è¦ä¸å®é™…æœç´¢å¼•æ“äº’åŠ¨çš„RLæ¡†æ¶ã€‚é€šè¿‡è½»é‡çº§çš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºè¯¾ç¨‹çš„æ»šåŠ¨ç­–ç•¥ï¼ŒZeroSearchèƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„æœç´¢èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚
[08.05.2025 09:13] Mistral response. {"id": "3285b276745a408788c02b32498fe497", "object": "chat.completion", "created": 1746695585, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the importance of enhancing the search capabilities of large language models (LLMs). Recent research has employed reinforcement learning (RL) to interact with real-time search engines to improve the search capabilities of LLMs, but this approach faces challenges such as uncontrollable document quality and high API costs. To address these issues, the authors propose ZeroSearch, an RL framework that does not require interaction with actual search engines. By utilizing lightweight supervised fine-tuning and a curriculum-based rolling strategy, ZeroSearch can effectively enhance the search capabilities of LLMs and performs well across models of different parameter sizes."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 200, "total_tokens": 330, "completion_tokens": 130}}
[08.05.2025 09:13] Response: This article discusses the importance of enhancing the search capabilities of large language models (LLMs). Recent research has employed reinforcement learning (RL) to interact with real-time search engines to improve the search capabilities of LLMs, but this approach faces challenges such as uncontrollable document quality and high API costs. To address these issues, the authors propose ZeroSearch, an RL framework that does not require interaction with actual search engines. By utilizing lightweight supervised fine-tuning and a curriculum-based rolling strategy, ZeroSearch can effectively enhance the search capabilities of LLMs and performs well across models of different parameter sizes.
[08.05.2025 09:13] Renaming data file.
[08.05.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-05-08.json
[08.05.2025 09:13] Saving new data file.
[08.05.2025 09:13] Generating page.
[08.05.2025 09:13] Renaming previous page.
[08.05.2025 09:13] Renaming previous data. index.html to ./d/2025-05-08.html
[08.05.2025 09:13] [Experimental] Generating Chinese page for reading.
[08.05.2025 09:13] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'æå‡', 'pinyin': 'tÃ­ shÄ“ng', 'trans': 'improve'}, {'word': 'å¤§å‹', 'pinyin': 'dÃ  xÃ­ng', 'trans': 'large-scale'}, {'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'language model'}, {'word': 'æœç´¢', 'pinyin': 'sÅu suÇ’', 'trans': 'search'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ© xÃ­', 'trans': 'reinforcement learning'}, {'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interact'}, {'word': 'æ”¹è¿›', 'pinyin': 'gÇi jÃ¬n', 'trans': 'improve'}, {'word': 'æ–‡æ¡£', 'pinyin': 'wÃ©n dÃ ng', 'trans': 'document'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'ä¸å¯æ§', 'pinyin': 'bÃ¹ kÄ› kÃ²ng', 'trans': 'uncontrollable'}, {'word': 'API', 'pinyin': 'API', 'trans': 'API'}, {'word': 'è´¹ç”¨', 'pinyin': 'fÃ¨i yÃ²ng', 'trans': 'cost'}, {'word': 'é«˜æ˜‚', 'pinyin': 'gÄo Ã¡ng', 'trans': 'high'}, {'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'ZeroSearch', 'pinyin': 'ZeroSearch', 'trans': 'ZeroSearch'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'è½»é‡çº§', 'pinyin': 'qÄ«ng liÃ ng jÃ­', 'trans': 'lightweight'}, {'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervised'}, {'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'}, {'word': 'åŸºäº', 'pinyin': 'jÄ« yÃº', 'trans': 'based on'}, {'word': 'è¯¾ç¨‹', 'pinyin': 'kÃ¨ chÃ©ng', 'trans': 'course'}, {'word': 'æ»šåŠ¨', 'pinyin': 'gÇ”n dÃ²ng', 'trans': 'rolling'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'è‰¯å¥½', 'pinyin': 'liÃ¡ng hÇo', 'trans': 'good'}, {'word': 'å‚æ•°', 'pinyin': 'cÄn shÇ”', 'trans': 'parameter'}, {'word': 'è§„æ¨¡', 'pinyin': 'guÄ« mÃ³', 'trans': 'scale'}]
[08.05.2025 09:13] Renaming previous Chinese page.
[08.05.2025 09:13] Renaming previous data. zh.html to ./d/2025-05-07_zh_reading_task.html
[08.05.2025 09:13] Writing Chinese reading task.
[08.05.2025 09:13] Writing result.
[08.05.2025 09:13] Renaming log file.
[08.05.2025 09:13] Renaming previous data. log.txt to ./logs/2025-05-08_last_log.txt
