[17.10.2025 04:14] Read previous papers.
[17.10.2025 04:14] Generating top page (month).
[17.10.2025 04:14] Writing top page (month).
[17.10.2025 05:12] Read previous papers.
[17.10.2025 05:12] Get feed.
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14975
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14979
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14545
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14359
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14972
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14967
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14943
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14973
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14528
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13998
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10518
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13217
[17.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.14300
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09033
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14276
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14978
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14969
[17.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.14211
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13454
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13054
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14974
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14958
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14880
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14252
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10390
[17.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.14976
[17.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14351
[17.10.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.10.2025 05:12] No deleted papers detected.
[17.10.2025 05:12] Downloading and parsing papers (pdf, html). Total: 27.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14975.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14975.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14975.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14979.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14979.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14979.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14545.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14545.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14545.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14359.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14359.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14359.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14972.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14972.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14972.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14967.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14967.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14967.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14943.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14943.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14943.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14973.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14973.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14973.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14528.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14528.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14528.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13998.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13998.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13998.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.10518.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.10518.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.10518.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13217.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13217.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13217.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14300.
[17.10.2025 05:12] Downloading paper 2510.14300 from http://arxiv.org/pdf/2510.14300v1...
[17.10.2025 05:12] Extracting affiliations from text.
[17.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 0 0 3 4 1 . 0 1 5 2 : r Preprint EXPERTISE NEED NOT MONOPOLIZE: ACTIONSPECIALIZED MIXTURE OF EXPERTS FOR VISIONLANGUAGE-ACTION LEARNING Weijie Shen1,2,8, Yitian Liu1,3, Yuhao Wu5, Zhixuan Liang6,4, Sijia Gu7 Dehui Wang1,2,8, Tian Nian1, Lei Xu3,10, Yusen Qin8, Jiangmiao Pang4, Xinping Guan2,9 Xiaokang Yang1,3, Yao Mu1,3,4 1MoE key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 2School of Automation and Intelligent Sensing, Shanghai Jiao Tong University 3School of Computer Science, Shanghai Jiao Tong University 4Shanghai AI Laboratory 5Tsinghua Shenzhen International Graduate School, Tsinghua University 6The University of Hong Kong 9Key Laboratory of System Control and Information Processing, Ministry of Education of China 10Shanghai Key Laboratory of Integrated Administration Technologies for Information Security {shenweijie,muyao}@sjtu.edu.cn, zxliang@cs.hku.hk 7Tongji University 8D-Robotics "
[17.10.2025 05:12] Response: ```python
[
    "MoE key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
    "School of Automation and Intelligent Sensing, Shanghai Jiao Tong University",
    "School of Computer Science, Shanghai Jiao Tong University",
    "Shanghai AI Laboratory",
    "Tsinghua Shenzhen International Graduate School, Tsinghua University",
    "The University of Hong Kong",
    "Key Laboratory of System Control and Information Processing, Ministry of Education of China",
    "Shanghai Key Laboratory of Integrated Administration Technologies for Information Security",
    "Tongji University",
    "D-Robotics"
]
```
[17.10.2025 05:12] Deleting PDF ./assets/pdf/2510.14300.pdf.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.09033.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.09033.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.09033.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14276.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14276.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14276.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14978.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14978.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14978.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14969.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14969.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14969.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14211.
[17.10.2025 05:12] Downloading paper 2510.14211 from http://arxiv.org/pdf/2510.14211v1...
[17.10.2025 05:12] Extracting affiliations from text.
[17.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning Beomseok Kang, Jiwon Song, Jae-Joon Kim Seoul National University {beomseok, jiwon.song, kimjaejoon}@snu.ac.kr 5 2 0 2 6 1 ] . [ 1 1 1 2 4 1 . 0 1 5 2 : r a "
[17.10.2025 05:12] Response: ```python
["Seoul National University"]
```
[17.10.2025 05:12] Deleting PDF ./assets/pdf/2510.14211.pdf.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13454.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13454.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13454.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.13054.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.13054.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.13054.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14974.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14974.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14974.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14958.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14958.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14958.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14880.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14880.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14880.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14252.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14252.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14252.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.10390.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.10390.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.10390.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14976.
[17.10.2025 05:12] Downloading paper 2510.14976 from http://arxiv.org/pdf/2510.14976v1...
[17.10.2025 05:12] Extracting affiliations from text.
[17.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 6 7 9 4 1 . 0 1 5 2 : r Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation Shaowei Liu Chuan Guo2 Bing Zhou2 1University of Illinois Urbana-Champaign Jian Wang2 2Snap Inc. https://stevenlsw.github.io/ponimator/ Generated Interaction Animation (leftright: time steps) Two-person image Estimated interactive pose Single-person image Generated interactive pose Single-person image + Lift the other onto his back Figure 1. Ponimator enables versatile interaction animation applications anchored on interactive poses. For two-person images (top), Ponimator generates contextual dynamics from estimated interactive poses (green box). For single-person images (middle) with optional text prompts (bottom), Ponimator first generates partner interactive poses (magenta box) and then fulfill the interaction dynamics. Generated interactive pose "
[17.10.2025 05:12] Response: ```python
["University of Illinois Urbana-Champaign", "Snap Inc."]
```
[17.10.2025 05:12] Deleting PDF ./assets/pdf/2510.14976.pdf.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.14351.
[17.10.2025 05:12] Extra JSON file exists (./assets/json/2510.14351.json), skip PDF parsing.
[17.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.14351.json), skip HTML parsing.
[17.10.2025 05:12] Success.
[17.10.2025 05:12] Enriching papers with extra data.
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 0. A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.  					AI-generated summary 				 Identity-consistent generation has become an important focus in text-to...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 1. NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.  					AI-generated summary 				 The edifice of native Vision-Language Models (VLMs) has emerged ...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 2. AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.  					AI-generated summary 				 Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn,...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 3. Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.  					AI-generated summary 				 In an era where AI is evolving from a passive tool into an active and adaptive companio...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 4. Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.  					AI-generated summary 				 Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural lan...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 5. Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 6. LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RL...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 7. Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.  					AI-generated summary 				 This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to ...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 8. PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.  					AI-generated summary 				 In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 9. BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.  					AI-generated summary 				 In this paper, we present ...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 10. VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) have substantially improved post-training ...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 11. LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.  					AI-generated summary 				 Modern IR systems are increasingly tasked with answering c...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 12. AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models are experiencing rapid development...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 13. LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.  					AI-generated summary 				 Recent work suggests that large ...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 14. Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.  					AI-generated summary 				 As large language models (LLMs) become more capable and widely used, ensuring the safety of their ou...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 15. A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.  					AI-generated summary 				 Recent image editing models have achieved impressive results while following natura...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 16. ...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 17. LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 Multi-stage reasoning has emerged as an effective strateg...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 18. VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.  					AI-generated summary 				 The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new p...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 19. ...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 20. Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.  					AI-generated summary 				 Few-step diffusion or flow-based generative models typically distill a velocity-...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 21. MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.  					AI-generated summary 				 While Large Language Mo...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 22. mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.  					AI-generated summary 				 In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 3...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 23. The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.  					AI-generated summary 				 The traditional RAG paradigm, which typically enga...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 24. RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.  					AI-generated summary 				 The ability of language models in RAG systems to selective...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 25. Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  					AI-generated summary 				 Close-proximity human-human interactive poses convey rich contextual information about interaction dynami...
[17.10.2025 05:12] ********************************************************************************
[17.10.2025 05:12] Abstract 26. Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as role-playing agents, yet their cap...
[17.10.2025 05:12] Read previous papers.
[17.10.2025 05:12] Generating reviews via LLM API.
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#cv", "#training", "#diffusion", "#benchmark"], "emoji": "🎭", "ru": {"title": "Генерация лиц без копипаста: баланс идентичности и разнообразия", "desc": "Исследователи решают проблему «копипаста» в text-to-image моделях, когда AI просто копирует референсное лицо вместо с
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#alignment", "#architecture", "#open_source"], "emoji": "🔗", "ru": {"title": "NEO: нативные Vision-Language модели с единым представлением", "desc": "Статья представляет NEO — новое семейство нативных Vision-Language Models (VLM), которые интегрируют визуальну
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#rl", "#training", "#optimization"], "emoji": "⚖️", "ru": {"title": "Балансировка энтропии для стабильного обучения веб-агентов", "desc": "AEPO — это алгоритм обучения с подкреплением для агентов, который решает проблемы, связанные с энтропией при обучении веб-агентов. Ал
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#agi", "#multimodal", "#optimization", "#games", "#interpretability"], "emoji": "🤖", "ru": {"title": "Проактивный AI: помощник, который предугадывает ваши нужды", "desc": "В статье представлена система Alpha-Service, которая использует AI-очки для проактивной помощи польз
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#data", "#plp", "#dataset", "#architecture"], "emoji": "🔤", "ru": {"title": "Проблема токенизации кода: когда пробелы меняют поведение модели", "desc": "Исследователи обнаружили серьёзную проблему в языковых моделях для кода: статистические токениз
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "🎯", "ru": {"title": "Плотные награды через прирост информации для многошагового обучения агентов", "desc": "Статья представляет метод IGPO для улучшения обучения LLM-агентов с помощью reinforcement lear
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "🎯", "ru": {"title": "Самооценка через последний токен для улучшения reasoning", "desc": "LaSeR — это алгоритм reinforcement learning, который улучшает reasoning способности LLM путём объединения генерации решений 
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#diffusion"], "emoji": "⚡", "ru": {"title": "Умное кэширование для ускорения диффузионных языковых моделей", "desc": "Статья предлагает метод Elastic-Cache для оптимизации управления key-value кэшем в диффузионных LLM. Авт
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#science", "#low_resource", "#benchmark"], "emoji": "📄", "ru": {"title": "Эффективное распознавание документов с минимальными ресурсами", "desc": "PaddleOCR-VL — это компактная vision-language модель для парсинга документов, объединяющая визуальный
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#small_models"], "emoji": "🔽", "ru": {"title": "Сжатие LLM до тернарных весов с сохранением качества", "desc": "В статье представлен BitNet Distillation (BitDistill) — легковесный метод для дистилляции полноточных LLM в мо
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#multimodal", "#open_source", "#long_context", "#benchmark"], "emoji": "🎬", "ru": {"title": "Обучение AI размышлять визуально при оценке видео", "desc": "Статья представляет VideoReward Thinker — новый подход к мультимодальным моделям вознаграждения, который поз
[17.10.2025 05:12] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#optimization", "#benchmark"], "emoji": "🌳", "ru": {"title": "Поиск через семантическое дерево с логарифмической сложностью", "desc": "LATTICE — это фреймворк для information retrieval, который организует большие коллекции документов в виде семантического дерев
[17.10.2025 05:12] Querying the API.
[17.10.2025 05:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.
[17.10.2025 05:12] Response: ```json
{
  "title": "Совместная работа экспертов для эффективного управления роботами",
  "emoji": "🤖",
  "desc": "Архитектура AdaMoE использует подход Mixture-of-Experts для масштабирования Vision-Language-Action моделей, сохраняя при этом предобученные веса и вычислительную эффективность. Ключевая инновация — механизм разделения выбора экспертов и их весов через специальный адаптер, что позволяет экспертам работать совместно, а не конкурировать по принципу «победитель получает всё». Модель показывает улучшение производительности на 1.8% в бенчмарке LIBERO, 9.3% в RoboTwin и впечатляющие 21.5% в реальных робототехнических задачах. Подход решает проблему дефицита робототехнических данных и высоких вычислительных требований при обучении VLA моделей с нуля."
}
```
[17.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks."

[17.10.2025 05:12] Response: ```python
['ARCHITECTURE', 'ROBOTICS', 'BENCHMARK']
```
[17.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks."

[17.10.2025 05:12] Response: ```python
["OPTIMIZATION", "AGI"]
```
[17.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AdaMoE is a Mixture-of-Experts architecture designed to enhance Vision-Language-Action (VLA) models for robotic manipulation. It effectively utilizes pretrained weights from dense VLA models to improve computational efficiency and performance. By implementing a decoupling technique, AdaMoE allows for collaborative expert utilization, where multiple experts can contribute to decision-making without competing against each other. This innovative approach leads to significant performance improvements in real-world robotic tasks, demonstrating its practical effectiveness.","title":"Collaborative Expertise for Enhanced Robotic Manipulation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AdaMoE is a Mixture-of-Experts architecture designed to enhance Vision-Language-Action (VLA) models for robotic manipulation. It effectively utilizes pretrained weights from dense VLA models to improve computational efficiency and performance. By implementing a decoupling technique, AdaMoE allows for collaborative expert utilization, where multiple experts can contribute to decision-making without competing against each other. This innovative approach leads to significant performance improvements in real-world robotic tasks, demonstrating its practical effectiveness.', title='Collaborative Expertise for Enhanced Robotic Manipulation'))
[17.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"AdaMoE是一种专家混合架构，旨在通过利用预训练权重和提高计算效率来增强视觉-语言-行动（VLA）模型的性能。该方法解决了从头训练VLA模型所需的高计算资源和数据集稀缺的问题。AdaMoE通过将前馈层替换为稀疏激活的专家层，优化了模型的容量与计算效率的平衡。实验结果表明，AdaMoE在多个基准测试中表现优异，尤其在实际应用中实现了21.5%的性能提升，证明了其在机器人操作任务中的有效性。","title":"AdaMoE：提升机器人操作的专家混合架构"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='AdaMoE是一种专家混合架构，旨在通过利用预训练权重和提高计算效率来增强视觉-语言-行动（VLA）模型的性能。该方法解决了从头训练VLA模型所需的高计算资源和数据集稀缺的问题。AdaMoE通过将前馈层替换为稀疏激活的专家层，优化了模型的容量与计算效率的平衡。实验结果表明，AdaMoE在多个基准测试中表现优异，尤其在实际应用中实现了21.5%的性能提升，证明了其在机器人操作任务中的有效性。', title='AdaMoE：提升机器人操作的专家混合架构'))
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#data", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "🎭", "ru": {"title": "LLM не знают, что они не знают: галлюцинации неотличимы от фактов", "desc": "Исследование показывает, что LLM обрабатывают фактические запросы и галлюцинации схожим образом, когда они связ
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#data", "#open_source", "#alignment", "#training", "#ethics", "#multilingual", "#low_resource", "#benchmark"], "emoji": "🛡️", "ru": {"title": "Многоязычная защита LLM с трёхуровневой классификацией и проверкой в реальном времени", "desc": "Qwen3Guard — это семейство многоязычных мод
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#cv", "#rlhf", "#training", "#optimization", "#synthetic", "#diffusion", "#benchmark"], "emoji": "✂️", "ru": {"title": "Редактирование изображений без парных данных через обратную связь от VLM", "desc": "Авторы предлагают новую парадигму обучения моделей редактирования изображений, 
[17.10.2025 05:13] Using data from previous issue: {"categories": [], "emoji": "🤝", "ru": {"title": "Когда AI модель не уверена — лучше спросить человека", "desc": "Исследователи предлагают метод, который позволяет LLM определять, когда они недостаточно уверены в ответе и нуждаются в помощи человека. Система использует специальный подход к калибровк
[17.10.2025 05:13] Querying the API.
[17.10.2025 05:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.
[17.10.2025 05:13] Response: ```json
{
  "title": "Ускорение многоэтапного рассуждения через умный пропуск слоёв",
  "desc": "LiteStage — это фреймворк для ускорения многоэтапного рассуждения в малых языковых моделях путём пропуска слоёв нейронной сети. Система учитывает, что разные этапы рассуждения имеют разную чувствительность к пропуску слоёв, и оптимально распределяет вычислительный бюджет между этапами. Дополнительно используется механизм раннего выхода при генерации, который останавливает декодирование избыточных токенов на основе уверенности модели. В результате достигается ускорение до 1.70 раз при потере точности менее 4%, что превосходит предыдущие методы пропуска слоёв без дообучения.",
  "emoji": "⚡",
  "desc_chars": 543
}
```
[17.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods."

[17.10.2025 05:13] Response: ```python
["INFERENCE", "TRAINING", "SMALL_MODELS"]
```
[17.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods."

[17.10.2025 05:13] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[17.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LiteStage is a framework designed to improve the efficiency of multi-stage reasoning in small language models by optimizing how layers are used. It addresses the challenges of varying sensitivity to layer skipping and the issue of generating unnecessary output tokens. By implementing a two-part approach that includes an offline search for optimal layer budgets and an online method for early exits based on confidence, LiteStage minimizes latency while maintaining accuracy. Experiments demonstrate that it can significantly speed up processing times with only a small decrease in accuracy compared to previous methods.","title":"Speed Up Multi-Stage Reasoning with LiteStage!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LiteStage is a framework designed to improve the efficiency of multi-stage reasoning in small language models by optimizing how layers are used. It addresses the challenges of varying sensitivity to layer skipping and the issue of generating unnecessary output tokens. By implementing a two-part approach that includes an offline search for optimal layer budgets and an online method for early exits based on confidence, LiteStage minimizes latency while maintaining accuracy. Experiments demonstrate that it can significantly speed up processing times with only a small decrease in accuracy compared to previous methods.', title='Speed Up Multi-Stage Reasoning with LiteStage!'))
[17.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LiteStage是一个关注延迟的层跳过框架，旨在优化多阶段推理的效率。它通过分配最佳层预算和抑制冗余输出标记，显著提高了推理速度，同时保持了较小的准确性损失。该框架结合了阶段性离线搜索和基于置信度的在线生成早期退出策略，以解决现有技术在效率和准确性之间的平衡问题。实验结果表明，LiteStage在多个基准测试中实现了高达1.70倍的加速，且准确性损失低于4.0%。","title":"LiteStage：提升推理速度的智能层跳过框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LiteStage是一个关注延迟的层跳过框架，旨在优化多阶段推理的效率。它通过分配最佳层预算和抑制冗余输出标记，显著提高了推理速度，同时保持了较小的准确性损失。该框架结合了阶段性离线搜索和基于置信度的在线生成早期退出策略，以解决现有技术在效率和准确性之间的平衡问题。实验结果表明，LiteStage在多个基准测试中实现了高达1.70倍的加速，且准确性损失低于4.0%。', title='LiteStage：提升推理速度的智能层跳过框架'))
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#alignment", "#training", "#optimization"], "emoji": "🎬", "ru": {"title": "От текста к 3D через видео: сшивание моделей для создания сцен", "desc": "VIST3A — это новый фреймворк для генерации 3D-сцен из текста, который объединяет latent text-to-video модели с с
[17.10.2025 05:13] Using data from previous issue: {"categories": [], "emoji": "🎯", "ru": {"title": "Оптимизация промптов через градиентный спуск в пространстве эмбеддингов", "desc": "Исследователи предлагают метод автоматической оптимизации промптов для языковых моделей, используя градиентный спуск непосредственно в пространстве эмбеддингов токенов
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#diffusion"], "emoji": "🌊", "ru": {"title": "Динамические траектории для быстрой генерации изображений", "desc": "Статья представляет pi-Flow — новый подход к дистилляции flow-based моделей генерации изображений. Вместо предсказания прямого пути 
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#games", "#math", "#benchmark"], "emoji": "📐", "ru": {"title": "Визуальная цепочка рассуждений для математики", "desc": "MathCanvas — это фреймворк для обучения больших мультимодальных моделей решению математических задач с использованием виз
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#long_context", "#training", "#dataset", "#optimization", "#benchmark", "#small_models"], "emoji": "🔍", "ru": {"title": "Мощный поиск в кармане: компактные модели побеждают гигантов", "desc": "Исследователи представили mxbai-edge-colbert-v0 — компактные модели для поиска информации 
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#multimodal", "#training", "#interpretability"], "emoji": "🧠", "ru": {"title": "От пассивных чанков к активной памяти документов", "desc": "Статья представляет фреймворк MoM, который улучшает RAG-системы, превращая пассивное разделение текста на чанки в активно
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#security", "#hallucinations", "#alignment", "#benchmark", "#rag"], "emoji": "🚫", "ru": {"title": "Когда AI должен сказать «не знаю»: тестируем умение моделей отказываться отвечать", "desc": "Исследователи обнаружили критическую проблему в RAG-системах: языковые модели плохо справля
[17.10.2025 05:13] Querying the API.
[17.10.2025 05:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  					AI-generated summary 				 Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.
[17.10.2025 05:13] Response: ```json
{
  "title": "Анимация взаимодействий через позы близкого контакта",
  "emoji": "🤝",
  "desc": "Ponimator — это фреймворк для генерации анимаций взаимодействия двух людей на основе поз близкого контакта из motion capture данных. Система использует два conditional diffusion models: один генерирует динамические последовательности движений из интерактивных поз, второй синтезирует сами интерактивные позы из одиночной позы, текста или их комбинации. Подход опирается на сильные prior'ы человеческого поведения, которые позволяют предсказывать контекст и динамику взаимодействия по позам близкого контакта. Система поддерживает различные задачи: анимацию по изображению, генерацию реакций и синтез взаимодействий по текстовому описанию, эффективно переносяя знания из качественных mocap данных в открытые сценарии."
}
```
[17.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  					AI-generated summary 				 Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework."

[17.10.2025 05:13] Response: ```python
['CV', 'MULTIMODAL', 'DATASET']
```
[17.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  					AI-generated summary 				 Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework."

[17.10.2025 05:13] Response: ```python
['DIFFUSION', 'TRANSFER_LEARNING']
```
[17.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Ponimator is a framework that utilizes conditional diffusion models to create and synthesize interactive poses based on motion capture data. It focuses on close-contact human interactions, allowing for the generation of dynamic motion sequences and interactive poses. The framework consists of two main components: a pose animator for generating motion sequences and a pose generator for synthesizing poses from various inputs. Through empirical testing, Ponimator shows its ability to effectively transfer interaction knowledge to various animation tasks, enhancing versatility in animation applications.","title":"Transforming Motion Capture into Interactive Animation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Ponimator is a framework that utilizes conditional diffusion models to create and synthesize interactive poses based on motion capture data. It focuses on close-contact human interactions, allowing for the generation of dynamic motion sequences and interactive poses. The framework consists of two main components: a pose animator for generating motion sequences and a pose generator for synthesizing poses from various inputs. Through empirical testing, Ponimator shows its ability to effectively transfer interaction knowledge to various animation tasks, enhancing versatility in animation applications.', title='Transforming Motion Capture into Interactive Animation'))
[17.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Ponimator 是一个基于条件扩散模型的框架，用于从动作捕捉数据生成和合成互动姿势。该模型利用近距离人际互动姿势，能够捕捉丰富的上下文信息，帮助人类直观推测互动的动态。Ponimator 通过两个条件扩散模型来实现：一个用于生成动态运动序列，另一个用于从单一姿势或文本合成互动姿势。实验结果表明，Ponimator 在多种数据集和应用中表现出良好的通用性和有效性。","title":"Ponimator：互动动画的智能生成"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Ponimator 是一个基于条件扩散模型的框架，用于从动作捕捉数据生成和合成互动姿势。该模型利用近距离人际互动姿势，能够捕捉丰富的上下文信息，帮助人类直观推测互动的动态。Ponimator 通过两个条件扩散模型来实现：一个用于生成动态运动序列，另一个用于从单一姿势或文本合成互动姿势。实验结果表明，Ponimator 在多种数据集和应用中表现出良好的通用性和有效性。', title='Ponimator：互动动画的智能生成'))
[17.10.2025 05:13] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#multimodal", "#alignment", "#ethics", "#benchmark"], "emoji": "🦸", "ru": {"title": "Супергерои из разных вселенных: проверка LLM на последовательность ролевой игры", "desc": "Исследователи создали бенчмарк Beyond One World для оценки способности LLM последо
[17.10.2025 05:13] Renaming data file.
[17.10.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-10-17.json
[17.10.2025 05:13] Saving new data file.
[17.10.2025 05:13] Generating page.
[17.10.2025 05:13] Renaming previous page.
[17.10.2025 05:13] Renaming previous data. index.html to ./d/2025-10-17.html
[17.10.2025 05:13] Writing result.
[17.10.2025 05:13] Renaming log file.
[17.10.2025 05:13] Renaming previous data. log.txt to ./logs/2025-10-17_last_log.txt
