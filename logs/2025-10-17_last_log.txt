[17.10.2025 06:19] Read previous papers.
[17.10.2025 06:19] Generating top page (month).
[17.10.2025 06:19] Writing top page (month).
[17.10.2025 07:12] Read previous papers.
[17.10.2025 07:12] Get feed.
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14545
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14975
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14359
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14979
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14967
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14972
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14943
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14973
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14528
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10518
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13998
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14958
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14763
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14616
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13217
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14300
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14276
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09033
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14978
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14211
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13054
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14969
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13454
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14880
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14974
[17.10.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.14955
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14949
[17.10.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.14902
[17.10.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.14847
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14252
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13996
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14976
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14351
[17.10.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10390
[17.10.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.06694
[17.10.2025 07:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.10.2025 07:12] No deleted papers detected.
[17.10.2025 07:12] Downloading and parsing papers (pdf, html). Total: 35.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14545.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14545.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14545.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14975.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14975.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14975.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14359.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14359.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14359.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14979.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14979.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14979.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14967.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14967.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14967.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14972.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14972.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14972.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14943.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14943.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14943.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14973.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14973.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14973.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14528.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14528.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14528.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.10518.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.10518.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.10518.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.13998.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.13998.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.13998.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14958.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14958.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14958.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14763.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14763.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14763.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14616.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14616.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14616.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.13217.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.13217.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.13217.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14300.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14300.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14300.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14276.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14276.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14276.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.09033.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.09033.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.09033.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14978.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14978.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14978.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14211.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14211.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14211.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.13054.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.13054.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.13054.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14969.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14969.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14969.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.13454.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.13454.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.13454.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14880.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14880.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14880.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14974.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14974.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14974.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14955.
[17.10.2025 07:12] Downloading paper 2510.14955 from http://arxiv.org/pdf/2510.14955v1...
[17.10.2025 07:12] Extracting affiliations from text.
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"REALDPO: REAL OR NOT REAL, THAT IS THE PREFERENCE Guo Cheng3 Danni Yang1 Ziqi Huang2 1Shanghai Artificial Intelligence Laboratory 3University of Electronic Science and Technology of China 5SenseTime Research Project Page: Vchitect.github.io/RealDPO-Project Jianlou Si5 Chenyang Si4 Ziwei Liu2(cid:66) 2S-Lab, Nanyang Technological University 4Nanjing University 5 2 0 O 6 1 ] . [ 1 5 5 9 4 1 . 0 1 5 2 : r Figure 1: Can we align video generative models using real data as preference data without reward model? (a) Comparison between using the reward model to score synthetic data for preference learning and our RealDPO method, which uses high-quality real data as win samples. Our method avoids the limitations of the reward model and the associated hacking issues. (b) Comparison between the video generated by the pretrained model and the real video for the same scene. The three scores on the right represent the scores given by the reward model from VisionReward (Xu et al., 2024a), the human action metric from VBench (Huang et al., 2024a;b), and human preference, respectively. It can be observed that while the existing reward model and VBench can evaluate semantic correctness, they are limited in assessing human motion quality. (c) Three model-generated examples from the same prompt, each with different initial noise, exhibit poor limb interaction, making it challenging for human annotators to identify which sample should be chosen as the win sample for reward model training. "
[17.10.2025 07:12] Response: ```python
["Shanghai Artificial Intelligence Laboratory", "University of Electronic Science and Technology of China", "SenseTime Research", "S-Lab, Nanyang Technological University", "Nanjing University"]
```
[17.10.2025 07:12] Deleting PDF ./assets/pdf/2510.14955.pdf.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14949.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14949.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14949.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14902.
[17.10.2025 07:12] Downloading paper 2510.14902 from http://arxiv.org/pdf/2510.14902v1...
[17.10.2025 07:12] Extracting affiliations from text.
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VLA2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation Han Zhao,1,2, Jiaxuan Zhang,2,3, Wenxuan Song4, Pengxiang Ding1,2, Donglin Wang*,2, 1Zhejiang University, China 2MiLAB, Westlake University, China 3Southern University of Science and Technology, China 4Hong Kong University of Science and Technology (Guangzhou), China 5 2 0 2 6 1 ] . [ 1 2 0 9 4 1 . 0 1 5 2 : r Abstract Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multitask capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose novel agentic framework, VLA2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current stateof-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA2 achieves 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla2.github.io. I. INTRODUCTION In recent years, foundation models have profoundly influenced the development of artificial intelligence research. This impact spans visual encoders [1][3], multi-modal large language models [4][6"
[17.10.2025 07:12] Response: ```python
[
    "Zhejiang University, China",
    "MiLAB, Westlake University, China",
    "Southern University of Science and Technology, China",
    "Hong Kong University of Science and Technology (Guangzhou), China"
]
```
[17.10.2025 07:12] Deleting PDF ./assets/pdf/2510.14902.pdf.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14847.
[17.10.2025 07:12] Downloading paper 2510.14847 from http://arxiv.org/pdf/2510.14847v1...
[17.10.2025 07:12] Extracting affiliations from text.
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints Meiqi Wu1,3 Bingze Song2 1 UCAS 2 AMAP, Alibaba Group 3 CRISE 4 THU 5 SEU Jiashu Zhu2 Xiaokun Feng1,3 Fangyuan Mao2 Chubin Chen4 Chen Zhu Jiahong Wu2 Xiangxiang Chu2 Kaiqi Huang1,3 5 2 0 2 6 1 ] . [ 1 7 4 8 4 1 . 0 1 5 2 : r Figure 1: The motivation of ImagerySearch. The figure illustrates two semantic dependency scenarios related to camels. Left: The distance depicts the corresponding strength of prompt tokens during the denoising process. LDT-Bench consists of imaginative scenarios with long-distance semantics, whose semantic dependencies are typically weak. Right: Wan2.1 performs well on short-distance semantics but fails under long-distance. Test time scaling methods (e.g., Video T1 (Liu et al., 2025a), Evosearch (He et al., 2025a)) also struggle. However, ImagerySearch generates coherent, context-aware motions (orange box). "
[17.10.2025 07:12] Response: ```python
["UCAS", "AMAP, Alibaba Group", "CRISE", "THU", "SEU"]
```
[17.10.2025 07:12] Deleting PDF ./assets/pdf/2510.14847.pdf.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14252.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14252.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14252.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.13996.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.13996.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.13996.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14976.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14976.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14976.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.14351.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.14351.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.14351.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.10390.
[17.10.2025 07:12] Extra JSON file exists (./assets/json/2510.10390.json), skip PDF parsing.
[17.10.2025 07:12] Paper image links file exists (./assets/img_data/2510.10390.json), skip HTML parsing.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2510.06694.
[17.10.2025 07:12] Downloading paper 2510.06694 from http://arxiv.org/pdf/2510.06694v1...
[17.10.2025 07:12] Extracting affiliations from text.
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 4 9 6 6 0 . 0 1 5 2 : r Published in Transactions on Machine Learning Research (06/2025) SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis Jipeng Lyu University of Illinois Urbana-Champaign Jiahua Dong University of Illinois Urbana-Champaign Yu-Xiong Wang University of Illinois Urbana-Champaign Reviewed on OpenReview: https: // openreview. net/ forum? id= YkycjbKjYP lvjipenglv@gmail.com jiahuad2@illinois.edu yxw@illinois.edu Figure 1: Our method achieves satisfying rendering results with 100 training iterations per frame. Leveraging learned deformation information, we also demonstrate successful articulated object segmentation. "
[17.10.2025 07:12] Response: ```python
["University of Illinois Urbana-Champaign"]
```
[17.10.2025 07:12] Deleting PDF ./assets/pdf/2510.06694.pdf.
[17.10.2025 07:12] Success.
[17.10.2025 07:12] Enriching papers with extra data.
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 0. AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.  					AI-generated summary 				 Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn,...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 1. A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.  					AI-generated summary 				 Identity-consistent generation has become an important focus in text-to...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 2. Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.  					AI-generated summary 				 In an era where AI is evolving from a passive tool into an active and adaptive companio...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 3. NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.  					AI-generated summary 				 The edifice of native Vision-Language Models (VLMs) has emerged ...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 4. Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 5. Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.  					AI-generated summary 				 Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural lan...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 6. LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RL...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 7. Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.  					AI-generated summary 				 This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to ...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 8. PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.  					AI-generated summary 				 In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 9. VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) have substantially improved post-training ...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 10. BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.  					AI-generated summary 				 In this paper, we present ...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 11. MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.  					AI-generated summary 				 While Large Language Mo...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 12. COIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.  					AI-generated summary 				 Large language models exhibit systematic deficien...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 13. Generative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.  					AI-generated summary 				 Current preferenc...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 14. LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.  					AI-generated summary 				 Modern IR systems are increasingly tasked with answering c...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 15. AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models are experiencing rapid development...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 16. Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.  					AI-generated summary 				 As large language models (LLMs) become more capable and widely used, ensuring the safety of their ou...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 17. LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.  					AI-generated summary 				 Recent work suggests that large ...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 18. A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.  					AI-generated summary 				 Recent image editing models have achieved impressive results while following natura...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 19. LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 Multi-stage reasoning has emerged as an effective strateg...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 20. ...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 21. ...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 22. VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.  					AI-generated summary 				 The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new p...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 23. mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.  					AI-generated summary 				 In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 3...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 24. Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.  					AI-generated summary 				 Few-step diffusion or flow-based generative models typically distill a velocity-...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 25. RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  					AI-generated summary 				 Video generative models have recently achieved notable advancements in synthes...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 26. A new benchmark and encoder-based mitigation strategy improve multimodal generative models' performance on dialectal textual input without degrading performance on Standard American English.  					AI-generated summary 				 Contact languages like English exhibit rich regional variations in the form o...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 27. A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  					AI-generated summary 				 Current vision-language-action (VLA) models, pre-trained on l...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 28. ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  					AI-generated summary 				 Video generation models hav...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 29. The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.  					AI-generated summary 				 The traditional RAG paradigm, which typically enga...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 30. The German Commons provides a large-scale, openly licensed dataset for training German language models, addressing the scarcity of such data.  					AI-generated summary 				 Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, li...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 31. Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  					AI-generated summary 				 Close-proximity human-human interactive poses convey rich contextual information about interaction dynami...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 32. Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as role-playing agents, yet their cap...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 33. RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.  					AI-generated summary 				 The ability of language models in RAG systems to selective...
[17.10.2025 07:12] ********************************************************************************
[17.10.2025 07:12] Abstract 34. SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  					AI-generated summary 				 Persistent dynamic scene modeling for trackin...
[17.10.2025 07:12] Read previous papers.
[17.10.2025 07:12] Generating reviews via LLM API.
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#agents", "#rl", "#training", "#optimization"], "emoji": "⚖️", "ru": {"title": "Балансировка энтропии для стабильного обучения веб-агентов", "desc": "AEPO — это алгоритм обучения с подкреплением для агентов, который решает проблемы, связанные с энтропией при обучении веб-агентов. Ал
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#cv", "#training", "#diffusion", "#benchmark"], "emoji": "🎭", "ru": {"title": "Генерация лиц без копипаста: баланс идентичности и разнообразия", "desc": "Исследователи решают проблему «копипаста» в text-to-image моделях, когда AI просто копирует референсное лицо вместо с
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#agents", "#agi", "#multimodal", "#optimization", "#games", "#interpretability"], "emoji": "🤖", "ru": {"title": "Проактивный AI: помощник, который предугадывает ваши нужды", "desc": "В статье представлена система Alpha-Service, которая использует AI-очки для проактивной помощи польз
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#alignment", "#architecture", "#open_source"], "emoji": "🔗", "ru": {"title": "NEO: нативные Vision-Language модели с единым представлением", "desc": "Статья представляет NEO — новое семейство нативных Vision-Language Models (VLM), которые интегрируют визуальну
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#agents", "#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "🎯", "ru": {"title": "Плотные награды через прирост информации для многошагового обучения агентов", "desc": "Статья представляет метод IGPO для улучшения обучения LLM-агентов с помощью reinforcement lear
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#data", "#plp", "#dataset", "#architecture"], "emoji": "🔤", "ru": {"title": "Проблема токенизации кода: когда пробелы меняют поведение модели", "desc": "Исследователи обнаружили серьёзную проблему в языковых моделях для кода: статистические токениз
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "🎯", "ru": {"title": "Самооценка через последний токен для улучшения reasoning", "desc": "LaSeR — это алгоритм reinforcement learning, который улучшает reasoning способности LLM путём объединения генерации решений 
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#diffusion"], "emoji": "⚡", "ru": {"title": "Умное кэширование для ускорения диффузионных языковых моделей", "desc": "Статья предлагает метод Elastic-Cache для оптимизации управления key-value кэшем в диффузионных LLM. Авт
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#science", "#low_resource", "#benchmark"], "emoji": "📄", "ru": {"title": "Эффективное распознавание документов с минимальными ресурсами", "desc": "PaddleOCR-VL — это компактная vision-language модель для парсинга документов, объединяющая визуальный
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#multimodal", "#open_source", "#long_context", "#benchmark"], "emoji": "🎬", "ru": {"title": "Обучение AI размышлять визуально при оценке видео", "desc": "Статья представляет VideoReward Thinker — новый подход к мультимодальным моделям вознаграждения, который поз
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#small_models"], "emoji": "🔽", "ru": {"title": "Сжатие LLM до тернарных весов с сохранением качества", "desc": "В статье представлен BitNet Distillation (BitDistill) — легковесный метод для дистилляции полноточных LLM в мо
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#games", "#math", "#benchmark"], "emoji": "📐", "ru": {"title": "Визуальная цепочка рассуждений для математики", "desc": "MathCanvas — это фреймворк для обучения больших мультимодальных моделей решению математических задач с использованием виз
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#story_generation", "#multilingual", "#low_resource", "#dataset"], "emoji": "✍️", "ru": {"title": "Креативность AI требует баланса логики и языка", "desc": "Исследователи создали датасет COIG-Writer для обучения LLM креативному письму на китайском языке, содержащий 1665 примеров с п
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#reasoning", "#low_resource", "#dataset", "#story_generation"], "emoji": "✍️", "ru": {"title": "Рассуждения важнее классификации для оценки творческих текстов", "desc": "Исследователи создали WritingPreferenceBench — датасет из 1800 примеров творческих текстов
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#optimization", "#benchmark"], "emoji": "🌳", "ru": {"title": "Поиск через семантическое дерево с логарифмической сложностью", "desc": "LATTICE — это фреймворк для information retrieval, который организует большие коллекции документов в виде семантического дерев
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agi", "#benchmark", "#architecture"], "emoji": "🤖", "ru": {"title": "Совместная работа экспертов для эффективного управления роботами", "desc": "Архитектура AdaMoE использует подход Mixture-of-Experts для масштабирования Vision-Language-Action моделей,
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#data", "#open_source", "#alignment", "#training", "#ethics", "#multilingual", "#low_resource", "#benchmark"], "emoji": "🛡️", "ru": {"title": "Многоязычная защита LLM с трёхуровневой классификацией и проверкой в реальном времени", "desc": "Qwen3Guard — это семейство многоязычных мод
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#data", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "🎭", "ru": {"title": "LLM не знают, что они не знают: галлюцинации неотличимы от фактов", "desc": "Исследование показывает, что LLM обрабатывают фактические запросы и галлюцинации схожим образом, когда они связ
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#cv", "#rlhf", "#training", "#optimization", "#synthetic", "#diffusion", "#benchmark"], "emoji": "✂️", "ru": {"title": "Редактирование изображений без парных данных через обратную связь от VLM", "desc": "Авторы предлагают новую парадигму обучения моделей редактирования изображений, 
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#reasoning", "#inference"], "emoji": "⚡", "ru": {"title": "Ускорение многоэтапного рассуждения через умный пропуск слоёв", "desc": "LiteStage — это фреймворк для ускорения многоэтапного рассуждения в малых языковых моделях путём пропуск
[17.10.2025 07:12] Using data from previous issue: {"categories": [], "emoji": "🎯", "ru": {"title": "Оптимизация промптов через градиентный спуск в пространстве эмбеддингов", "desc": "Исследователи предлагают метод автоматической оптимизации промптов для языковых моделей, используя градиентный спуск непосредственно в пространстве эмбеддингов токенов
[17.10.2025 07:12] Using data from previous issue: {"categories": [], "emoji": "🤝", "ru": {"title": "Когда AI модель не уверена — лучше спросить человека", "desc": "Исследователи предлагают метод, который позволяет LLM определять, когда они недостаточно уверены в ответе и нуждаются в помощи человека. Система использует специальный подход к калибровк
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#alignment", "#training", "#optimization"], "emoji": "🎬", "ru": {"title": "От текста к 3D через видео: сшивание моделей для создания сцен", "desc": "VIST3A — это новый фреймворк для генерации 3D-сцен из текста, который объединяет latent text-to-video модели с с
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#long_context", "#training", "#dataset", "#optimization", "#benchmark", "#small_models"], "emoji": "🔍", "ru": {"title": "Мощный поиск в кармане: компактные модели побеждают гигантов", "desc": "Исследователи представили mxbai-edge-colbert-v0 — компактные модели для поиска информации 
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#diffusion"], "emoji": "🌊", "ru": {"title": "Динамические траектории для быстрой генерации изображений", "desc": "Статья представляет pi-Flow — новый подход к дистилляции flow-based моделей генерации изображений. Вместо предсказания прямого пути 
[17.10.2025 07:12] Querying the API.
[17.10.2025 07:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  					AI-generated summary 				 Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.
[17.10.2025 07:12] Response: ```json
{
  "desc": "RealDPO — новый метод обучения генеративных видео-моделей, который использует реальные видео как эталонные примеры для улучшения качества синтеза движений. В отличие от стандартного supervised fine-tuning, метод применяет Direct Preference Optimization (DPO), сравнивая реальные видео с ошибочными результатами модели для итеративного самокорректирования. Авторы представляют датасет RealAction-5K с высококачественными видео повседневной активности людей с детальными движениями. Эксперименты показывают значительное улучшение реалистичности движений, качества видео и соответствия текстовым описаниям по сравнению с существующими моделями.",
  "emoji": "🎬",
  "title": "Обучение на реальных видео делает AI-движения естественными"
}
```
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  					AI-generated summary 				 Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques."

[17.10.2025 07:12] Response: ```python
['DATASET', 'VIDEO', 'TRAINING']
```
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  					AI-generated summary 				 Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques."

[17.10.2025 07:12] Response: ```python
["ALIGNMENT", "OPTIMIZATION"]
```
[17.10.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RealDPO is a new method for improving how video generative models create realistic motions by using real-world data for preference learning. It addresses the challenge of generating smooth and contextually accurate movements, which has been a limitation in current models. By using Direct Preference Optimization (DPO) and a special loss function, RealDPO allows the model to learn from real videos and correct its mistakes iteratively. The introduction of the RealAction-5K dataset provides high-quality examples of human activities, further enhancing the model\'s ability to produce lifelike motion in videos.","title":"Enhancing Motion Realism in Video Generation with RealDPO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="RealDPO is a new method for improving how video generative models create realistic motions by using real-world data for preference learning. It addresses the challenge of generating smooth and contextually accurate movements, which has been a limitation in current models. By using Direct Preference Optimization (DPO) and a special loss function, RealDPO allows the model to learn from real videos and correct its mistakes iteratively. The introduction of the RealAction-5K dataset provides high-quality examples of human activities, further enhancing the model's ability to produce lifelike motion in videos.", title='Enhancing Motion Realism in Video Generation with RealDPO'))
[17.10.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RealDPO是一种新颖的偏好学习范式，利用真实世界数据来增强视频生成模型中的运动真实感。它通过直接偏好优化（DPO）和迭代自我修正的方法，解决了生成复杂运动时的挑战。与传统的监督微调（SFT）不同，RealDPO使用定制的损失函数，提供更有效的反馈，从而提高运动合成的准确性。通过对比真实视频和模型输出的错误，RealDPO能够逐步改进运动质量，显著提升视频质量和运动真实感。","title":"真实数据驱动的运动合成优化"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RealDPO是一种新颖的偏好学习范式，利用真实世界数据来增强视频生成模型中的运动真实感。它通过直接偏好优化（DPO）和迭代自我修正的方法，解决了生成复杂运动时的挑战。与传统的监督微调（SFT）不同，RealDPO使用定制的损失函数，提供更有效的反馈，从而提高运动合成的准确性。通过对比真实视频和模型输出的错误，RealDPO能够逐步改进运动质量，显著提升视频质量和运动真实感。', title='真实数据驱动的运动合成优化'))
[17.10.2025 07:12] Using data from previous issue: {"categories": ["#multimodal", "#low_resource", "#benchmark", "#synthetic"], "emoji": "🗣️", "ru": {"title": "Обучение генеративных моделей понимать диалекты английского языка", "desc": "Исследователи создали новый бенчмарк для оценки мультимодальных генеративных моделей на шести диалектах английског
[17.10.2025 07:12] Querying the API.
[17.10.2025 07:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  					AI-generated summary 				 Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io.
[17.10.2025 07:12] Response: ```json
{
  "desc": "Статья представляет VLA^2 — новый агентный фреймворк для улучшения vision-language-action моделей в робототехнике. Основная проблема существующих VLA моделей заключается в значительном снижении точности при работе с объектами, которых не было в обучающих данных. VLA^2 решает эту проблему, интегрируя внешние модули, такие как поиск в интернете и детекция объектов, которые предоставляют дополнительную визуальную и текстовую информацию о целевых объектах. На новом бенчмарке авторы достигли улучшения на 44.2% по сравнению с базовой моделью OpenVLA на самом сложном уровне тестирования.",
  "emoji": "🤖",
  "title": "Агентный подход к обобщению VLA моделей на невиденные объекты"
}
```
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  					AI-generated summary 				 Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io."

[17.10.2025 07:12] Response: ```python
['AGENTS', 'CV', 'BENCHMARK']
```
[17.10.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  					AI-generated summary 				 Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io."

[17.10.2025 07:12] Response: ```python
["AGI", "OPTIMIZATION"]
```
[17.10.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces VLA^2, a new framework that enhances vision-language-action (VLA) models by incorporating external modules like web retrieval and object detection. This integration allows the model to better generalize to unseen objects and descriptions, addressing the limitations of existing models that struggle with out-of-distribution data. By utilizing the OpenVLA execution backbone, VLA^2 significantly improves the success rate in challenging scenarios, achieving a 44.2% increase in performance on a hard-level benchmark. The framework demonstrates robust capabilities across various environments without compromising performance on familiar tasks.","title":"VLA^2: Enhancing Generalization in Vision-Language-Action Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces VLA^2, a new framework that enhances vision-language-action (VLA) models by incorporating external modules like web retrieval and object detection. This integration allows the model to better generalize to unseen objects and descriptions, addressing the limitations of existing models that struggle with out-of-distribution data. By utilizing the OpenVLA execution backbone, VLA^2 significantly improves the success rate in challenging scenarios, achieving a 44.2% increase in performance on a hard-level benchmark. The framework demonstrates robust capabilities across various environments without compromising performance on familiar tasks.', title='VLA^2: Enhancing Generalization in Vision-Language-Action Models'))
[17.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新颖的代理框架VLA^2，旨在增强视觉-语言-动作（VLA）模型的能力。通过整合外部模块，如网络检索和物体检测，VLA^2能够提高模型对未见物体和描述的泛化能力。该框架在LIBERO仿真环境中进行了评估，成功超越了当前最先进的模型，尤其在困难级别的基准测试中表现出色。与传统的OpenVLA基线相比，VLA^2在困难级别基准测试中的成功率提高了44.2%。","title":"VLA^2：提升视觉-语言-动作模型的泛化能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新颖的代理框架VLA^2，旨在增强视觉-语言-动作（VLA）模型的能力。通过整合外部模块，如网络检索和物体检测，VLA^2能够提高模型对未见物体和描述的泛化能力。该框架在LIBERO仿真环境中进行了评估，成功超越了当前最先进的模型，尤其在困难级别的基准测试中表现出色。与传统的OpenVLA基线相比，VLA^2在困难级别基准测试中的成功率提高了44.2%。', title='VLA^2：提升视觉-语言-动作模型的泛化能力'))
[17.10.2025 07:13] Querying the API.
[17.10.2025 07:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  					AI-generated summary 				 Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.
[17.10.2025 07:13] Response: ```json
{
  "desc": "Статья представляет ImagerySearch - метод адаптивного поиска во время инференса для улучшения генерации видео в творческих сценариях. Основная проблема существующих моделей - они плохо справляются с промптами, содержащими редко встречающиеся вместе концепции с далёкими семантическими связями. ImagerySearch динамически настраивает пространство поиска и функцию вознаграждения в зависимости от семантических отношений в промпте. Авторы также представили LDT-Bench - первый бенчмарк для оценки генерации видео по промптам с концепциями, имеющими большую семантическую дистанцию.",
  "emoji": "🎨",
  "title": "Адаптивный поиск для креативной видеогенерации"
}
```
[17.10.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  					AI-generated summary 				 Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation."

[17.10.2025 07:13] Response: ```python
['VIDEO', 'BENCHMARK']
```
[17.10.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  					AI-generated summary 				 Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation."

[17.10.2025 07:13] Response: ```python
["OPTIMIZATION", "LONG_CONTEXT"]
```
[17.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ImagerySearch is a novel strategy designed to improve video generation in imaginative scenarios by adapting search spaces and reward functions based on prompts. Traditional video generation models struggle with rare concepts and long-distance semantic relationships, leading to poor performance in creative contexts. By dynamically adjusting the inference process, ImagerySearch enhances the coherence and visual quality of generated videos. The introduction of LDT-Bench provides a new benchmark for evaluating these capabilities, showcasing the effectiveness of ImagerySearch over existing methods.","title":"Dynamic Adaptation for Imaginative Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ImagerySearch is a novel strategy designed to improve video generation in imaginative scenarios by adapting search spaces and reward functions based on prompts. Traditional video generation models struggle with rare concepts and long-distance semantic relationships, leading to poor performance in creative contexts. By dynamically adjusting the inference process, ImagerySearch enhances the coherence and visual quality of generated videos. The introduction of LDT-Bench provides a new benchmark for evaluating these capabilities, showcasing the effectiveness of ImagerySearch over existing methods.', title='Dynamic Adaptation for Imaginative Video Generation'))
[17.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ImagerySearch是一种基于提示的自适应测试时间搜索策略，旨在提高视频生成在富有想象力场景中的表现。该方法通过动态调整搜索空间和奖励函数，能够更好地处理长距离语义关系的提示，从而生成更连贯和视觉上可信的视频。与现有方法相比，ImagerySearch在新的基准LDT-Bench上表现优越，展示了其在创造性生成能力上的优势。我们还引入了LDT-Bench，这是第一个专门针对长距离语义提示的基准，包含多样的概念对和自动评估协议。","title":"ImagerySearch：提升想象力视频生成的自适应策略"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ImagerySearch是一种基于提示的自适应测试时间搜索策略，旨在提高视频生成在富有想象力场景中的表现。该方法通过动态调整搜索空间和奖励函数，能够更好地处理长距离语义关系的提示，从而生成更连贯和视觉上可信的视频。与现有方法相比，ImagerySearch在新的基准LDT-Bench上表现优越，展示了其在创造性生成能力上的优势。我们还引入了LDT-Bench，这是第一个专门针对长距离语义提示的基准，包含多样的概念对和自动评估协议。', title='ImagerySearch：提升想象力视频生成的自适应策略'))
[17.10.2025 07:13] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#multimodal", "#training", "#interpretability"], "emoji": "🧠", "ru": {"title": "От пассивных чанков к активной памяти документов", "desc": "Статья представляет фреймворк MoM, который улучшает RAG-системы, превращая пассивное разделение текста на чанки в активно
[17.10.2025 07:13] Using data from previous issue: {"categories": ["#open_source", "#data", "#low_resource", "#dataset"], "emoji": "🇩🇪", "ru": {"title": "Немецкие общины: открытый датасет для немецких языковых моделей", "desc": "Исследователи представили German Commons — крупнейшую коллекцию текстов на немецком языке с открытыми лицензиями для обуче
[17.10.2025 07:13] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#transfer_learning", "#multimodal", "#cv"], "emoji": "🤝", "ru": {"title": "Анимация взаимодействий через позы близкого контакта", "desc": "Ponimator — это фреймворк для генерации анимаций взаимодействия двух людей на основе поз близкого контакта из motion c
[17.10.2025 07:13] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#multimodal", "#alignment", "#ethics", "#benchmark"], "emoji": "🦸", "ru": {"title": "Супергерои из разных вселенных: проверка LLM на последовательность ролевой игры", "desc": "Исследователи создали бенчмарк Beyond One World для оценки способности LLM последо
[17.10.2025 07:13] Using data from previous issue: {"categories": ["#security", "#hallucinations", "#alignment", "#benchmark", "#rag"], "emoji": "🚫", "ru": {"title": "Когда AI должен сказать «не знаю»: тестируем умение моделей отказываться отвечать", "desc": "Исследователи обнаружили критическую проблему в RAG-системах: языковые модели плохо справля
[17.10.2025 07:13] Querying the API.
[17.10.2025 07:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  					AI-generated summary 				 Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.
[17.10.2025 07:13] Response: ```json
{
  "title": "Быстрое моделирование динамических сцен через каскадную оптимизацию",
  "desc": "SCas4D — это новый фреймворк для моделирования динамических 3D-сцен с использованием 3D Gaussian Splatting. Ключевая идея заключается в том, что деформации объектов в реальном мире часто имеют иерархическую структуру, где группы точек двигаются похожим образом. Метод последовательно уточняет деформации от грубого уровня частей объекта до детального уровня отдельных точек, что позволяет достичь сходимости всего за 100 итераций на кадр. Система показывает отличные результаты в задачах сегментации артикулированных объектов, синтеза новых ракурсов и отслеживания точек, работая в 20 раз быстрее существующих методов.",
  "emoji": "🎯",
  "desc_en": ""
}
```
[17.10.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  					AI-generated summary 				 Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks."

[17.10.2025 07:13] Response: ```python
['3D']
```
[17.10.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  					AI-generated summary 				 Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks."

[17.10.2025 07:13] Response: ```python
["OPTIMIZATION"]
```
[17.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SCas4D is a novel framework designed for efficiently modeling dynamic scenes using 3D Gaussian Splatting. It focuses on capturing hierarchical deformation patterns, which allows for faster convergence and high-quality outputs. By refining deformations from a coarse to a fine level, SCas4D can achieve results comparable to existing methods while significantly reducing training iterations. This approach is particularly effective in tasks such as self-supervised articulated object segmentation and novel view synthesis.","title":"Efficient Dynamic Scene Modeling with SCas4D"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SCas4D is a novel framework designed for efficiently modeling dynamic scenes using 3D Gaussian Splatting. It focuses on capturing hierarchical deformation patterns, which allows for faster convergence and high-quality outputs. By refining deformations from a coarse to a fine level, SCas4D can achieve results comparable to existing methods while significantly reducing training iterations. This approach is particularly effective in tasks such as self-supervised articulated object segmentation and novel view synthesis.', title='Efficient Dynamic Scene Modeling with SCas4D'))
[17.10.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SCas4D是一种级联优化框架，利用3D高斯点云有效建模动态场景。该方法通过层次化变形模式，快速收敛并在多种任务中实现高质量结果。SCas4D的关键在于真实世界的变形通常呈现层次化模式，多个高斯共享相似的变换。通过从粗到细逐步优化变形，SCas4D在每个时间帧内仅需100次迭代即可收敛，且训练迭代次数仅为现有方法的五分之一。","title":"高效动态场景建模的层次化优化框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SCas4D是一种级联优化框架，利用3D高斯点云有效建模动态场景。该方法通过层次化变形模式，快速收敛并在多种任务中实现高质量结果。SCas4D的关键在于真实世界的变形通常呈现层次化模式，多个高斯共享相似的变换。通过从粗到细逐步优化变形，SCas4D在每个时间帧内仅需100次迭代即可收敛，且训练迭代次数仅为现有方法的五分之一。', title='高效动态场景建模的层次化优化框架'))
[17.10.2025 07:13] Renaming data file.
[17.10.2025 07:13] Renaming previous data. hf_papers.json to ./d/2025-10-17.json
[17.10.2025 07:13] Saving new data file.
[17.10.2025 07:13] Generating page.
[17.10.2025 07:13] Renaming previous page.
[17.10.2025 07:13] Renaming previous data. index.html to ./d/2025-10-17.html
[17.10.2025 07:13] Writing result.
[17.10.2025 07:13] Renaming log file.
[17.10.2025 07:13] Renaming previous data. log.txt to ./logs/2025-10-17_last_log.txt
