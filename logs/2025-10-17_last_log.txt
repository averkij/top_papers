[17.10.2025 11:11] Read previous papers.
[17.10.2025 11:11] Generating top page (month).
[17.10.2025 11:11] Writing top page (month).
[17.10.2025 12:22] Read previous papers.
[17.10.2025 12:22] Get feed.
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14545
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14975
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14359
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14979
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14847
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14943
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14972
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14967
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14973
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04849
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14528
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13998
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10518
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14958
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14763
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09033
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14902
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14616
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14300
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13217
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14276
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13054
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14880
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14978
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14211
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14969
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13996
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13454
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14974
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14955
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14949
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14252
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13697
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14976
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14351
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.13910
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10390
[17.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06694
[17.10.2025 12:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.10.2025 12:22] No deleted papers detected.
[17.10.2025 12:22] Downloading and parsing papers (pdf, html). Total: 38.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14545.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14545.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14545.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14975.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14975.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14975.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14359.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14359.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14359.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14979.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14979.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14979.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14847.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14847.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14847.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14943.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14943.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14943.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14972.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14972.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14972.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14967.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14967.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14967.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14973.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14973.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14973.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.04849.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.04849.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.04849.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14528.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14528.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14528.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.13998.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.13998.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.13998.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.10518.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.10518.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.10518.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14958.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14958.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14958.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14763.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14763.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14763.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09033.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09033.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09033.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14902.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14902.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14902.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14616.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14616.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14616.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14300.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14300.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14300.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.13217.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.13217.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.13217.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14276.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14276.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14276.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.13054.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.13054.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.13054.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14880.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14880.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14880.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14978.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14978.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14978.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14211.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14211.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14211.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14969.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14969.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14969.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.13996.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.13996.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.13996.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.13454.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.13454.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.13454.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14974.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14974.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14974.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14955.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14955.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14955.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14949.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14949.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14949.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14252.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14252.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14252.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.13697.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.13697.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.13697.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14976.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14976.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14976.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.14351.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.14351.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.14351.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.13910.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.13910.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.13910.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.10390.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.10390.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.10390.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.06694.
[17.10.2025 12:22] Extra JSON file exists (./assets/json/2510.06694.json), skip PDF parsing.
[17.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.06694.json), skip HTML parsing.
[17.10.2025 12:22] Success.
[17.10.2025 12:22] Enriching papers with extra data.
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 0. AEPO, an agentic RL algorithm, addresses entropy-related challenges in web agent training, enhancing performance and stability across various datasets.  					AI-generated summary 				 Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn,...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 1. A diffusion-based model addresses copy-paste artifacts in text-to-image generation by using a large-scale paired dataset and a contrastive identity loss to balance identity fidelity and variation.  					AI-generated summary 				 Identity-consistent generation has become an important focus in text-to...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 2. Alpha-Service, a unified framework for proactive AI assistance, uses a multi-agent system on AI glasses to detect service opportunities and provide timely, personalized assistance.  					AI-generated summary 				 In an era where AI is evolving from a passive tool into an active and adaptive companio...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 3. NEO, a novel family of native Vision-Language Models, addresses fundamental constraints and integrates vision and language within a unified framework, achieving competitive performance with limited data.  					AI-generated summary 				 The edifice of native Vision-Language Models (VLMs) has emerged ...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 4. ImagerySearch, a prompt-guided adaptive test-time search strategy, enhances video generation in imaginative scenarios by dynamically adjusting search spaces and reward functions, outperforming existing methods on a new benchmark, LDT-Bench.  					AI-generated summary 				 Video generation models hav...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 5. LaSeR, a reinforcement learning algorithm, enhances Large Language Models by aligning last-token self-rewarding scores with verifier-based reasoning rewards, improving reasoning performance and inference-time scaling.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RL...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 6. Misaligned tokenization in large language models for code leads to inconsistent model behavior, necessitating grammar-aware tokenization.  					AI-generated summary 				 Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural lan...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 7. Information Gain-based Policy Optimization (IGPO) enhances multi-turn reasoning in large language models by providing dense intrinsic rewards derived from the model's belief updates, improving accuracy and sample efficiency.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 8. Elastic-Cache optimizes key-value cache management in diffusion large language models to reduce decoding latency without sacrificing prediction accuracy.  					AI-generated summary 				 This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to ...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 9. PsiloQA, a multilingual dataset with span-level hallucinations, enhances hallucination detection in large language models across 14 languages using an automated pipeline and encoder-based models.  					AI-generated summary 				 Hallucination detection remains a fundamental challenge for the safe and...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 10. PaddleOCR-VL, a vision-language model combining NaViT-style visual encoder and ERNIE-4.5 language model, achieves state-of-the-art performance in document parsing with minimal resource consumption.  					AI-generated summary 				 In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 11. BitNet Distillation fine-tunes large language models to 1.58-bit precision using SubLN, multi-head attention distillation, and continual pre-training, achieving comparable performance with significant memory and inference speed improvements.  					AI-generated summary 				 In this paper, we present ...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 12. VideoReward Thinker enhances multimodal reward models with visual reasoning operations and a configurable memory window, improving accuracy on video preference benchmarks.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) have substantially improved post-training ...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 13. MathCanvas enhances Large Multimodal Models with Visual Chain-of-Thought capabilities for mathematics through pre-training on diagram generation and fine-tuning on visual-textual reasoning, achieving significant improvements on math benchmarks.  					AI-generated summary 				 While Large Language Mo...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 14. COIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.  					AI-generated summary 				 Large language models exhibit systematic deficien...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 15. LLMs process factual queries and hallucinations similarly when associated with subject knowledge, leading to indistinguishable internal representations, but produce distinct representations for hallucinations without subject knowledge.  					AI-generated summary 				 Recent work suggests that large ...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 16. A novel agentic framework, VLA^2, enhances vision-language-action models by integrating external modules like web retrieval and object detection, improving generalization to unseen objects and descriptions.  					AI-generated summary 				 Current vision-language-action (VLA) models, pre-trained on l...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 17. Generative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.  					AI-generated summary 				 Current preferenc...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 18. AdaMoE, a Mixture-of-Experts architecture, enhances VLA models by leveraging pretrained weights and improving computational efficiency, achieving superior performance in robotic manipulation tasks.  					AI-generated summary 				 Vision-Language-Action (VLA) models are experiencing rapid development...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 19. LATTICE, a hierarchical retrieval framework, enables efficient and accurate reasoning over large document collections using a semantic tree structure and a traversal algorithm that calibrates relevance scores.  					AI-generated summary 				 Modern IR systems are increasingly tasked with answering c...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 20. Qwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.  					AI-generated summary 				 As large language models (LLMs) become more capable and widely used, ensuring the safety of their ou...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 21. ...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 22. mxbai-edge-colbert-v0 models, with 17M and 32M parameters, demonstrate superior retrieval performance on short-text and long-context benchmarks compared to ColBERTv2.  					AI-generated summary 				 In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 3...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 23. A new training paradigm for image editing models uses unrolled diffusion models and vision-language feedback to achieve performance comparable to supervised models without paired data.  					AI-generated summary 				 Recent image editing models have achieved impressive results while following natura...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 24. LiteStage, a latency-aware layer skipping framework, enhances multi-stage reasoning by optimizing layer budgets and suppressing redundant output tokens, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 Multi-stage reasoning has emerged as an effective strateg...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 25. ...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 26. The German Commons provides a large-scale, openly licensed dataset for training German language models, addressing the scarcity of such data.  					AI-generated summary 				 Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, li...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 27. VIST3A combines latent text-to-video models and 3D reconstruction systems to generate high-quality 3D scenes from text, improving upon prior methods.  					AI-generated summary 				 The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new p...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 28. Policy-based flow models enable efficient and high-quality image generation by distilling teacher models into student models with dynamic flow velocities, improving diversity and quality.  					AI-generated summary 				 Few-step diffusion or flow-based generative models typically distill a velocity-...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 29. RealDPO, a novel preference learning paradigm using real-world data, enhances motion realism in video generative models through Direct Preference Optimization and iterative self-correction.  					AI-generated summary 				 Video generative models have recently achieved notable advancements in synthes...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 30. A new benchmark and encoder-based mitigation strategy improve multimodal generative models' performance on dialectal textual input without degrading performance on Standard American English.  					AI-generated summary 				 Contact languages like English exhibit rich regional variations in the form o...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 31. The MoM framework enhances RAG by transforming text processing from passive chunking to proactive understanding, enabling LLMs to generate structured document memories and SLMs to develop human-like reading abilities.  					AI-generated summary 				 The traditional RAG paradigm, which typically enga...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 32. Extending the context window and adapting to a new rotary positional embedding scaling parameter improve repository-level code completion in OpenCoder, achieving performance comparable to larger models with less data.  					AI-generated summary 				 Repository-level pretraining is commonly used to e...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 33. Ponimator uses conditional diffusion models to generate and synthesize interactive poses from motion capture data, enabling versatile interaction animation tasks.  					AI-generated summary 				 Close-proximity human-human interactive poses convey rich contextual information about interaction dynami...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 34. Beyond One World benchmark evaluates LLMs' ability to consistently portray version-specific superheroes across different canons through factual recall and ethical reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as role-playing agents, yet their cap...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 35. RAGCap-Bench evaluates intermediate tasks in agentic RAG workflows, highlighting the importance of enhancing these capabilities for better end-to-end performance.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as fa...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 36. RefusalBench evaluates the selective refusal capability of language models in RAG systems using programmatically generated test cases, revealing systematic failure patterns and offering a path for improvement.  					AI-generated summary 				 The ability of language models in RAG systems to selective...
[17.10.2025 12:22] ********************************************************************************
[17.10.2025 12:22] Abstract 37. SCas4D, a cascaded optimization framework using 3D Gaussian Splatting, efficiently models dynamic scenes by leveraging hierarchical deformation patterns, enabling fast convergence and high-quality results in various tasks.  					AI-generated summary 				 Persistent dynamic scene modeling for trackin...
[17.10.2025 12:22] Read previous papers.
[17.10.2025 12:22] Generating reviews via LLM API.
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#agents", "#rl", "#training", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "AEPO ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç–Ω—Ç—Ä–æ–ø–∏–µ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤. –ê–ª
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#dataset", "#cv", "#training", "#diffusion", "#benchmark"], "emoji": "üé≠", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∏—Ü –±–µ–∑ –∫–æ–ø–∏–ø–∞—Å—Ç–∞: –±–∞–ª–∞–Ω—Å –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É ¬´–∫–æ–ø–∏–ø–∞—Å—Ç–∞¬ª –≤ text-to-image –º–æ–¥–µ–ª—è—Ö, –∫–æ–≥–¥–∞ AI –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–µ –ª–∏—Ü–æ –≤–º–µ—Å—Ç–æ —Å
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#agents", "#agi", "#multimodal", "#optimization", "#games", "#interpretability"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π AI: –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞–µ—Ç –≤–∞—à–∏ –Ω—É–∂–¥—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Alpha-Service, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç AI-–æ—á–∫–∏ –¥–ª—è –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–π –ø–æ–º–æ—â–∏ –ø–æ–ª—å–∑
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#alignment", "#architecture", "#open_source"], "emoji": "üîó", "ru": {"title": "NEO: –Ω–∞—Ç–∏–≤–Ω—ã–µ Vision-Language –º–æ–¥–µ–ª–∏ —Å –µ–¥–∏–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NEO ‚Äî –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –Ω–∞—Ç–∏–≤–Ω—ã—Ö Vision-Language Models (VLM), –∫–æ—Ç–æ—Ä—ã–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –≤–∏–∑—É–∞–ª—å–Ω—É
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#video", "#benchmark", "#optimization", "#long_context"], "emoji": "üé®", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–π –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ImagerySearch - –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –û—Å
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "üéØ", "ru": {"title": "–°–∞–º–æ–æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è reasoning", "desc": "LaSeR ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç reasoning —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø—É—Ç—ë–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏–π 
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#data", "#plp", "#dataset", "#architecture"], "emoji": "üî§", "ru": {"title": "–ü—Ä–æ–±–ª–µ–º–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞: –∫–æ–≥–¥–∞ –ø—Ä–æ–±–µ–ª—ã –º–µ–Ω—è—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–µ—Ä—å—ë–∑–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –∫–æ–¥–∞: —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ–∫–µ–Ω–∏–∑
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#agents", "#rl", "#reasoning", "#rlhf", "#training", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ü–ª–æ—Ç–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –ø—Ä–∏—Ä–æ—Å—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ IGPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é reinforcement lear
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Elastic-Cache –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è key-value –∫—ç—à–µ–º –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö LLM. –ê–≤—Ç
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#hallucinations", "#low_resource", "#multilingual", "#dataset"], "emoji": "üçÑ", "ru": {"title": "–ü–æ–π–º–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—é: –¥–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ 14 —è–∑—ã–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PsiloQA ‚Äî –∫—Ä—É–ø–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ LLM –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö 
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#science", "#low_resource", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏", "desc": "PaddleOCR-VL ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω—ã–π
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization", "#small_models"], "emoji": "üîΩ", "ru": {"title": "–°–∂–∞—Ç–∏–µ LLM –¥–æ —Ç–µ—Ä–Ω–∞—Ä–Ω—ã—Ö –≤–µ—Å–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω BitNet Distillation (BitDistill) ‚Äî –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã—Ö LLM –≤ –º–æ
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#multimodal", "#open_source", "#long_context", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI —Ä–∞–∑–º—ã—à–ª—è—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoReward Thinker ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#multimodal", "#games", "#math", "#benchmark"], "emoji": "üìê", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "MathCanvas ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–∑
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#story_generation", "#multilingual", "#low_resource", "#dataset"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å AI —Ç—Ä–µ–±—É–µ—Ç –±–∞–ª–∞–Ω—Å–∞ –ª–æ–≥–∏–∫–∏ –∏ —è–∑—ã–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç COIG-Writer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–º—É –ø–∏—Å—å–º—É –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 1665 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#data", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "üé≠", "ru": {"title": "LLM –Ω–µ –∑–Ω–∞—é—Ç, —á—Ç–æ –æ–Ω–∏ –Ω–µ –∑–Ω–∞—é—Ç: –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã –æ—Ç —Ñ–∞–∫—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ LLM –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Å—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–æ–≥–¥–∞ –æ–Ω–∏ —Å–≤—è–∑
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#agi", "#benchmark", "#optimization", "#cv", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–æ–±—â–µ–Ω–∏—é VLA –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ–≤–∏–¥–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLA^2 ‚Äî –Ω–æ–≤—ã–π –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è vision-language-action –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –û—Å–Ω–æ–≤–Ω–∞
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#reasoning", "#low_resource", "#dataset", "#story_generation"], "emoji": "‚úçÔ∏è", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–∞–∂–Ω–µ–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ WritingPreferenceBench ‚Äî –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 1800 –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#robotics", "#agi", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AdaMoE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ Mixture-of-Experts –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π,
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#optimization", "#benchmark"], "emoji": "üå≥", "ru": {"title": "–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é", "desc": "LATTICE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è information retrieval, –∫–æ—Ç–æ—Ä—ã–π –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–∏–¥–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–µ—Ä–µ–≤
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#data", "#open_source", "#alignment", "#training", "#ethics", "#multilingual", "#low_resource", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –∑–∞—â–∏—Ç–∞ LLM —Å —Ç—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "Qwen3Guard ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥
[17.10.2025 12:22] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#long_context", "#training", "#dataset", "#optimization", "#benchmark", "#small_models"], "emoji": "üîç", "ru": {"title": "–ú–æ—â–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∫–∞—Ä–º–∞–Ω–µ: –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ mxbai-edge-colbert-v0 ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ 
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#rlhf", "#training", "#optimization", "#synthetic", "#diffusion", "#benchmark"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç VLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, 
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#reasoning", "#inference"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –ø—Ä–æ–ø—É—Å–∫ —Å–ª–æ—ë–≤", "desc": "LiteStage ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –ø—Ä–æ–ø—É—Å–∫
[17.10.2025 12:22] Using data from previous issue: {"categories": [], "emoji": "ü§ù", "ru": {"title": "–ö–æ–≥–¥–∞ AI –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ ‚Äî –ª—É—á—à–µ —Å–ø—Ä–æ—Å–∏—Ç—å —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –∫–æ–≥–¥–∞ –æ–Ω–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ –∏ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –ø–æ–º–æ—â–∏ —á–µ–ª–æ–≤–µ–∫–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞–ª–∏–±—Ä–æ–≤–∫
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#open_source", "#data", "#low_resource", "#dataset"], "emoji": "üá©üá™", "ru": {"title": "–ù–µ–º–µ—Ü–∫–∏–µ –æ–±—â–∏–Ω—ã: –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –Ω–µ–º–µ—Ü–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ German Commons ‚Äî –∫—Ä—É–ø–Ω–µ–π—à—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º —è–∑—ã–∫–µ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –ª–∏—Ü–µ–Ω–∑–∏—è–º–∏ –¥–ª—è –æ–±—É—á–µ
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#alignment", "#training", "#optimization"], "emoji": "üé¨", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ 3D —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ: —Å—à–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ü–µ–Ω", "desc": "VIST3A ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç latent text-to-video –º–æ–¥–µ–ª–∏ —Å —Å
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#diffusion"], "emoji": "üåä", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç pi-Flow ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ flow-based –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –í–º–µ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä—è–º–æ–≥–æ –ø—É—Ç–∏ 
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#video", "#dataset", "#alignment", "#optimization", "#training"], "emoji": "üé¨", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–µ–ª–∞–µ—Ç AI-–¥–≤–∏–∂–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏", "desc": "RealDPO ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã 
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#multimodal", "#low_resource", "#benchmark", "#synthetic"], "emoji": "üó£Ô∏è", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–∏–∞–ª–µ–∫—Ç—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —à–µ—Å—Ç–∏ –¥–∏–∞–ª–µ–∫—Ç–∞—Ö –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#multimodal", "#training", "#interpretability"], "emoji": "üß†", "ru": {"title": "–û—Ç –ø–∞—Å—Å–∏–≤–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∫ –∞–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MoM, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—ã, –ø—Ä–µ–≤—Ä–∞—â–∞—è –ø–∞—Å—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –≤ –∞–∫—Ç–∏–≤–Ω–æ
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#transfer_learning", "#long_context", "#dataset", "#benchmark", "#training", "#data", "#architecture"], "emoji": "üìÅ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É–ª—É—á—à–∏–ª–∏ –º–æ–¥–µ–ª—å OpenCoder –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ —É
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#transfer_learning", "#multimodal", "#cv"], "emoji": "ü§ù", "ru": {"title": "–ê–Ω–∏–º–∞—Ü–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –ø–æ–∑—ã –±–ª–∏–∑–∫–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞", "desc": "Ponimator ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∏–º–∞—Ü–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–≤—É—Ö –ª—é–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑ –±–ª–∏–∑–∫–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞ –∏–∑ motion c
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#multimodal", "#alignment", "#ethics", "#benchmark"], "emoji": "ü¶∏", "ru": {"title": "–°—É–ø–µ—Ä–≥–µ—Ä–æ–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤—Å–µ–ª–µ–Ω–Ω—ã—Ö: –ø—Ä–æ–≤–µ—Ä–∫–∞ LLM –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–æ–ª–µ–≤–æ–π –∏–≥—Ä—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ Beyond One World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø–æ—Å–ª–µ–¥–æ
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#rag", "#reasoning", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö RAG-—Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RAGCap-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –∞–≥–µ–Ω—Ç–Ω—ã—Ö RAG-—Å
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#security", "#hallucinations", "#alignment", "#benchmark", "#rag"], "emoji": "üö´", "ru": {"title": "–ö–æ–≥–¥–∞ AI –¥–æ–ª–∂–µ–Ω —Å–∫–∞–∑–∞—Ç—å ¬´–Ω–µ –∑–Ω–∞—é¬ª: —Ç–µ—Å—Ç–∏—Ä—É–µ–º —É–º–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –æ—Ç–≤–µ—á–∞—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö: —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è
[17.10.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#3d"], "emoji": "üéØ", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –∫–∞—Å–∫–∞–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "SCas4D ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 3D Gaussian Splatting. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –¥–µ—Ñ–æ
[17.10.2025 12:22] Renaming data file.
[17.10.2025 12:22] Renaming previous data. hf_papers.json to ./d/2025-10-17.json
[17.10.2025 12:22] Saving new data file.
[17.10.2025 12:22] Generating page.
[17.10.2025 12:22] Renaming previous page.
[17.10.2025 12:22] Renaming previous data. index.html to ./d/2025-10-17.html
[17.10.2025 12:22] Writing result.
[17.10.2025 12:22] Renaming log file.
[17.10.2025 12:22] Renaming previous data. log.txt to ./logs/2025-10-17_last_log.txt
