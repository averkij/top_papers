[16.04.2025 18:15] Read previous papers.
[16.04.2025 18:15] Generating top page (month).
[16.04.2025 18:15] Writing top page (month).
[16.04.2025 19:08] Read previous papers.
[16.04.2025 19:08] Get feed.
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10481
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08672
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10337
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10766
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11346
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10465
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11442
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10462
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10903
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10559
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11427
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10342
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10277
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10188
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11343
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11001
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11456
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11455
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11447
[16.04.2025 19:08] Extract page data from URL. URL: https://huggingface.co/papers/2504.11393
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11326
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08846
[16.04.2025 19:08] Extract page data from URL. URL: https://huggingface.co/papers/2504.11409
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.09454
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06949
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11042
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10443
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10049
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11457
[16.04.2025 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11080
[16.04.2025 19:08] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.04.2025 19:08] No deleted papers detected.
[16.04.2025 19:08] Downloading and parsing papers (pdf, html). Total: 30.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10481.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10481.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10481.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.08672.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.08672.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.08672.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10337.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10337.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10337.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10766.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10766.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10766.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11346.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11346.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11346.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10465.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10465.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10465.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11442.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11442.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11442.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10462.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10462.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10462.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10903.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10903.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10903.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10559.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10559.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10559.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11427.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11427.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11427.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10342.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10342.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10342.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10277.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10277.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10277.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.10188.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.10188.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.10188.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11343.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11343.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11343.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11001.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11001.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11001.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11456.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11456.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11456.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11455.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11455.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11455.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11447.
[16.04.2025 19:08] Extra JSON file exists (./assets/json/2504.11447.json), skip PDF parsing.
[16.04.2025 19:08] Paper image links file exists (./assets/img_data/2504.11447.json), skip HTML parsing.
[16.04.2025 19:08] Success.
[16.04.2025 19:08] Downloading and parsing paper https://huggingface.co/papers/2504.11393.
[16.04.2025 19:08] Downloading paper 2504.11393 from http://arxiv.org/pdf/2504.11393v1...
[16.04.2025 19:09] Extracting affiliations from text.
[16.04.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"How to Predict Best Pretraining Data with Small Experiments Ian Magnusson, Nguyen Tai, Ben Bogin, David Heineman, Jena Hwang, Luca Soldaini, Akshita Bhagia, Jiacheng Liu, Dirk Groeneveld, Oyvind Tafjord, Noah A. Smith, Pang Wei Koh, Jesse Dodge Allen Institute for AI University of Pennsylvania University of Washington equal contribution "
[16.04.2025 19:09] Response: ```python
["Allen Institute for AI", "University of Pennsylvania", "University of Washington"]
```
[16.04.2025 19:09] Deleting PDF ./assets/pdf/2504.11393.pdf.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.11326.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.11326.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.11326.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.08846.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.08846.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.08846.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.11409.
[16.04.2025 19:09] Downloading paper 2504.11409 from http://arxiv.org/pdf/2504.11409v1...
[16.04.2025 19:09] Extracting affiliations from text.
[16.04.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 9 0 4 1 1 . 4 0 5 2 : r 2025-4-16 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Ali Taghibakhshi*, Sharath Turuvekere Sreenivas*, Saurav Muralidharan*, Marcin Chochowski*, Yashaswi Karnati*, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi Olabiyi, Daniel Korzekwa, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro, Ashwath Aithal, Nima Tajbakhsh, Pavlo Molchanov Abstract: Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve stateof-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier. Recent advances in language modeling have led to the development of hybrid architectures that combine Transformer layers [1] with State Space Models (SSMs) [2, 3]. These hybrid models leverage the complementary strengths of both approaches: Transformers excel at capturing global dependencies through self-attention mechanisms, while SSMs provide efficient sequence processing with ùëÇ(ùëÅ ) scaling during training"
[16.04.2025 19:09] Response: ```python
[]
```
[16.04.2025 19:09] Extracting affiliations from text.
[16.04.2025 19:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 9 0 4 1 1 . 4 0 5 2 : r 2025-4-16 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Ali Taghibakhshi*, Sharath Turuvekere Sreenivas*, Saurav Muralidharan*, Marcin Chochowski*, Yashaswi Karnati*, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi Olabiyi, Daniel Korzekwa, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro, Ashwath Aithal, Nima Tajbakhsh, Pavlo Molchanov Abstract: Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve stateof-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier.Recent advances in language modeling have led to the development of hybrid architectures that combine Transformer layers [1] with State Space Models (SSMs) [2, 3]. These hybrid models leverage the complementary strengths of both approaches: Transformers excel at capturing global dependencies through self-attention mechanisms, while SSMs provide efficient sequence processing with ùëÇ(ùëÅ ) scaling during training and ùëÇ(1) cache size during inference. Mamba [2, 3] in particular is popular SSM designed for efficient sequence modeling with linear-time complexity and support for long contexts and is often the preferred choice for non-attention layers in hybrid architectures. Despite their improved efficiency, many hybrid LLMs remain incredibly large, often spanning billions of parameters - this motivates the need for efficiently creating smaller hybrid models suitable for deployment in various resource-constrained environments. Model pruningthe removal of redundant parameters while preserving accuracyhas recently emerged as promising approach for compressing LLMs. In particular, methods that combine structured pruning (i.e., pruning of entire parameter blocks such as neurons, attention heads, etc.) with knowledge distillation [4] have proven effective at simultaneously reducing model memory footprint while improving runtime performance and accuracy [5]. While pruning techniques have been extensively studied for Transformer architectures [5, 6, 7], their application to hybrid models remains significantly underexplored. Some early work on Mamba and SSM pruning includes Mamba-Shredder [8], which removes the entire state space module from the Mamba layers, leaving only linear projections and convolution layer. In concurrent study, Ghattas et al. [9] proposed method for pruning Mamba architectures by focusing on three aspects: state space dimension reduction, Mamba head dimension pruning, and Mamba head merging. To the best of our knowledge, no existing work on SSM/Mamba pruning presents holistic compression strategy that simultaneously combines various aspects of SSM pruning with the pruning of other network components such as FFN neurons, embedding channels, and network depth; we believe such an approach is essential for obtaining the best combination of runtime performance and model accuracy. In this paper, we introduce novel pruning method for Mamba architectures that compresses multiple dimensions (Mamba heads, head channels). We also present unified pruning recipe that combines Mamba pruning with FFN, embedding dimension, and layer pruning to maximize accuracy and runtime performance. This paper makes the following key contributions: Introduces group-aware pruning method for Mamba layers that preserves SSM block structure and sequence modeling capabilities. Presents novel hybrid pruning recipe that effectively combines Mamba pruning with the pruning of other network components such as FFN neu- * Equal contribution. 2025 NVIDIA. All rights reserved. Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Figure 1 Comparison of Nemotron-H 4B model accuracy w.r.t. inference throughput (left), and training budget for the base model (right) to similarly-sized community models. Inference throughput is measured at an input and output sequence length of 65536 and 1024, respectively. rons, embedding channels and layers. Presents findings on the sensitivity of Mamba to pruning, along with block components accuracy-throughput trade-offs when combined with pruning of other network components. Utilizes the proposed hybrid pruning recipe to compress the Nemotron-H 8B model to 4B parameters through pruning and knowledge distillation. The resulting model requires up to 40x fewer training tokens compared to others in the same size range. It also achieves state-of-the-art accuracy on benchmarks, along with 2x speedup in throughput compared to models of similar size, significantly pushing the Pareto frontier.State Space Models (SSMs). SSMs are class of sequence models that process inputs through hidden states evolving over time [3]. The general form of an SSM is given by: ‚Ñéùë° = ùê¥‚Ñéùë°1 + ùêµùë•ùë° ùë¶ùë° = ùê∂ ‚Ñéùë° + ùê∑ùë•ùë° (1) (2) Here, ‚Ñéùë° represents the hidden state, ùë•ùë° the input, ùë¶ùë° the output, and ùê¥, ùêµ, ùê∂, and ùê∑ are parameter matrices. The above equations describe linear time-invariant (LTI) SSMs, where the parameters remain constant across timesteps. The Mamba architecture [3] introduced selective SSM variant with time-varying parameters: ‚Ñéùë° = ùê¥ùë°‚Ñéùë°1 + ùêµùë°ùë•ùë° ùë¶ùë° = ùê∂ ùë° ‚Ñéùë° + ùê∑ùë°ùë•ùë° (3) (4) This selective mechanism allows the model to adapt dynamically to the input sequence, improving performance on complex tasks. Mamba2 [3] builds upon the selective SSM framework and introduces several enhancements to improve efficiency and scalability. It leverages the Structured State Space Duality (SSD), which connects SSMs and attention mechanisms through semi-se"
[16.04.2025 19:09] Mistral response. {"id": "26376666c5c94990a6a4970b66654fd8", "object": "chat.completion", "created": 1744830565, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"NVIDIA\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1760, "total_tokens": 1772, "completion_tokens": 12}}
[16.04.2025 19:09] Response: ```python
["NVIDIA"]
```
[16.04.2025 19:09] Deleting PDF ./assets/pdf/2504.11409.pdf.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.09454.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.09454.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.09454.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.06949.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.06949.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.06949.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.11042.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.11042.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.11042.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.10443.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.10443.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.10443.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.10049.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.10049.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.10049.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.11457.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.11457.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.11457.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2504.11080.
[16.04.2025 19:09] Extra JSON file exists (./assets/json/2504.11080.json), skip PDF parsing.
[16.04.2025 19:09] Paper image links file exists (./assets/img_data/2504.11080.json), skip HTML parsing.
[16.04.2025 19:09] Success.
[16.04.2025 19:09] Enriching papers with extra data.
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 0. With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle t...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 1. Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reason...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 2. An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently ...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 3. As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quali...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 4. We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesth...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 5. Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In thi...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 6. TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 7. This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 8. Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this "slow-thinking" paradigm, with numerous tokens generated in sequence, inevitably introdu...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 9. Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain ...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 10. Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing met...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 11. Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing ...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 12. Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains under...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 13. Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down co...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 14. Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood....
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 15. Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without exp...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 16. The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer f...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 17. This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with ...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 18. The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. ...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 19. Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield th...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 20. This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 21. We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos,...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 22. Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 23. Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the nat...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 24. The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each time...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 25. Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve t...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 26. Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent meth...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 27. Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of automatic summarization of multimodal presentations usin...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 28. With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely ...
[16.04.2025 19:09] ********************************************************************************
[16.04.2025 19:09] Abstract 29. Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Visi...
[16.04.2025 19:09] Read previous papers.
[16.04.2025 19:09] Generating reviews via LLM API.
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#dataset", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "xVerify: —Ç–æ—á–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç xVerify - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. xVerify —Å–ø–æ—Å–æ–±–µ–Ω 
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rl"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ò–ò: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Genius –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–Ω–µ—à–Ω–µ–≥–æ –Ω–∞–¥–∑–æ
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#long_context", "#optimization", "#training", "#rl", "#math"], "emoji": "üîç", "ru": {"title": "Heimdall: –ò–ò-–≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Heimdall - –º–æ–¥–µ–ª—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#data", "#training"], "emoji": "üß†", "ru": {"title": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–µ–∫—Ä–µ—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–æ–π–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, –≤—ã–∑–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã–º–∏ —Ä–∞–∑–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ 
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#alignment", "#data", "#optimization", "#dataset", "#multimodal", "#architecture", "#training"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: Seedream 3.0 –ø–æ–¥–Ω–∏–º–∞–µ—Ç –ø–ª–∞–Ω–∫—É", "desc": "Seedream 3.0 - —ç—Ç–æ –¥–≤—É—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —É–ª—É—á—à–∞—é—â–∞—è –ø—Ä–µ–¥—ã–¥—É—â
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#survey", "#optimization", "#benchmark", "#multimodal", "#training", "#architecture", "#games"], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Pixel-SAIL - –µ–¥–∏–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º–∞—à–∏
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#agents", "#games", "#benchmark", "#open_source"], "emoji": "üéÆ", "ru": {"title": "TextArena: –ê—Ä–µ–Ω–∞ –¥–ª—è –æ—Ç—Ç–∞—á–∏–≤–∞–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "TextArena - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –Ω–∞–±–æ—Ä —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–≥—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "SAIL: –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "SAIL - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ –≤ –µ–¥–∏–Ω–æ–π
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#training", "#inference", "#reasoning", "#survey", "#optimization", "#data", "#small_models"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –∫–æ—Ä–æ—á–µ, –º–µ–Ω—å—à–µ, –±—ã—Å—Ç—Ä–µ–µ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–µ–π 
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#data", "#reasoning", "#optimization", "#benchmark", "#math", "#training"], "emoji": "üéØ", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ActPRM - –ø–æ–¥—Ö–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (PRM).
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#long_context", "#diffusion", "#video", "#training"], "emoji": "üé•", "ru": {"title": "NormalCrafter: –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–æ—Ä–º–∞–ª–µ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NormalCrafter - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞–ª–µ–π –≤ –≤–∏–¥
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal"], "emoji": "üß†", "ru": {"title": "VisualPuzzles: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ—Ü–µ–Ω–∫—É –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VisualPuzzles –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –í –æ—Ç–ª–∏—á
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#ethics", "#data", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "RealHarm: —Ä–µ–∞–ª—å–Ω—ã–µ —É–≥—Ä–æ–∑—ã –ò–ò-—Å–∏—Å—Ç–µ–º –ø–æ–¥ –º–∏–∫—Ä–æ—Å–∫–æ–ø–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç RealHarm, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –ø—É
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Embedded Representation Wa
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#rl", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#rl", "#rag"], "emoji": "üîç", "ru": {"title": "–ù–∞—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –æ–∫—É–ø–∞–µ—Ç—Å—è: ReZero –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ ReZero –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#math", "#dataset", "#data", "#open_source", "#rl"], "emoji": "üßÆ", "ru": {"title": "DeepMath-103K: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "DeepMath-103K - —ç—Ç–æ –Ω–æ–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ–∫–æ–ª–æ 103 —Ç—ã—Å—è—á –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#training", "#inference", "#open_source", "#benchmark", "#optimization", "#small_models", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "SimpleAR - —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –ø—Ä–æ—Å—Ç—É—é –∞—Ä—Ö–∏—Ç
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#open_source", "#3d", "#rlhf", "#training", "#diffusion", "#optimization"], "emoji": "üöó", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω LiDAR —Å –ø–æ–º–æ—â—å—é Distillation-DPO", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Distillation-DPO –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è
[16.04.2025 19:09] Querying the API.
[16.04.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.
[16.04.2025 19:09] Response: {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç DataDecide - –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π, –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π (150M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) —Ö–æ—Ä–æ—à–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–µ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ (1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤). –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –≤ –Ω–µ–±–æ–ª—å—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä—è–¥–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é >80% –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Å–µ–≥–æ 0.01% –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.",
  "emoji": "üß†",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[16.04.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute."

[16.04.2025 19:09] Response: ```python
["DATASET", "BENCHMARK", "DATA", "TRAINING", "SMALL_MODELS"]
```
[16.04.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute."

[16.04.2025 19:09] Response: ```python
["OPEN_SOURCE", "OPTIMIZATION"]
```
[16.04.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of efficiently selecting datasets for pretraining large language models by utilizing smaller-scale experiments. It introduces DataDecide, a comprehensive suite that allows researchers to evaluate the impact of different datasets and model sizes on performance. The authors conduct extensive experiments across various corpora and find that performance rankings at smaller model sizes can effectively predict outcomes at larger scales. Additionally, they demonstrate that using continuous likelihood metrics can significantly enhance the predictability of benchmarks with minimal computational resources.","title":"Optimize Dataset Selection for Large Language Models with DataDecide"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of efficiently selecting datasets for pretraining large language models by utilizing smaller-scale experiments. It introduces DataDecide, a comprehensive suite that allows researchers to evaluate the impact of different datasets and model sizes on performance. The authors conduct extensive experiments across various corpora and find that performance rankings at smaller model sizes can effectively predict outcomes at larger scales. Additionally, they demonstrate that using continuous likelihood metrics can significantly enhance the predictability of benchmarks with minimal computational resources.', title='Optimize Dataset Selection for Large Language Models with DataDecide'))
[16.04.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÂ∞èËßÑÊ®°ÂÆûÈ™åÊù•ÈÄâÊã©Êï∞ÊçÆÈõÜÔºå‰ª•Èôç‰ΩéÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÊàêÊú¨„ÄÇÊàë‰ª¨ÂèëÂ∏É‰∫ÜDataDecideÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊîæÁöÑÊ®°Âûã„ÄÅÊï∞ÊçÆÂíåËØÑ‰º∞Â•ó‰ª∂ÔºåÊó®Âú®Â∏ÆÂä©Á†îÁ©∂ËÄÖÊé¢Á¥¢ÊúÄ‰Ω≥Êï∞ÊçÆÈõÜÈÄâÊã©„ÄÇÈÄöËøáÂú®25‰∏™‰∏çÂêåÊù•Ê∫êÁöÑËØ≠ÊñôÂ∫ì‰∏äËøõË°åÊéßÂà∂È¢ÑËÆ≠ÁªÉÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞Â∞èËßÑÊ®°Ê®°ÂûãÁöÑÊéíÂêçÂèØ‰ª•ÊúâÊïàÈ¢ÑÊµãÂ§ßËßÑÊ®°Ê®°ÂûãÁöÑË°®Áé∞„ÄÇ‰ΩøÁî®ËøûÁª≠‰ººÁÑ∂Â∫¶ÊåáÊ†á‰Ωú‰∏∫Â∞èËßÑÊ®°ÂÆûÈ™åÁöÑ‰ª£ÁêÜÔºåÂèØ‰ª•Âú®ÁõÆÊ†áËßÑÊ®°‰∏ãÂÆûÁé∞Ë∂ÖËøá80%ÁöÑÂèØÈ¢ÑÊµãÊÄß„ÄÇ","title":"Â∞èËßÑÊ®°ÂÆûÈ™åÂä©ÂäõÂ§ßÊ®°ÂûãÊï∞ÊçÆÈÄâÊã©"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÂ∞èËßÑÊ®°ÂÆûÈ™åÊù•ÈÄâÊã©Êï∞ÊçÆÈõÜÔºå‰ª•Èôç‰ΩéÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÊàêÊú¨„ÄÇÊàë‰ª¨ÂèëÂ∏É‰∫ÜDataDecideÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊîæÁöÑÊ®°Âûã„ÄÅÊï∞ÊçÆÂíåËØÑ‰º∞Â•ó‰ª∂ÔºåÊó®Âú®Â∏ÆÂä©Á†îÁ©∂ËÄÖÊé¢Á¥¢ÊúÄ‰Ω≥Êï∞ÊçÆÈõÜÈÄâÊã©„ÄÇÈÄöËøáÂú®25‰∏™‰∏çÂêåÊù•Ê∫êÁöÑËØ≠ÊñôÂ∫ì‰∏äËøõË°åÊéßÂà∂È¢ÑËÆ≠ÁªÉÂÆûÈ™åÔºåÊàë‰ª¨ÂèëÁé∞Â∞èËßÑÊ®°Ê®°ÂûãÁöÑÊéíÂêçÂèØ‰ª•ÊúâÊïàÈ¢ÑÊµãÂ§ßËßÑÊ®°Ê®°ÂûãÁöÑË°®Áé∞„ÄÇ‰ΩøÁî®ËøûÁª≠‰ººÁÑ∂Â∫¶ÊåáÊ†á‰Ωú‰∏∫Â∞èËßÑÊ®°ÂÆûÈ™åÁöÑ‰ª£ÁêÜÔºåÂèØ‰ª•Âú®ÁõÆÊ†áËßÑÊ®°‰∏ãÂÆûÁé∞Ë∂ÖËøá80%ÁöÑÂèØÈ¢ÑÊµãÊÄß„ÄÇ', title='Â∞èËßÑÊ®°ÂÆûÈ™åÂä©ÂäõÂ§ßÊ®°ÂûãÊï∞ÊçÆÈÄâÊã©'))
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã 4-–≥–æ –∫–æ–Ω–∫—É—Ä—Å–∞ PVUW –ø–æ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π, –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –≤ —Ä–∞–º–∫–∞—Ö CVPR 2025. –ö–æ–Ω–∫—É—Ä—Å –≤–∫–ª—é—á–∞–ª –¥–≤–∞
[16.04.2025 19:09] Using data from previous issue: {"categories": ["#low_resource", "#multimodal", "#training", "#open_source", "#science", "#rag"], "emoji": "üéì", "ru": {"title": "AI-U: –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AI University (AI-U) - –≥–∏–±–∫—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–æ–¥–∞—á–∏ —É—á–µ–±–Ω–æ–≥
[16.04.2025 19:09] Querying the API.
[16.04.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier.
[16.04.2025 19:09] Response: {
  "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, —Å–æ—á–µ—Ç–∞—é—â–∏—Ö –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ–±—Ä–µ–∑–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –±–ª–æ–∫–æ–≤ SSM. –ü—Ä–∏–º–µ–Ω—è—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ —Å–∂–∞—Ç–∏—è –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, –∏–º —É–¥–∞–ª–æ—Å—å —É–º–µ–Ω—å—à–∏—Ç—å –º–æ–¥–µ–ª—å Nemotron-H 8B –¥–æ 4 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å. –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –≤ 2 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏.",
  "emoji": "‚úÇÔ∏è",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞"
}
[16.04.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier."

[16.04.2025 19:09] Response: ```python
["ARCHITECTURE", "INFERENCE", "TRAINING", "SMALL_MODELS"]
```
[16.04.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier."

[16.04.2025 19:09] Response: ```python
["OPTIMIZATION"]
```
[16.04.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses a new method for improving Hybrid LLM architectures that use both Attention and State Space Models (SSMs). The authors propose a group-aware pruning strategy that maintains the effectiveness of SSMs while reducing the model size. They show that this pruning, combined with knowledge distillation, leads to smaller models that are not only more accurate but also faster in making predictions. The results indicate that their approach can significantly enhance model performance while using fewer training resources.","title":"Efficient Hybrid Models: Pruning for Performance and Accuracy"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses a new method for improving Hybrid LLM architectures that use both Attention and State Space Models (SSMs). The authors propose a group-aware pruning strategy that maintains the effectiveness of SSMs while reducing the model size. They show that this pruning, combined with knowledge distillation, leads to smaller models that are not only more accurate but also faster in making predictions. The results indicate that their approach can significantly enhance model performance while using fewer training resources.', title='Efficient Hybrid Models: Pruning for Performance and Accuracy'))
[16.04.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÊ∑∑ÂêàÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊû∂ÊûÑÁöÑÂéãÁº©ÊïàÊûúÔºåËøô‰∫õÊû∂ÊûÑÁªìÂêà‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁæ§‰ΩìÊÑüÁü•Ââ™ÊûùÁ≠ñÁï•ÔºåËÉΩÂ§ü‰øùÊåÅSSMÊ®°ÂùóÁöÑÁªìÊûÑÂÆåÊï¥ÊÄßÂíåÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõ„ÄÇÈÄöËøáËøôÁßçÂéãÁº©ÊñπÊ≥ïÔºåÊàë‰ª¨Â∞ÜNemotron-H 8BÊ∑∑ÂêàÊ®°ÂûãÂéãÁº©Âà∞4BÂèÇÊï∞ÔºåÂπ∂ÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉÊâÄÈúÄÁöÑÊ†áËÆ∞Êï∞Èáè„ÄÇÊúÄÁªàÂæóÂà∞ÁöÑÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÈÄüÂ∫¶‰∏äÂùá‰ºò‰∫éÂêåÁ≠âËßÑÊ®°ÁöÑÊ®°ÂûãÔºåÊòæËëóÊé®Âä®‰∫ÜÊ®°ÂûãÊÄßËÉΩÁöÑËæπÁïå„ÄÇ","title":"ÂéãÁº©Ê∑∑ÂêàÊ®°ÂûãÔºåÊèêÂçáÊÄßËÉΩ‰∏éÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÊ∑∑ÂêàÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊû∂ÊûÑÁöÑÂéãÁº©ÊïàÊûúÔºåËøô‰∫õÊû∂ÊûÑÁªìÂêà‰∫ÜÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÁä∂ÊÄÅÁ©∫Èó¥Ê®°ÂûãÔºàSSMÔºâ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁæ§‰ΩìÊÑüÁü•Ââ™ÊûùÁ≠ñÁï•ÔºåËÉΩÂ§ü‰øùÊåÅSSMÊ®°ÂùóÁöÑÁªìÊûÑÂÆåÊï¥ÊÄßÂíåÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõ„ÄÇÈÄöËøáËøôÁßçÂéãÁº©ÊñπÊ≥ïÔºåÊàë‰ª¨Â∞ÜNemotron-H 8BÊ∑∑ÂêàÊ®°ÂûãÂéãÁº©Âà∞4BÂèÇÊï∞ÔºåÂπ∂ÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉÊâÄÈúÄÁöÑÊ†áËÆ∞Êï∞Èáè„ÄÇÊúÄÁªàÂæóÂà∞ÁöÑÊ®°ÂûãÂú®ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÈÄüÂ∫¶‰∏äÂùá‰ºò‰∫éÂêåÁ≠âËßÑÊ®°ÁöÑÊ®°ÂûãÔºåÊòæËëóÊé®Âä®‰∫ÜÊ®°ÂûãÊÄßËÉΩÁöÑËæπÁïå„ÄÇ', title='ÂéãÁº©Ê∑∑ÂêàÊ®°ÂûãÔºåÊèêÂçáÊÄßËÉΩ‰∏éÊïàÁéá'))
[16.04.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#open_source", "#cv", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–∂–∞—Ç–∏–µ –∏ –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏
[16.04.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –Ω–µ —Ö—É–∂–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Adaptive Computation Pruning (ACP) - –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –æ–±—Ä–µ–∑–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–∏ Forgetting Transformer (FoX
[16.04.2025 19:10] Using data from previous issue: {"categories": ["#open_source", "#data", "#dataset", "#science", "#training"], "emoji": "üß†", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –ª–µ–Ω–∏–≤—ã–º –º—ã—à–ª–µ–Ω–∏–µ–º –≤ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ \"–ª–µ–Ω–∏–≤–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è\" –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ—Ü–µ–Ω–∑–∏–π. –ê–≤—Ç–æ—Ä—ã 
[16.04.2025 19:10] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#multimodal", "#video", "#benchmark"], "emoji": "üéûÔ∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (L
[16.04.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#video", "#interpretability", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π —Å –ø–æ–º–æ—â—å—é VLM", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü
[16.04.2025 19:10] Using data from previous issue: {"categories": ["#training", "#multimodal", "#alignment", "#cv", "#diffusion"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ—Ü–µ—Å—Å —à
[16.04.2025 19:10] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#optimization"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é State Space Models", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å Change State Space Model –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥–µ
[16.04.2025 19:10] Loading Chinese text from previous data.
[16.04.2025 19:10] Renaming data file.
[16.04.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-04-16.json
[16.04.2025 19:10] Saving new data file.
[16.04.2025 19:10] Generating page.
[16.04.2025 19:10] Renaming previous page.
[16.04.2025 19:10] Renaming previous data. index.html to ./d/2025-04-16.html
[16.04.2025 19:10] [Experimental] Generating Chinese page for reading.
[16.04.2025 19:10] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠ shƒìng', 'trans': 'improve'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√† x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«î y√°n m√≥ x√≠ng', 'trans': 'language model'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ÂÖ¥Ë∂£', 'pinyin': 'x√¨ng q√π', 'trans': 'interest'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´ l√†i', 'trans': 'depend on'}, {'word': 'ÁõëÁù£', 'pinyin': 'ji√†n d≈´', 'trans': 'supervised'}, {'word': '‰ø°Âè∑', 'pinyin': 'x√¨n h√†o', 'trans': 'signal'}, {'word': 'ÂèØÊâ©Â±ïÊÄß', 'pinyin': 'kƒõ ku√≤ zh«én x√¨ng', 'trans': 'scalability'}, {'word': 'È´ò', 'pinyin': 'gƒÅo', 'trans': 'high'}, {'word': 'Ê†áÊ≥®', 'pinyin': 'biƒÅo zh√π', 'trans': 'annotation'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ng bƒõn', 'trans': 'cost'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Êó†ÁõëÁù£', 'pinyin': 'w√∫ ji√†n d≈´', 'trans': 'unsupervised'}, {'word': 'Ëá™ËÆ≠ÁªÉ', 'pinyin': 'z√¨ x√πn li√†n', 'trans': 'self-training'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'Âêç‰∏∫', 'pinyin': 'm√≠ng w√©i', 'trans': 'named'}, {'word': 'Ê≠•ËøõÂºè', 'pinyin': 'b√π j√¨n sh√¨', 'trans': 'stepwise'}, {'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'prediction'}, {'word': 'ÈáçÈááÊ†∑', 'pinyin': 'ch√≥ng c«éi y√†ng', 'trans': 'resampling'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': '‰ºòÂäø', 'pinyin': 'y≈çu sh√¨', 'trans': 'advantage'}, {'word': 'Ê†°ÂáÜ', 'pinyin': 'ji√†o zh«în', 'trans': 'calibration'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çu hu√†', 'trans': 'optimization'}, {'word': 'ÊçüÂ§±ÂáΩÊï∞', 'pinyin': 's«în shƒ´ h√°n sh√π', 'trans': 'loss function'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Â§ñÈÉ®', 'pinyin': 'w√†i b√π', 'trans': 'external'}, {'word': 'ÂèëÂ∏É', 'pinyin': 'fƒÅ b√π', 'trans': 'release'}]
[16.04.2025 19:10] Renaming previous Chinese page.
[16.04.2025 19:10] Renaming previous data. zh.html to ./d/2025-04-15_zh_reading_task.html
[16.04.2025 19:10] Writing Chinese reading task.
[16.04.2025 19:10] Writing result.
[16.04.2025 19:10] Renaming log file.
[16.04.2025 19:10] Renaming previous data. log.txt to ./logs/2025-04-16_last_log.txt
