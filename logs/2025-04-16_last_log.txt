[16.04.2025 05:13] Read previous papers.
[16.04.2025 05:13] Generating top page (month).
[16.04.2025 05:13] Writing top page (month).
[16.04.2025 06:16] Read previous papers.
[16.04.2025 06:16] Get feed.
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10481
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10337
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10766
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11442
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10465
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11346
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10462
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10559
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11427
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11343
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11447
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10188
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06949
[16.04.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2504.11326
[16.04.2025 06:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.04.2025 06:16] No deleted papers detected.
[16.04.2025 06:16] Downloading and parsing papers (pdf, html). Total: 14.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.10481.
[16.04.2025 06:16] Extra JSON file exists (./assets/json/2504.10481.json), skip PDF parsing.
[16.04.2025 06:16] Paper image links file exists (./assets/img_data/2504.10481.json), skip HTML parsing.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.10337.
[16.04.2025 06:16] Extra JSON file exists (./assets/json/2504.10337.json), skip PDF parsing.
[16.04.2025 06:16] Paper image links file exists (./assets/img_data/2504.10337.json), skip HTML parsing.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.10766.
[16.04.2025 06:16] Extra JSON file exists (./assets/json/2504.10766.json), skip PDF parsing.
[16.04.2025 06:16] Paper image links file exists (./assets/img_data/2504.10766.json), skip HTML parsing.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.11442.
[16.04.2025 06:16] Downloading paper 2504.11442 from http://arxiv.org/pdf/2504.11442v1...
[16.04.2025 06:16] Extracting affiliations from text.
[16.04.2025 06:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 2 4 4 1 1 . 4 0 5 2 : r a Leon Guertlerp,*, Bobby ChengN,, Simon YuB, Bo LiuR, Leshem ChoshenQ, and Cheston Tanp,N Centre for Frontier AI Research (CFAR), A*STAR Institute of High Performance Computing, A*STAR Northeastern University National University of Singapore MIT, MIT-IBM Watson AI Lab (cid:147) Play: https://www.textarena.ai/ (cid:140) Leaderboard: https://www.textarena.ai/leaderboard Code: https://github.com/LeonGuertler/TextArena Figure 1: TextArena Soft-skill comparison. Frontier models and Humanity are compared across ten key skills. Each skill is normalised separately for presentation; see the leaderboard for full data. "
[16.04.2025 06:16] Response: ```python
["Centre for Frontier AI Research (CFAR), A*STAR Institute of High Performance Computing, A*STAR", "Northeastern University", "National University of Singapore", "MIT, MIT-IBM Watson AI Lab"]
```
[16.04.2025 06:16] Deleting PDF ./assets/pdf/2504.11442.pdf.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.10465.
[16.04.2025 06:16] Downloading paper 2504.10465 from http://arxiv.org/pdf/2504.10465v1...
[16.04.2025 06:16] Extracting affiliations from text.
[16.04.2025 06:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding Tao Zhang1,2 Xiangtai Li1 Zilong Huang1 Yanwei Li1 Weixian Lei Xueqing Deng1 Shihao Chen2 1 Bytedance Seed Shunping Ji2 2 WHU Jiashi Feng1 5 2 0 A 4 1 ] . [ 1 5 6 4 0 1 . 4 0 5 2 : r Project Page: https://zhang-tao-whu.github.io/project/pixelsail Figure 1. Comparison of current MLLMs for pixel-wise understanding with our method. (a) and (b). Current MLLMs for pixel-wise understanding feature highly complex system architectures, including an LLM, CLIP-like vision backbone, an object token extraction model, segmentation vision backbone, and SAM-like decoder. (c). Our method employs only single transformer. "
[16.04.2025 06:16] Response: ```python
["Bytedance Seed", "WHU"]
```
[16.04.2025 06:16] Deleting PDF ./assets/pdf/2504.10465.pdf.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.11346.
[16.04.2025 06:16] Extra JSON file exists (./assets/json/2504.11346.json), skip PDF parsing.
[16.04.2025 06:16] Paper image links file exists (./assets/img_data/2504.11346.json), skip HTML parsing.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.10462.
[16.04.2025 06:16] Downloading paper 2504.10462 from http://arxiv.org/pdf/2504.10462v1...
[16.04.2025 06:16] Extracting affiliations from text.
[16.04.2025 06:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 2 6 4 0 1 . 4 0 5 2 : r The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with Single Transformer Weixian Lei* Jiacong Wang* Haochen Wang* Jun Hao Liew Jiashi Feng Zilong Huang Equal contribution, Project Lead Bytedance Seed "
[16.04.2025 06:16] Response: ```python
["Bytedance Seed"]
```
[16.04.2025 06:16] Deleting PDF ./assets/pdf/2504.10462.pdf.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.10559.
[16.04.2025 06:16] Extra JSON file exists (./assets/json/2504.10559.json), skip PDF parsing.
[16.04.2025 06:16] Paper image links file exists (./assets/img_data/2504.10559.json), skip HTML parsing.
[16.04.2025 06:16] Success.
[16.04.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2504.11427.
[16.04.2025 06:16] Downloading paper 2504.11427 from http://arxiv.org/pdf/2504.11427v1...
[16.04.2025 06:16] Extracting affiliations from text.
[16.04.2025 06:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 7 2 4 1 1 . 4 0 5 2 : r NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors Yanrui Bin1 Wenbo Hu2 Haoyuan Wang3 Xinya Chen4 Bing Wang1 1Spatial Intelligence Group, The Hong Kong Polytechnic University 2ARC Lab, Tencent PCG 3City University of Hong Kong 4Huazhong University of Science and Technology https://normalcrafter.github.io/ Input Marigold-E2E-FT Ours Figure 1. We innovate NormalCrafter, novel video normal estimation model, that can generate temporally consistent normal sequences with fine-grained details from open-world videos with arbitrary lengths. Compared to results from state-of-the-art image normal estimators, Marigold-E2E-FT [26], our results exhibit both higher spatial fidelity and temporal consistency, as shown in the frame visualizations and temporal profiles (marked by the red lines and rectangles). "
[16.04.2025 06:17] Response: ```python
[
    "Spatial Intelligence Group, The Hong Kong Polytechnic University",
    "ARC Lab, Tencent PCG",
    "City University of Hong Kong",
    "Huazhong University of Science and Technology"
]
```
[16.04.2025 06:17] Deleting PDF ./assets/pdf/2504.11427.pdf.
[16.04.2025 06:17] Success.
[16.04.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2504.11343.
[16.04.2025 06:17] Downloading paper 2504.11343 from http://arxiv.org/pdf/2504.11343v1...
[16.04.2025 06:17] Extracting affiliations from text.
[16.04.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 3 4 3 1 1 . 4 0 5 2 : r Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce Wei Xiong Jiarui Yao Yuhui Xu Bo Pang Lei Wang Doyen Sahoo Junnan Li Nan Jiang Tong Zhang Caiming Xiong Hanze Dong Salesforce AI Research University of Illinois Urbana-Champaign Abstract Reinforcement learning (RL) has become prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPOs main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training. We investigate reinforcement learning (RL) algorithms in the context of fine-tuning large language models (LLMs) with verifiable rewards. Our focus is on mathematical reasoning tasks, which have recently received significant attention following the release of models such as OpenAIs O1 Model (Jaech et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025). The dominant approach"
[16.04.2025 06:17] Response: ```python
["Salesforce AI Research", "University of Illinois Urbana-Champaign"]
```
[16.04.2025 06:17] Deleting PDF ./assets/pdf/2504.11343.pdf.
[16.04.2025 06:17] Success.
[16.04.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2504.11447.
[16.04.2025 06:17] Extra JSON file exists (./assets/json/2504.11447.json), skip PDF parsing.
[16.04.2025 06:17] Paper image links file exists (./assets/img_data/2504.11447.json), skip HTML parsing.
[16.04.2025 06:17] Success.
[16.04.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2504.10188.
[16.04.2025 06:17] Downloading paper 2504.10188 from http://arxiv.org/pdf/2504.10188v1...
[16.04.2025 06:17] Extracting affiliations from text.
[16.04.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Deyuan Liu1* Peng Sun2,1* Xufeng Li1,3 1Westlake University 2Zhejiang University Tao Lin1 3Nanjing University 5 2 0 2 4 ] . [ 1 8 8 1 0 1 . 4 0 5 2 : r Figure 1. Embedded Representation Warmup (ERW). Throughout the training process, we demonstrate that incorporating representations at the early stages is highly beneficial. To this end, we propose representation warmup stage that employs representation alignment loss to integrate representations from models such as Dinov2 [38] into the ERW. This initialized representation region is subsequently embedded into the diffusion model pipeline, providing strong starting point for training. Our ERW method thus enhances both efficiency and effectiveness, leading to faster convergence and superior performance compared to the REPA [54] method, thereby establishing new state-of-the-art. "
[16.04.2025 06:17] Response: ```python
["Westlake University", "Zhejiang University", "Nanjing University"]
```
[16.04.2025 06:17] Deleting PDF ./assets/pdf/2504.10188.pdf.
[16.04.2025 06:17] Success.
[16.04.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2504.06949.
[16.04.2025 06:17] Downloading paper 2504.06949 from http://arxiv.org/pdf/2504.06949v1...
[16.04.2025 06:18] Extracting affiliations from text.
[16.04.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 9 4 9 6 0 . 4 0 5 2 : r Preprint. Under review. Adaptive Computation Pruning for the Forgetting Transformer Zhixuan Lin Mila & Universite de Montreal zxlin.cs@gmail.com Johan Obando-Ceron Mila & Universite de Montreal jobando0730@gmail.com Xu Owen He MakerMaker AI owen.hexu@gmail.com Aaron Courville Mila & Universite de Montreal courvila@mila.quebec "
[16.04.2025 06:18] Response: ```python
["Mila & Universite de Montreal", "MakerMaker AI"]
```
[16.04.2025 06:18] Deleting PDF ./assets/pdf/2504.06949.pdf.
[16.04.2025 06:18] Success.
[16.04.2025 06:18] Downloading and parsing paper https://huggingface.co/papers/2504.11326.
[16.04.2025 06:18] Downloading paper 2504.11326 from http://arxiv.org/pdf/2504.11326v1...
[16.04.2025 06:18] Extracting affiliations from text.
[16.04.2025 06:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 6 2 3 1 1 . 4 0 5 2 : r PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild Henghui Ding*, Chang Liu*, Nikhila Ravi*, Shuting He*, Yunchao Wei*, Song Bai*, Philip Torr* Kehuan Song, Xinglin Xie, Kexin Zhang, Licheng Jiao, Lingling Li, Shuyuan Yang Xuqiang Cao, Linnan Zhao, Jiaxuan Zhao, Mengjiao Wang, Junpei Zhang, Hao Fang, Runmin Cong, Xu Liu, Xiankai Lu, Yuting Yang, Zhiyang Che, Tianming Liang, Haichao Jiang, Wei-Shi Zheng, Haobo Yuan, Xiangtai Li, Tao Zhang, https://pvuw.github.io/ Lu Qi, Jian-Fang Hu Ming-Hsuan Yang "
[16.04.2025 06:18] Response: ```python
[]
```
[16.04.2025 06:18] Extracting affiliations from text.
[16.04.2025 06:18] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 6 2 3 1 1 . 4 0 5 2 : r PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild Henghui Ding*, Chang Liu*, Nikhila Ravi*, Shuting He*, Yunchao Wei*, Song Bai*, Philip Torr* Kehuan Song, Xinglin Xie, Kexin Zhang, Licheng Jiao, Lingling Li, Shuyuan Yang Xuqiang Cao, Linnan Zhao, Jiaxuan Zhao,Mengjiao Wang, Junpei Zhang, Hao Fang, Runmin Cong, Xu Liu, Xiankai Lu, Yuting Yang, Zhiyang Che,Tianming Liang, Haichao Jiang, Wei-Shi Zheng, Haobo Yuan, Xiangtai Li, Tao Zhang, https://pvuw.github.io/ Lu Qi, Jian-Fang Hu Ming-Hsuan YangThis report provides comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and The challenge features two future research directions. tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motionguided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/. 1. Introduction Pixel-level understanding of dynamic and complex visual scenes remains core yet unresolved problem in computer vision [9, 10, 19, 23, 34, 42]. While traditional research has predominantly focused on semantic segmentation within static images [57], such approaches fall short in capturing the temporal continuity of the real world. In contrast, video segmentation [9, 10, 20, 21, 36, 37] offers more realistic framework, aligning better with applications that demand spatiotemporal reasoningsuch as autonomous driving, aerial navigation, and mobile video editing. These use cases underscore growing shift toward scene under- *Authors are CVPR 2025 PVUW Challenge organizers. All others are challenge participants from the top-3 teams of MOSE and MeViS tracks. (cid:0) Corresponding to Henghui Ding (henghui.ding@gmail.com), the Institute of Big Data, Fudan University, Shanghai, China. standing methods that are not only spatially precise but also temporally coherent. To advance research in this direction, we introduce the Pixel-level Video Understanding in the Wild (PVUW) workshop, which emphasizes the challenges posed by unconstrained, real-world environments [13]. PVUW seeks to narrow the gap between static and dynamic scene understanding, encouraging the development of robust algorithms that can generalize across diverse, time-varying visual conditions. Through this initiative, we aim to catalyze innovation toward deploying perception systems capable of reliable operation in the wild. Recent advances in Large Language Models and multimodal LLMs have significantly reshaped computer vision [35]. Alongside, foundational models like SAM2 [34] have leveraged large-scale data to achieve strong generalization. Notably, progress in tasks such as Video Object Segmentation (VOS) [10] and Referring Video Object Segmentation (RVOS) [9] highlights the fields continued momentum toward more robust and unified vision systems. Building on these developments, the goal of our workshop and challenge is to keep pace with cutting-edge research, offer challenging, yet realistic benchmark to evaluate state-of-the-art models, and provide valuable insights into both the current trends and future directions of video understanding. Following past challenges, we aim to continuously provide challenging and diverse benchmarking data that are taken in real world, and in this year, we have added more latest data that are first time released. 2. The PVUW 2025 Challenge This year, we center our challenge around two focused tracks: the MOSE Track, which benchmarks advanced VOS methods in complex and densely populated scenes; and the MeViS Track, which evaluates models on language-guided 1 Table 1. MOSE Track results and top 20 of the final rankings. Table 2. MeViS Track results and top 20 of the final rankings. Rank Team (cid:140) 1 5 2 5 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 BrainyBots DeepSegMa JIO SCU Leung wulutuluman mima LK186******96 STELATOS9 MaxBitter XiaomiYU7 menghaoran zjy05140514 keeper zhaojinhui LuxeedR7 HuaweiAITOM9 YuLinLin ccHub ZhiMu ppbb 83.59 82.50 80.28 79.93 79.89 79.80 79.80 79.65 79.64 79.47 79.59 79.46 79.48 79.44 79.40 79.23 79.15 78.93 78.79 78. &F Rank Team 90.92 90.07 87.57 87.33 87.21 87.21 87.10 87.16 87.10 86.92 86.79 86.85 86.83 86.83 86.85 86.58 86.55 86.68 86.59 86.11 87.26 86.28 83.92 83.63 83.55 83.51 83.45 83.41 83.37 83.20 83.19 83.15 83.15 83.14 83.12 82.91 82.85 82.80 82.69 82. (cid:140) 1 5 2 5 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 MVP-Lab ReferDINO-iSEE Sa2VA Pengsong ssam2s strong kimchi seilvik90 yiweima xmu maclab xinming zhangtao-whu yiweima TransVG321 xmu-xiaoma666 MYOLO kker101 X-CLIP tbao LuQiLXX mengyuan 58.83 56.79 52.68 53.06 52.00 51.78 50.61 50.93 50.63 51.24 51.22 50.49 50.10 49.86 49.80 50.02 49.64 49.05 48.48 48.63 &F 65.14 64.07 59.84 58.76 58.33 58.27 59.22 58.65 58.32 57.33 57.19 57.30 57.30 56.92 56.97 56.55 56.84 56.59 54.69 54.42 61.98 60.43 56.26 55.91 55.16 55.02 54.91 54.79 54.48 54.28 54.21 53.90 53.70 53.39 53.38 53.29 53.24 52.82 51.59 51.53 video segmentation, with particular emphasis on motionguided language expressions. 2.1. Two Video Segmentation Tracks Track 1: MOSE Track Complex Video Object Segmentation (MOSE) [10] aims to track and segment objects in videos of complex environments. This track is based on the MOSE [10] dataset, which is new video object segmentation benchmark designed to study object tracking and segmentation in complex, real-world scenes. Unlike previous video segmentation datasets [32, 43] that focus on salient and isolated objects, MOSE features crowded environments, frequent occlusions, and object disappearances. It consists of 2,149 video clips and 5,200 objects across 36 categories, with over 430,000 high-quality segmentation masks. MOSE challenges existing VOS models and highlights the performance gap in complex scenarios, encouraging further research into robust segmentation techniques. This years testing set is part of MOSE testing set, but with more challenging newly taken data added. The ground truths of all videos in the testing sets are confidential"
[16.04.2025 06:18] Mistral response. {"id": "a51d624b429a4676ad43c20657eee294", "object": "chat.completion", "created": 1744784301, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"the Institute of Big Data, Fudan University, Shanghai, China\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 2421, "total_tokens": 2444, "completion_tokens": 23}}
[16.04.2025 06:18] Response: ```python
["the Institute of Big Data, Fudan University, Shanghai, China"]
```
[16.04.2025 06:18] Deleting PDF ./assets/pdf/2504.11326.pdf.
[16.04.2025 06:18] Success.
[16.04.2025 06:18] Enriching papers with extra data.
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 0. With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle t...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 1. An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently ...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 2. As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quali...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 3. TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 4. Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In thi...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 5. We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesth...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 6. This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 7. Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain ...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 8. Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing met...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 9. Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood....
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 10. The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. ...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 11. Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down co...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 12. The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each time...
[16.04.2025 06:18] ********************************************************************************
[16.04.2025 06:18] Abstract 13. This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which...
[16.04.2025 06:18] Read previous papers.
[16.04.2025 06:18] Generating reviews via LLM API.
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#dataset", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "xVerify: —Ç–æ—á–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç xVerify - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. xVerify —Å–ø–æ—Å–æ–±–µ–Ω 
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#long_context", "#optimization", "#training", "#rl", "#math"], "emoji": "üîç", "ru": {"title": "Heimdall: –ò–ò-–≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Heimdall - –º–æ–¥–µ–ª—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#data", "#training"], "emoji": "üß†", "ru": {"title": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–µ–∫—Ä–µ—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–æ–π–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, –≤—ã–∑–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã–º–∏ —Ä–∞–∑–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ 
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#agents", "#games", "#benchmark", "#open_source"], "emoji": "üéÆ", "ru": {"title": "TextArena: –ê—Ä–µ–Ω–∞ –¥–ª—è –æ—Ç—Ç–∞—á–∏–≤–∞–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "TextArena - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –Ω–∞–±–æ—Ä —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–≥—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#survey", "#optimization", "#benchmark", "#multimodal", "#training", "#architecture", "#games"], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Pixel-SAIL - –µ–¥–∏–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º–∞—à–∏
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#alignment", "#data", "#optimization", "#dataset", "#multimodal", "#architecture", "#training"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: Seedream 3.0 –ø–æ–¥–Ω–∏–º–∞–µ—Ç –ø–ª–∞–Ω–∫—É", "desc": "Seedream 3.0 - —ç—Ç–æ –¥–≤—É—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —É–ª—É—á—à–∞—é—â–∞—è –ø—Ä–µ–¥—ã–¥—É—â
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#multimodal", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "SAIL: –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "SAIL - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞ –≤ –µ–¥–∏–Ω–æ–π
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#data", "#reasoning", "#optimization", "#benchmark", "#math", "#training"], "emoji": "üéØ", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ActPRM - –ø–æ–¥—Ö–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (PRM).
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#cv", "#long_context", "#diffusion", "#video", "#training"], "emoji": "üé•", "ru": {"title": "NormalCrafter: –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–æ—Ä–º–∞–ª–µ–π –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NormalCrafter - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞–ª–µ–π –≤ –≤–∏–¥
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#rl", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#open_source", "#3d", "#rlhf", "#training", "#diffusion", "#optimization"], "emoji": "üöó", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω LiDAR —Å –ø–æ–º–æ—â—å—é Distillation-DPO", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Distillation-DPO –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Embedded Representation Wa
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –Ω–µ —Ö—É–∂–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Adaptive Computation Pruning (ACP) - –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –æ–±—Ä–µ–∑–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–∏ Forgetting Transformer (FoX
[16.04.2025 06:18] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã 4-–≥–æ –∫–æ–Ω–∫—É—Ä—Å–∞ PVUW –ø–æ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π, –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –≤ —Ä–∞–º–∫–∞—Ö CVPR 2025. –ö–æ–Ω–∫—É—Ä—Å –≤–∫–ª—é—á–∞–ª –¥–≤–∞
[16.04.2025 06:18] Loading Chinese text from previous data.
[16.04.2025 06:18] Renaming data file.
[16.04.2025 06:18] Renaming previous data. hf_papers.json to ./d/2025-04-16.json
[16.04.2025 06:18] Saving new data file.
[16.04.2025 06:18] Generating page.
[16.04.2025 06:18] Renaming previous page.
[16.04.2025 06:18] Renaming previous data. index.html to ./d/2025-04-16.html
[16.04.2025 06:18] [Experimental] Generating Chinese page for reading.
[16.04.2025 06:18] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'ÈáçÂ§ß', 'pinyin': 'zh√≤ng d√†', 'trans': 'major'}, {'word': 'ËøõÊ≠•', 'pinyin': 'j√¨n b√π', 'trans': 'progress'}, {'word': 'ÈááÁî®', 'pinyin': 'c«éi y√≤ng', 'trans': 'adopt'}, {'word': 'ËåÉÂºè', 'pinyin': 'f√†n sh√¨', 'trans': 'paradigm'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-training'}, {'word': 'Èò∂ÊÆµ', 'pinyin': 'jiƒì du√†n', 'trans': 'stage'}, {'word': 'Ëé∑Âèñ', 'pinyin': 'hu√≤ q«î', 'trans': 'obtain'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unified'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'Â§çÊùÇÊÄß', 'pinyin': 'f√π z√° x√¨ng', 'trans': 'complexity'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨ q√≠', 'trans': 'alignment'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}, {'word': 'ÂèØÂèò', 'pinyin': 'kƒõ bi√†n', 'trans': 'variable'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': '‰ΩçÁΩÆ', 'pinyin': 'w√®i zh√¨', 'trans': 'position'}, {'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'encoding'}, {'word': 'ÂÖàËøõ', 'pinyin': 'xiƒÅn j√¨n', 'trans': 'advanced'}, {'word': 'ÂêéËÆ≠ÁªÉ', 'pinyin': 'h√≤u x√πn li√†n', 'trans': 'post-training'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨ sh√π', 'trans': 'technology'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': 'ÁâπÂà´', 'pinyin': 't√® bi√©', 'trans': 'especially'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÊµãËØï', 'pinyin': 'c√® sh√¨', 'trans': 'test'}, {'word': 'ÂàÜÊï∞', 'pinyin': 'fƒìn sh√π', 'trans': 'score'}, {'word': 'Á∫™ÂΩï', 'pinyin': 'j√¨ l√π', 'trans': 'record'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'ÂèëÂ∏É', 'pinyin': 'fƒÅ b√π', 'trans': 'release'}, {'word': 'ÊùÉÈáç', 'pinyin': 'qu√°n zh√≤ng', 'trans': 'weights'}, {'word': '‰øÉËøõ', 'pinyin': 'c√π j√¨n', 'trans': 'promote'}, {'word': '‰∏ã‰∏Ä‰ª£', 'pinyin': 'xi√† yƒ´ d√†i', 'trans': 'next generation'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÂºÄÂèë', 'pinyin': 'kƒÅi fƒÅ', 'trans': 'development'}]
[16.04.2025 06:18] Renaming previous Chinese page.
[16.04.2025 06:18] Renaming previous data. zh.html to ./d/2025-04-15_zh_reading_task.html
[16.04.2025 06:18] Writing Chinese reading task.
[16.04.2025 06:18] Writing result.
[16.04.2025 06:18] Renaming log file.
[16.04.2025 06:18] Renaming previous data. log.txt to ./logs/2025-04-16_last_log.txt
