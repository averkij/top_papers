[26.02.2026 07:47] Read previous papers.
[26.02.2026 07:47] Generating top page (month).
[26.02.2026 07:47] Writing top page (month).
[26.02.2026 08:38] Read previous papers.
[26.02.2026 08:38] Get feed.
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12160
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21534
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21818
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18283
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22190
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22208
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22010
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19163
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20122
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15030
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14878
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22144
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21472
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18993
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18527
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21835
[26.02.2026 08:38] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19004
[26.02.2026 08:38] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2026 08:38] No deleted papers detected.
[26.02.2026 08:38] Downloading and parsing papers (pdf, html). Total: 17.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.12160.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.12160.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.12160.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.21534.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.21534.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.21534.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.21818.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.21818.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.21818.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.18283.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.18283.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.18283.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.22190.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.22190.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.22190.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.22208.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.22208.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.22208.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.22010.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.22010.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.22010.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.19163.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.19163.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.19163.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.20122.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.20122.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.20122.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.15030.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.15030.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.15030.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.14878.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.14878.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.14878.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.22144.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.22144.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.22144.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.21472.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.21472.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.21472.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.18993.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.18993.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.18993.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.18527.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.18527.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.18527.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.21835.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.21835.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.21835.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Downloading and parsing paper https://huggingface.co/papers/2602.19004.
[26.02.2026 08:38] Extra JSON file exists (./assets/json/2602.19004.json), skip PDF parsing.
[26.02.2026 08:38] Paper image links file exists (./assets/img_data/2602.19004.json), skip HTML parsing.
[26.02.2026 08:38] Success.
[26.02.2026 08:38] Enriching papers with extra data.
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 0. Abstract DreamID-Omni is a unified framework for controllable human-centric audio-video generation that uses a symmetric conditional diffusion transformer with dual-level disentanglement and multi-task progressive training to achieve state-of-the-art performance.  					AI-generated summary Recent ad...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 1. Abstract ARLArena framework analyzes training stability in agentic reinforcement learning and proposes SAMPO method for stable policy optimization across diverse tasks.  					AI-generated summary Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training a...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 2. Abstract SkyReels V4 is a unified multimodal video foundation model that generates, edits, and inpaints video and audio simultaneously using a dual-stream architecture with shared text encoding and efficient high-resolution processing.  					AI-generated summary SkyReels V4 is a unified multi modal ...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 3. Abstract HyTRec addresses the challenge of modeling long user behavior sequences by combining linear and softmax attention mechanisms with a temporal-aware delta network to balance efficiency and retrieval precision.  					AI-generated summary Modeling long sequences of user behaviors has emerged as...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 4. Abstract GUI-Libra addresses limitations in open-source GUI agents through specialized training methods that improve reasoning-grounding alignment and reinforcement learning under partial verifiability, demonstrating enhanced task completion across web and mobile platforms.  					AI-generated summar...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 5. Abstract Solaris is a multiplayer video world model that simulates consistent multi-view observations through a novel data collection system and staged training approach.  					AI-generated summary Existing action-conditioned video generation models (video world models) are limited to single-agent p...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 6. Abstract World Guidance framework enhances Vision-Language-Action models by mapping future observations into compact conditions for improved action generation and generalization.  					AI-generated summary Leveraging future observation modeling to facilitate action generation presents a promising av...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 7. Abstract JavisDiT++ presents a unified framework for joint audio-video generation using modality-specific mixture-of-experts, temporal-aligned RoPE, and audio-video direct preference optimization to achieve high-quality, synchronized multimedia synthesis.  					AI-generated summary AIGC has rapidly ...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 8. Abstract NanoKnow benchmark enables analysis of knowledge sources in LLMs by partitioning questions based on pre-training data presence, revealing how parametric and external knowledge interact in model responses.  					AI-generated summary How do large language models (LLMs) know what they know? An...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 9. Abstract The Sphere Encoder is an efficient generative model that produces images in a single forward pass by mapping images to a spherical latent space and decoding from random points on that sphere, achieving diffusion-like quality with significantly reduced inference costs.  					AI-generated sum...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 10. Abstract Foundation model agents rely on natural language tool descriptions for effective interaction with external systems, but poor description quality significantly impacts performance and efficiency.  					AI-generated summary The Model Context Protocol (MCP) introduces a standard specification ...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 11. Abstract Object hallucinations in LVLMs are primarily caused by language decoder priors, leading to the development of a training-free framework that suppresses these priors to reduce hallucinations.  					AI-generated summary Object hallucination is a critical issue in Large Vision-Language Models ...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 12. Abstract A large-scale study of tri-modal discrete diffusion models demonstrates improved performance across text, image, and speech generation tasks through systematic analysis of scaling laws and optimized inference methods.  					AI-generated summary Discrete diffusion models have emerged as stro...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 13. Abstract Spectral-Evolution-Aware Cache (SeaCache) improves diffusion model inference speed by using spectrally aligned representations to optimize intermediate output reuse, achieving better latency-quality trade-offs than previous methods.  					AI-generated summary Diffusion models are a strong b...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 14. Abstract JAEGER extends audio-visual large language models to 3D space by integrating RGB-D observations and multi-channel audio to improve spatial reasoning and source localization.  					AI-generated summary Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D pe...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 15. Abstract UniVBench introduces a comprehensive benchmark for evaluating video foundation models across multiple capabilities including understanding, generation, editing, and reconstruction using high-quality, diverse video content and a unified evaluation system.  					AI-generated summary Video fou...
[26.02.2026 08:38] ********************************************************************************
[26.02.2026 08:38] Abstract 16. Abstract MoBind learns joint representations between IMU signals and 2D pose sequences through hierarchical contrastive learning to achieve cross-modal retrieval, temporal synchronization, and action recognition with fine-grained alignment.  					AI-generated summary We aim to learn a joint represen...
[26.02.2026 08:38] Read previous papers.
[26.02.2026 08:38] Generating reviews via LLM API.
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#audio", "#architecture", "#training", "#multimodal", "#open_source", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ", "desc": "DreamID-Omni –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#benchmark", "#training", "#agents", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ ARLArena –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –≤ –∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#audio", "#architecture", "#video", "#open_source", "#diffusion", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—å —Å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–∞", "desc": "SkyReels V4 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç, —Ä–µ
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#training", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—Ä–∏—è–º–∏", "desc": "HyTRec ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ª–∏–Ω–µ
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#dataset", "#agents", "#reasoning", "#data", "#training", "#rlhf", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π –≤ GUI-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è GUI-Libra ‚Äî –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#games", "#dataset", "#training", "#benchmark", "#agents", "#open_source", "#video"], "emoji": "üéÆ", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –≤–∏–¥–µ–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–∏—Ä–∞ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "Solaris ‚Äî —ç—Ç–æ –º–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ –¥–ª—è –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#training", "#robotics", "#multimodal", "#cv"], "emoji": "üåç", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è—é—â–∏–µ –º–∏—Ä—ã: –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ World Guidance –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –∫–æ
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#rlhf", "#audio", "#architecture", "#dataset", "#training", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º–æ–¥–∞–ª—å–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π", "desc": "JavisDiT++ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#small_models", "#rag"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–≥–∞–¥—ã–≤–∞—è –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π LLM: –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –≤ –µ–¥–∏–Ω—Å—Ç–≤–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ NanoKnow –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è 
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#inference", "#cv", "#architecture"], "emoji": "üîÆ", "ru": {"title": "–û–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ", "desc": "Sphere Encoder ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥, –æ—Ç–æ–±—Ä–∞–∂–∞—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Å—Ñ–µ
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#agents", "#open_source"], "emoji": "üîß", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–æ –æ–ø–∏—Å–∞–Ω–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ foundation models", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—è–∑—ã—á–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —ç–∫–æ—Å–∏—Å
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#open_source", "#hallucinations"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–∏–æ—Ä–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –≤—ã–∑–≤–∞–Ω—ã —Å–∏–ª—å–Ω—ã–º–∏ –∞–ø—Ä–∏–æ
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#open_source", "#inference", "#diffusion", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ç—Ä—ë—Ö–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –∫—ç—à", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SeaCache ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#audio", "#3d", "#dataset", "#benchmark", "#reasoning", "#open_source", "#multimodal"], "emoji": "üéµ", "ru": {"title": "–û—Ç 2D –∫ 3D: –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —á–µ—Ä–µ–∑ –∑–≤—É–∫ –∏ –≥–ª—É–±–∏–Ω—É", "desc": "JAEGER —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –∏–Ω—Ç–µ
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#benchmark", "#video", "#agents", "#survey", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "UniVBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –ø–æ–Ω–∏
[26.02.2026 08:38] Using data from previous issue: {"categories": ["#cv", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–Ω—Å–æ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "MoBind ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤
[26.02.2026 08:38] Renaming data file.
[26.02.2026 08:38] Renaming previous data. hf_papers.json to ./d/2026-02-26.json
[26.02.2026 08:38] Saving new data file.
[26.02.2026 08:38] Generating page.
[26.02.2026 08:38] Renaming previous page.
[26.02.2026 08:38] Renaming previous data. index.html to ./d/2026-02-26.html
[26.02.2026 08:38] Writing result.
[26.02.2026 08:38] Renaming log file.
[26.02.2026 08:38] Renaming previous data. log.txt to ./logs/2026-02-26_last_log.txt
