[26.02.2026 14:06] Read previous papers.
[26.02.2026 14:06] Generating top page (month).
[26.02.2026 14:06] Writing top page (month).
[26.02.2026 15:46] Read previous papers.
[26.02.2026 15:46] Get feed.
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18283
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17602
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12160
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21818
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21534
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22190
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19163
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15030
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22208
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22010
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21548
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21461
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20122
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21778
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21472
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18993
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14878
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22144
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20857
[26.02.2026 15:46] Extract page data from URL. URL: https://huggingface.co/papers/2602.20273
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19594
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18527
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21835
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19004
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18964
[26.02.2026 15:46] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18589
[26.02.2026 15:46] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.02.2026 15:46] No deleted papers detected.
[26.02.2026 15:46] Downloading and parsing papers (pdf, html). Total: 26.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.18283.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.18283.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.18283.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.17602.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.17602.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.17602.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.12160.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.12160.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.12160.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.21818.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.21818.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.21818.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.21534.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.21534.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.21534.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.22190.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.22190.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.22190.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.19163.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.19163.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.19163.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.15030.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.15030.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.15030.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.22208.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.22208.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.22208.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.22010.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.22010.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.22010.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.21548.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.21548.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.21548.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.21461.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.21461.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.21461.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.20122.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.20122.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.20122.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.21778.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.21778.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.21778.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.21472.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.21472.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.21472.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.18993.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.18993.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.18993.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.14878.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.14878.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.14878.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.22144.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.22144.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.22144.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.20857.
[26.02.2026 15:46] Extra JSON file exists (./assets/json/2602.20857.json), skip PDF parsing.
[26.02.2026 15:46] Paper image links file exists (./assets/img_data/2602.20857.json), skip HTML parsing.
[26.02.2026 15:46] Success.
[26.02.2026 15:46] Downloading and parsing paper https://huggingface.co/papers/2602.20273.
[26.02.2026 15:46] Downloading paper 2602.20273 from https://arxiv.org/pdf/2602.20273v1...
[26.02.2026 15:47] Extracting affiliations from text.
[26.02.2026 15:47] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zhuofan Josh Ying 1 Shauli Ravfogel 2 Nikolaus Kriegeskorte 1 3 Peter Hase "
[26.02.2026 15:47] Response: ```python
[]
```
[26.02.2026 15:47] Extracting affiliations from text.
[26.02.2026 15:47] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zhuofan Josh Ying 1 Shauli Ravfogel 2 Nikolaus Kriegeskorte 1 3 Peter Hase1. Introduction 6 2 0 2 3 2 ] . [ 1 3 7 2 0 2 . 2 0 6 2 : r Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this findings generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes nearperfectly predicts cross-domain generalization (R2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, posttraining reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting representational basis for chat models sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry.1 1Department of Psychology, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 2New York University, New York, NY 3Department of Neuroscience, Columbia University, New York, NY 4Stanford University, Stanford, CA 5Schmidt Sciences, New York, NY. Correspondence to: Zhuofan Josh Ying <zy2559@columbia.edu>. Preprint. February 25, 2026. 1Code for all experiments is provided in https://github. com/zfying/truth spec. Large language models (LLMs) often generate false or misleading information that, by all appearances, they know to be untrue (Pan et al., 2023; Scheurer et al., 2023; Abdulhai et al., 2025). While we cannot always verify the truthfulness of model generations, we can probe them to detect when models themselves represent their generations to be false (Goldowsky-Dill et al., 2025). Therefore, understanding how LLMs internally represent truthfulness has become critical challenge for their safe deployment. Recent work suggests that LLMs develop linear representation of truthfulness that can be extracted via probing (Marks & Tegmark, 2023). If such representations are sufficiently general, they should enable reliable detection of model falsehoods regardless of domain, and potentially allow interventions to improve honesty (Li et al., 2023; Zou et al., 2023; Turner et al., 2023; Cundy & Gleave, 2025; Ravfogel et al., 2025). Although some works show that these truth directions exhibit remarkable generalization across various domains and strategic deception scenarios (Burns et al., 2023; Azaria & Mitchell, 2023; Marks & Tegmark, 2023; Liu et al., 2024; Burger et al., 2024; Goldowsky-Dill et al., 2025), others show that probes fail to generalize in some cases and argue that LLMs encode multiple, distinct notions of truth (Levinstein & Herrmann, 2024; Sky et al., 2024; Azizian et al., 2025; Orgad et al., 2025). We argue that these seemingly contradictory findings can be reconciled. Prior work has treated cross-domain generalization failure and geometric dissimilarity between probes as evidence against domain-general truth encoding. However, this inference is flawed: generalization may fail not because domain-general directions do not exist, but because we fail to discover them. Conversely, high probe generalization performance and high probe direction similarity do not preclude the existence of highly domain-specific directions. We propose the truthfulness spectrum hypothesis: rather than exhibiting either single domain-general truth direction or entirely separate domain-specific directions, LLMs encode truthfulness along spectrum of generality, with directions at varying levels of generality coexisting in the representational space (Figure 1). At one end lies fully domain-general direction; at the other, fully domain-specific directions share no common structure; in between, direcThe Truthfulness Spectrum Hypothesis Figure 1. Truth representations in LLMs are graded in generality and reshaped by post-training. Left: Different truth types share partially overlapping but distinct sets of truth directions. These directions lie on spectrum from domain-general to domain-specific. The geometry of truth representations changes through post-training, pushing sycophancy into more distant subspace from other truth types. This reorganization causes probes trained on factual truth to fail on sycophancy detection, and vice versa (X). However, training on all domains still yields domain-general direction (X). Right: Concept erasure analysis further reveals the full spectrum of truth directions. tions generalize across some domains but not others. probe trained on one domain may capture superposition of these directions, combining domain-specific and more general features. The distribution of models truth representations along this spectrum has important implications for lie detection and alignment interventions. Our experiments are based on our FLEED dataset, large set of carefully controlled truthfulness datasets spanning five fundamental truth types: definitional, empirical, logical, fictional, and ethical. We additionally construct two novel deception datasets: sycophantic lying dataset where models alter their answers to align with user-stated beliefs, and an expectation-inverted dataset where the user expects models to make false claims, making true generations violate the user expectation and count as lies. We also evaluate on prior honesty benchmarks (Scheurer et al., 2023; Benton et al., 2024; Goldowsky-Dill et al., 2025). Our findings support the truthfulness spectrum hypothesis. Linear probes generalize well across our five fundamental truth types and most honesty benchmarks, yet fail almost entirely on sycophantic and expectation-inverted lying (AUROC 0.55). At t"
[26.02.2026 15:47] Mistral response. {"id": "372d3ddb25244966872df5642898dba2", "created": 1772120851, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1458, "total_tokens": 1535, "completion_tokens": 77, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Psychology, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY\",\n    \"New York University, New York, NY\",\n    \"Department of Neuroscience, Columbia University, New York, NY\",\n    \"Stanford University, Stanford, CA\",\n    \"Schmidt Sciences, New York, NY\"\n]\n```"}}]}
[26.02.2026 15:47] Response: ```python
[
    "Department of Psychology, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY",
    "New York University, New York, NY",
    "Department of Neuroscience, Columbia University, New York, NY",
    "Stanford University, Stanford, CA",
    "Schmidt Sciences, New York, NY"
]
```
[26.02.2026 15:47] Deleting PDF ./assets/pdf/2602.20273.pdf.
[26.02.2026 15:47] Success.
[26.02.2026 15:47] Downloading and parsing paper https://huggingface.co/papers/2602.19594.
[26.02.2026 15:47] Extra JSON file exists (./assets/json/2602.19594.json), skip PDF parsing.
[26.02.2026 15:47] Paper image links file exists (./assets/img_data/2602.19594.json), skip HTML parsing.
[26.02.2026 15:47] Success.
[26.02.2026 15:47] Downloading and parsing paper https://huggingface.co/papers/2602.18527.
[26.02.2026 15:47] Extra JSON file exists (./assets/json/2602.18527.json), skip PDF parsing.
[26.02.2026 15:47] Paper image links file exists (./assets/img_data/2602.18527.json), skip HTML parsing.
[26.02.2026 15:47] Success.
[26.02.2026 15:47] Downloading and parsing paper https://huggingface.co/papers/2602.21835.
[26.02.2026 15:47] Extra JSON file exists (./assets/json/2602.21835.json), skip PDF parsing.
[26.02.2026 15:47] Paper image links file exists (./assets/img_data/2602.21835.json), skip HTML parsing.
[26.02.2026 15:47] Success.
[26.02.2026 15:47] Downloading and parsing paper https://huggingface.co/papers/2602.19004.
[26.02.2026 15:47] Extra JSON file exists (./assets/json/2602.19004.json), skip PDF parsing.
[26.02.2026 15:47] Paper image links file exists (./assets/img_data/2602.19004.json), skip HTML parsing.
[26.02.2026 15:47] Success.
[26.02.2026 15:47] Downloading and parsing paper https://huggingface.co/papers/2602.18964.
[26.02.2026 15:47] Extra JSON file exists (./assets/json/2602.18964.json), skip PDF parsing.
[26.02.2026 15:47] Paper image links file exists (./assets/img_data/2602.18964.json), skip HTML parsing.
[26.02.2026 15:47] Success.
[26.02.2026 15:47] Downloading and parsing paper https://huggingface.co/papers/2602.18589.
[26.02.2026 15:47] Extra JSON file exists (./assets/json/2602.18589.json), skip PDF parsing.
[26.02.2026 15:47] Paper image links file exists (./assets/img_data/2602.18589.json), skip HTML parsing.
[26.02.2026 15:47] Success.
[26.02.2026 15:47] Enriching papers with extra data.
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 0. Abstract HyTRec addresses the challenge of modeling long user behavior sequences by combining linear and softmax attention mechanisms with a temporal-aware delta network to balance efficiency and retrieval precision.  					AI-generated summary Modeling long sequences of user behaviors has emerged as...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 1. Abstract MolHIT presents a hierarchical discrete diffusion model for molecular graph generation that achieves superior chemical validity and property-guided synthesis compared to existing 1D and graph-based approaches.  					AI-generated summary Molecular generation with diffusion models has emerged...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 2. Abstract DreamID-Omni is a unified framework for controllable human-centric audio-video generation that uses a symmetric conditional diffusion transformer with dual-level disentanglement and multi-task progressive training to achieve state-of-the-art performance.  					AI-generated summary Recent ad...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 3. Abstract SkyReels V4 is a unified multimodal video foundation model that generates, edits, and inpaints video and audio simultaneously using a dual-stream architecture with shared text encoding and efficient high-resolution processing.  					AI-generated summary SkyReels V4 is a unified multi modal ...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 4. Abstract ARLArena framework analyzes training stability in agentic reinforcement learning and proposes SAMPO method for stable policy optimization across diverse tasks.  					AI-generated summary Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training a...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 5. Abstract GUI-Libra addresses limitations in open-source GUI agents through specialized training methods that improve reasoning-grounding alignment and reinforcement learning under partial verifiability, demonstrating enhanced task completion across web and mobile platforms.  					AI-generated summar...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 6. Abstract JavisDiT++ presents a unified framework for joint audio-video generation using modality-specific mixture-of-experts, temporal-aligned RoPE, and audio-video direct preference optimization to achieve high-quality, synchronized multimedia synthesis.  					AI-generated summary AIGC has rapidly ...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 7. Abstract The Sphere Encoder is an efficient generative model that produces images in a single forward pass by mapping images to a spherical latent space and decoding from random points on that sphere, achieving diffusion-like quality with significantly reduced inference costs.  					AI-generated sum...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 8. Abstract Solaris is a multiplayer video world model that simulates consistent multi-view observations through a novel data collection system and staged training approach.  					AI-generated summary Existing action-conditioned video generation models (video world models) are limited to single-agent p...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 9. Abstract World Guidance framework enhances Vision-Language-Action models by mapping future observations into compact conditions for improved action generation and generalization.  					AI-generated summary Leveraging future observation modeling to facilitate action generation presents a promising av...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 10. Abstract DualPath addresses KV-cache storage I/O bottlenecks in multi-turn LLM inference by introducing dual-path loading and dynamic load balancing across prefill and decode engines.  					AI-generated summary The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cach...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 11. Abstract VecGlypher is a multimodal language model that generates high-fidelity vector glyphs directly from text or image inputs, bypassing traditional raster-to-vector processes and enabling direct SVG path generation.  					AI-generated summary Vector glyphs are the atomic units of digital typogra...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 12. Abstract NanoKnow benchmark enables analysis of knowledge sources in LLMs by partitioning questions based on pre-training data presence, revealing how parametric and external knowledge interact in model responses.  					AI-generated summary How do large language models (LLMs) know what they know? An...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 13. Abstract PhysicEdit enhances image editing by incorporating physical state transitions through a dual-thinking mechanism combining frozen vision-language models with learnable transition queries in a diffusion framework.  					AI-generated summary Instruction-based image editing has achieved remarka...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 14. Abstract A large-scale study of tri-modal discrete diffusion models demonstrates improved performance across text, image, and speech generation tasks through systematic analysis of scaling laws and optimized inference methods.  					AI-generated summary Discrete diffusion models have emerged as stro...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 15. Abstract Spectral-Evolution-Aware Cache (SeaCache) improves diffusion model inference speed by using spectrally aligned representations to optimize intermediate output reuse, achieving better latency-quality trade-offs than previous methods.  					AI-generated summary Diffusion models are a strong b...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 16. Abstract Foundation model agents rely on natural language tool descriptions for effective interaction with external systems, but poor description quality significantly impacts performance and efficiency.  					AI-generated summary The Model Context Protocol (MCP) introduces a standard specification ...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 17. Abstract Object hallucinations in LVLMs are primarily caused by language decoder priors, leading to the development of a training-free framework that suppresses these priors to reduce hallucinations.  					AI-generated summary Object hallucination is a critical issue in Large Vision-Language Models ...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 18. Abstract Functional Continuous Decomposition enables parametric, continuous optimization of time-series data with guaranteed continuity for capturing local and global patterns, enhancing machine learning model performance through improved feature extraction.  					AI-generated summary The analysis o...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 19. Abstract Large language models contain truth directions ranging from domain-general to domain-specific in their representational space, with linear probes showing varying generalization capabilities and causal interventions revealing differential effectiveness of these directions.  					AI-generated...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 20. Abstract ISO-Bench evaluates coding agents on real-world LLM inference optimization tasks from popular serving frameworks, using combined execution and LLM-based metrics to assess performance.  					AI-generated summary We introduce ISO-Bench, a benchmark for coding agents to test their capabilities...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 21. Abstract JAEGER extends audio-visual large language models to 3D space by integrating RGB-D observations and multi-channel audio to improve spatial reasoning and source localization.  					AI-generated summary Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D pe...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 22. Abstract UniVBench introduces a comprehensive benchmark for evaluating video foundation models across multiple capabilities including understanding, generation, editing, and reconstruction using high-quality, diverse video content and a unified evaluation system.  					AI-generated summary Video fou...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 23. Abstract MoBind learns joint representations between IMU signals and 2D pose sequences through hierarchical contrastive learning to achieve cross-modal retrieval, temporal synchronization, and action recognition with fine-grained alignment.  					AI-generated summary We aim to learn a joint represen...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 24. Abstract A new gold-standard dataset for sarcasm detection in Yor√πb√° is introduced with high inter-annotator agreement and soft labels for uncertainty-aware modeling.  					AI-generated summary Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve di...
[26.02.2026 15:47] ********************************************************************************
[26.02.2026 15:47] Abstract 25. Abstract Diffusion models are evaluated for computed tomography reconstruction through a comprehensive benchmark addressing practical challenges like correlated noise and artifact structures.  					AI-generated summary Diffusion models have recently emerged as powerful priors for solving inverse pro...
[26.02.2026 15:47] Read previous papers.
[26.02.2026 15:47] Generating reviews via LLM API.
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#training", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—Ä–∏—è–º–∏", "desc": "HyTRec ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ª–∏–Ω–µ
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#diffusion", "#science"], "emoji": "üß™", "ru": {"title": "–ò–¥–µ–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤", "desc": "MolHIT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#audio", "#architecture", "#training", "#multimodal", "#open_source", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ", "desc": "DreamID-Omni –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#audio", "#architecture", "#video", "#open_source", "#diffusion", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—å —Å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–∞", "desc": "SkyReels V4 ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç, —Ä–µ
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#benchmark", "#training", "#agents", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ ARLArena –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –≤ –∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#dataset", "#agents", "#reasoning", "#data", "#training", "#rlhf", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π –≤ GUI-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è GUI-Libra ‚Äî –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#rlhf", "#audio", "#architecture", "#dataset", "#training", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º–æ–¥–∞–ª—å–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π", "desc": "JavisDiT++ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#inference", "#cv", "#architecture"], "emoji": "üîÆ", "ru": {"title": "–û–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ", "desc": "Sphere Encoder ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥, –æ—Ç–æ–±—Ä–∞–∂–∞—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Å—Ñ–µ
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#games", "#dataset", "#training", "#benchmark", "#agents", "#open_source", "#video"], "emoji": "üéÆ", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –≤–∏–¥–µ–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–∏—Ä–∞ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "Solaris ‚Äî —ç—Ç–æ –º–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ –¥–ª—è –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#training", "#robotics", "#multimodal", "#cv"], "emoji": "üåç", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è—é—â–∏–µ –º–∏—Ä—ã: –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ World Guidance –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –∫–æ
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–≤—É—Ö–ø—É—Ç–µ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ KV-cache –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–Ω–æ–≥–æ–æ–±–æ—Ä–æ—Ç–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞", "desc": "DualPath —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —É–∑–∫–∏—Ö –º–µ—Å—Ç –ø—Ä–∏ –≤–≤–æ–¥–µ-–≤—ã–≤–æ–¥–µ KV-cache –≤–æ –≤—Ä–µ–º—è –º–Ω–æ–≥–æ–æ–±–æ—Ä–æ—Ç–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –≤–≤–µ–¥–µ–Ω–∏—è –¥–≤—É
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#training", "#multimodal"], "emoji": "‚úèÔ∏è", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤ –ø—Ä—è–º–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "VecGlypher ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –≥–ª–∏—Ñ—ã –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä—è–º–æ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö 
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#small_models", "#rag"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–≥–∞–¥—ã–≤–∞—è –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π LLM: –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –≤ –µ–¥–∏–Ω—Å—Ç–≤–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ NanoKnow –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è 
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#dataset", "#training", "#cv", "#diffusion", "#multimodal", "#open_source", "#synthetic", "#science"], "emoji": "üåä", "ru": {"title": "–§–∏–∑–∏–∫–∞ –≤ –ø–∏–∫—Å–µ–ª—è—Ö: —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π", "desc": "PhysicEdit ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –º–æ–¥–µ–ª–∏—Ä
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#open_source", "#inference", "#diffusion", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ç—Ä—ë—Ö–º–æ–¥–∞–ª—å–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –∫—ç—à", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SeaCache ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#agents", "#open_source"], "emoji": "üîß", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–æ –æ–ø–∏—Å–∞–Ω–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ foundation models", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—è–∑—ã—á–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —ç–∫–æ—Å–∏—Å
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#open_source", "#hallucinations"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–∏–æ—Ä–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –≤—ã–∑–≤–∞–Ω—ã —Å–∏–ª—å–Ω—ã–º–∏ –∞–ø—Ä–∏–æ
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üìà", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è (FCD) ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—â–∏–π –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å –ø—Ä–∏
[26.02.2026 15:47] Querying the API.
[26.02.2026 15:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Large language models contain truth directions ranging from domain-general to domain-specific in their representational space, with linear probes showing varying generalization capabilities and causal interventions revealing differential effectiveness of these directions.  					AI-generated summary Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.
[26.02.2026 15:47] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–¥–∏—Ä—É—é—Ç –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å –Ω–µ –æ–¥–Ω–∏–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º, –∞ —Å–ø–µ–∫—Ç—Ä–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π ‚Äî –æ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –¥–æ —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–ª–∏ –æ–±–æ–±—â–µ–Ω–∏–µ –ª–∏–Ω–µ–π–Ω—ã—Ö –∑–æ–Ω–¥–æ–≤ –Ω–∞ –ø—è—Ç—å —Ç–∏–ø–æ–≤ –∏—Å—Ç–∏–Ω—ã –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –æ–Ω–∏ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –¥–æ–º–µ–Ω–æ–≤, –Ω–æ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –ª–∂–∏–≤—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏, —É–≥–æ–∂–¥–∞—é—â–∏–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é. –ì–µ–æ–º–µ—Ç—Ä–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∑–æ–Ω–¥–æ–≤, –∏–∑–º–µ—Ä–µ–Ω–Ω–∞—è —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ú–∞—Ö–∞–ª–∞–Ω–æ–±–∏—Å–∞, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–∞–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏. –ü—Ä–∏—á–∏–Ω–Ω—ã–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ, —á–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ, –∞ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é –∏—Å—Ç–∏–Ω—ã, —É—Å–∏–ª–∏–≤–∞—è —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —É–≥–æ–∂–¥–µ–Ω–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.",
  "emoji": "üß≠",
  "title": "–°–ø–µ–∫—Ç—Ä –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç–∏: –æ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º –∏—Å—Ç–∏–Ω—ã –≤ LLM"
}
```
[26.02.2026 15:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Large language models contain truth directions ranging from domain-general to domain-specific in their representational space, with linear probes showing varying generalization capabilities and causal interventions revealing differential effectiveness of these directions.  					AI-generated summary Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec."

[26.02.2026 15:47] Response: ```python
["TRAINING", "ARCHITECTURE"]
```
[26.02.2026 15:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Large language models contain truth directions ranging from domain-general to domain-specific in their representational space, with linear probes showing varying generalization capabilities and causal interventions revealing differential effectiveness of these directions.  					AI-generated summary Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec."

[26.02.2026 15:47] Response: ```python
["INTERPRETABILITY"]
```
[26.02.2026 15:47] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper explores how large language models (LLMs) represent truthfulness in their internal structure, proposing a spectrum of truth directions that range from general to specific. The authors use linear probes to evaluate how well these models generalize across different types of truth, such as definitional and ethical truths. They find that while linear probes perform well in most areas, they struggle with certain types of lying, indicating that domain-specific directions are more effective. The study concludes that the geometry of these truth directions can be reshaped through training, which influences how models handle different truth types, particularly in relation to sycophantic behavior.","title":"Exploring the Spectrum of Truth in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how large language models (LLMs) represent truthfulness in their internal structure, proposing a spectrum of truth directions that range from general to specific. The authors use linear probes to evaluate how well these models generalize across different types of truth, such as definitional and ethical truths. They find that while linear probes perform well in most areas, they struggle with certain types of lying, indicating that domain-specific directions are more effective. The study concludes that the geometry of these truth directions can be reshaped through training, which influences how models handle different truth types, particularly in relation to sycophantic behavior.', title='Exploring the Spectrum of Truth in Language Models'))
[26.02.2026 15:47] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÖ∂Ë°®Á§∫Á©∫Èó¥‰∏≠ÂåÖÂê´‰ªéÈ¢ÜÂüüÈÄöÁî®Âà∞È¢ÜÂüüÁâπÂÆöÁöÑÁúüÁêÜÊñπÂêë„ÄÇÈÄöËøáÁ∫øÊÄßÊé¢ÊµãÂô®ÁöÑËØÑ‰º∞ÔºåÂèëÁé∞Ëøô‰∫õÊñπÂêëÂú®‰∏çÂêåÁúüÁêÜÁ±ªÂûã‰∏äÁöÑÊ≥õÂåñËÉΩÂäõÂêÑÂºÇ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ¢ÜÂüüÁâπÂÆöÁöÑÊñπÂêëÂú®ÂºïÂØºÁúüÁêÜÊó∂ÊØîÈ¢ÜÂüüÈÄöÁî®ÁöÑÊñπÂêëÊõ¥ÊúâÊïà„ÄÇËÆ≠ÁªÉÂêéÔºåÊ®°ÂûãÁöÑÁúüÁêÜÂá†‰ΩïÂΩ¢Áä∂ÂèëÁîüÂèòÂåñÔºåËøõ‰∏ÄÊ≠•Êé®Âä®‰∫ÜÂØπË∞ÑÂ™öÊÄßË∞éË®ÄÁöÑÁêÜËß£„ÄÇ","title":"ÁúüÁêÜÊñπÂêëÁöÑÂ§öÊ†∑ÊÄß‰∏éÈ¢ÜÂüüÁâπÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÖ∂Ë°®Á§∫Á©∫Èó¥‰∏≠ÂåÖÂê´‰ªéÈ¢ÜÂüüÈÄöÁî®Âà∞È¢ÜÂüüÁâπÂÆöÁöÑÁúüÁêÜÊñπÂêë„ÄÇÈÄöËøáÁ∫øÊÄßÊé¢ÊµãÂô®ÁöÑËØÑ‰º∞ÔºåÂèëÁé∞Ëøô‰∫õÊñπÂêëÂú®‰∏çÂêåÁúüÁêÜÁ±ªÂûã‰∏äÁöÑÊ≥õÂåñËÉΩÂäõÂêÑÂºÇ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ¢ÜÂüüÁâπÂÆöÁöÑÊñπÂêëÂú®ÂºïÂØºÁúüÁêÜÊó∂ÊØîÈ¢ÜÂüüÈÄöÁî®ÁöÑÊñπÂêëÊõ¥ÊúâÊïà„ÄÇËÆ≠ÁªÉÂêéÔºåÊ®°ÂûãÁöÑÁúüÁêÜÂá†‰ΩïÂΩ¢Áä∂ÂèëÁîüÂèòÂåñÔºåËøõ‰∏ÄÊ≠•Êé®Âä®‰∫ÜÂØπË∞ÑÂ™öÊÄßË∞éË®ÄÁöÑÁêÜËß£„ÄÇ', title='ÁúüÁêÜÊñπÂêëÁöÑÂ§öÊ†∑ÊÄß‰∏éÈ¢ÜÂüüÁâπÊÄß'))
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#optimization", "#inference", "#agents", "#open_source", "#benchmark", "#plp"], "emoji": "‚ö°", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ISO-Bench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫–æ–¥–∏—Ä—É—é—â–∏
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#audio", "#3d", "#dataset", "#benchmark", "#reasoning", "#open_source", "#multimodal"], "emoji": "üéµ", "ru": {"title": "–û—Ç 2D –∫ 3D: –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —á–µ—Ä–µ–∑ –∑–≤—É–∫ –∏ –≥–ª—É–±–∏–Ω—É", "desc": "JAEGER —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –∏–Ω—Ç–µ
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#benchmark", "#video", "#agents", "#survey", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≤–∏–¥–µ–æ—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "UniVBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –ø–æ–Ω–∏
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#cv", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–Ω—Å–æ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "MoBind ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#open_source", "#dataset"], "emoji": "üé≠", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–º—ã—Å–ª–∞: –¥–∞—Ç–∞—Å–µ—Ç —Å–∞—Ä–∫–∞–∑–º–∞ –¥–ª—è –π–æ—Ä—É–±–∞ —Å –º—è–≥–∫–∏–º–∏ –º–µ—Ç–∫–∞–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∞—Ä–∫–∞–∑–º–∞ –Ω–∞ —è–∑—ã–∫–µ –π–æ—Ä—É–±–∞ (Yor-Sarc), —Å–æ–¥–µ—Ä–∂–∞
[26.02.2026 15:47] Using data from previous issue: {"categories": ["#healthcare", "#diffusion", "#open_source", "#benchmark", "#science", "#dataset"], "emoji": "üè•", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç–æ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω comprehensive benchmark DM4CT –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–∏—Ñ—Ñ—É–∑–∏
[26.02.2026 15:47] Renaming data file.
[26.02.2026 15:47] Renaming previous data. hf_papers.json to ./d/2026-02-26.json
[26.02.2026 15:47] Saving new data file.
[26.02.2026 15:47] Generating page.
[26.02.2026 15:47] Renaming previous page.
[26.02.2026 15:47] Renaming previous data. index.html to ./d/2026-02-26.html
[26.02.2026 15:47] Writing result.
[26.02.2026 15:47] Renaming log file.
[26.02.2026 15:47] Renaming previous data. log.txt to ./logs/2026-02-26_last_log.txt
