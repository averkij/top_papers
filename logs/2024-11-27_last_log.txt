[27.11.2024 07:11] Read previous papers.
[27.11.2024 07:11] Generating top page (month).
[27.11.2024 07:11] Writing top page (month).
[27.11.2024 07:13] Read previous papers.
[27.11.2024 07:13] Get feed.
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17465
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17116
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17686
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.15296
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.14740
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17673
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17467
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16819
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17223
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16173
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17691
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16754
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16856
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17383
[27.11.2024 07:13] Get page data from previous paper. URL: https://huggingface.co/papers/2411.15411
[27.11.2024 07:13] Downloading and parsing papers (pdf, html). Total: 15.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17465.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17465.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17465.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17116.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17116.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17116.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17686.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17686.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17686.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.15296.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.15296.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.15296.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.14740.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.14740.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.14740.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17673.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17673.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17673.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17467.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17467.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17467.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.16819.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.16819.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.16819.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17223.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17223.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17223.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.16173.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.16173.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.16173.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17691.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17691.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17691.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.16754.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.16754.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.16754.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.16856.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.16856.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.16856.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.17383.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.17383.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.17383.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Downloading and parsing paper https://huggingface.co/papers/2411.15411.
[27.11.2024 07:13] Extra JSON file exists (./assets/json/2411.15411.json), skip PDF parsing.
[27.11.2024 07:13] Paper image links file exists (./assets/img_data/2411.15411.json), skip HTML parsing.
[27.11.2024 07:13] Success.
[27.11.2024 07:13] Enriching papers with extra data.
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 0. Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visual...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 1. Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention ac...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 2. To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research. We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects rema...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 3. As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that ...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 4. While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for test-time opt...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 5. Sketching serves as a versatile tool for externalizing ideas, enabling rapid exploration and visual communication that spans various disciplines. While artificial systems have driven substantial advances in content creation and human-computer interaction, capturing the dynamic and abstract nature of...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 6. Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to s...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 7. Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image....
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 8. Subject-driven image inpainting has emerged as a popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces DreamMix, a dif...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 9. Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With th...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 10. We reveal that low-bit quantization favors undertrained large language models (LLMs) by observing that models with larger sizes or fewer training tokens experience less quantization-induced degradation (QiD) when applying low-bit quantization, whereas smaller models with extensive training tokens su...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 11. The proliferation of AI techniques for image generation, coupled with their increasing accessibility, has raised significant concerns about the potential misuse of these images to spread misinformation. Recent AI-generated image detection (AGID) methods include CNNDetection, NPR, DM Image Detection,...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 12. Autoregressive models have demonstrated remarkable success across various fields, from large language models (LLMs) to large multimodal models (LMMs) and 2D content generation, moving closer to artificial general intelligence (AGI). Despite these advances, applying autoregressive approaches to 3D ob...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 13. The automatic generation of anchor-style product promotion videos presents promising opportunities in online commerce, advertising, and consumer engagement. However, this remains a challenging task despite significant advancements in pose-guided human video generation. In addressing this challenge, ...
[27.11.2024 07:13] ********************************************************************************
[27.11.2024 07:13] Abstract 14. The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabiliti...
[27.11.2024 07:13] Read previous papers.
[27.11.2024 07:13] Generating reviews via LLM API.
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#training", "#data", "#games", "#optimization", "#cv", "#agents", "#graphs", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "ShowUI: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ShowUI - –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#long_context", "#inference", "#optimization", "#architecture"], "emoji": "‚≠ê", "ru": {"title": "–ó–≤–µ–∑–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: —É—Å–∫–æ—Ä–µ–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Star Attention –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (LL
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ MLLM: –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≤—ã–≤–æ–¥–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#benchmark", "#agi", "#survey", "#multimodal"], "emoji": "üß†", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –∫–ª—é—á
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#3d", "#games", "#architecture", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Ç–µ–∫—Å—Ç—É—Ä: –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä—è–º–æ –≤ UV-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä–Ω—ã—Ö –∫–∞—Ä—Ç –¥–ª—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –∫—Ä—É–ø–Ω—É
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#multimodal", "#agents"], "emoji": "‚úèÔ∏è", "ru": {"title": "SketchAgent: –¥–∏–∞–ª–æ–≥–æ–≤–æ–µ —Ä–∏—Å–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SketchAgent - –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Å–∫–∏–∑–æ–≤, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —è–∑—ã–∫–æ–º. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —ç—Å–∫–∏–∑—ã —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#3d", "#dataset", "#transfer_learning", "#synthetic"], "emoji": "üßä", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –≥–µ–æ–º–µ—Ç—Ä–∏—è –≤–∞–∂–Ω–µ–µ —Å–º—ã—Å–ª–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑ –Ω–µ–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ —Ç–æ
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#diffusion", "#video", "#cv", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Å—Ç–∞—Ä—É—é –∑–∞–¥–∞—á—É", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ–æ
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#open_source", "#cv"], "emoji": "üé®", "ru": {"title": "DreamMix: –í—Å—Ç–∞–≤–∫–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–∞", "desc": "DreamMix - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ —Ç–æ–ª—å–∫
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#architecture", "#video", "#long_context", "#multimodal", "#dataset"], "emoji": "üé•", "ru": {"title": "SALOVA: —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "SALOVA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#open_source", "#inference"], "emoji": "üß†", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Ç–∞–π–Ω—ã –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Å –Ω–∏–∑–∫–∏–º –±–∏—Ç—Ä–µ–π—Ç–æ–º –º–µ–Ω–µ–µ –≤—Ä–µ–¥–Ω–æ –¥–ª—è –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —á–µ–º 
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#open_source", "#cv", "#benchmark", "#security", "#ethics", "#dataset"], "emoji": "üïµÔ∏è", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—è–≤–ª–µ–Ω–∏—é –ò–ò-–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º (–ò–ò), –≤ –∫–æ–Ω
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#architecture", "#games", "#agi"], "emoji": "üßä", "ru": {"title": "SAR3D: –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ 3D –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SAR3D –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. S
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#training", "#video", "#cv", "#diffusion"], "emoji": "üé¨", "ru": {"title": "AnchorCrafter: –ò–ò —Å–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–º–æ-–≤–∏–¥–µ–æ —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ç–æ–≤–∞—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AnchorCrafter - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —á–µ–ª
[27.11.2024 07:13] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#cv", "#dataset", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å FINECAPTION, —Å–ø–æ—Å–æ–±–Ω
[27.11.2024 07:13] Loading Chinese text from previous data.
[27.11.2024 07:13] Renaming data file.
[27.11.2024 07:13] Renaming previous data. hf_papers.json to ./d/2024-11-27.json
[27.11.2024 07:13] Saving new data file.
[27.11.2024 07:13] Generating page.
[27.11.2024 07:13] Renaming previous page.
[27.11.2024 07:13] Renaming previous data. index.html to ./d/2024-11-27.html
[27.11.2024 07:13] [Experimental] Generating Chinese page for reading.
[27.11.2024 07:13] Chinese vocab [{'word': 'Â±ïÁ§∫', 'pinyin': 'zh«énsh√¨', 'trans': 'display'}, {'word': 'Material', 'pinyin': 'M√©it√®ri«él', 'trans': 'Material'}, {'word': 'Anything', 'pinyin': 'ƒín√¨th√¨ng', 'trans': 'Anything'}, {'word': 'Ëá™Âä®Âåñ', 'pinyin': 'z√¨d√≤nghu√†', 'trans': 'automated'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íngyƒ´', 'trans': 'unified'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤s√†n', 'trans': 'diffusion'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«êz√†i', 'trans': 'aim to'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': 'Áâ©ÁêÜ', 'pinyin': 'w√πl«ê', 'trans': 'physics'}, {'word': 'ÊùêË¥®', 'pinyin': 'c√°izh√¨', 'trans': 'material'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´l√†i', 'trans': 'rely on'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√πz√°', 'trans': 'complex'}, {'word': 'ÊµÅÊ∞¥Á∫ø', 'pinyin': 'li√∫shu«êxi√†n', 'trans': 'pipeline'}, {'word': 'ÁâπÂÆö', 'pinyin': 't√®d√¨ng', 'trans': 'specific'}, {'word': 'Ê°à‰æã', 'pinyin': '√†nl√¨', 'trans': 'case'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çuhu√†', 'trans': 'optimization'}, {'word': 'ÂÅ•Â£Æ', 'pinyin': 'ji√†nzhu√†ng', 'trans': 'robust'}, {'word': 'Á´ØÂà∞Á´Ø', 'pinyin': 'duƒÅnd√†oduƒÅn', 'trans': 'end-to-end'}, {'word': 'Ëß£ÂÜ≥ÊñπÊ°à', 'pinyin': 'jiƒõju√© fƒÅng√†n', 'trans': 'solution'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨y√≤ng', 'trans': 'utilize'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√πx√πnli√†n', 'trans': 'pre-trained'}, {'word': 'ÂõæÂÉè', 'pinyin': 't√∫xi√†ng', 'trans': 'image'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√©h√©', 'trans': 'combine'}, {'word': '‰∏âÂ§¥Êû∂ÊûÑ', 'pinyin': 'sƒÅnt√≥u ji√†g√≤u', 'trans': 'three-headed architecture'}, {'word': 'Ê∏≤Êüì', 'pinyin': 'xu√†nr√°n', 'trans': 'rendering'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«înshƒ´', 'trans': 'loss'}, {'word': 'Á®≥ÂÆöÊÄß', 'pinyin': 'wƒõnd√¨ngx√¨ng', 'trans': 'stability'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨li√†ng', 'trans': 'quality'}, {'word': 'ÁΩÆ‰ø°', 'pinyin': 'zh√¨x√¨n', 'trans': 'confidence'}, {'word': 'Êé©Á†Å', 'pinyin': 'y«énm«é', 'trans': 'mask'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ngt√†i', 'trans': 'dynamic'}, {'word': 'ÂºÄÂÖ≥', 'pinyin': 'kƒÅiguƒÅn', 'trans': 'switch'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íuxi√†o', 'trans': 'effective'}, {'word': 'Á∫πÁêÜ', 'pinyin': 'w√©nl«ê', 'trans': 'texture'}, {'word': 'Ê∏êËøõ', 'pinyin': 'ji√†nj√¨n', 'trans': 'progressive'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√®l√º√®', 'trans': 'strategy'}, {'word': 'UVÁ©∫Èó¥', 'pinyin': 'UV k≈çngjiƒÅn', 'trans': 'UV space'}, {'word': 'Á≤æÁÇºÂô®', 'pinyin': 'jƒ´ngli√†nq√¨', 'trans': 'refiner'}, {'word': 'Á°Æ‰øù', 'pinyin': 'qu√®b«éo', 'trans': 'ensure'}, {'word': '‰∏ÄËá¥', 'pinyin': 'yƒ´zh√¨', 'trans': 'consistent'}, {'word': 'ÂáÜÂ§áÂ•Ω', 'pinyin': 'zh«înb√®i h«éo', 'trans': 'ready'}, {'word': 'ÂπøÊ≥õ', 'pinyin': 'gu«éngf√†n', 'trans': 'extensive'}, {'word': 'Á±ªÂà´', 'pinyin': 'l√®ibi√©', 'trans': 'category'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'}]
[27.11.2024 07:13] Renaming previous Chinese page.
[27.11.2024 07:13] Renaming previous data. zh.html to ./d/2024-11-26_zh_reading_task.html
[27.11.2024 07:13] Writing Chinese reading task.
[27.11.2024 07:13] Writing result.
[27.11.2024 07:13] Renaming log file.
[27.11.2024 07:13] Renaming previous data. log.txt to ./logs/2024-11-27_last_log.txt
