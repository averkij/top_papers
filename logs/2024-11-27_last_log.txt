[27.11.2024 20:11] Read previous papers.
[27.11.2024 20:11] Generating top page (month).
[27.11.2024 20:11] Writing top page (month).
[27.11.2024 21:09] Read previous papers.
[27.11.2024 21:09] Get feed.
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17465
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17116
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16819
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17686
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.15296
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.14740
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17673
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17451
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17467
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.15411
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16856
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17223
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17691
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16173
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16801
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.14721
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.16754
[27.11.2024 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17383
[27.11.2024 21:10] Downloading and parsing papers (pdf, html). Total: 18.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17465.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17465.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17465.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17116.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17116.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17116.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.16819.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.16819.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.16819.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17686.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17686.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17686.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.15296.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.15296.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.15296.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.14740.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.14740.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.14740.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17673.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17673.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17673.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17451.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17451.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17451.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17467.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17467.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17467.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.15411.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.15411.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.15411.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.16856.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.16856.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.16856.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17223.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17223.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17223.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17691.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17691.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17691.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.16173.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.16173.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.16173.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.16801.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.16801.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.16801.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.14721.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.14721.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.14721.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.16754.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.16754.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.16754.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Downloading and parsing paper https://huggingface.co/papers/2411.17383.
[27.11.2024 21:10] Extra JSON file exists (./assets/json/2411.17383.json), skip PDF parsing.
[27.11.2024 21:10] Paper image links file exists (./assets/img_data/2411.17383.json), skip HTML parsing.
[27.11.2024 21:10] Success.
[27.11.2024 21:10] Enriching papers with extra data.
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 0. Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visual...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 1. Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention ac...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 2. Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image....
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 3. To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research. We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects rema...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 4. As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that ...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 5. While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for test-time opt...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 6. Sketching serves as a versatile tool for externalizing ideas, enabling rapid exploration and visual communication that spans various disciplines. While artificial systems have driven substantial advances in content creation and human-computer interaction, capturing the dynamic and abstract nature of...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 7. Vision-language generative reward models (VL-GenRMs) play a crucial role in aligning and evaluating multimodal AI systems, yet their own evaluation remains under-explored. Current assessment methods primarily rely on AI-annotated preference labels from traditional VL tasks, which can introduce biase...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 8. Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to s...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 9. The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabiliti...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 10. Autoregressive models have demonstrated remarkable success across various fields, from large language models (LLMs) to large multimodal models (LMMs) and 2D content generation, moving closer to artificial general intelligence (AGI). Despite these advances, applying autoregressive approaches to 3D ob...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 11. Subject-driven image inpainting has emerged as a popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces DreamMix, a dif...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 12. We reveal that low-bit quantization favors undertrained large language models (LLMs) by observing that models with larger sizes or fewer training tokens experience less quantization-induced degradation (QiD) when applying low-bit quantization, whereas smaller models with extensive training tokens su...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 13. Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With th...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 14. We present BootComp, a novel framework based on text-to-image diffusion models for controllable human image generation with multiple reference garments. Here, the main bottleneck is data acquisition for training: collecting a large-scale dataset of high-quality reference garment images per human sub...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 15. Molecule discovery is a pivotal research field, impacting everything from the medicines we take to the materials we use. Recently, Large Language Models (LLMs) have been widely adopted in molecule understanding and generation, yet the alignments between molecules and their corresponding captions rem...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 16. The proliferation of AI techniques for image generation, coupled with their increasing accessibility, has raised significant concerns about the potential misuse of these images to spread misinformation. Recent AI-generated image detection (AGID) methods include CNNDetection, NPR, DM Image Detection,...
[27.11.2024 21:10] ********************************************************************************
[27.11.2024 21:10] Abstract 17. The automatic generation of anchor-style product promotion videos presents promising opportunities in online commerce, advertising, and consumer engagement. However, this remains a challenging task despite significant advancements in pose-guided human video generation. In addressing this challenge, ...
[27.11.2024 21:10] Read previous papers.
[27.11.2024 21:10] Generating reviews via LLM API.
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#training", "#data", "#games", "#optimization", "#cv", "#agents", "#graphs", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "ShowUI: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ShowUI - –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏–Ω
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#long_context", "#inference", "#optimization", "#architecture"], "emoji": "‚≠ê", "ru": {"title": "–ó–≤–µ–∑–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: —É—Å–∫–æ—Ä–µ–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Star Attention –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ (LL
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#diffusion", "#video", "#cv", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Å—Ç–∞—Ä—É—é –∑–∞–¥–∞—á—É", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ–æ
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ MLLM: –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≤—ã–≤–æ–¥–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#benchmark", "#agi", "#survey", "#multimodal"], "emoji": "üß†", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –∫–ª—é—á
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#3d", "#games", "#architecture", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-—Ç–µ–∫—Å—Ç—É—Ä: –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä—è–º–æ –≤ UV-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä–Ω—ã—Ö –∫–∞—Ä—Ç –¥–ª—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –∫—Ä—É–ø–Ω—É
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#multimodal", "#agents"], "emoji": "‚úèÔ∏è", "ru": {"title": "SketchAgent: –¥–∏–∞–ª–æ–≥–æ–≤–æ–µ —Ä–∏—Å–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SketchAgent - –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Å–∫–∏–∑–æ–≤, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —è–∑—ã–∫–æ–º. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —ç—Å–∫–∏–∑—ã —á–µ—Ä–µ–∑ –¥–∏–∞–ª–æ–≥
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#optimization", "#hallucinations", "#reasoning", "#benchmark", "#multimodal", "#alignment"], "emoji": "üß†", "ru": {"title": "VL-RewardBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VL-RewardBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#3d", "#dataset", "#transfer_learning", "#synthetic"], "emoji": "üßä", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –≥–µ–æ–º–µ—Ç—Ä–∏—è –≤–∞–∂–Ω–µ–µ —Å–º—ã—Å–ª–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑ –Ω–µ–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ —Ç–æ
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#training", "#multimodal", "#games", "#cv", "#dataset", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å FINECAPTION, —Å–ø–æ—Å–æ–±–Ω
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#architecture", "#games", "#agi"], "emoji": "üßä", "ru": {"title": "SAR3D: –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ 3D –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ SAR3D –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. S
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#open_source", "#cv"], "emoji": "üé®", "ru": {"title": "DreamMix: –í—Å—Ç–∞–≤–∫–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–∞", "desc": "DreamMix - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ —Ç–æ–ª—å–∫
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#open_source", "#inference"], "emoji": "üß†", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Ç–∞–π–Ω—ã –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Å –Ω–∏–∑–∫–∏–º –±–∏—Ç—Ä–µ–π—Ç–æ–º –º–µ–Ω–µ–µ –≤—Ä–µ–¥–Ω–æ –¥–ª—è –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —á–µ–º 
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#architecture", "#video", "#long_context", "#multimodal", "#dataset"], "emoji": "üé•", "ru": {"title": "SALOVA: —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "SALOVA - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#data", "#diffusion", "#synthetic", "#cv", "#dataset"], "emoji": "üëö", "ru": {"title": "BootComp: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª—é–¥–µ–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –æ–¥–µ–∂–¥–æ–π –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω BootComp - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª—é–¥–µ–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π 
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#alignment", "#multimodal", "#dataset", "#interpretability", "#reasoning", "#training"], "emoji": "üß™", "ru": {"title": "MolReFlect: –¢–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–ª–µ–∫—É–ª –∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MolReFlect - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –º–æ–ª–µ–∫—É–ª–∞–º–∏ 
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#open_source", "#cv", "#benchmark", "#security", "#ethics", "#dataset"], "emoji": "üïµÔ∏è", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—è–≤–ª–µ–Ω–∏—é –ò–ò-–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º (–ò–ò), –≤ –∫–æ–Ω
[27.11.2024 21:10] Using data from previous issue: {"categories": ["#training", "#video", "#cv", "#diffusion"], "emoji": "üé¨", "ru": {"title": "AnchorCrafter: –ò–ò —Å–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–º–æ-–≤–∏–¥–µ–æ —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ç–æ–≤–∞—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AnchorCrafter - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —á–µ–ª
[27.11.2024 21:10] Loading Chinese text from previous data.
[27.11.2024 21:10] Renaming data file.
[27.11.2024 21:10] Renaming previous data. hf_papers.json to ./d/2024-11-27.json
[27.11.2024 21:10] Saving new data file.
[27.11.2024 21:10] Generating page.
[27.11.2024 21:10] Renaming previous page.
[27.11.2024 21:10] Renaming previous data. index.html to ./d/2024-11-27.html
[27.11.2024 21:10] [Experimental] Generating Chinese page for reading.
[27.11.2024 21:10] Chinese vocab [{'word': 'ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢', 'pinyin': 't√∫ x√≠ng y√≤ng h√π jiƒì mi√†n', 'trans': 'graphical user interface'}, {'word': 'Âä©Êâã', 'pinyin': 'zh√π sh«íu', 'trans': 'assistant'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'vision'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«î y√°n', 'trans': 'language'}, {'word': 'Âä®‰Ωú', 'pinyin': 'd√≤ng zu√≤', 'trans': 'action'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'Áîü‰∫ßÂäõ', 'pinyin': 'shƒìng ch«én l√¨', 'trans': 'productivity'}, {'word': 'ÂºïÂØº', 'pinyin': 'y«ên d«éo', 'trans': 'guide'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅo j√¨', 'trans': 'mark'}, {'word': 'ÈÄâÊã©', 'pinyin': 'xu«én z√©', 'trans': 'selection'}, {'word': '‰∫§Èîô', 'pinyin': 'jiƒÅo cu√≤', 'trans': 'interleave'}, {'word': 'ÊµÅ', 'pinyin': 'li√∫', 'trans': 'flow'}, {'word': 'ËßÑÊ®°', 'pinyin': 'guƒ´ m√≥', 'trans': 'scale'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«ê l√¨ng', 'trans': 'command'}, {'word': 'Ë∑üÈöè', 'pinyin': 'gƒìn su√≠', 'trans': 'follow'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨ su√†n', 'trans': 'computation'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ng bƒõn', 'trans': 'cost'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'training'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†o l«ú', 'trans': 'efficiency'}, {'word': 'Èõ∂Ê†∑Êú¨', 'pinyin': 'l√≠ng y√†ng bƒõn', 'trans': 'zero-shot'}, {'word': 'Êà™Âõæ', 'pinyin': 'ji√© t√∫', 'trans': 'screenshot'}, {'word': 'ÂÆö‰Ωç', 'pinyin': 'd√¨ng w√®i', 'trans': 'localization'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n wu', 'trans': 'task'}, {'word': 'ÂáÜÁ°ÆÁéá', 'pinyin': 'zh«în qu√® l«ú', 'trans': 'accuracy'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°n j√¨ng', 'trans': 'environment'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]
[27.11.2024 21:10] Renaming previous Chinese page.
[27.11.2024 21:10] Renaming previous data. zh.html to ./d/2024-11-26_zh_reading_task.html
[27.11.2024 21:10] Writing Chinese reading task.
[27.11.2024 21:10] Writing result.
[27.11.2024 21:10] Renaming log file.
[27.11.2024 21:10] Renaming previous data. log.txt to ./logs/2024-11-27_last_log.txt
