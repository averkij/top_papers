[21.03.2025 14:10] Read previous papers.
[21.03.2025 14:10] Generating top page (month).
[21.03.2025 14:10] Writing top page (month).
[21.03.2025 15:11] Read previous papers.
[21.03.2025 15:11] Get feed.
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13358
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16419
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16416
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16302
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14487
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16397
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15558
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16212
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16365
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16257
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16418
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15299
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16356
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16421
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16322
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16057
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13657
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16413
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16428
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16422
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16425
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16278
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16188
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16055
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10625
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16375
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15851
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15567
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14237
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16252
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16219
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15451
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12689
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16429
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16194
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16031
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15855
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15242
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13834
[21.03.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.13356
[21.03.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16091
[21.03.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.09949
[21.03.2025 15:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.03.2025 15:11] No deleted papers detected.
[21.03.2025 15:11] Downloading and parsing papers (pdf, html). Total: 42.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.13358.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.13358.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.13358.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16419.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16419.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16419.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16416.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16416.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16416.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16302.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16302.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16302.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.14487.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.14487.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.14487.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16397.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16397.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16397.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.15558.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.15558.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.15558.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16212.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16212.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16212.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16365.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16365.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16365.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16257.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16257.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16257.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16418.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16418.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16418.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.15299.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.15299.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.15299.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16356.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16356.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16356.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16421.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16421.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16421.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16322.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16322.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16322.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16057.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16057.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16057.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.13657.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.13657.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.13657.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16413.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16413.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16413.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16428.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16428.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16428.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16422.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16422.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16422.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16425.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16425.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16425.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16278.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16278.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16278.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16188.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16188.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16188.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16055.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16055.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16055.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.10625.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.10625.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.10625.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16375.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16375.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16375.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.15851.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.15851.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.15851.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.15567.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.15567.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.15567.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.14237.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.14237.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.14237.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16252.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16252.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16252.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16219.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16219.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16219.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.15451.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.15451.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.15451.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.12689.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.12689.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.12689.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16429.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16429.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16429.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16194.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16194.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16194.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16031.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16031.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16031.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.15855.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.15855.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.15855.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.15242.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.15242.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.15242.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.13834.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.13834.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.13834.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.13356.
[21.03.2025 15:11] Downloading paper 2503.13356 from http://arxiv.org/pdf/2503.13356v1...
[21.03.2025 15:11] Extracting affiliations from text.
[21.03.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Agents Play Thousands of 3D Video Games Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu and Wei Yang Tencent 5 2 0 2 7 1 ] . [ 1 6 5 3 3 1 . 3 0 5 2 : r Abstract: We present PORTAL1, novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. dual-feedback mechanism incorporating quantitative game metrics and visionlanguage model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTALs effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents significant advancement in game AI development, offering practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on our project website. An agent must learn from its experience. Rich Sutton 1. Introduction Games have served as foundational testing grounds for artificial intelligence development since the fields inception [2, 9, 15, 17, 26, 27]. They pro"
[21.03.2025 15:11] Response: ```python
["Tencent"]
```
[21.03.2025 15:11] Deleting PDF ./assets/pdf/2503.13356.pdf.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.16091.
[21.03.2025 15:11] Extra JSON file exists (./assets/json/2503.16091.json), skip PDF parsing.
[21.03.2025 15:11] Paper image links file exists (./assets/img_data/2503.16091.json), skip HTML parsing.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2503.09949.
[21.03.2025 15:11] Downloading paper 2503.09949 from http://arxiv.org/pdf/2503.09949v1...
[21.03.2025 15:11] Extracting affiliations from text.
[21.03.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"UVE: Are MLLMs Unified Evaluators for AI-Generated Videos? Yuanxin Liu 1 Rui Zhu 2 Shuhuai Ren 1 Jiacong Wang 3 Haoyuan Guo 2 Xu Sun 1 Lu Jiang 2 5 2 0 2 3 1 ] . [ 1 9 4 9 9 0 . 3 0 5 2 : r Abstract With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AIgenerated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an indepth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE. 1. Introduction Video generative models (VGMs) have rapidly evolved in recent years, greatly aiding visual content creation in numer1Peking University 2ByteDance Seed 3School of Artificial Intelligence, University of Chinese Academy of Sciences. Correspondence to: Yuanxin Liu <li"
[21.03.2025 15:11] Response: ```python
["Peking University", "ByteDance Seed", "School of Artificial Intelligence, University of Chinese Academy of Sciences"]
```
[21.03.2025 15:11] Deleting PDF ./assets/pdf/2503.09949.pdf.
[21.03.2025 15:11] Success.
[21.03.2025 15:11] Enriching papers with extra data.
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 0. Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 1. Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 2. The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 3. 3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 4. Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heteroge...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 5. We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 6. Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 7. Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variati...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 8. Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often ne...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 9. Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bot...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 10. Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 11. This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a form...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 12. Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 13. Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 14. Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 15. Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transfor...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 16. Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM agents collaborate to accomplish tasks, their performance gains across popular benchmarks remain minimal compared to single-agent frameworks. This gap highlights the need to analyze the challenges hindering MAS effectivenes...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 17. We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rende...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 18. Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and effi...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 19. 4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 20. This paper proposes a fundamentally new paradigm for image generation through set-based tokenization and distribution modeling. Unlike conventional methods that serialize images into fixed-position latent codes with a uniform compression ratio, we introduce an unordered token set representation to d...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 21. Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI fo...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 22. Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 23. The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tun...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 24. Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 25. In this paper, we explore the task of generating expansive outdoor scenes, ranging from castles to high-rises. Unlike indoor scene generation, which has been a primary focus of prior work, outdoor scene generation presents unique challenges, including wide variations in scene heights and the need fo...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 26. Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars w...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 27. 3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D c...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 28. Popular video training methods mainly operate on a fixed number of tokens sampled from a predetermined spatiotemporal grid, resulting in sub-optimal accuracy-computation trade-offs due to inherent video redundancy. They also lack adaptability to varying computational budgets for downstream tasks, hi...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 29. Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 30. Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 31. This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are const...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 32. Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during tr...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 33. In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on represent...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 34. Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quan...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 35. This paper presents the Deceptive Humor Dataset (DHD), a novel resource for studying humor derived from fabricated claims and misinformation. In an era of rampant misinformation, understanding how humor intertwines with deception is essential. DHD consists of humor-infused comments generated from fa...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 36. We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary te...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 37. We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 38. Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to "dominant modality bias.'' This bias significantly hurts performance, especially when one modality is impaired. In this study, we...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 39. We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to g...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 40. Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways ...
[21.03.2025 15:11] ********************************************************************************
[21.03.2025 15:11] Abstract 41. With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluator...
[21.03.2025 15:11] Read previous papers.
[21.03.2025 15:11] Generating reviews via LLM API.
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#synthetic", "#hallucinations", "#training", "#diffusion", "#cv", "#optimization"], "emoji": "ðŸ”", "ru": {"title": "RSD: Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ ÑÐ²ÐµÑ€Ñ…Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸ Ð´Ð»Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ²ÐµÑ€Ñ…Ñ€
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#reasoning", "#rl", "#dataset", "#survey", "#training", "#benchmark"], "emoji": "ðŸ§ ", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð² LLM: Ð¿Ñ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ Ð¾Ð±Ð·Ð¾Ñ€ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð¿
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#reasoning", "#agents"], "emoji": "ðŸ¤–", "ru": {"title": "ÐšÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² Ð¾Ñ†ÐµÐ½ÐºÐ¸ AI-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾ÐºÐ¾Ð»ÐµÐ½Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð¾Ð±Ð·Ð¾Ñ€ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³Ð¸Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM). ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð°Ð½
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#open_source", "#3d", "#diffusion", "#optimization", "#inference"], "emoji": "ðŸš€", "ru": {"title": "ÐœÐ¾Ð»Ð½Ð¸ÐµÐ½Ð¾ÑÐ½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ 3D-Ñ„Ð¾Ñ€Ð¼ Ñ FlashVDM", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ FlashVDM - ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ 3D-Ñ„Ð¾Ñ€Ð¼ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (VDM). ÐÐ²Ñ‚Ð¾Ñ€
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#diffusion", "#architecture"], "emoji": "ðŸ–¼ï¸", "ru": {"title": "DiffMoE: Ð£Ð¼Ð½Ð°Ñ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "DiffMoE - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑŽ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐžÐ½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿ÑƒÐ» Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#cv", "#optimization"], "emoji": "ðŸ”", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼", "desc": "SwD - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð´Ð»Ñ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð°Ñ Ð¸Ð´ÐµÑŽ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ Ð¼Ð°ÑÑˆÑ‚Ð°
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#training", "#rl", "#benchmark", "#open_source", "#agents", "#reasoning", "#multimodal"], "emoji": "ðŸ¤–", "ru": {"title": "Ð¤Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð˜Ð˜: Ð¾Ñ‚ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð¼Ð¸Ñ€Ð° Ðº Ð²Ð¾Ð¿Ð»Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¼ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÑÐ¼", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Cosmos-Reason1, ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¼Ð¸Ñ€ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ 
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#training", "#synthetic", "#open_source", "#math", "#reasoning", "#dataset", "#benchmark", "#data"], "emoji": "ðŸ§®", "ru": {"title": "Ð¡Ð»Ð¸ÑÐ½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡ Ð´Ð»Ñ Ð¿Ñ€Ð¾ÐºÐ°Ñ‡ÐºÐ¸ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð˜Ð˜", "desc": "MathFusion - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑŽ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ Ñƒ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#open_source", "#games", "#cv", "#agents", "#training"], "emoji": "ðŸ¤–", "ru": {"title": "Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ VLA Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ Ð² Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ÐµÐ´Ð°Ñ…", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ (VLA) Ð´Ð»Ñ Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ Ð² Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ÐµÐ´
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#video", "#long_context", "#inference", "#optimization"], "emoji": "ðŸŽ¥", "ru": {"title": "VidKV: Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ ÐºÑÑˆÐ° Ð´Ð»Ñ Ð²Ð¸Ð´ÐµÐ¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ VidKV - Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÐºÑÑˆÐ° ÐºÐ»ÑŽÑ‡-Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ (KV) Ð´Ð»Ñ Ð²Ð¸Ð´ÐµÐ¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (VideoLLMs). ÐÐ²Ñ‚Ð¾Ñ€
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#architecture", "#synthetic", "#training"], "emoji": "ðŸ–¼ï¸", "ru": {"title": "InfiniteYou: ÐÐ¾Ð²Ñ‹Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸", "desc": "InfiniteYou (InfU) - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒ
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#hallucinations", "#multimodal", "#data", "#benchmark", "#interpretability"], "emoji": "ðŸ§ ", "ru": {"title": "Ð¡ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð·Ð½Ð°Ð½Ð¸Ñ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…: Ð±Ð¾Ð»ÑŒÑˆÐµ, Ñ‡ÐµÐ¼ ÐºÐ°Ð¶ÐµÑ‚ÑÑ Ð½Ð° Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð²Ð·Ð³Ð»ÑÐ´", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³Ð¸ÑŽ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… (LLM). ÐÐ²Ñ‚
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#training", "#open_source", "#reasoning", "#dataset", "#data"], "emoji": "ðŸ§ ", "ru": {"title": "CaKE: ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð˜Ð˜ Ñ‡ÐµÑ€ÐµÐ· ÑƒÐ¼Ð½Ð¾Ðµ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð·Ð½Ð°Ð½Ð¸Ð¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ CaKE. Ð­Ñ‚Ð¾Ñ‚ Ð¼ÐµÑ‚Ð¾
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#video", "#games", "#dataset", "#benchmark"], "emoji": "ðŸŽ¥", "ru": {"title": "MagicMotion: Ñ‚Ð¾Ñ‡Ð½Ð¾Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð² Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼Ð¾Ð¼ Ð²Ð¸Ð´ÐµÐ¾", "desc": "MagicMotion - Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÐµÐ¼ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ 
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#optimization", "#data", "#diffusion", "#cv", "#synthetic", "#training", "#benchmark"], "emoji": "ðŸ–¼ï¸", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ ÑÐ²ÐµÑ€Ñ…Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð³Ð¾ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð°Ð±Ð¾Ñ€ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¹ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ URAE Ð´Ð»Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#cv", "#training", "#optimization"], "emoji": "ðŸŽï¸", "ru": {"title": "Race-DiT: Ð“Ð¾Ð½ÐºÐ¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð² Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Race-DiT - Ð½Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Mixture of Experts (MoE) Ð´Ð»Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð¾Ð² Ñ Ð³Ð¸Ð±ÐºÐ¾Ð¹ ÑÑ‚
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#dataset", "#agi", "#agents", "#open_source"], "emoji": "ðŸ¤–", "ru": {"title": "Ð Ð°ÑÐºÑ€Ñ‹Ð²Ð°Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼: Ð¿ÑƒÑ‚ÑŒ Ðº ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¼Ñƒ ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ñƒ Ð˜Ð˜", "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ (ÐœÐÐ¡) Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð±Ð¾Ð»
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#optimization", "#games", "#3d", "#multimodal"], "emoji": "ðŸ§ ", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ 3D Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð²Ð¾ÑÐ¿Ñ€Ð¸ÑÑ‚Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ 3D ÐŸÑ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²ÐµÐ½Ð½ÑƒÑŽ ÐœÑƒÐ»ÑŒÑ‚Ð¸ÐœÐ¾Ð´Ð°Ð»ÑŒÐ½ÑƒÑŽ ÐŸÐ°Ð¼ÑÑ‚ÑŒ (M3) - ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ, Ð¿Ñ€ÐµÐ´Ð½Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½ÑƒÑŽ Ð´Ð»
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#inference", "#architecture", "#long_context", "#optimization", "#video", "#benchmark"], "emoji": "ðŸš€", "ru": {"title": "XAttention: Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð¾Ð² Ð±ÐµÐ· Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸", "desc": "XAttention - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð² Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ðµ
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#3d", "#inference", "#optimization"], "emoji": "ðŸš€", "ru": {"title": "4DGS-1K: Ð¡Ð²ÐµÑ€Ñ…Ð±Ñ‹ÑÑ‚Ñ€Ð°Ñ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ 4DGS-1K Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ 4D Ð³Ð°ÑƒÑÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐ¿Ð»Ð°Ñ‚Ñ‚Ð¸Ð½Ð³Ð° Ð² Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€ÐµÑˆÐ°ÑŽÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ñ…Ñ€Ð°Ð½
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#cv", "#open_source", "#diffusion", "#video"], "emoji": "ðŸ§©", "ru": {"title": "Ð ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹: Ð¾Ñ‚ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹ Ðº Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð°Ð¼", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼Ñƒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð² Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ€Ð°Ñ
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#science", "#training", "#architecture", "#3d", "#optimization", "#inference"], "emoji": "ðŸ§¬", "ru": {"title": "Uni-3DAR: Ð•Ð´Ð¸Ð½Ð°Ñ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ 3D Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Uni-3DAR - ÑƒÐ½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡ 3D Ð³ÐµÐ½ÐµÑ€Ð°Ñ†
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#transfer_learning", "#optimization", "#cv", "#multimodal", "#training"], "emoji": "ðŸ¤–", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¾Ñ‚ÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ðµ Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ñ‹ Ð² ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð˜Ð˜", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ñƒ
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#healthcare", "#training", "#optimization"], "emoji": "ðŸ§ ", "ru": {"title": "SALT: Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ñ… Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ SALT Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#architecture"], "emoji": "ðŸ§‘â€ðŸ¦°", "ru": {"title": "Ð ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð°Ð½Ð¸Ð¼Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ñ… 3D-Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑŽ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð¾Ð²", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ LHM (Large Animatable Human Reconstruction Model) - Ð½Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð°Ð½Ð¸Ð¼
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#synthetic", "#3d"], "emoji": "ðŸ™ï¸", "ru": {"title": "Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ñ‹Ñ… Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ†ÐµÐ½: Ð¾Ñ‚ Ð·Ð°Ð¼ÐºÐ¾Ð² Ð´Ð¾ Ð½ÐµÐ±Ð¾ÑÐºÑ€ÐµÐ±Ð¾Ð²", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð¾Ð±ÑˆÐ¸Ñ€Ð½Ñ‹Ñ… Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ†ÐµÐ½, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð·Ð°Ð¼ÐºÐ¸ Ð¸ Ð½ÐµÐ±Ð¾ÑÐºÑ€ÐµÐ±Ñ‹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´, ÐºÐ¾Ð´Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¹ Ñ„Ñ€Ð°Ð³
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#data", "#diffusion", "#dataset", "#synthetic", "#video", "#open_source"], "emoji": "ðŸŽ­", "ru": {"title": "Zero-1-to-A: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ñ… 3D-Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ð±ÐµÐ· Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Zero-1-to-A Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð°Ð½Ð¸Ð¼Ð¸Ñ€ÑƒÐµÐ¼Ñ‹Ñ… 3D-Ð°Ð²Ð°Ñ‚Ð°Ñ€Ð¾Ð² Ð±ÐµÐ· Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÐ°Ð»ÑŒ
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#benchmark", "#3d", "#optimization", "#dataset"], "emoji": "ðŸ§ª", "ru": {"title": "Ð•Ð´Ð¸Ð½Ð¾Ðµ Ð»Ð°Ñ‚ÐµÐ½Ñ‚Ð½Ð¾Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ 3D Ð¼Ð¾Ð»ÐµÐºÑƒÐ»", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚Ñ€ÐµÑ…Ð¼ÐµÑ€Ð½Ñ‹Ñ… Ð¼Ð¾Ð»ÐµÐºÑƒÐ» Ð´Ð»Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð»ÐµÐºÐ°Ñ€ÑÑ‚Ð² Ð¸ Ð¼Ð°Ñ‚ÐµÑ€
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#video", "#open_source", "#training", "#optimization"], "emoji": "ðŸŽ¥", "ru": {"title": "Flux: Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Flux. ÐžÐ½ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÑƒ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¸Ð·
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#training", "#rl", "#architecture", "#reasoning", "#dataset", "#healthcare"], "emoji": "ðŸ’¹", "ru": {"title": "Fin-R1: ÐœÐ¾Ñ‰Ð½Ð°Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÑŽÑ‚ Fin-R1 - ÑÐ·Ñ‹ÐºÐ¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð´Ð»Ñ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ ÑÐµÐºÑ‚Ð¾Ñ€
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#dataset", "#small_models", "#reasoning", "#open_source", "#training", "#optimization"], "emoji": "ðŸ§ ", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð¼Ð°Ð»Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ RL", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#games", "#video", "#long_context"], "emoji": "ðŸ¤–", "ru": {"title": "ÐŸÐ»Ð°Ð²Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ MotionStreamer - Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¹ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹ Ð² 
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#video", "#multimodal"], "emoji": "ðŸŽ­", "ru": {"title": "MagicID: ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð²Ð¸Ð´ÐµÐ¾ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¸ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¾Ð¹", "desc": "MagicID - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ñ… Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹. ÐžÐ½Ð° 
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#3d", "#optimization", "#transfer_learning"], "emoji": "ðŸŒ", "ru": {"title": "Sonata: Ð½Ð¾Ð²Ð°Ñ ÑÑ€Ð° ÑÐ°Ð¼Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ñ 3D Ð¾Ð±Ð»Ð°ÐºÐ¾Ð² Ñ‚Ð¾Ñ‡ÐµÐº", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð¾Ð±Ð»Ð°ÐºÐ°Ð¼Ð¸ Ñ‚Ð¾Ñ‡ÐµÐº Ð² 3D, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡ Ñ
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#architecture"], "emoji": "ðŸ–¼ï¸", "ru": {"title": "ÐžÑ‚ Ð³Ñ€ÑƒÐ±Ð¾Ð³Ð¾ Ðº Ñ‚Ð¾Ñ‡Ð½Ð¾Ð¼Ñƒ: Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿Ñ€
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#ethics", "#low_resource", "#benchmark", "#dataset", "#multilingual"], "emoji": "ðŸ¤¡", "ru": {"title": "Ð¡Ð¼ÐµÑ… ÑÐºÐ²Ð¾Ð·ÑŒ Ð»Ð¾Ð¶ÑŒ: Ð½Ð¾Ð²Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¾Ð±Ð¼Ð°Ð½Ñ‡Ð¸Ð²Ð¾Ð³Ð¾ ÑŽÐ¼Ð¾Ñ€Ð°", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… DHD (Deceptive Humor Dataset) Ð´Ð»Ñ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑŽÐ¼Ð¾Ñ€Ð°, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð½Ð° 
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#video", "#3d"], "emoji": "ðŸŽ¥", "ru": {"title": "Ð ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ðµ 3D-ÑÑ†ÐµÐ½Ñ‹ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸", "desc": "VideoRFSplat - ÑÑ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ñ… 3D-ÑÑ†ÐµÐ½ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹. ÐžÐ½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¼Ð½
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#synthetic", "#benchmark", "#dataset", "#plp", "#reasoning"], "emoji": "â±ï¸", "ru": {"title": "BigO(Bench): ÐžÑ†ÐµÐ½ÐºÐ° Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸", "desc": "BigO(Bench) - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ benchmark Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð¸ ÑÐ¾Ð·Ð´Ð°
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#optimization", "#training"], "emoji": "âš–ï¸", "ru": {"title": "Ð‘Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ° Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ðµ Ð´Ð¾Ð¼Ð¸Ð½Ð¸Ñ€ÑƒÑŽÑ‰ÐµÐ¹ Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð² Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ 
[21.03.2025 15:11] Querying the API.
[21.03.2025 15:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal .
[21.03.2025 15:11] Response: {
  "desc": "PORTAL - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð˜Ð˜-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð², ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ñ… Ð¸Ð³Ñ€Ð°Ñ‚ÑŒ Ð² Ñ‚Ñ‹ÑÑÑ‡Ð¸ 3D-Ð²Ð¸Ð´ÐµÐ¾Ð¸Ð³Ñ€ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸Ðº Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐ·Ñ‹ÐºÐ°. ÐžÐ½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð´ÐµÑ€ÐµÐ²ÑŒÐµÐ² Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ, Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð½Ð¾-Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¼ ÑÐ·Ñ‹ÐºÐµ. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð²Ð²Ð¾Ð´Ð¸Ñ‚ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸, ÑÐ¾Ñ‡ÐµÑ‚Ð°ÑŽÑ‰ÑƒÑŽ ÑƒÐ·Ð»Ñ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð» Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹. PORTAL Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð² ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸, Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸Ðº Ð¸ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ð¸ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°Ð¼Ð¸.",
  "emoji": "ðŸŽ®",
  "title": "Ð¯Ð·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð¾ÐºÐ¾Ñ€ÑÑŽÑ‚ Ð¼Ð¸Ñ€ 3D-Ð¸Ð³Ñ€"
}
[21.03.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal ."

[21.03.2025 15:11] Response: ```python
['AGENTS', '3D', 'VIDEO', 'RL']
```
[21.03.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal ."

[21.03.2025 15:11] Response: ```python
['GAMES', 'AGI', 'INTERPRETABILITY', 'REASONING']
```
[21.03.2025 15:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PORTAL is a new framework that enables AI agents to play many 3D video games by using language to guide their decision-making. It turns complex decision problems into tasks that large language models can handle, generating behavior trees in a specific language. This approach reduces the heavy computational needs of traditional reinforcement learning while maintaining strategic depth and adaptability. By combining rule-based and neural network elements, PORTAL allows for both high-level strategy and detailed control, making it easier to create versatile game-playing agents.","title":"PORTAL: Revolutionizing Game AI with Language-Guided Policies"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PORTAL is a new framework that enables AI agents to play many 3D video games by using language to guide their decision-making. It turns complex decision problems into tasks that large language models can handle, generating behavior trees in a specific language. This approach reduces the heavy computational needs of traditional reinforcement learning while maintaining strategic depth and adaptability. By combining rule-based and neural network elements, PORTAL allows for both high-level strategy and detailed control, making it easier to create versatile game-playing agents.', title='PORTAL: Revolutionizing Game AI with Language-Guided Policies'))
[21.03.2025 15:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PORTALæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æž¶ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿé€šè¿‡è¯­è¨€å¼•å¯¼ç­–ç•¥ç”Ÿæˆæ¥çŽ©æ•°åƒæ¬¾3Dè§†é¢‘æ¸¸æˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚è¯¥æ–¹æ³•å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ç”Ÿæˆä»¥é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰è¡¨ç¤ºçš„è¡Œä¸ºæ ‘ã€‚é€šè¿‡ç»“åˆåŸºäºŽè§„åˆ™çš„èŠ‚ç‚¹å’Œç¥žç»ç½‘ç»œç»„ä»¶ï¼ŒPORTALå®žçŽ°äº†é«˜å±‚æ¬¡çš„æˆ˜ç•¥æŽ¨ç†å’Œç²¾ç¡®çš„ä½Žå±‚æ¬¡æŽ§åˆ¶ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒPORTALåœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆä¸­è¡¨çŽ°å‡ºæ˜¾è‘—çš„å¼€å‘æ•ˆçŽ‡æå‡ã€ç­–ç•¥æ³›åŒ–å’Œè¡Œä¸ºå¤šæ ·æ€§ã€‚","title":"PORTALï¼šæ¸¸æˆAIçš„æ–°çºªå…ƒ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PORTALæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æž¶ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿé€šè¿‡è¯­è¨€å¼•å¯¼ç­–ç•¥ç”Ÿæˆæ¥çŽ©æ•°åƒæ¬¾3Dè§†é¢‘æ¸¸æˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚è¯¥æ–¹æ³•å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ç”Ÿæˆä»¥é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰è¡¨ç¤ºçš„è¡Œä¸ºæ ‘ã€‚é€šè¿‡ç»“åˆåŸºäºŽè§„åˆ™çš„èŠ‚ç‚¹å’Œç¥žç»ç½‘ç»œç»„ä»¶ï¼ŒPORTALå®žçŽ°äº†é«˜å±‚æ¬¡çš„æˆ˜ç•¥æŽ¨ç†å’Œç²¾ç¡®çš„ä½Žå±‚æ¬¡æŽ§åˆ¶ã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒPORTALåœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆä¸­è¡¨çŽ°å‡ºæ˜¾è‘—çš„å¼€å‘æ•ˆçŽ‡æå‡ã€ç­–ç•¥æ³›åŒ–å’Œè¡Œä¸ºå¤šæ ·æ€§ã€‚', title='PORTALï¼šæ¸¸æˆAIçš„æ–°çºªå…ƒ'))
[21.03.2025 15:11] Using data from previous issue: {"categories": ["#science", "#architecture", "#healthcare", "#optimization", "#open_source", "#training"], "emoji": "ðŸ’Š", "ru": {"title": "Ð˜Ð˜ Ð½Ð° ÑÑ‚Ñ€Ð°Ð¶Ðµ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ: Ñ‚Ð¾Ñ‡Ð½Ð¾Ðµ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¸ÐµÐ¼Ð° Ð»ÐµÐºÐ°Ñ€ÑÑ‚Ð²", "desc": "Ð”Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ AIMI (Adherence Forecasting and Intervention wi
[21.03.2025 15:11] Querying the API.
[21.03.2025 15:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE.
[21.03.2025 15:11] Response: {
  "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ñ†ÐµÐ½ÐºÐµ Ð²Ð¸Ð´ÐµÐ¾, ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð¾Ð¼ (AIGV), Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (MLLM). ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº UVE-Bench Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð² ÑƒÐ½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐµ AIGV. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¾, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ MLLM, Ñ…Ð¾Ñ‚Ñ Ð¸ ÑƒÑÑ‚ÑƒÐ¿Ð°ÑŽÑ‚ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¸Ð¼ Ð¾Ñ†ÐµÐ½Ñ‰Ð¸ÐºÐ°Ð¼, Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ Ð¼Ð½Ð¾Ð³Ð¾Ð¾Ð±ÐµÑ‰Ð°ÑŽÑ‰Ð¸Ðµ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð² ÑƒÐ½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐµ AIGV, Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð¾Ñ†ÐµÐ½ÐºÐ¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ‚Ð°ÐºÐ¶Ðµ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹, Ð²Ð»Ð¸ÑÑŽÑ‰Ð¸Ðµ Ð½Ð° Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¾Ñ†ÐµÐ½Ñ‰Ð¸ÐºÐ¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ MLLM.",
  "emoji": "ðŸŽ¥",
  "title": "MLLM ÐºÐ°Ðº ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ†ÐµÐ½Ñ‰Ð¸Ðº AI-Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾"
}
[21.03.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE."

[21.03.2025 15:11] Response: ```python
['VIDEO', 'BENCHMARK', 'MULTIMODAL']
```
[21.03.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE."

[21.03.2025 15:11] Response: ```python
['GAMES', 'INTERPRETABILITY', 'OPTIMIZATION']
```
[21.03.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the need for effective evaluation metrics for AI-generated videos (AIGVs) as video generative models (VGMs) become more prevalent. It critiques existing methods that either depend on models designed for different tasks or require human assessments, which limits their scalability and comprehensiveness. The authors propose using multimodal large language models (MLLMs) as a unified evaluator, capitalizing on their capabilities in visual perception and language understanding. They introduce UVE-Bench, a benchmark for evaluating AIGVs, and find that while MLLMs do not yet match human evaluators, they outperform traditional evaluation methods, providing insights for future improvements in AIGV assessment.","title":"Harnessing MLLMs for Comprehensive AI-Generated Video Evaluation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the need for effective evaluation metrics for AI-generated videos (AIGVs) as video generative models (VGMs) become more prevalent. It critiques existing methods that either depend on models designed for different tasks or require human assessments, which limits their scalability and comprehensiveness. The authors propose using multimodal large language models (MLLMs) as a unified evaluator, capitalizing on their capabilities in visual perception and language understanding. They introduce UVE-Bench, a benchmark for evaluating AIGVs, and find that while MLLMs do not yet match human evaluators, they outperform traditional evaluation methods, providing insights for future improvements in AIGV assessment.', title='Harnessing MLLMs for Comprehensive AI-Generated Video Evaluation'))
[21.03.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"éšç€è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼ˆVGMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€å‘å¯é çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯¹äºŽAIç”Ÿæˆè§†é¢‘ï¼ˆAIGVï¼‰å˜å¾—è‡³å…³é‡è¦ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽä¸ºå…¶ä»–ä»»åŠ¡ä¼˜åŒ–çš„çŽ°æˆæ¨¡åž‹æˆ–äººç±»è¯„ä¼°æ•°æ®æ¥è®­ç»ƒä¸“é—¨çš„è¯„ä¼°å™¨ï¼Œè¿™é™åˆ¶äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æŽ¢è®¨äº†ä½¿ç”¨å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰ä½œä¸ºAIGVçš„ç»Ÿä¸€è¯„ä¼°å™¨çš„å¯è¡Œæ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºUVE-Benchçš„åŸºå‡†ï¼Œæ”¶é›†äº†ç”±æœ€å…ˆè¿›çš„VGMç”Ÿæˆçš„è§†é¢‘åŠå…¶äººç±»åå¥½æ³¨é‡Šã€‚æˆ‘ä»¬çš„å®žè¯ç»“æžœè¡¨æ˜Žï¼Œå°½ç®¡å…ˆè¿›çš„MLLMä»ç„¶è½åŽäºŽäººç±»è¯„ä¼°è€…ï¼Œä½†åœ¨ç»Ÿä¸€AIGVè¯„ä¼°ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†çŽ°æœ‰çš„ä¸“é—¨è¯„ä¼°æ–¹æ³•ã€‚","title":"åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹æå‡AIç”Ÿæˆè§†é¢‘è¯„ä¼°çš„ç»Ÿä¸€æ€§"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='éšç€è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼ˆVGMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€å‘å¯é çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯¹äºŽAIç”Ÿæˆè§†é¢‘ï¼ˆAIGVï¼‰å˜å¾—è‡³å…³é‡è¦ã€‚çŽ°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºŽä¸ºå…¶ä»–ä»»åŠ¡ä¼˜åŒ–çš„çŽ°æˆæ¨¡åž‹æˆ–äººç±»è¯„ä¼°æ•°æ®æ¥è®­ç»ƒä¸“é—¨çš„è¯„ä¼°å™¨ï¼Œè¿™é™åˆ¶äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æŽ¢è®¨äº†ä½¿ç”¨å¤šæ¨¡æ€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆMLLMï¼‰ä½œä¸ºAIGVçš„ç»Ÿä¸€è¯„ä¼°å™¨çš„å¯è¡Œæ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºUVE-Benchçš„åŸºå‡†ï¼Œæ”¶é›†äº†ç”±æœ€å…ˆè¿›çš„VGMç”Ÿæˆçš„è§†é¢‘åŠå…¶äººç±»åå¥½æ³¨é‡Šã€‚æˆ‘ä»¬çš„å®žè¯ç»“æžœè¡¨æ˜Žï¼Œå°½ç®¡å…ˆè¿›çš„MLLMä»ç„¶è½åŽäºŽäººç±»è¯„ä¼°è€…ï¼Œä½†åœ¨ç»Ÿä¸€AIGVè¯„ä¼°ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†çŽ°æœ‰çš„ä¸“é—¨è¯„ä¼°æ–¹æ³•ã€‚', title='åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡åž‹æå‡AIç”Ÿæˆè§†é¢‘è¯„ä¼°çš„ç»Ÿä¸€æ€§'))
[21.03.2025 15:12] Loading Chinese text from previous data.
[21.03.2025 15:12] Renaming data file.
[21.03.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-03-21.json
[21.03.2025 15:12] Saving new data file.
[21.03.2025 15:12] Generating page.
[21.03.2025 15:12] Renaming previous page.
[21.03.2025 15:12] Renaming previous data. index.html to ./d/2025-03-21.html
[21.03.2025 15:12] [Experimental] Generating Chinese page for reading.
[21.03.2025 15:12] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇŽo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§åž‹', 'pinyin': 'dÃ  xÃ­ng', 'trans': 'large-scale'}, {'word': 'è¯­è¨€æ¨¡åž‹', 'pinyin': 'yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'language model'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇŽn zhÃ¹', 'trans': 'significant'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'æŽ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervised'}, {'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tuning'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ© xÃ­', 'trans': 'reinforcement learning'}, {'word': 'æŠ€æœ¯', 'pinyin': 'jÃ¬ shÃ¹', 'trans': 'technology'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'é¢†åŸŸ', 'pinyin': 'lÇng yÃ¹', 'trans': 'field'}, {'word': 'åºåˆ—', 'pinyin': 'xÃ¹ liÃ¨', 'trans': 'sequence'}, {'word': 'è®¡ç®—', 'pinyin': 'jÃ¬ suÃ n', 'trans': 'computation'}, {'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'}, {'word': 'çŽ°è±¡', 'pinyin': 'xiÃ n xiÃ ng', 'trans': 'phenomenon'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'}, {'word': 'è°ƒæŸ¥', 'pinyin': 'diÃ o chÃ¡', 'trans': 'investigate'}, {'word': 'è¿›å±•', 'pinyin': 'jÃ¬n zhÇŽn', 'trans': 'progress'}, {'word': 'æ–¹å‘', 'pinyin': 'fÄng xiÃ ng', 'trans': 'direction'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'}, {'word': 'è¾“å‡º', 'pinyin': 'shÅ« chÅ«', 'trans': 'output'}, {'word': 'è¾“å…¥', 'pinyin': 'shÅ« rÃ¹', 'trans': 'input'}, {'word': 'æç¤º', 'pinyin': 'tÃ­ shÃ¬', 'trans': 'prompt'}, {'word': 'æŽ¢è®¨', 'pinyin': 'tÃ n tÇŽo', 'trans': 'explore'}, {'word': 'é«˜æ•ˆ', 'pinyin': 'gÄo xiÃ o', 'trans': 'efficient'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹ jÃ¹', 'trans': 'data'}, {'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹n liÃ n', 'trans': 'training'}, {'word': 'å°åž‹', 'pinyin': 'xiÇŽo xÃ­ng', 'trans': 'small-scale'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluation'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇŽ', 'trans': 'method'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'æµ‹è¯•', 'pinyin': 'cÃ¨ shÃ¬', 'trans': 'test'}]
[21.03.2025 15:12] Renaming previous Chinese page.
[21.03.2025 15:12] Renaming previous data. zh.html to ./d/2025-03-20_zh_reading_task.html
[21.03.2025 15:12] Writing Chinese reading task.
[21.03.2025 15:12] Writing result.
[21.03.2025 15:12] Renaming log file.
[21.03.2025 15:12] Renaming previous data. log.txt to ./logs/2025-03-21_last_log.txt
