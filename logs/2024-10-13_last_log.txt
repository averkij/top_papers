[14.10.2024 18:50] Get feed.
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08565
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08261
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06456
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07133
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08815
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07035
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09008
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09009
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07656
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08102
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08391
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08168
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09045
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07331
[14.10.2024 18:50] Extract page data from URL. URL: https://huggingface.co/papers/2410.09038
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07536
[14.10.2024 18:50] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09037
[14.10.2024 18:50] Extract page data from URL. URL: https://huggingface.co/papers/2410.08193
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 0. The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at ...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 1. Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregr...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 2. Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enha...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 3. Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiti...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 4. Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This charact...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 5. Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length cons...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 6. Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 7. Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 8. Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers,...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 9. Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 10. Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for ...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 11. We present ZeroComp, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors, together opera...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 12. The proliferation of inflammatory or misleading "fake" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 13. We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding a...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 14. Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to ...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 15. Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrap...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 16. Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine...
[14.10.2024 18:50] ********************************************************************************
[14.10.2024 18:50] Abstract 17. Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Te...
[14.10.2024 18:50] Read previous papers.
[14.10.2024 18:50] Generating reviews via LLM API.
[14.10.2024 18:50] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Baichuan-Omni - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ–ø—ã—Ç. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ö–µ–º—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á
[14.10.2024 18:50] Using data from previous issue: {"desc": "Meissonic - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –ø–æ–¥—Ö–æ–¥–µ masked image modeling (MIM). –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏
[14.10.2024 18:50] Using data from previous issue: {"desc": "VITask - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∑–∞–¥–∞—á–∞–º. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –æ—Ç–≤–µ—Ç–æ–≤. VITask –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç
[14.10.2024 18:50] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EvolveDirector - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç API —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –î–ª—è
[14.10.2024 18:50] Using data from previous issue: {"desc": "StructRAG - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π. –û–Ω –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ RAG, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—è –∏—Å—Ö–æ–¥–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–∑–æ–º. StructRAG –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–∏–±–æ
[14.10.2024 18:50] Using data from previous issue: {"desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω–∏ –≤–≤–æ–¥—è—Ç PositionID Prompting –∏ PositionID Fine-Tuning –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ PositionID CP Prompting –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –∫–æ
[14.10.2024 18:50] Using data from previous issue: {"desc": "SuperCorrect - —ç—Ç–æ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª–µ–Ω—å–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å-—É—á–∏—Ç–µ–ª—è –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –º–µ–Ω—å—à–µ–π –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã –º
[14.10.2024 18:50] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π - Semantic Score Distillation Sampling (SemanticSDS). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤. SemanticSDS —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —ç
[14.10.2024 18:50] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SAE Match –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏ (SAE), –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –ú–µ—Ç–æ–¥ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –æ—à–∏–±–∫—É –º–µ–∂–¥—É —Å–≤–µ—Ä–Ω—É—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ SAE, —É—á–∏—Ç—ã–≤–∞—è –ø–æ—Ä–æ–≥–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Gemma 
[14.10.2024 18:50] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ö–∞–∂–¥—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å—Ç—É–ø–∞–µ—Ç –≤ —Ä–æ–ª–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∞ –∫–æ–Ω—Å–æ–ª—å –∞–≥–µ–Ω—Ç–æ–≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è LLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å
[14.10.2024 18:50] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º KV Prediction –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ KV-–∫—ç—à–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (TTFT). –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω
[14.10.2024 18:50] Using data from previous issue: {"desc": "ZeroComp ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –ø–∞—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—Ü–µ–Ω –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ControlNet –¥–ª—è –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –µ–≥–æ —Å –º–æ–¥–µ–ª—å—é Stable Diffusion –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –µ—ë –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω
[14.10.2024 18:50] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MiRAGeNews, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 12 500 –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ–¥–ø–∏—Å–µ–π, –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã—Ö, —Ç–∞–∫ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ò–ò. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–∞–∫ –¥–ª—è –ª—é–¥–µ–π (60% F1-–º–µ—Ä—ã), —Ç–∞–∫ –∏ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (
[14.10.2024 18:50] Using data from previous issue: {"desc": "DA-Code - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö
[14.10.2024 18:50] Querying the API.
[14.10.2024 18:50] Got response. {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SimpleStrat. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏–º–µ–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏. SimpleStrat –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º—É —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∞ —Å—Ç—Ä–∞—Ç—ã, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞—Ç–µ–º –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–∞—è —Å—Ç—Ä–∞—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç CoverageQA —Å –Ω–µ–¥–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, –∏–º–µ—é—â–∏–º–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ SimpleStrat –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ recall –∏ –º–µ–Ω—å—à–µ–≥–æ KL-—Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",

  "tags": ["#SimpleStrat", "#CoverageQA", "#—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ_–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"],

  "categories": ["#nlp", "#dataset", "#benchmark"],

  "emoji": "üé≠",

  "title": "SimpleStrat: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
}
[14.10.2024 18:50] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ I-Max –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö —Å –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º (RFT) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. I-Max –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Projected Flow –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è –≤—ã–≤–æ–¥–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –æ–±–æ–±—â
[14.10.2024 18:50] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mentor-KD –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–µ–Ω—å—à–∏–º –º–æ–¥–µ–ª—è–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å-–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π Chain-of-Thought –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏
[14.10.2024 18:50] Querying the API.
[14.10.2024 18:50] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GenARM - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ LLM –∫ –∂–µ–ª–∞–µ–º–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GenARM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ —Ç–∞–∫–∂–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ—Ç —Å–ª–∞–±–æ–≥–æ –∫ —Å–∏–ª—å–Ω–æ–º—É –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.",
  "tags": ["#–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è_–º–æ–¥–µ–ª—å", "#–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è_–º–æ–¥–µ–ª—å", "#–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ_LLM"],
  "categories": ["#nlp", "#rlhf"],
  "emoji": "üéØ",
  "title": "GenARM: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ LLM –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
}
[14.10.2024 18:50] Trying to get texts in Chinese.
[14.10.2024 18:50] Mistral request. Write simple and brief explanation (2-3 sentences) of an article in Chinese. Text:

The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.
[14.10.2024 18:50] Mistral response. {"message": "Unauthorized", "request_id": "8c2ccb51cf622738e3bf6f5d0a56a914"}
[14.10.2024 18:50] Failed to get Chinese text: 'choices'
[14.10.2024 18:50] Renaming data file.
[14.10.2024 18:50] Renaming previous data. hf_papers.json to 2024-10-13_hf_papers.json
[14.10.2024 18:50] Saving new data file.
[14.10.2024 18:50] Generating page.
[14.10.2024 18:50] Renaming previous page.
[14.10.2024 18:50] Renaming previous data. index.html to 2024-10-13_hf_papers.html
[14.10.2024 18:50] Writing result.
[14.10.2024 18:50] Renaming log file.
[14.10.2024 18:50] Renaming previous data. log.txt to 2024-10-13_last_log.txt
