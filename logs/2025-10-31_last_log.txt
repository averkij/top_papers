[31.10.2025 11:10] Read previous papers.
[31.10.2025 11:10] Generating top page (month).
[31.10.2025 11:10] Writing top page (month).
[31.10.2025 12:23] Read previous papers.
[31.10.2025 12:23] Get feed.
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26583
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15510
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26802
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26692
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26768
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26794
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19949
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26800
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26658
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25992
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26298
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25628
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26213
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25897
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25779
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26787
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26140
[31.10.2025 12:23] Extract page data from URL. URL: https://huggingface.co/papers/2510.25867
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26474
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26160
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25132
[31.10.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26781
[31.10.2025 12:23] Extract page data from URL. URL: https://huggingface.co/papers/2510.25364
[31.10.2025 12:23] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[31.10.2025 12:23] No deleted papers detected.
[31.10.2025 12:23] Downloading and parsing papers (pdf, html). Total: 23.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26583.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26583.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26583.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.15510.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.15510.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.15510.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26802.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26802.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26802.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26692.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26692.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26692.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26768.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26768.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26768.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26794.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26794.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26794.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.19949.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.19949.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.19949.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26800.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26800.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26800.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26658.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26658.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26658.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.25992.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.25992.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.25992.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26298.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26298.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26298.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.25628.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.25628.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.25628.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26213.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26213.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26213.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.25897.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.25897.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.25897.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.25779.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.25779.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.25779.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26787.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26787.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26787.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26140.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26140.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26140.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.25867.
[31.10.2025 12:23] Downloading paper 2510.25867 from http://arxiv.org/pdf/2510.25867v1...
[31.10.2025 12:23] Extracting affiliations from text.
[31.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SYNTHESIZING HIGH-QUALITY VISUAL QUESTION ANSWERING FROM MEDICAL DOCUMENTS WITH GENERATOR-VERIFIER LMMS Xiaoke Huang1, Ningsen Wang1,2, Hui Liu3, Xianfeng Tang3, Yuyin Zhou1 1 UC Santa Cruz indicates equal contribution https://github.com/UCSC-VLAA/MedVLSynther 3 Amazon Research 2 Fudan University 5 2 0 2 9 ] . [ 1 7 6 8 5 2 . 0 1 5 2 : r a "
[31.10.2025 12:23] Response: ```python
["UC Santa Cruz", "Fudan University", "Amazon Research"]
```
[31.10.2025 12:23] Deleting PDF ./assets/pdf/2510.25867.pdf.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26474.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26474.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26474.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26160.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26160.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26160.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.25132.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.25132.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.25132.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.26781.
[31.10.2025 12:23] Extra JSON file exists (./assets/json/2510.26781.json), skip PDF parsing.
[31.10.2025 12:23] Paper image links file exists (./assets/img_data/2510.26781.json), skip HTML parsing.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2510.25364.
[31.10.2025 12:23] Downloading paper 2510.25364 from http://arxiv.org/pdf/2510.25364v1...
[31.10.2025 12:23] Extracting affiliations from text.
[31.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 4 6 3 5 2 . 0 1 5 2 : r CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs Luca Capone1* and Alessandro Bondielli1,2 and Alessandro Lenci1 1CoLing Lab, Department of Philology, Literature and Linguistics, University of Pisa 2Department of Computer Science, University of Pisa luca.capone@fileli.unipi.it, {alessandro.bondielli, alessandro.lenci}@unipi.it "
[31.10.2025 12:23] Response: ```python
["CoLing Lab, Department of Philology, Literature and Linguistics, University of Pisa", "Department of Computer Science, University of Pisa"]
```
[31.10.2025 12:23] Deleting PDF ./assets/pdf/2510.25364.pdf.
[31.10.2025 12:23] Success.
[31.10.2025 12:23] Enriching papers with extra data.
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 0. We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily de...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 1. ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.  					AI-generated summary 				 While pre-trained visual representations have significantly advanced imitation learning, they are...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 2. Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question ...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 3. We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA)...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 4. We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models ...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 5. Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 6. Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 7. There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance th...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 8. We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with lar...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 9. Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 10. OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynam...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 11. Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and l...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 12. Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers wi...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 13. Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 14. As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understa...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 15. AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economical...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 16. Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among ...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 17. Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 18. Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating h...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 19. Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no compr...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 20. Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. T...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 21. Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding...
[31.10.2025 12:23] ********************************************************************************
[31.10.2025 12:23] Abstract 22. This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-t...
[31.10.2025 12:23] Read previous papers.
[31.10.2025 12:23] Generating reviews via LLM API.
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#inference", "#multimodal", "#rl", "#agi", "#cv", "#games"], "emoji": "🌍", "ru": {"title": "Мультимодальная модель мира с единым предсказанием следующего токена", "desc": "Представлена модель Emu3.5 — крупномасштабная мультимодальная модель, которая 
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#agents", "#optimization", "#benchmark", "#diffusion", "#cv", "#robotics"], "emoji": "🤖", "ru": {"title": "Обучаемые промпты для адаптации диффузионных моделей к робототехнике", "desc": "ORCA использует обучаемые промпты для адаптации предобученных text-to-image диффузионных моделей
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#video", "#reasoning", "#benchmark"], "emoji": "🎬", "ru": {"title": "Видео-модели как визуальные движки: обещания и ограничения рассуждений", "desc": "Исследователи проверили, может ли современная модель генерации видео Veo-3 выступать в роли zero-shot reasoner для сложных задач виз
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#optimization", "#long_context", "#training"], "emoji": "⚡", "ru": {"title": "Линейное внимание обходит полное: эффективность встречается с производительностью", "desc": "Представлена архитектура Kimi Linear с гибридным линейным вниманием, которая вп
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#math", "#benchmark", "#reasoning"], "emoji": "🏆", "ru": {"title": "Олимпиадная математика ставит LLM в тупик", "desc": "Исследователи представили AMO-Bench — новый бенчмарк для оценки математических способностей LLM с задачами уровня Международной математической олимпиады и выше. Б
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#benchmark", "#data", "#dataset", "#open_source", "#cv", "#multimodal", "#diffusion", "#transfer_learning"], "emoji": "🎬", "ru": {"title": "От видео к движению: перенос знаний для генерации 3D-анимации человека", "desc": "Исследователи предложили новый подход к генерации 3D-движений
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#agents", "#agi", "#optimization"], "emoji": "🏄", "ru": {"title": "Универсальный визуальный агент для управления любыми интерфейсами", "desc": "Представлена система Surfer 2 — универсальная архитектура AI-агента, которая работает исключительно на основ
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#3d", "#synthetic", "#games"], "emoji": "🌐", "ru": {"title": "От панорам к реалистичным 3D-сценам с физически корректным рендерингом", "desc": "Статья представляет OmniX — универсальную систему для создания графически готовых 3D-сцен из панорамных изображе
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#rl", "#inference", "#math", "#reasoning", "#agents", "#optimization"], "emoji": "🔀", "ru": {"title": "Асинхронное мышление: AI-агенты решают задачи параллельно и коллаборативно", "desc": "В статье представлена концепция асинхронного мышления (AsyncThink) для LLM, где модель организ
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#optimization", "#agents", "#training", "#small_models", "#reasoning", "#rl"], "emoji": "🎯", "ru": {"title": "Обучение через пошаговое подражание экспертам", "desc": "Статья представляет метод Supervised Reinforcement Learning (SRL), который помогает небольшим языковым моделям решат
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#reasoning", "#benchmark", "#games"], "emoji": "🎮", "ru": {"title": "Атлас от OpenAI: силён в логике, слаб в реакции", "desc": "Исследователи протестировали новую модель ChatGPT Atlas, которая умеет взаимодействовать с веб-страницами через курсор и клавиату
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#healthcare", "#reasoning", "#training", "#science"], "emoji": "🏥", "ru": {"title": "EHR-R1: AI-модель с продвинутыми рассуждениями для анализа медицинских карт", "desc": "Статья представляет EHR-R1 — специализированную LLM для анализа электронных медицинск
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#training", "#transfer_learning", "#data", "#architecture"], "emoji": "📰", "ru": {"title": "OmniLayout: Миллион разнообразных макетов для генерации документов", "desc": "Исследователи представили OmniLayout-1M — первый датасет с миллионом разнообразных ма
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#alignment", "#training", "#optimization", "#dataset", "#benchmark", "#cv"], "emoji": "🎯", "ru": {"title": "Обучение с учётом предпочтений пользователей напрямую", "desc": "Современные модели генерации изображений обучаются на огромных неотфильтрованных датасетах, что не всегда соот
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#multimodal", "#ethics", "#reasoning", "#agents"], "emoji": "🤝", "ru": {"title": "Агенты на рынке: скорость побеждает качество", "desc": "Исследователи изучают поведение LLM-агентов в двусторонних рынках, где одни агенты представляют потребителей, а другие — конкурирующие бизнесы. Д
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#agents", "#science", "#reasoning", "#benchmark"], "emoji": "🏢", "ru": {"title": "Реальная автоматизация труда: AI пока справляется только с 2.5% задач", "desc": "Исследователи представили Remote Labor Index (RLI) — новый бенчмарк для оценки способности AI-агентов выполнять реальные
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#3d", "#dataset"], "emoji": "🧩", "ru": {"title": "Полноразмерная генерация частей: каждой детали своё пространство", "desc": "Статья представляет FullPart — новый подход к генерации 3D-объектов по частям, который комбинирует неявные (implicit) и явные (
[31.10.2025 12:23] Querying the API.
[31.10.2025 12:23] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.
[31.10.2025 12:23] Response: ```json
{
  "desc": "Исследователи представили MedVLSynther — систему для автоматической генерации качественных медицинских вопросов с вариантами ответов на основе изображений и текста из открытой научной литературы. Система использует генератор вопросов и многоступенчатый верификатор, который проверяет корректность, клиническую валидность и согласованность изображения с текстом. На основе этого метода был создан датасет MedSynVQA из 13,087 вопросов по 14,803 медицинским изображениям различных модальностей. Обучение LLM с reinforcement learning на этих данных значительно улучшило точность ответов на медицинские визуальные вопросы по сравнению с существующими медицинскими моделями.",
  "emoji": "🏥",
  "title": "Синтетические медицинские вопросы из научной литературы для обучения AI"
}
```
[31.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data."

[31.10.2025 12:23] Response: ```python
['DATASET', 'DATA', 'MULTIMODAL', 'HEALTHCARE', 'RL', 'TRAINING']
```
[31.10.2025 12:23] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data."

[31.10.2025 12:23] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE', 'REASONING']
```
[31.10.2025 12:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces MedVLSynther, a framework designed to generate high-quality medical visual question answering (VQA) items from open biomedical literature. It utilizes a generator-verifier approach, where the generator creates multiple-choice questions based on figures and text, while the verifier ensures the questions meet specific criteria for quality and validity. By applying this method to PubMed Central, the authors produced a dataset called MedSynVQA, which includes over 13,000 questions linked to various medical images. The framework enhances the training of large multimodal models through reinforcement learning, leading to improved performance on medical VQA benchmarks.","title":"Generating Quality Medical Questions from Open Literature"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MedVLSynther, a framework designed to generate high-quality medical visual question answering (VQA) items from open biomedical literature. It utilizes a generator-verifier approach, where the generator creates multiple-choice questions based on figures and text, while the verifier ensures the questions meet specific criteria for quality and validity. By applying this method to PubMed Central, the authors produced a dataset called MedSynVQA, which includes over 13,000 questions linked to various medical images. The framework enhances the training of large multimodal models through reinforcement learning, leading to improved performance on medical VQA benchmarks.', title='Generating Quality Medical Questions from Open Literature'))
[31.10.2025 12:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"大型多模态模型（LMMs）在回答需要图像和文本联合推理的医学问题方面越来越有能力，但由于缺乏大型、开放可用的高质量语料库，训练通用医学视觉问答系统受到阻碍。我们提出了MedVLSynther，这是一种基于规则的生成-验证框架，能够直接从开放的生物医学文献中合成高质量的多项选择视觉问答题。该框架通过条件生成图像、标题和文本引用，生成自包含的题干和相互排斥的选项，并通过多阶段验证器确保题目的有效性和一致性。通过在PubMed Central应用该流程，我们获得了MedSynVQA，包含13,087个经过审核的问题，覆盖14,803张图像，训练开放权重的LMMs显著提高了在多个医学视觉问答基准上的准确性。","title":"开放文献驱动的医学视觉问答系统"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='大型多模态模型（LMMs）在回答需要图像和文本联合推理的医学问题方面越来越有能力，但由于缺乏大型、开放可用的高质量语料库，训练通用医学视觉问答系统受到阻碍。我们提出了MedVLSynther，这是一种基于规则的生成-验证框架，能够直接从开放的生物医学文献中合成高质量的多项选择视觉问答题。该框架通过条件生成图像、标题和文本引用，生成自包含的题干和相互排斥的选项，并通过多阶段验证器确保题目的有效性和一致性。通过在PubMed Central应用该流程，我们获得了MedSynVQA，包含13,087个经过审核的问题，覆盖14,803张图像，训练开放权重的LMMs显著提高了在多个医学视觉问答基准上的准确性。', title='开放文献驱动的医学视觉问答系统'))
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#cv", "#training"], "emoji": "⚖️", "ru": {"title": "Борьба с эффектом Матфея в самообучении визуальных моделей", "desc": "Исследователи обнаружили проблему в процессе самообучения больших визуально-языковых моделей (LVLMs): модели хорошо справляются с 
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#benchmark", "#dataset"], "emoji": "👓", "ru": {"title": "CRAG-MM: Бенчмарк для умных очков с мультимодальными диалогами", "desc": "Исследователи создали новый бенчмарк CRAG-MM для оценки систем, отвечающих на вопросы об окружающем мире через носимые устройства
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#data", "#dataset", "#training"], "emoji": "🧬", "ru": {"title": "Генерация ферментов под конкретные субстраты с контролем каталитических свойств", "desc": "Исследователи представили EnzyControl — метод для генерации структур ферментов с учётом специфич
[31.10.2025 12:23] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#cv", "#reasoning"], "emoji": "📊", "ru": {"title": "ChartAB: новый стандарт для оценки понимания графиков в VLM", "desc": "Статья представляет новый бенчмарк ChartAB для оценки способностей vision-language моделей (VLM) в задачах детального понимания
[31.10.2025 12:23] Querying the API.
[31.10.2025 12:23] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits.
[31.10.2025 12:24] Response: ```json
{
  "desc": "Исследование проверяет, могут ли небольшие языковые модели (100M и 140M параметров) получить пользу от instruction tuning. Сравниваются разные датасеты для обучения диалогам и ответам на вопросы, применяемые либо совместно, либо последовательно. Результаты показывают небольшие, но стабильные улучшения при файн-тюнинге, особенно при последовательном обучении, но эти улучшения не всегда переносятся на zero-shot задачи. Работа демонстрирует компромисс между адаптацией к интерактивным задачам и общей лингвистической обобщающей способностью маленьких LLM.",
  "emoji": "🎓",
  "title": "Обучение маленьких моделей через инструкции: потенциал и ограничения"
}
```
[31.10.2025 12:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits."

[31.10.2025 12:24] Response: ```python
['SMALL_MODELS', 'TRAINING']
```
[31.10.2025 12:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits."

[31.10.2025 12:24] Response: ```python
['TRANSFER_LEARNING', 'LOW_RESOURCE']
```
[31.10.2025 12:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the impact of instruction tuning on small-scale language models (LMs) with 100M and 140M parameters. It compares the effectiveness of using conversational and question-answering datasets in either a merged or sequential curriculum format. The study finds that instruction tuning leads to slight but consistent improvements in fine-tuning tasks, particularly with sequential curricula being more effective than merged datasets. However, these enhancements do not always carry over to zero-shot tasks, indicating a balance between focused learning and broader language understanding.","title":"Unlocking Potential: Instruction Tuning for Small-Scale LMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the impact of instruction tuning on small-scale language models (LMs) with 100M and 140M parameters. It compares the effectiveness of using conversational and question-answering datasets in either a merged or sequential curriculum format. The study finds that instruction tuning leads to slight but consistent improvements in fine-tuning tasks, particularly with sequential curricula being more effective than merged datasets. However, these enhancements do not always carry over to zero-shot tasks, indicating a balance between focused learning and broader language understanding.', title='Unlocking Potential: Instruction Tuning for Small-Scale LMs'))
[31.10.2025 12:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究探讨了小规模语言模型（LMs）是否能从指令调优中受益。我们比较了对话和问答指令调优数据集，采用合并或顺序课程的方式，使用参数为1亿和1.4亿的解码器模型。评估涵盖了微调（SuperGLUE）和零样本（BLiMP、EWoK、WUGs、实体跟踪和心理语言学相关性）设置。结果表明，指令调优在微调场景中带来了小但一致的提升，顺序课程优于合并数据，但这种提升并未在零样本任务中一致转移，表明了以互动为中心的适应与广泛语言泛化之间的权衡。","title":"小规模语言模型的指令调优潜力与限制"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究探讨了小规模语言模型（LMs）是否能从指令调优中受益。我们比较了对话和问答指令调优数据集，采用合并或顺序课程的方式，使用参数为1亿和1.4亿的解码器模型。评估涵盖了微调（SuperGLUE）和零样本（BLiMP、EWoK、WUGs、实体跟踪和心理语言学相关性）设置。结果表明，指令调优在微调场景中带来了小但一致的提升，顺序课程优于合并数据，但这种提升并未在零样本任务中一致转移，表明了以互动为中心的适应与广泛语言泛化之间的权衡。', title='小规模语言模型的指令调优潜力与限制'))
[31.10.2025 12:24] Renaming data file.
[31.10.2025 12:24] Renaming previous data. hf_papers.json to ./d/2025-10-31.json
[31.10.2025 12:24] Saving new data file.
[31.10.2025 12:24] Generating page.
[31.10.2025 12:24] Renaming previous page.
[31.10.2025 12:24] Renaming previous data. index.html to ./d/2025-10-31.html
[31.10.2025 12:24] Writing result.
[31.10.2025 12:24] Renaming log file.
[31.10.2025 12:24] Renaming previous data. log.txt to ./logs/2025-10-31_last_log.txt
