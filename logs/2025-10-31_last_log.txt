[31.10.2025 15:11] Read previous papers.
[31.10.2025 15:11] Generating top page (month).
[31.10.2025 15:11] Writing top page (month).
[31.10.2025 16:15] Read previous papers.
[31.10.2025 16:15] Get feed.
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26697
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26583
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15510
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26692
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26802
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26768
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26794
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19949
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26800
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26658
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26298
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25992
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25897
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25628
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26213
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25779
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25867
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26787
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26140
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26474
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26160
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25132
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22282
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26781
[31.10.2025 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25364
[31.10.2025 16:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[31.10.2025 16:15] No deleted papers detected.
[31.10.2025 16:15] Downloading and parsing papers (pdf, html). Total: 25.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26697.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26697.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26697.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26583.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26583.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26583.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.15510.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.15510.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.15510.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26692.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26692.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26692.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26802.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26802.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26802.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26768.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26768.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26768.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26794.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26794.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26794.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.19949.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.19949.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.19949.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26800.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26800.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26800.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26658.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26658.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26658.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26298.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26298.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26298.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.25992.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.25992.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.25992.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.25897.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.25897.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.25897.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.25628.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.25628.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.25628.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26213.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26213.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26213.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.25779.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.25779.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.25779.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.25867.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.25867.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.25867.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26787.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26787.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26787.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26140.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26140.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26140.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26474.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26474.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26474.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26160.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26160.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26160.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.25132.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.25132.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.25132.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.22282.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.22282.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.22282.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.26781.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.26781.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.26781.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Downloading and parsing paper https://huggingface.co/papers/2510.25364.
[31.10.2025 16:15] Extra JSON file exists (./assets/json/2510.25364.json), skip PDF parsing.
[31.10.2025 16:15] Paper image links file exists (./assets/img_data/2510.25364.json), skip HTML parsing.
[31.10.2025 16:15] Success.
[31.10.2025 16:15] Enriching papers with extra data.
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 0. The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by lear...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 1. We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily de...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 2. ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.  					AI-generated summary 				 While pre-trained visual representations have significantly advanced imitation learning, they are...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 3. We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA)...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 4. Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question ...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 5. We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models ...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 6. Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 7. Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 8. There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance th...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 9. We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with lar...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 10. OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynam...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 11. Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 12. Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 13. Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and l...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 14. Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers wi...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 15. As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understa...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 16. Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 17. AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economical...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 18. Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among ...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 19. Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating h...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 20. Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no compr...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 21. Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. T...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 22. Harnessing publicly available, large-scale web data, such as street view and satellite imagery, urban socio-economic sensing is of paramount importance for achieving global sustainable development goals. With the emergence of Large Vision-Language Models (LVLMs), new opportunities have arisen to sol...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 23. Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding...
[31.10.2025 16:15] ********************************************************************************
[31.10.2025 16:15] Abstract 24. This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-t...
[31.10.2025 16:15] Read previous papers.
[31.10.2025 16:15] Generating reviews via LLM API.
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#alignment", "#optimization", "#benchmark"], "emoji": "üéõÔ∏è", "ru": {"title": "–ú–æ–¥–µ–ª—å —Å–∞–º–∞ —É—á–∏—Ç—Å—è —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–≤–æ–µ–π —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ AutoDeco ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#inference", "#multimodal", "#rl", "#agi", "#cv", "#games"], "emoji": "üåç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ —Å –µ–¥–∏–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Emu3.5 ‚Äî –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è 
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#optimization", "#benchmark", "#diffusion", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–∞–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "ORCA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–∞–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö text-to-image –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#optimization", "#long_context", "#training"], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—Ö–æ–¥–∏—Ç –ø–æ–ª–Ω–æ–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Kimi Linear —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –≤–ø
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#video", "#reasoning", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–≤–∏–∂–∫–∏: –æ–±–µ—â–∞–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏, –º–æ–∂–µ—Ç –ª–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ Veo-3 –≤—ã—Å—Ç—É–ø–∞—Ç—å –≤ —Ä–æ–ª–∏ zero-shot reasoner –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –≤–∏–∑
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#math", "#benchmark", "#reasoning"], "emoji": "üèÜ", "ru": {"title": "–û–ª–∏–º–ø–∏–∞–¥–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ —Å—Ç–∞–≤–∏—Ç LLM –≤ —Ç—É–ø–∏–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ AMO-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM —Å –∑–∞–¥–∞—á–∞–º–∏ —É—Ä–æ–≤–Ω—è –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã –∏ –≤—ã—à–µ. –ë
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#data", "#dataset", "#open_source", "#cv", "#multimodal", "#diffusion", "#transfer_learning"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –ø–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∞–Ω–∏–º–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–¥–≤–∏–∂–µ–Ω–∏–π
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#agents", "#agi", "#optimization"], "emoji": "üèÑ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ª—é–±—ã–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Surfer 2 ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI-–∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#3d", "#synthetic", "#games"], "emoji": "üåê", "ru": {"title": "–û—Ç –ø–∞–Ω–æ—Ä–∞–º –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º 3D-—Å—Ü–µ–Ω–∞–º —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniX ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏ –≥–æ—Ç–æ–≤—ã—Ö 3D-—Å—Ü–µ–Ω –∏–∑ –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#rl", "#inference", "#math", "#reasoning", "#agents", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: AI-–∞–≥–µ–Ω—Ç—ã —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è (AsyncThink) –¥–ª—è LLM, –≥–¥–µ –º–æ–¥–µ–ª—å –æ—Ä–≥–∞–Ω–∏–∑
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#reasoning", "#benchmark", "#games"], "emoji": "üéÆ", "ru": {"title": "–ê—Ç–ª–∞—Å –æ—Ç OpenAI: —Å–∏–ª—ë–Ω –≤ –ª–æ–≥–∏–∫–µ, —Å–ª–∞–± –≤ —Ä–µ–∞–∫—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å ChatGPT Atlas, –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ —á–µ—Ä–µ–∑ –∫—É—Ä—Å–æ—Ä –∏ –∫–ª–∞–≤–∏–∞—Ç—É
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#agents", "#training", "#small_models", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤–æ–µ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Supervised Reinforcement Learning (SRL), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Ä–µ—à–∞—Ç
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#alignment", "#training", "#optimization", "#dataset", "#benchmark", "#cv"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞–ø—Ä—è–º—É—é", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –Ω–µ–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, —á—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–æ–æ—Ç
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#healthcare", "#reasoning", "#training", "#science"], "emoji": "üè•", "ru": {"title": "EHR-R1: AI-–º–æ–¥–µ–ª—å —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫–∞—Ä—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EHR-R1 ‚Äî —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#training", "#transfer_learning", "#data", "#architecture"], "emoji": "üì∞", "ru": {"title": "OmniLayout: –ú–∏–ª–ª–∏–æ–Ω —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–∞–∫–µ—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ OmniLayout-1M ‚Äî –ø–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–∏–ª–ª–∏–æ–Ω–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–∞
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#ethics", "#reasoning", "#agents"], "emoji": "ü§ù", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–∞ —Ä—ã–Ω–∫–µ: —Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ–±–µ–∂–¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Ä—ã–Ω–∫–∞—Ö, –≥–¥–µ –æ–¥–Ω–∏ –∞–≥–µ–Ω—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π, –∞ –¥—Ä—É–≥–∏–µ ‚Äî –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ –±–∏–∑–Ω–µ—Å—ã. –î
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#data", "#rl", "#multimodal", "#training", "#healthcare", "#reasoning", "#synthetic", "#open_source", "#dataset"], "emoji": "üè•", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MedVLSynther ‚Äî —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#agents", "#science", "#reasoning", "#benchmark"], "emoji": "üè¢", "ru": {"title": "–†–µ–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç—Ä—É–¥–∞: AI –ø–æ–∫–∞ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å 2.5% –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Remote Labor Index (RLI) ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#3d", "#dataset"], "emoji": "üß©", "ru": {"title": "–ü–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–∞—Å—Ç–µ–π: –∫–∞–∂–¥–æ–π –¥–µ—Ç–∞–ª–∏ —Å–≤–æ—ë –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FullPart ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ —á–∞—Å—Ç—è–º, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ—è–≤–Ω—ã–µ (implicit) –∏ —è–≤–Ω—ã–µ (
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#cv", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å —ç—Ñ—Ñ–µ–∫—Ç–æ–º –ú–∞—Ç—Ñ–µ—è –≤ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLMs): –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å 
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#benchmark", "#dataset"], "emoji": "üëì", "ru": {"title": "CRAG-MM: –ë–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —É–º–Ω—ã—Ö –æ—á–∫–æ–≤ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ª–æ–≥–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CRAG-MM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º, –æ—Ç–≤–µ—á–∞—é—â–∏—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ–± –æ–∫—Ä—É–∂–∞—é—â–µ–º –º–∏—Ä–µ —á–µ—Ä–µ–∑ –Ω–æ—Å–∏–º—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#data", "#dataset", "#training"], "emoji": "üß¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–µ—Ä–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—É–±—Å—Ç—Ä–∞—Ç—ã —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞—Ç–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ EnzyControl ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä —Ñ–µ—Ä–º–µ–Ω—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#rl", "#dataset", "#multimodal", "#training"], "emoji": "üåÜ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ –±–ª–∞–≥–æ—Å–æ—Å—Ç–æ—è–Ω–∏–∏ –≥–æ—Ä–æ–¥–æ–≤ –ø–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ CityRiSE ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Å
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#cv", "#reasoning"], "emoji": "üìä", "ru": {"title": "ChartAB: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ VLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ChartAB –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π vision-language –º–æ–¥–µ–ª–µ–π (VLM) –≤ –∑–∞–¥–∞—á–∞—Ö –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
[31.10.2025 16:15] Using data from previous issue: {"categories": ["#low_resource", "#small_models", "#training", "#transfer_learning"], "emoji": "üéì", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–≥—É—Ç –ª–∏ –Ω–µ–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (100M –∏ 140M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª—å–∑—É –æ—Ç 
[31.10.2025 16:15] Renaming data file.
[31.10.2025 16:15] Renaming previous data. hf_papers.json to ./d/2025-10-31.json
[31.10.2025 16:15] Saving new data file.
[31.10.2025 16:15] Generating page.
[31.10.2025 16:15] Renaming previous page.
[31.10.2025 16:15] Renaming previous data. index.html to ./d/2025-10-31.html
[31.10.2025 16:15] Writing result.
[31.10.2025 16:15] Renaming log file.
[31.10.2025 16:15] Renaming previous data. log.txt to ./logs/2025-10-31_last_log.txt
