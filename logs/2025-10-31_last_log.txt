[31.10.2025 10:13] Read previous papers.
[31.10.2025 10:13] Generating top page (month).
[31.10.2025 10:13] Writing top page (month).
[31.10.2025 11:10] Read previous papers.
[31.10.2025 11:10] Get feed.
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26583
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15510
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26802
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26692
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26768
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26794
[31.10.2025 11:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.19949
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26800
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26658
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25992
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26298
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25628
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25897
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26213
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25779
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26787
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26140
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26474
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26160
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.25132
[31.10.2025 11:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.26781
[31.10.2025 11:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[31.10.2025 11:10] No deleted papers detected.
[31.10.2025 11:10] Downloading and parsing papers (pdf, html). Total: 21.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26583.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26583.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26583.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.15510.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.15510.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.15510.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26802.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26802.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26802.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26692.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26692.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26692.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26768.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26768.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26768.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26794.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26794.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26794.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.19949.
[31.10.2025 11:10] Downloading paper 2510.19949 from http://arxiv.org/pdf/2510.19949v2...
[31.10.2025 11:10] Extracting affiliations from text.
[31.10.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 2 9 4 9 9 1 . 0 1 5 2 : r Surfer 2 The Next Generation of Cross-Platform Computer Use Agents M. Andreux, M. Bakler, Y. Barbier, H. Benchekroun, E. Bire, A. Bonnet, R. Bordie, N. Bout, M. Brunel, A. Cambray, P.-L. Cedoz, A. Chassang, G. Cloix, E. Connelly, A. D. Constantinou, R. De Coster, H. de La Jonqui`ere, A. Delfosse, M. Delpit, A. Deprez, A. Derupti, M. Diaz, S. DSouza, J. Dujardin, A. Edmund, M. Eickenberg, A. Fatalot, W. Felissi, I. Herring, X. Koegler, E. Le Jumeau de Kergaradec, A. Lac, M. Langevin, C. Lauverjat, A. Loison, A. Manevich, A. Moyal, A. Nguyen Kerbel, M. Parovic, J. Revelle, G. Richard, M.L. Richter, R. Riochet, M. Santos, R. Savidan, L. Sifre, M. Theillard, M. Thibault, I. Valentini, T. Wu, L. Yie, K. Yuan, J. Zubovskij Company Alphabetical order October 2025 Abstract Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for next-generation vision language model to achieve Pareto-optimal cost-efficiency. Figure 1: Surfer 2 state-of-the-art performance on WebVoyager, WebArena, OSWorld E2E, and AndroidWorld"
[31.10.2025 11:10] Response: ```python
[]
```
[31.10.2025 11:10] Extracting affiliations from text.
[31.10.2025 11:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 2 9 4 9 9 1 . 0 1 5 2 : r Surfer 2 The Next Generation of Cross-Platform Computer Use Agents M. Andreux, M. Bakler, Y. Barbier, H. Benchekroun, E. Bire, A. Bonnet, R. Bordie, N. Bout, M. Brunel, A. Cambray, P.-L. Cedoz, A. Chassang, G. Cloix, E. Connelly, A. D. Constantinou, R. De Coster, H. de La Jonqui`ere, A. Delfosse, M. Delpit, A. Deprez, A. Derupti, M. Diaz, S. DSouza, J. Dujardin, A. Edmund, M. Eickenberg, A. Fatalot, W. Felissi, I. Herring, X. Koegler, E. Le Jumeau de Kergaradec, A. Lac, M. Langevin, C. Lauverjat, A. Loison, A. Manevich, A. Moyal, A. Nguyen Kerbel, M. Parovic, J. Revelle, G. Richard, M.L. Richter, R. Riochet, M. Santos, R. Savidan, L. Sifre, M. Theillard, M. Thibault, I. Valentini, T. Wu, L. Yie, K. Yuan, J. Zubovskij Company Alphabetical order October 2025 Abstract Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for next-generation vision language model to achieve Pareto-optimal cost-efficiency. Figure 1: Surfer 2 state-of-the-art performance on WebVoyager, WebArena, OSWorld E2E, and AndroidWorld. Human performance is indicated when available.Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) unlocked remarkable reasoning capabilities for agentic use cases [1]. However, turning these capabilities into reliable, generalpurpose agents that operate autonomously in Graphical User Interfaces (GUIs) on complex, real-world tasks remains challenging. In recent years, one dominant path has been to train increasingly large models with minimal scaffolding like tool-call [2, 3] to improve agentic capabilities [4, 5, 6]. In contrast, this work presents an alternative perspective: with proper orchestration and system design, existing state-of-the-art models can achieve human-level performance and exceed prior systems across multiple benchmarks. Prior approaches require environment-specific adaptations, such as DOM parsers for web navigation, accessibility trees for mobile interfaces, or specialized APIs for desktop applications, limiting generalization across diverse digital environments. This work introduces Surfer 2, unified, hierarchical agent architecture designed for complex tasks across desktop, web, and mobile environments using purely visual interaction. Surfer 2 comprises three components: Orchestrator (optional high-level planner), Navigator (lowlevel GUI executor), and Validator (evaluation module). Surfer 2 integrates third-party frontier models and Companys Holo1.5 models [7] in design that separates long-term strategic planning from shortterm tactical execution. key design insight in Surfer 2 is architectural flexibility. It can enable or bypass the Orchestrator to match task complexity. For long-horizon problems, the Orchestrator runs as highlevel planner in the plan-and-act style [8], where it decomposes the user task into verifiable goals, plans ahead, and delegates targeted subtasks to the Navigator. For simple tasks, the Orchestrator is bypassed and the Navigator is invoked directly. Built on our previous SurferH agent [9], the Navigator follows ReAct (reason+act) loop [10]. It perceives the environment purely via screenshots, reasons about the next step, and executes constrained keyboard and mouse-level controls with pixel-accurate UI localization from Holo1.5. Upon subtask completion, Validator inspects the latest screenshots, the execution history, and the proposed answer to assess subtask success in one of two ways: (1) if the Orchestrator is enabled, it leverages the Validators report and either advances to the next subgoal or replans accordingly, or (2) if the Orchestrator is disabled, the Validators feedback is sent directly to the Navigator, which integrates it into its reasoning and continues the ReAct loop until the task is completed or termination condition is reached. Without task-specific fine-tuning, Surfer 2 attains state-of-the-art results on four major benchmarks spanning different Computer Use environments (web browser, desktop, mobile): WebVoyager [11] and WebArena [12], OSWorld [13], and AndroidWorld [14]. On OSWorld and AndroidWorld, Surfer 2 surpasses the human baseline, underscoring that expert agent design is as crucial as model capability.The development of Computer Use agents capable of controlling computers, web browsers, and mobile devices represents key frontier in AI. In many real-world scenarios, agents encounter tools and software for which no API or Model Context Protocol (MCP) is available, leaving GUIs as the only viable control surface. Consequently, recent research has focused on enabling general-purpose agents to perceive, reason about, and act within GUIs, transforming visual interaction into universal interface for autonomous computer use."
[31.10.2025 11:10] Mistral response. {"id": "ba90783389d54297aec894ac71ce0ca0", "created": 1761909035, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1383, "total_tokens": 1391, "completion_tokens": 8}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Company\"]\n```"}}]}
[31.10.2025 11:10] Response: ```python
["Company"]
```
[31.10.2025 11:10] Deleting PDF ./assets/pdf/2510.19949.pdf.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26800.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26800.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26800.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26658.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26658.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26658.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.25992.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.25992.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.25992.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26298.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26298.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26298.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.25628.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.25628.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.25628.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.25897.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.25897.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.25897.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26213.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26213.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26213.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.25779.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.25779.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.25779.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26787.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26787.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26787.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26140.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26140.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26140.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26474.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26474.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26474.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26160.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26160.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26160.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.25132.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.25132.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.25132.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2510.26781.
[31.10.2025 11:10] Extra JSON file exists (./assets/json/2510.26781.json), skip PDF parsing.
[31.10.2025 11:10] Paper image links file exists (./assets/img_data/2510.26781.json), skip HTML parsing.
[31.10.2025 11:10] Success.
[31.10.2025 11:10] Enriching papers with extra data.
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 0. We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily de...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 1. ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.  					AI-generated summary 				 While pre-trained visual representations have significantly advanced imitation learning, they are...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 2. Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question ...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 3. We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA)...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 4. We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models ...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 5. Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 6. Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 7. There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance th...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 8. We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with lar...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 9. Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 10. OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynam...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 11. Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and l...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 12. Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 13. Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers wi...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 14. As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understa...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 15. AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economical...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 16. Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among ...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 17. Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating h...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 18. Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no compr...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 19. Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. T...
[31.10.2025 11:10] ********************************************************************************
[31.10.2025 11:10] Abstract 20. Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding...
[31.10.2025 11:10] Read previous papers.
[31.10.2025 11:10] Generating reviews via LLM API.
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#inference", "#multimodal", "#rl", "#agi", "#cv", "#games"], "emoji": "üåç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ —Å –µ–¥–∏–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Emu3.5 ‚Äî –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è 
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#agents", "#optimization", "#benchmark", "#diffusion", "#cv", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–∞–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "ORCA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–∞–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö text-to-image –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#video", "#reasoning", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–≤–∏–∂–∫–∏: –æ–±–µ—â–∞–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏–ª–∏, –º–æ–∂–µ—Ç –ª–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ Veo-3 –≤—ã—Å—Ç—É–ø–∞—Ç—å –≤ —Ä–æ–ª–∏ zero-shot reasoner –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –≤–∏–∑
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#optimization", "#long_context", "#training"], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—Ö–æ–¥–∏—Ç –ø–æ–ª–Ω–æ–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Kimi Linear —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –≤–ø
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#math", "#benchmark", "#reasoning"], "emoji": "üèÜ", "ru": {"title": "–û–ª–∏–º–ø–∏–∞–¥–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ —Å—Ç–∞–≤–∏—Ç LLM –≤ —Ç—É–ø–∏–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ AMO-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM —Å –∑–∞–¥–∞—á–∞–º–∏ —É—Ä–æ–≤–Ω—è –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã –∏ –≤—ã—à–µ. –ë
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#data", "#dataset", "#open_source", "#cv", "#multimodal", "#diffusion", "#transfer_learning"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –ø–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∞–Ω–∏–º–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–¥–≤–∏–∂–µ–Ω–∏–π
[31.10.2025 11:10] Querying the API.
[31.10.2025 11:10] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency.
[31.10.2025 11:10] Response: ```json
{
  "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ª—é–±—ã–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏",
  "emoji": "üèÑ",
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Surfer 2 ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI-–∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ —Å–ø–æ—Å–æ–±–Ω–∞ —É–ø—Ä–∞–≤–ª—è—Ç—å –≤–µ–±-–±—Ä–∞—É–∑–µ—Ä–∞–º–∏, –¥–µ—Å–∫—Ç–æ–ø–Ω—ã–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏ –∏ –º–æ–±–∏–ª—å–Ω—ã–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫—É —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö –∑–∞–¥–∞—á. Surfer 2 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö (97.1% –Ω–∞ WebVoyager, 69.6% –Ω–∞ WebArena, 60.1% –Ω–∞ OSWorld –∏ 87.1% –Ω–∞ AndroidWorld) –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–æ–±—É—á–∫–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏. –ü—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ–ø—ã—Ç–∫–∞—Ö —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è —É—Å–∏–ª–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ foundation models –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ."
}
```
[31.10.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency."

[31.10.2025 11:10] Response: ```python
['AGENTS', 'ARCHITECTURE', 'BENCHMARK']
```
[31.10.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency."

[31.10.2025 11:10] Response: ```python
["AGI", "OPTIMIZATION"]
```
[31.10.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Surfer 2 is a new system designed to help agents work well across different platforms like web, desktop, and mobile without needing special adjustments for each one. It uses visual observations to understand and interact with these environments, achieving high accuracy in various tasks. The system combines advanced techniques like hierarchical context management and self-verification to ensure it can handle long and complex tasks reliably. Surfer 2 not only surpasses previous systems in performance but also exceeds human capabilities in several benchmarks, highlighting the potential of unified architectures in machine learning.","title":"Surfer 2: Unifying Cross-Platform Agent Performance through Visual Interaction"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Surfer 2 is a new system designed to help agents work well across different platforms like web, desktop, and mobile without needing special adjustments for each one. It uses visual observations to understand and interact with these environments, achieving high accuracy in various tasks. The system combines advanced techniques like hierarchical context management and self-verification to ensure it can handle long and complex tasks reliably. Surfer 2 not only surpasses previous systems in performance but also exceeds human capabilities in several benchmarks, highlighting the potential of unified architectures in machine learning.', title='Surfer 2: Unifying Cross-Platform Agent Performance through Visual Interaction'))
[31.10.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜSurfer 2ÔºåËøôÊòØ‰∏ÄÁßçÁªü‰∏ÄÊû∂ÊûÑÁöÑÊô∫ËÉΩ‰ΩìÔºåËÉΩÂ§üÂú®ÁΩëÈ°µ„ÄÅÊ°åÈù¢ÂíåÁßªÂä®ÁéØÂ¢É‰∏≠ËøõË°åË∑®Âπ≥Âè∞Êìç‰Ωú„ÄÇËØ•Á≥ªÁªü‰ªÖ‰æùËµñËßÜËßâËßÇÂØüÔºåÈõÜÊàê‰∫ÜÂ±ÇÊ¨°Âåñ‰∏ä‰∏ãÊñáÁÆ°ÁêÜ„ÄÅËß£ËÄ¶ÁöÑËßÑÂàí‰∏éÊâßË°å‰ª•ÂèäËá™ÊàëÈ™åËØÅ‰∏éËá™ÈÄÇÂ∫îÊÅ¢Â§çÂäüËÉΩ„ÄÇSurfer 2Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂáÜÁ°ÆÁéáÂàÜÂà´‰∏∫WebVoyager 97.1%„ÄÅWebArena 69.6%„ÄÅOSWorld 60.1%ÂíåAndroidWorld 87.1%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÈÄöËøáÁ≥ªÁªüÂåñÁöÑÂçèË∞ÉÔºåSurfer 2ËÉΩÂ§üÊòæËëóÊèêÂçáÂü∫Á°ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåÂÆûÁé∞‰ªÖÈÄöËøáËßÜËßâ‰∫§‰∫íËøõË°åÈÄöÁî®ËÆ°ÁÆóÊú∫ÊéßÂà∂„ÄÇ","title":"Ë∑®Âπ≥Âè∞Êô∫ËÉΩ‰ΩìÁöÑËßÜËßâ‰∫§‰∫íÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜSurfer 2ÔºåËøôÊòØ‰∏ÄÁßçÁªü‰∏ÄÊû∂ÊûÑÁöÑÊô∫ËÉΩ‰ΩìÔºåËÉΩÂ§üÂú®ÁΩëÈ°µ„ÄÅÊ°åÈù¢ÂíåÁßªÂä®ÁéØÂ¢É‰∏≠ËøõË°åË∑®Âπ≥Âè∞Êìç‰Ωú„ÄÇËØ•Á≥ªÁªü‰ªÖ‰æùËµñËßÜËßâËßÇÂØüÔºåÈõÜÊàê‰∫ÜÂ±ÇÊ¨°Âåñ‰∏ä‰∏ãÊñáÁÆ°ÁêÜ„ÄÅËß£ËÄ¶ÁöÑËßÑÂàí‰∏éÊâßË°å‰ª•ÂèäËá™ÊàëÈ™åËØÅ‰∏éËá™ÈÄÇÂ∫îÊÅ¢Â§çÂäüËÉΩ„ÄÇSurfer 2Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂáÜÁ°ÆÁéáÂàÜÂà´‰∏∫WebVoyager 97.1%„ÄÅWebArena 69.6%„ÄÅOSWorld 60.1%ÂíåAndroidWorld 87.1%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåÈÄöËøáÁ≥ªÁªüÂåñÁöÑÂçèË∞ÉÔºåSurfer 2ËÉΩÂ§üÊòæËëóÊèêÂçáÂü∫Á°ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåÂÆûÁé∞‰ªÖÈÄöËøáËßÜËßâ‰∫§‰∫íËøõË°åÈÄöÁî®ËÆ°ÁÆóÊú∫ÊéßÂà∂„ÄÇ', title='Ë∑®Âπ≥Âè∞Êô∫ËÉΩ‰ΩìÁöÑËßÜËßâ‰∫§‰∫íÊñ∞Á™ÅÁ†¥'))
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#3d", "#synthetic", "#games"], "emoji": "üåê", "ru": {"title": "–û—Ç –ø–∞–Ω–æ—Ä–∞–º –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º 3D-—Å—Ü–µ–Ω–∞–º —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniX ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏ –≥–æ—Ç–æ–≤—ã—Ö 3D-—Å—Ü–µ–Ω –∏–∑ –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#rl", "#inference", "#math", "#reasoning", "#agents", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: AI-–∞–≥–µ–Ω—Ç—ã —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è (AsyncThink) –¥–ª—è LLM, –≥–¥–µ –º–æ–¥–µ–ª—å –æ—Ä–≥–∞–Ω–∏–∑
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#optimization", "#agents", "#training", "#small_models", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤–æ–µ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Supervised Reinforcement Learning (SRL), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Ä–µ—à–∞—Ç
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#reasoning", "#benchmark", "#games"], "emoji": "üéÆ", "ru": {"title": "–ê—Ç–ª–∞—Å –æ—Ç OpenAI: —Å–∏–ª—ë–Ω –≤ –ª–æ–≥–∏–∫–µ, —Å–ª–∞–± –≤ —Ä–µ–∞–∫—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å ChatGPT Atlas, –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ —á–µ—Ä–µ–∑ –∫—É—Ä—Å–æ—Ä –∏ –∫–ª–∞–≤–∏–∞—Ç—É
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#healthcare", "#reasoning", "#training", "#science"], "emoji": "üè•", "ru": {"title": "EHR-R1: AI-–º–æ–¥–µ–ª—å —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫–∞—Ä—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EHR-R1 ‚Äî —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#alignment", "#training", "#optimization", "#dataset", "#benchmark", "#cv"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞–ø—Ä—è–º—É—é", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –Ω–µ–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, —á—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–æ–æ—Ç
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#training", "#transfer_learning", "#data", "#architecture"], "emoji": "üì∞", "ru": {"title": "OmniLayout: –ú–∏–ª–ª–∏–æ–Ω —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–∞–∫–µ—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ OmniLayout-1M ‚Äî –ø–µ—Ä–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–∏–ª–ª–∏–æ–Ω–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º–∞
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#multimodal", "#ethics", "#reasoning", "#agents"], "emoji": "ü§ù", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã –Ω–∞ —Ä—ã–Ω–∫–µ: —Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ–±–µ–∂–¥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Ä—ã–Ω–∫–∞—Ö, –≥–¥–µ –æ–¥–Ω–∏ –∞–≥–µ–Ω—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π, –∞ –¥—Ä—É–≥–∏–µ ‚Äî –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ –±–∏–∑–Ω–µ—Å—ã. –î
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#agents", "#science", "#reasoning", "#benchmark"], "emoji": "üè¢", "ru": {"title": "–†–µ–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç—Ä—É–¥–∞: AI –ø–æ–∫–∞ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å 2.5% –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Remote Labor Index (RLI) ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#3d", "#dataset"], "emoji": "üß©", "ru": {"title": "–ü–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–∞—Å—Ç–µ–π: –∫–∞–∂–¥–æ–π –¥–µ—Ç–∞–ª–∏ —Å–≤–æ—ë –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FullPart ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ —á–∞—Å—Ç—è–º, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ—è–≤–Ω—ã–µ (implicit) –∏ —è–≤–Ω—ã–µ (
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#cv", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å —ç—Ñ—Ñ–µ–∫—Ç–æ–º –ú–∞—Ç—Ñ–µ—è –≤ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLMs): –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å 
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#benchmark", "#dataset"], "emoji": "üëì", "ru": {"title": "CRAG-MM: –ë–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —É–º–Ω—ã—Ö –æ—á–∫–æ–≤ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ª–æ–≥–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CRAG-MM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º, –æ—Ç–≤–µ—á–∞—é—â–∏—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ–± –æ–∫—Ä—É–∂–∞—é—â–µ–º –º–∏—Ä–µ —á–µ—Ä–µ–∑ –Ω–æ—Å–∏–º—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#data", "#dataset", "#training"], "emoji": "üß¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–µ—Ä–º–µ–Ω—Ç–æ–≤ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—É–±—Å—Ç—Ä–∞—Ç—ã —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞—Ç–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ EnzyControl ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä —Ñ–µ—Ä–º–µ–Ω—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á
[31.10.2025 11:10] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#cv", "#reasoning"], "emoji": "üìä", "ru": {"title": "ChartAB: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ VLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ChartAB –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π vision-language –º–æ–¥–µ–ª–µ–π (VLM) –≤ –∑–∞–¥–∞—á–∞—Ö –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
[31.10.2025 11:10] Renaming data file.
[31.10.2025 11:10] Renaming previous data. hf_papers.json to ./d/2025-10-31.json
[31.10.2025 11:10] Saving new data file.
[31.10.2025 11:10] Generating page.
[31.10.2025 11:10] Renaming previous page.
[31.10.2025 11:10] Renaming previous data. index.html to ./d/2025-10-31.html
[31.10.2025 11:10] Writing result.
[31.10.2025 11:10] Renaming log file.
[31.10.2025 11:10] Renaming previous data. log.txt to ./logs/2025-10-31_last_log.txt
