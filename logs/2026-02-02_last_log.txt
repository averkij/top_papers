[02.02.2026 17:41] Read previous papers.
[02.02.2026 17:41] Generating top page (month).
[02.02.2026 17:41] Writing top page (month).
[02.02.2026 18:47] Read previous papers.
[02.02.2026 18:47] Get feed.
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21558
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23143
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22813
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22975
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22628
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21192
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23265
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23184
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23182
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22491
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20218
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21998
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21716
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13097
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22642
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22636
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21957
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21468
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18241
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21358
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23228
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23161
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22904
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22837
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23188
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21419
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20732
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15625
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22664
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22141
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21525
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23134
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22666
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22108
[02.02.2026 18:47] Extract page data from URL. URL: https://huggingface.co/papers/2601.22032
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21709
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21526
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22680
[02.02.2026 18:47] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21666
[02.02.2026 18:47] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.02.2026 18:47] No deleted papers detected.
[02.02.2026 18:47] Downloading and parsing papers (pdf, html). Total: 39.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21558.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21558.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21558.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23143.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23143.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23143.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22813.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22813.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22813.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22975.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22975.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22975.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22628.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22628.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22628.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21192.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21192.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21192.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23265.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23265.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23265.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23184.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23184.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23184.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23182.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23182.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23182.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22491.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22491.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22491.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.20218.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.20218.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.20218.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21998.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21998.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21998.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21716.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21716.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21716.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.13097.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.13097.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.13097.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22642.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22642.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22642.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22636.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22636.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22636.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21957.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21957.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21957.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21468.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21468.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21468.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.18241.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.18241.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.18241.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21358.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21358.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21358.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23228.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23228.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23228.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23161.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23161.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23161.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22904.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22904.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22904.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22837.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22837.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22837.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23188.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23188.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23188.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21419.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21419.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21419.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.20732.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.20732.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.20732.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.15625.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.15625.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.15625.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22664.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22664.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22664.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22141.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22141.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22141.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21525.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21525.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21525.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.23134.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.23134.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.23134.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22666.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22666.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22666.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22108.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22108.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22108.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22032.
[02.02.2026 18:47] Downloading paper 2601.22032 from https://arxiv.org/pdf/2601.22032v1...
[02.02.2026 18:47] Extracting affiliations from text.
[02.02.2026 18:47] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving Linhan Wang * 1 Zichong Yang 2 Chen Bai 3 Guoxiang Zhang 3 Xiaotong Liu 3 Xiaoyin Zheng 3 Xiao-Xiao Long 4 Chang-Tien Lu 1 Cheng Lu 3 6 2 0 2 9 2 ] . [ 1 2 3 0 2 2 . 1 0 6 2 : r Abstract End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with simple transformerbased decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting new state-of-the-art. The code is available at https://github.com/linhanwang/Drive-JEPA. 1. Introduction End-to-end autonomous driving (Pomerleau, 1988; Chitta et al., 2022; Hu et al., 2023b) has emerged as promising paradigm that directly maps raw sensor observations to driving actions using unified neural model. By eliminating hand-designed intermediate representations used in * Work done during an internship at XPENG Motors. 1Virginia Tech 2Purdue University 3XPENG Motors 4Nan"
[02.02.2026 18:47] Response: ```python
[
    "Virginia Tech",
    "Purdue University",
    "XPENG Motors",
    "Nanjing University of Aeronautics and Astronautics"
]
```
[02.02.2026 18:47] Deleting PDF ./assets/pdf/2601.22032.pdf.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21709.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21709.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21709.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21526.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21526.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21526.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.22680.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.22680.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.22680.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Downloading and parsing paper https://huggingface.co/papers/2601.21666.
[02.02.2026 18:47] Extra JSON file exists (./assets/json/2601.21666.json), skip PDF parsing.
[02.02.2026 18:47] Paper image links file exists (./assets/img_data/2601.21666.json), skip HTML parsing.
[02.02.2026 18:47] Success.
[02.02.2026 18:47] Enriching papers with extra data.
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 0. ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for mu...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 1. ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) a...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 2. Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  					AI-generated summary 				 The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to al...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 3. Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.  					AI-generated summary 				 ...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 4. TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the ...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 5. Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.  					AI-generated summary 				 State-of-the-art embedding models are increasingly de...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 6. _paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generatin...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 7. ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performa...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 8. Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.  					AI-generated summ...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 9. Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with ver...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 10. DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in huma...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 11. Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.  					AI-generated summary 				 This work highlights that video world modeling, alongside vision-language...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 12. DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character ima...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 13. RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.  					AI-generated summary 				 We present RM-RF, a lightweight reward model for run-free evaluation of automatica...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 14. A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their s...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 15. A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.  					AI-generated summary 				 Large Language Models (LLMs) are typica...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 16. A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.  					AI-generated summary 				 We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-a...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 17. MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 18. TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.  					AI-generated summary 				 While Large Language Models (LLMs) have shown promise in software e...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 19. PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but re...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 20. Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.  					AI-generated summary 				 While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning mult...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 21. DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio languag...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 22. A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 23. NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically ...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 24. Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.  					AI-generated summary 				 Deep search agents powered by large language models have demonstrated strong capab...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 25. Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.  					AI-generated summary 				 Recent advances in diffusion and flow matching mod...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 26. Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.  					AI-generated summary 				 As digital environments (data distributi...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 27. A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.  					AI-generated summary 				 Large language models (LLMs) can call tools effectively, yet ...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 28. RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 29. Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 30. Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many d...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 31. A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.  					AI-generated summary 				 In the post-Dennard era, optimizing embedded systems requi...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 32. ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.  					AI-generated summary 				 Open-vocabulary grounding requires accurate vi...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 33. V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.  					AI-generated summary 				 Can a small amount of verified goal information steer the expensive self-supervised pretraining of f...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 34. Drive-JEPA combines V-JEPA video pretraining with multimodal trajectory distillation to achieve state-of-the-art performance in end-to-end autonomous driving.  					AI-generated summary 				 End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable ...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 35. Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-s...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 36. KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.  					AI-generated summary 				 We introduce KAPSO, a modular framework for ...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 37. A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.  					AI-generated summary 				 We introduce the Visual Person...
[02.02.2026 18:47] ********************************************************************************
[02.02.2026 18:47] Abstract 38. A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are a major focus of re...
[02.02.2026 18:47] Read previous papers.
[02.02.2026 18:47] Generating reviews via LLM API.
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#data", "#benchmark", "#open_source", "#rl", "#agents", "#optimization", "#synthetic", "#training", "#reasoning"], "emoji": "üõ†Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "ASTRA ‚Äî —ç—Ç–æ –∞–≤—Ç
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#open_source", "#rl", "#training", "#reasoning"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–µ–∑ —É—á–∏—Ç–µ–ª–µ–π: —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "ThinkSafe ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞
[02.02.2026 18:47] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "Quartet II: –ø–æ–ª–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è NVFP4 –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ Quartet II –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ NVFP4 –Ω–∞ GPU NVIDIA Blackwell. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#rl", "#data", "#reasoning", "#small_models", "#dataset", "#training"], "emoji": "ü™ø", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è –∏–∑ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Golden Goose –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#benchmark", "#math", "#open_source", "#optimization", "#training", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è TTCS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ–ø–æ—Å—Ä
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#architecture", "#rlhf", "#interpretability", "#training"], "emoji": "üîÑ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ª—É—á—à–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è (embedding models), –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã", "desc": "PaperBanana ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –≤
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#multimodal", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–°–∂–∞—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "ReGuLaR –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üåä", "ru": {"title": "–û—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ –¥–µ—Ç–∞–ª—è–º: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–µ–¥—ë–Ω –∞–Ω–∞–ª–∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —á—Ç–æ –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ–¥–∏—Ä—É
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#rl", "#optimization", "#agents", "#reasoning"], "emoji": "üéæ", "ru": {"title": "–¢–æ—á–Ω—ã–π —É–¥–∞—Ä –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–µ—à–µ–Ω–∏–π: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "Sweet Spot Learning (SSL) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#benchmark", "#rlhf", "#diffusion", "#alignment", "#optimization", "#multimodal", "#training"], "emoji": "üéØ", "ru": {"title": "–ü–ª–æ—Ç–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –¥–µ–Ω–æ–π–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "DenseGRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow match
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#robotics", "#diffusion", "#video", "#architecture", "#open_source", "#training"], "emoji": "ü§ñ", "ru": {"title": "–í–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–µ–∑ —è–≤–Ω—ã—Ö –ø–æ–∑ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "DreamActor-M2 ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#optimization", "#training", "#science", "#plp", "#dataset", "#multilingual", "#open_source", "#small_models", "#data"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤ –±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç–∫–æ–Ω–æ–º–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ RM-RF ‚Äî –ª—ë–≥–∫–∞—è –º–æ–¥–µ–ª—å-–Ω–∞–≥—Ä–∞–¥–∞, –∫
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#benchmark", "#training", "#rlhf"], "emoji": "‚öôÔ∏è", "ru": {"title": "–§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∫ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–∞—è —Å–∏–ª–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –ª–æ–≥–∏—á–µ—Å–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#alignment", "#security"], "emoji": "üîì", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Å–ø–æ—Å–æ–± –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SABER –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ adversarial –∞—Ç–∞–∫–∞–º –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º sampling. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#small_models", "#cv", "#multimodal", "#open_source", "#dataset", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç PaddleOCR-VL-1.5, –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —Ä–∞–∑–º–µ—Ä
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#rl", "#agents", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–ª–≥–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "MemOCR ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–∂–∞—Ç–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#plp", "#agents", "#dataset", "#open_source", "#benchmark"], "emoji": "üß™", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ –ø–æ–ª–Ω–æ–º—É –∂–∏–∑–Ω–µ–Ω–Ω–æ–º—É —Ü–∏–∫–ª—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "TAM-Eval ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–¥
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–û—Ç–¥–µ–ª–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –æ—Ç —Å–ª–æ–≤: –≥–∏–±–∫–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —à–∞–≥–æ–≤", "desc": "PLaT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, —Ä–∞–∑–¥–µ–ª—è—è –ø—Ä–æ—Ü–µ—Å—Å
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#training", "#math"], "emoji": "ü§ù", "ru": {"title": "–ü–æ—à–∞–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –∫ —É—Å–ø–µ—Ö—É —Å–∏—Å—Ç–µ–º—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ MAPPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –æ—Ç AI –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#rlhf", "#diffusion", "#audio", "#training", "#open_source", "#architecture"], "emoji": "üéµ", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ", "desc": "DIFFA-2 ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–Ω
[02.02.2026 18:47] Using data from previous issue: {"categories": [], "emoji": "üåê", "ru": {"title": "–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä: –æ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DINO-SAE ‚Äî –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Vision Foundation Models —Å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–∏–∫—Å–µ–ª–µ–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
[02.02.2026 18:47] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "NativeTok –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤–≤–æ–¥–∏—Ç –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#agents"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Deep Search with Meta-Cognitive Monitoring (DS-MCM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Å –ø—Ä–µ–¥—Å–∫–∞
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üéØ", "ru": {"title": "–Ø–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø–æ—Ç–æ–∫–µ: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–¥–≤–∏–≥–∏ –≤ –¥–æ–º–µ–Ω–∞—Ö –∏ —Ä–∞–∑—Ä
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üîß", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫–∏ –≤ –∑–Ω–∞–Ω–∏—è: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–µ–µ—Å—è –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è tool-use –º–æ–¥–µ–ª–µ–π", "desc": "–§ission-GRPO ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –û—Å–Ω–æ–≤–Ω–∞
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#training", "#rlhf", "#alignment"], "emoji": "üéØ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã –≤ –º–µ—Ç–æ–¥–µ RLHF, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –Ω–∞–≥—Ä–∞–¥—ã –∏ –ø–æ–ª–∏—Ç–∏
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üé´", "ru": {"title": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–ª–µ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π ¬´Routing the Lottery¬ª, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Å–µ—Ç–µ
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context"], "emoji": "üß©", "ru": {"title": "–û—Ä–∏–µ–Ω—Ç–∏—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Landmark pooling, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Å–≤–µ—Ä—Ç
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –≥–µ—Ç–µ—Ä–æ
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#cv", "#architecture", "#alignment", "#multimodal"], "emoji": "üîó", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π", "desc": "ExpAlign ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ
[02.02.2026 18:47] Using data from previous issue: {"categories": ["#reasoning", "#small_models", "#optimization", "#transfer_learning", "#multimodal", "#training"], "emoji": "üéØ", "ru": {"title": "–¶–µ–ª–µ–≤–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞: –Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á", "desc": "V-Pretraining ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω
[02.02.2026 18:47] Querying the API.
[02.02.2026 18:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Drive-JEPA combines V-JEPA video pretraining with multimodal trajectory distillation to achieve state-of-the-art performance in end-to-end autonomous driving.  					AI-generated summary 				 End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.
[02.02.2026 18:47] Response: ```json
{
  "desc": "Drive-JEPA –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∏–¥–µ–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ V-JEPA —Å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–∫–≤–æ–∑–Ω–æ–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏. –ú–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Joint-Embedding Predictive Architecture –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–∞—Ö –≤–∏–¥–µ–æ –¥–∞–Ω–Ω—ã—Ö –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ (–∫–æ–≥–¥–∞ –æ–¥–Ω–∞ —Å—Ü–µ–Ω–∞ –∏–º–µ–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π) –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫, –¥–∏—Å—Ç–∏–ª–ª–∏—Ä—É—é—â–∏–π —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏–∑ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞ –≤–º–µ—Å—Ç–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ NAVSIM, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ 3 –ø—É–Ω–∫—Ç–∞ –ø–æ –º–µ—Ç—Ä–∏–∫–µ PDMS.",
  "emoji": "üöó",
  "title": "–í–∏–¥–µ–æ-–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è"
}
```
[02.02.2026 18:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Drive-JEPA combines V-JEPA video pretraining with multimodal trajectory distillation to achieve state-of-the-art performance in end-to-end autonomous driving.  					AI-generated summary 				 End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art."

[02.02.2026 18:47] Response: ```python
["VIDEO", "MULTIMODAL", "ARCHITECTURE", "TRAINING"]
```
[02.02.2026 18:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Drive-JEPA combines V-JEPA video pretraining with multimodal trajectory distillation to achieve state-of-the-art performance in end-to-end autonomous driving.  					AI-generated summary 				 End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art."

[02.02.2026 18:47] Response: ```python
["TRANSFER_LEARNING", "SYNTHETIC"]
```

**Justification:**

1. **TRANSFER_LEARNING**: The paper explicitly discusses "self-supervised video pretraining to learn transferable planning representations" and describes how V-JEPA pretraining produces "predictive representations" that transfer to the autonomous driving task.

2. **SYNTHETIC**: The paper mentions "simulator-generated trajectories" that are used alongside human trajectories for training the multimodal trajectory distillation component, which constitutes the use of synthetic data for training.
[02.02.2026 18:47] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "SYNTHETIC"]


**Justification:**

1. **TRANSFER_LEARNING**: The paper explicitly discusses "self-supervised video pretraining to learn transferable planning representations" and describes how V-JEPA pretraining produces "predictive representations" that transfer to the autonomous driving task.

2. **SYNTHETIC**: The paper mentions "simulator-generated trajectories" that are used alongside human trajectories for training the multimodal trajectory distillation component, which constitutes the use of synthetic data for training.
[02.02.2026 18:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Drive-JEPA is a novel framework that enhances end-to-end autonomous driving by combining V-JEPA video pretraining with multimodal trajectory distillation. It utilizes a Vision Transformer (ViT) encoder trained on extensive driving videos to create predictive representations that aid in trajectory planning. The framework also incorporates a proposal-centric planner that distills both simulator-generated and human trajectories, ensuring diverse and safe driving behaviors. As a result, Drive-JEPA achieves superior performance on the NAVSIM benchmark, setting new records in perception-free driving tasks.","title":"Drive-JEPA: Revolutionizing Autonomous Driving with Multimodal Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Drive-JEPA is a novel framework that enhances end-to-end autonomous driving by combining V-JEPA video pretraining with multimodal trajectory distillation. It utilizes a Vision Transformer (ViT) encoder trained on extensive driving videos to create predictive representations that aid in trajectory planning. The framework also incorporates a proposal-centric planner that distills both simulator-generated and human trajectories, ensuring diverse and safe driving behaviors. As a result, Drive-JEPA achieves superior performance on the NAVSIM benchmark, setting new records in perception-free driving tasks.', title='Drive-JEPA: Revolutionizing Autonomous Driving with Multimodal Learning'))
[02.02.2026 18:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Drive-JEPAÊòØ‰∏ÄÁßçÁªìÂêà‰∫ÜËßÜÈ¢ëËÅîÂêàÂµåÂÖ•È¢ÑÊµãÊû∂ÊûÑÔºàV-JEPAÔºâÂíåÂ§öÊ®°ÊÄÅËΩ®ËøπËí∏È¶èÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑËá™Âä®È©æÈ©∂„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®Â§ßËßÑÊ®°È©æÈ©∂ËßÜÈ¢ë‰∏äÈ¢ÑËÆ≠ÁªÉViTÁºñÁ†ÅÂô®ÔºåÁîüÊàê‰∏éËΩ®ËøπËßÑÂàíÂØπÈΩêÁöÑÈ¢ÑÊµãË°®Á§∫„ÄÇÂÆÉËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßç‰ª•ÊèêÊ°à‰∏∫‰∏≠ÂøÉÁöÑËßÑÂàíÂô®ÔºåËí∏È¶èÂ§öÊ†∑ÂåñÁöÑÊ®°ÊãüÂô®ÁîüÊàêËΩ®ËøπÂíå‰∫∫Á±ªËΩ®ËøπÔºå‰ª•‰øÉËøõÁ®≥ÂÆöÂíåÂÆâÂÖ®ÁöÑË°å‰∏∫„ÄÇDrive-JEPAÂú®NAVSIM‰∏äÁöÑËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂÖ∂ÊÄßËÉΩË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊñπÊ≥ïÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇ","title":"Drive-JEPAÔºöËá™Âä®È©æÈ©∂ÁöÑÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Drive-JEPAÊòØ‰∏ÄÁßçÁªìÂêà‰∫ÜËßÜÈ¢ëËÅîÂêàÂµåÂÖ•È¢ÑÊµãÊû∂ÊûÑÔºàV-JEPAÔºâÂíåÂ§öÊ®°ÊÄÅËΩ®ËøπËí∏È¶èÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞Á´ØÂà∞Á´ØÁöÑËá™Âä®È©æÈ©∂„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®Â§ßËßÑÊ®°È©æÈ©∂ËßÜÈ¢ë‰∏äÈ¢ÑËÆ≠ÁªÉViTÁºñÁ†ÅÂô®ÔºåÁîüÊàê‰∏éËΩ®ËøπËßÑÂàíÂØπÈΩêÁöÑÈ¢ÑÊµãË°®Á§∫„ÄÇÂÆÉËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßç‰ª•ÊèêÊ°à‰∏∫‰∏≠ÂøÉÁöÑËßÑÂàíÂô®ÔºåËí∏È¶èÂ§öÊ†∑ÂåñÁöÑÊ®°ÊãüÂô®ÁîüÊàêËΩ®ËøπÂíå‰∫∫Á±ªËΩ®ËøπÔºå‰ª•‰øÉËøõÁ®≥ÂÆöÂíåÂÆâÂÖ®ÁöÑË°å‰∏∫„ÄÇDrive-JEPAÂú®NAVSIM‰∏äÁöÑËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂÖ∂ÊÄßËÉΩË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊñπÊ≥ïÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇ', title='Drive-JEPAÔºöËá™Âä®È©æÈ©∂ÁöÑÊñ∞Á™ÅÁ†¥'))
[02.02.2026 18:48] Using data from previous issue: {"categories": [], "emoji": "‚è∞", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –µ–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ TAPPA –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ
[02.02.2026 18:48] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#plp", "#agents"], "emoji": "üß†", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º —á–µ—Ä–µ–∑ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é", "desc": "KAPSO ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º, –∫–æ—Ç–æ—Ä–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞
[02.02.2026 18:48] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#benchmark", "#video", "#rag", "#3d"], "emoji": "üé≠", "ru": {"title": "–ö–æ–≥–¥–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ–æ—Ç–ª–∏—á–∏–º –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Visual Personalization Turing Test (VPTT) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥
[02.02.2026 18:48] Using data from previous issue: {"categories": ["#dataset", "#audio", "#multimodal", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ-–∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç SONIC-O1, –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ
[02.02.2026 18:48] Renaming data file.
[02.02.2026 18:48] Renaming previous data. hf_papers.json to ./d/2026-02-02.json
[02.02.2026 18:48] Saving new data file.
[02.02.2026 18:48] Generating page.
[02.02.2026 18:48] Renaming previous page.
[02.02.2026 18:48] Renaming previous data. index.html to ./d/2026-02-02.html
[02.02.2026 18:48] Writing result.
[02.02.2026 18:48] Renaming log file.
[02.02.2026 18:48] Renaming previous data. log.txt to ./logs/2026-02-02_last_log.txt
