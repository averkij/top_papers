[02.02.2026 11:37] Read previous papers.
[02.02.2026 11:37] Generating top page (month).
[02.02.2026 11:37] Writing top page (month).
[02.02.2026 13:03] Read previous papers.
[02.02.2026 13:03] Get feed.
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21558
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23143
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22628
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22491
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23265
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23184
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23182
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21957
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21468
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20218
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21716
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13097
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22642
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22636
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21358
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23188
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22904
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21419
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20732
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18241
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15625
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22664
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22141
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21525
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23161
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23134
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22837
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21709
[02.02.2026 13:03] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23228
[02.02.2026 13:03] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.02.2026 13:03] No deleted papers detected.
[02.02.2026 13:03] Downloading and parsing papers (pdf, html). Total: 29.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21558.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21558.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21558.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23143.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23143.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23143.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22628.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22628.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22628.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22491.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22491.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22491.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23265.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23265.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23265.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23184.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23184.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23184.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23182.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23182.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23182.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21957.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21957.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21957.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21468.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21468.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21468.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.20218.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.20218.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.20218.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21716.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21716.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21716.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.13097.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.13097.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.13097.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22642.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22642.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22642.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22636.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22636.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22636.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21358.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21358.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21358.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23188.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23188.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23188.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22904.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22904.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22904.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21419.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21419.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21419.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.20732.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.20732.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.20732.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.18241.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.18241.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.18241.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.15625.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.15625.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.15625.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22664.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22664.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22664.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22141.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22141.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22141.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21525.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21525.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21525.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23161.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23161.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23161.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23134.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23134.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23134.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.22837.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.22837.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.22837.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.21709.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.21709.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.21709.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Downloading and parsing paper https://huggingface.co/papers/2601.23228.
[02.02.2026 13:03] Extra JSON file exists (./assets/json/2601.23228.json), skip PDF parsing.
[02.02.2026 13:03] Paper image links file exists (./assets/img_data/2601.23228.json), skip HTML parsing.
[02.02.2026 13:03] Success.
[02.02.2026 13:03] Enriching papers with extra data.
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 0. ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for mu...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 1. ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) a...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 2. TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the ...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 3. Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with ver...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 4. _paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generatin...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 5. ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performa...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 6. Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.  					AI-generated summ...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 7. A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.  					AI-generated summary 				 We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-a...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 8. MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 9. DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in huma...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 10. DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character ima...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 11. RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.  					AI-generated summary 				 We present RM-RF, a lightweight reward model for run-free evaluation of automatica...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 12. A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their s...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 13. A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.  					AI-generated summary 				 Large Language Models (LLMs) are typica...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 14. PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but re...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 15. Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.  					AI-generated summary 				 Deep search agents powered by large language models have demonstrated strong capab...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 16. A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 17. Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.  					AI-generated summary 				 Recent advances in diffusion and flow matching mod...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 18. Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.  					AI-generated summary 				 As digital environments (data distributi...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 19. TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.  					AI-generated summary 				 While Large Language Models (LLMs) have shown promise in software e...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 20. A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.  					AI-generated summary 				 Large language models (LLMs) can call tools effectively, yet ...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 21. RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 22. Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 23. Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many d...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 24. DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio languag...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 25. A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.  					AI-generated summary 				 In the post-Dennard era, optimizing embedded systems requi...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 26. NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically ...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 27. Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-s...
[02.02.2026 13:03] ********************************************************************************
[02.02.2026 13:03] Abstract 28. Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.  					AI-generated summary 				 While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning mult...
[02.02.2026 13:03] Read previous papers.
[02.02.2026 13:03] Generating reviews via LLM API.
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#data", "#benchmark", "#open_source", "#rl", "#agents", "#optimization", "#synthetic", "#training", "#reasoning"], "emoji": "üõ†Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "ASTRA ‚Äî —ç—Ç–æ –∞–≤—Ç
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#open_source", "#rl", "#training", "#reasoning"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–µ–∑ —É—á–∏—Ç–µ–ª–µ–π: —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "ThinkSafe ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#benchmark", "#math", "#open_source", "#optimization", "#training", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è TTCS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ–ø–æ—Å—Ä
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#rl", "#optimization", "#agents", "#reasoning"], "emoji": "üéæ", "ru": {"title": "–¢–æ—á–Ω—ã–π —É–¥–∞—Ä –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–µ—à–µ–Ω–∏–π: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "Sweet Spot Learning (SSL) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã", "desc": "PaperBanana ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –≤
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#multimodal", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–°–∂–∞—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "ReGuLaR –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üåä", "ru": {"title": "–û—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ –¥–µ—Ç–∞–ª—è–º: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–µ–¥—ë–Ω –∞–Ω–∞–ª–∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —á—Ç–æ –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ–¥–∏—Ä—É
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#small_models", "#cv", "#multimodal", "#open_source", "#dataset", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç PaddleOCR-VL-1.5, –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —Ä–∞–∑–º–µ—Ä
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#rl", "#agents", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–ª–≥–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "MemOCR ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–∂–∞—Ç–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#benchmark", "#rlhf", "#diffusion", "#alignment", "#optimization", "#multimodal", "#training"], "emoji": "üéØ", "ru": {"title": "–ü–ª–æ—Ç–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –¥–µ–Ω–æ–π–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "DenseGRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow match
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–µ–∑ —è–≤–Ω—ã—Ö –ø–æ–∑ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "DreamActor-M2 ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#optimization", "#training", "#science", "#plp", "#dataset", "#multilingual", "#open_source", "#small_models", "#data"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤ –±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç–∫–æ–Ω–æ–º–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ RM-RF ‚Äî –ª—ë–≥–∫–∞—è –º–æ–¥–µ–ª—å-–Ω–∞–≥—Ä–∞–¥–∞, –∫
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#benchmark", "#training", "#rlhf"], "emoji": "‚öôÔ∏è", "ru": {"title": "–§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∫ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–∞—è —Å–∏–ª–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –ª–æ–≥–∏—á–µ—Å–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#alignment", "#security"], "emoji": "üîì", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Å–ø–æ—Å–æ–± –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SABER –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ adversarial –∞—Ç–∞–∫–∞–º –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º sampling. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–û—Ç–¥–µ–ª–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –æ—Ç —Å–ª–æ–≤: –≥–∏–±–∫–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —à–∞–≥–æ–≤", "desc": "PLaT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, —Ä–∞–∑–¥–µ–ª—è—è –ø—Ä–æ—Ü–µ—Å—Å
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#agents"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Deep Search with Meta-Cognitive Monitoring (DS-MCM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ
[02.02.2026 13:03] Using data from previous issue: {"categories": [], "emoji": "üåê", "ru": {"title": "–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä: –æ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DINO-SAE ‚Äî –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Vision Foundation Models —Å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–∏–∫—Å–µ–ª–µ–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Å –ø—Ä–µ–¥—Å–∫–∞
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üéØ", "ru": {"title": "–Ø–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø–æ—Ç–æ–∫–µ: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–¥–≤–∏–≥–∏ –≤ –¥–æ–º–µ–Ω–∞—Ö –∏ —Ä–∞–∑—Ä
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#plp", "#agents", "#dataset", "#open_source", "#benchmark"], "emoji": "üß™", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ –ø–æ–ª–Ω–æ–º—É –∂–∏–∑–Ω–µ–Ω–Ω–æ–º—É —Ü–∏–∫–ª—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "TAM-Eval ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–¥
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üîß", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫–∏ –≤ –∑–Ω–∞–Ω–∏—è: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–µ–µ—Å—è –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è tool-use –º–æ–¥–µ–ª–µ–π", "desc": "–§ission-GRPO ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –û—Å–Ω–æ–≤–Ω–∞
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#training", "#rlhf", "#alignment"], "emoji": "üéØ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã –≤ –º–µ—Ç–æ–¥–µ RLHF, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –Ω–∞–≥—Ä–∞–¥—ã –∏ –ø–æ–ª–∏—Ç–∏
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üé´", "ru": {"title": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–ª–µ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π ¬´Routing the Lottery¬ª, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Å–µ—Ç–µ
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context"], "emoji": "üß©", "ru": {"title": "–û—Ä–∏–µ–Ω—Ç–∏—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Landmark pooling, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Å–≤–µ—Ä—Ç
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#rlhf", "#diffusion", "#audio", "#training", "#open_source", "#architecture"], "emoji": "üéµ", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ", "desc": "DIFFA-2 ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–Ω
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –≥–µ—Ç–µ—Ä–æ
[02.02.2026 13:03] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "NativeTok –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤–≤–æ–¥–∏—Ç –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏
[02.02.2026 13:03] Using data from previous issue: {"categories": [], "emoji": "‚è∞", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –µ–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ TAPPA –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ
[02.02.2026 13:03] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#training", "#math"], "emoji": "ü§ù", "ru": {"title": "–ü–æ—à–∞–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –∫ —É—Å–ø–µ—Ö—É —Å–∏—Å—Ç–µ–º—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ MAPPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –æ—Ç AI –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á
[02.02.2026 13:03] Renaming data file.
[02.02.2026 13:03] Renaming previous data. hf_papers.json to ./d/2026-02-02.json
[02.02.2026 13:03] Saving new data file.
[02.02.2026 13:03] Generating page.
[02.02.2026 13:03] Renaming previous page.
[02.02.2026 13:03] Renaming previous data. index.html to ./d/2026-02-02.html
[02.02.2026 13:03] Writing result.
[02.02.2026 13:03] Renaming log file.
[02.02.2026 13:03] Renaming previous data. log.txt to ./logs/2026-02-02_last_log.txt
