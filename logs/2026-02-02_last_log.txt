[02.02.2026 15:39] Read previous papers.
[02.02.2026 15:39] Generating top page (month).
[02.02.2026 15:39] Writing top page (month).
[02.02.2026 16:42] Read previous papers.
[02.02.2026 16:42] Get feed.
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21558
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23143
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22628
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22813
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22975
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23265
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23184
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23182
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22491
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20218
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21716
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13097
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22642
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22636
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21957
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21468
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18241
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21358
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23161
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22904
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22837
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23228
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23188
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21419
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20732
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15625
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22664
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22141
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21525
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23134
[02.02.2026 16:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.22666
[02.02.2026 16:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21709
[02.02.2026 16:42] Extract page data from URL. URL: https://huggingface.co/papers/2601.21526
[02.02.2026 16:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.02.2026 16:42] No deleted papers detected.
[02.02.2026 16:42] Downloading and parsing papers (pdf, html). Total: 33.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.21558.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.21558.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.21558.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23143.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23143.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23143.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22628.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22628.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22628.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22813.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22813.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22813.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22975.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22975.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22975.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23265.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23265.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23265.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23184.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23184.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23184.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23182.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23182.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23182.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22491.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22491.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22491.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.20218.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.20218.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.20218.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.21716.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.21716.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.21716.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.13097.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.13097.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.13097.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22642.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22642.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22642.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22636.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22636.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22636.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.21957.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.21957.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.21957.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.21468.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.21468.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.21468.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.18241.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.18241.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.18241.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.21358.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.21358.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.21358.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23161.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23161.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23161.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22904.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22904.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22904.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22837.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22837.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22837.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23228.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23228.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23228.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23188.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23188.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23188.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.21419.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.21419.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.21419.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.20732.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.20732.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.20732.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.15625.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.15625.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.15625.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22664.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22664.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22664.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22141.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.22141.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.22141.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.21525.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.21525.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.21525.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.23134.
[02.02.2026 16:42] Extra JSON file exists (./assets/json/2601.23134.json), skip PDF parsing.
[02.02.2026 16:42] Paper image links file exists (./assets/img_data/2601.23134.json), skip HTML parsing.
[02.02.2026 16:42] Success.
[02.02.2026 16:42] Downloading and parsing paper https://huggingface.co/papers/2601.22666.
[02.02.2026 16:43] Downloading paper 2601.22666 from https://arxiv.org/pdf/2601.22666v1...
[02.02.2026 16:43] Extracting affiliations from text.
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Junyi Hu 1 Tian Bai 2 3 Fengyi Wu 2 Wenyan Li 4 Zhenming Peng 2 Yi Zhang "
[02.02.2026 16:43] Response: ```python
[]
```
[02.02.2026 16:43] Extracting affiliations from text.
[02.02.2026 16:43] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding Junyi Hu 1 Tian Bai 2 3 Fengyi Wu 2 Wenyan Li 4 Zhenming Peng 2 Yi ZhangOpen-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, theoretically grounded vision-language alignment framework built on principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attentionbased soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including Top-K multi-positive contrastive objective and Geometry-Aware Consistency Objective derived from Lagrangianconstrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 APr on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inferenceefficient. 6 2 0 2 0 3 ] . [ 1 6 6 6 2 2 . 1 0 6 2 : r 1. Introduction Large vision-language models (VLMs) enable powerful zero-shot transfer by aligning images and texts in shared embedding space (Radford et al., 2021; Li et al., 2023; Jia et al., 2021). Despite significant progress, precise spatial grounding, which involves localizing free-form textual 1Department of Automation, Tsinghua University, China 2University of Electronic Science and Technology of China, China 3Lingsu Lab, China 4University of Copenhagen, Denmark. Correspondence to: Yi Zhang <zhyi@mail.tsinghua.edu.cn>. Preprint. February 2, 2026. concepts within images, remains key challenge in dense prediction tasks such as open-vocabulary detection and segmentation (Kamath et al., 2021; Cai et al., 2022). Recent open-vocabulary methods (Liu et al., 2024; Cheng et al., 2024; Wang et al., 2025; Fu et al., 2025) alleviate vocabulary constraints but often struggle with complex linguistic phenomena, including negation, relations, and compositional descriptions, when fine-grained localization is required. Recent theoretical analysis reveals an inherent limitation of CLIP-style joint embeddings: collapsing prompt into single global representation cannot simultaneously encode attribute binding, spatial relations, and negation under cosine similarity (Kang et al., 2025b). This geometric bottleneck motivates token-level vision-language alignment, where informative tokens are selectively emphasized rather than uniformly aggregated. However, incorporating tokenlevel reasoning into dense grounding remains nontrivial due to weak supervision and optimization instability. We propose ExpAlign, an expectation-guided visionlanguage alignment framework for open-vocabulary grounding. At its core is the Expectation Alignment Head (EAH), which produces prompt-conditioned spatial alignment maps by aggregating token-wise similarities through soft expectation mechanism. By treating spatial locations as latent instances and textual tokens as competing hypotheses, EAH performs implicit token selection without instance-level annotations, admitting natural interpretation as attentionbased soft pooling in multiple instance learning (MIL) (Ilse et al., 2018). To further improve discriminability and spatial coherence, we introduce two auxiliary objectives. multi-positive InfoNCE loss enforces prompt-level semantic separation under weak supervision, while Geometry-Aware Consistency Objective (GACO) regularizes alignment maps by emphasizing relatively consistent regions within each ground-truth mask. Together, they stabilize optimization and support both positive and negative prompts. Experiments on LVIS (Gupta et al., 2019), ODinW (Li et al., 2022), and RefCOCO/+/g (Yu et al., 2016) show that ExpAlign delivers strong open-vocabulary detection and segmentation performance under similar pre-training ExpAlign: Expectation-Guided VisionLanguage Alignment for Open-Vocabulary Grounding scale and model capacity to recent baselines. It achieves competitive or superior results on LVIS rare categories and ODinW subsets, while on RefCOCO/+/g it outperforms detection-focused methods such as YOLOE but trails specialized grounding models like Grounding DINO-T due to CLIPs limited spatial reasoning. dition, geometry-aware regularization has been explored in segmentation and structured prediction (Liang et al., 2021), but existing approaches typically rely on absolute geometric cues. In contrast, our geometry-aware consistency objective operates on relative instance statistics, encouraging coherent alignment without rigid spatial targets. 2. Related Work Sentence-level vision-language Alignment. visionlanguage pretraining methods such as CLIP (Radford et al., 2021) and BLIP (Li et al., 2023) align whole images with global text embeddings using contrastive objectives. These sentence-level alignment techniques have enabled strong zero-shot transfer for retrieval and classification, and have been adapted to open-vocabulary detection by using prompt embeddings as classifier proxies (Zhou et al., 2022). However, collapsing prompt into single vector can lose internal structure and limits fine-grained localization, motivating methods that exploit richer textual and visual interactions. Token-level and Phrase-level Alignment. To capture fine-grained semantics between language and vision, several works explicitly model interactions at the token or phrase level. For instance, GLIP and its variants unify localization and grounding by introducing region-word contrastive alignment and phrase grounding objectives that align phrases with corresponding image regions (Zhang et al., 2022), enabling the model to learn regiontoken correspondences beyond global text embeddings. Methods like X-VLM perform multi-grained visionlanguage pretraining that aligns text with visual concepts at varying granularities, leveraging patch-level or concept-level representations (Zeng et al., 2021). Works in temporal grounding also observe that treating all tokens uniformly under cross-modal attention fails to exploit word-le"
[02.02.2026 16:43] Mistral response. {"id": "d28fe354544f43f9aee6e68ceef0699d", "created": 1770050590, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1383, "total_tokens": 1434, "completion_tokens": 51, "num_cached_tokens": 1382}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Automation, Tsinghua University, China\",\n    \"University of Electronic Science and Technology of China, China\",\n    \"Lingsu Lab, China\",\n    \"University of Copenhagen, Denmark\"\n]\n```"}}]}
[02.02.2026 16:43] Response: ```python
[
    "Department of Automation, Tsinghua University, China",
    "University of Electronic Science and Technology of China, China",
    "Lingsu Lab, China",
    "University of Copenhagen, Denmark"
]
```
[02.02.2026 16:43] Deleting PDF ./assets/pdf/2601.22666.pdf.
[02.02.2026 16:43] Success.
[02.02.2026 16:43] Downloading and parsing paper https://huggingface.co/papers/2601.21709.
[02.02.2026 16:43] Extra JSON file exists (./assets/json/2601.21709.json), skip PDF parsing.
[02.02.2026 16:43] Paper image links file exists (./assets/img_data/2601.21709.json), skip HTML parsing.
[02.02.2026 16:43] Success.
[02.02.2026 16:43] Downloading and parsing paper https://huggingface.co/papers/2601.21526.
[02.02.2026 16:43] Downloading paper 2601.21526 from https://arxiv.org/pdf/2601.21526v1...
[02.02.2026 16:43] Extracting affiliations from text.
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 6 2 5 1 2 . 1 0 6 2 : r KAPSO: Knowledge-grounded framework for Autonomous Program Synthesis and Optimization Alireza Nadaf Alireza Mohammadshahi Majid Yazdani Leeroo Team {nadaf, alireza, my}@leeroo.com Abstract We introduce KAPSO, modular framework for autonomous program synthesis and optimization. Given natural-language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within long-horizon optimization loop, where progress is defined by evaluator outcomes. KAPSO targets long-horizon failures common in coding agentsincluding lost experimental state, brittle debugging, and weak reuse of domain expertiseby integrating three tightly coupled components. First, git-native experimentation engine isolates each attempt as branch, producing reproducible artifacts and preserving provenance across iterations. Second, knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence. We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance. Domain experts often know what they want to build, but turning that intent into reliable, runnable, and optimized software still requires repeated experimentation. In practice, successful development is an iterative process: "
[02.02.2026 16:43] Response: ```python
["Leeroo Team"]
```
[02.02.2026 16:43] Deleting PDF ./assets/pdf/2601.21526.pdf.
[02.02.2026 16:43] Success.
[02.02.2026 16:43] Enriching papers with extra data.
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 0. ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for mu...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 1. ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) a...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 2. TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the ...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 3. Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  					AI-generated summary 				 The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to al...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 4. Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.  					AI-generated summary 				 ...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 5. _paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generatin...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 6. ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performa...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 7. Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.  					AI-generated summ...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 8. Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with ver...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 9. DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in huma...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 10. DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character ima...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 11. RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.  					AI-generated summary 				 We present RM-RF, a lightweight reward model for run-free evaluation of automatica...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 12. A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their s...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 13. A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.  					AI-generated summary 				 Large Language Models (LLMs) are typica...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 14. A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.  					AI-generated summary 				 We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-a...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 15. MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 16. TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.  					AI-generated summary 				 While Large Language Models (LLMs) have shown promise in software e...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 17. PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but re...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 18. DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio languag...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 19. A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 20. NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically ...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 21. Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.  					AI-generated summary 				 While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning mult...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 22. Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.  					AI-generated summary 				 Deep search agents powered by large language models have demonstrated strong capab...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 23. Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.  					AI-generated summary 				 Recent advances in diffusion and flow matching mod...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 24. Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.  					AI-generated summary 				 As digital environments (data distributi...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 25. A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.  					AI-generated summary 				 Large language models (LLMs) can call tools effectively, yet ...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 26. RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 27. Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 28. Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many d...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 29. A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.  					AI-generated summary 				 In the post-Dennard era, optimizing embedded systems requi...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 30. ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.  					AI-generated summary 				 Open-vocabulary grounding requires accurate vi...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 31. Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-s...
[02.02.2026 16:43] ********************************************************************************
[02.02.2026 16:43] Abstract 32. KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.  					AI-generated summary 				 We introduce KAPSO, a modular framework for ...
[02.02.2026 16:43] Read previous papers.
[02.02.2026 16:43] Generating reviews via LLM API.
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#data", "#benchmark", "#open_source", "#rl", "#agents", "#optimization", "#synthetic", "#training", "#reasoning"], "emoji": "üõ†Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "ASTRA ‚Äî —ç—Ç–æ –∞–≤—Ç
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#open_source", "#rl", "#training", "#reasoning"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–µ–∑ —É—á–∏—Ç–µ–ª–µ–π: —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "ThinkSafe ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#benchmark", "#math", "#open_source", "#optimization", "#training", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è TTCS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ–ø–æ—Å—Ä
[02.02.2026 16:43] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "Quartet II: –ø–æ–ª–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è NVFP4 –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ Quartet II –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ NVFP4 –Ω–∞ GPU NVIDIA Blackwell. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#rl", "#data", "#reasoning", "#small_models", "#dataset", "#training"], "emoji": "ü™ø", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è –∏–∑ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Golden Goose –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã", "desc": "PaperBanana ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –≤
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#multimodal", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–°–∂–∞—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "ReGuLaR –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üåä", "ru": {"title": "–û—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ –¥–µ—Ç–∞–ª—è–º: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–µ–¥—ë–Ω –∞–Ω–∞–ª–∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —á—Ç–æ –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ–¥–∏—Ä—É
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#rl", "#optimization", "#agents", "#reasoning"], "emoji": "üéæ", "ru": {"title": "–¢–æ—á–Ω—ã–π —É–¥–∞—Ä –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–µ—à–µ–Ω–∏–π: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "Sweet Spot Learning (SSL) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#benchmark", "#rlhf", "#diffusion", "#alignment", "#optimization", "#multimodal", "#training"], "emoji": "üéØ", "ru": {"title": "–ü–ª–æ—Ç–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –¥–µ–Ω–æ–π–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "DenseGRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow match
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–µ–∑ —è–≤–Ω—ã—Ö –ø–æ–∑ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "DreamActor-M2 ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#optimization", "#training", "#science", "#plp", "#dataset", "#multilingual", "#open_source", "#small_models", "#data"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤ –±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç–∫–æ–Ω–æ–º–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ RM-RF ‚Äî –ª—ë–≥–∫–∞—è –º–æ–¥–µ–ª—å-–Ω–∞–≥—Ä–∞–¥–∞, –∫
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#benchmark", "#training", "#rlhf"], "emoji": "‚öôÔ∏è", "ru": {"title": "–§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∫ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–∞—è —Å–∏–ª–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –ª–æ–≥–∏—á–µ—Å–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#alignment", "#security"], "emoji": "üîì", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Å–ø–æ—Å–æ–± –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SABER –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ adversarial –∞—Ç–∞–∫–∞–º –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º sampling. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#small_models", "#cv", "#multimodal", "#open_source", "#dataset", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç PaddleOCR-VL-1.5, –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —Ä–∞–∑–º–µ—Ä
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#rl", "#agents", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–ª–≥–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "MemOCR ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–∂–∞—Ç–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#plp", "#agents", "#dataset", "#open_source", "#benchmark"], "emoji": "üß™", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ –ø–æ–ª–Ω–æ–º—É –∂–∏–∑–Ω–µ–Ω–Ω–æ–º—É —Ü–∏–∫–ª—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "TAM-Eval ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–¥
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–û—Ç–¥–µ–ª–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –æ—Ç —Å–ª–æ–≤: –≥–∏–±–∫–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —à–∞–≥–æ–≤", "desc": "PLaT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, —Ä–∞–∑–¥–µ–ª—è—è –ø—Ä–æ—Ü–µ—Å—Å
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#rlhf", "#diffusion", "#audio", "#training", "#open_source", "#architecture"], "emoji": "üéµ", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ", "desc": "DIFFA-2 ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–Ω
[02.02.2026 16:43] Using data from previous issue: {"categories": [], "emoji": "üåê", "ru": {"title": "–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä: –æ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DINO-SAE ‚Äî –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Vision Foundation Models —Å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–∏–∫—Å–µ–ª–µ–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
[02.02.2026 16:43] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "NativeTok –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤–≤–æ–¥–∏—Ç –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#training", "#math"], "emoji": "ü§ù", "ru": {"title": "–ü–æ—à–∞–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –∫ —É—Å–ø–µ—Ö—É —Å–∏—Å—Ç–µ–º—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ MAPPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –æ—Ç AI –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#agents"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Deep Search with Meta-Cognitive Monitoring (DS-MCM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Å –ø—Ä–µ–¥—Å–∫–∞
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üéØ", "ru": {"title": "–Ø–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø–æ—Ç–æ–∫–µ: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–¥–≤–∏–≥–∏ –≤ –¥–æ–º–µ–Ω–∞—Ö –∏ —Ä–∞–∑—Ä
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üîß", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫–∏ –≤ –∑–Ω–∞–Ω–∏—è: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–µ–µ—Å—è –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è tool-use –º–æ–¥–µ–ª–µ–π", "desc": "–§ission-GRPO ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –û—Å–Ω–æ–≤–Ω–∞
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#training", "#rlhf", "#alignment"], "emoji": "üéØ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã –≤ –º–µ—Ç–æ–¥–µ RLHF, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –Ω–∞–≥—Ä–∞–¥—ã –∏ –ø–æ–ª–∏—Ç–∏
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üé´", "ru": {"title": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–ª–µ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π ¬´Routing the Lottery¬ª, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Å–µ—Ç–µ
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context"], "emoji": "üß©", "ru": {"title": "–û—Ä–∏–µ–Ω—Ç–∏—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Landmark pooling, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Å–≤–µ—Ä—Ç
[02.02.2026 16:43] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –≥–µ—Ç–µ—Ä–æ
[02.02.2026 16:43] Querying the API.
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.  					AI-generated summary 				 Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP_r on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.
[02.02.2026 16:43] Response: ```json
{
  "desc": "ExpAlign ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—É–ª–∏–Ω–≥ –¥–ª—è –Ω–µ—è–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –î–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∞–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏ –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä—ë–º –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö.",
  "emoji": "üîó",
  "title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"
}
```
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.  					AI-generated summary 				 Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP_r on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient."

[02.02.2026 16:43] Response: ```python
['CV', 'MULTIMODAL', 'ARCHITECTURE']
```
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.  					AI-generated summary 				 Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP_r on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient."

[02.02.2026 16:43] Response: ```python
["ALIGNMENT"]
```
[02.02.2026 16:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ExpAlign is a framework designed to enhance vision-language alignment for tasks like open-vocabulary detection and zero-shot instance segmentation without needing extra annotations. It utilizes multiple instance learning and an attention-based pooling method to effectively align visual tokens with language descriptions. The framework introduces an Expectation Alignment Head that allows for implicit selection of relevant tokens and instances, improving the model\'s performance. Additionally, it incorporates a multi-scale consistency regularization approach to stabilize the learning process, leading to significant improvements in accuracy, especially for less common categories.","title":"Enhancing Vision-Language Alignment with ExpAlign"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="ExpAlign is a framework designed to enhance vision-language alignment for tasks like open-vocabulary detection and zero-shot instance segmentation without needing extra annotations. It utilizes multiple instance learning and an attention-based pooling method to effectively align visual tokens with language descriptions. The framework introduces an Expectation Alignment Head that allows for implicit selection of relevant tokens and instances, improving the model's performance. Additionally, it incorporates a multi-scale consistency regularization approach to stabilize the learning process, leading to significant improvements in accuracy, especially for less common categories.", title='Enhancing Vision-Language Alignment with ExpAlign'))
[02.02.2026 16:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ExpAlign ÊòØ‰∏Ä‰∏™ËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÊ°ÜÊû∂ÔºåÂà©Áî®Â§öÂÆû‰æãÂ≠¶‰π†ÂíåÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÊ±†ÂåñÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂºÄÊîæËØçÊ±áÊ£ÄÊµãÂíåÈõ∂Ê†∑Êú¨ÂÆû‰æãÂàÜÂâ≤ÁöÑÊïàÊûúÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÁöÑÊ†áÊ≥®„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊúüÊúõÂØπÈΩêÂ§¥ÂÆûÁé∞Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËΩØÂ§öÂÆû‰æãÊ±†ÂåñÔºåËÉΩÂ§üÂú®Ê≤°ÊúâÈ¢ùÂ§ñÊ†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÈöêÂºèÁöÑÊ†áËÆ∞ÂíåÂÆû‰æãÈÄâÊã©„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Á®≥ÂÆöÂØπÈΩêÂ≠¶‰π†ÔºåExpAlign ËøòÂºÄÂèë‰∫Ü‰∏ÄÁßçÂü∫‰∫éËÉΩÈáèÁöÑÂ§öÂ∞∫Â∫¶‰∏ÄËá¥ÊÄßÊ≠£ÂàôÂåñÊñπÊ°àÔºåÂåÖÊã¨ Top-K Â§öÊ≠£Ê†∑Êú¨ÂØπÊØîÁõÆÊ†áÂíåÂá†‰ΩïÊÑüÁü•‰∏ÄËá¥ÊÄßÁõÆÊ†á„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåExpAlign Âú®ÂºÄÊîæËØçÊ±áÊ£ÄÊµãÂíåÈõ∂Ê†∑Êú¨ÂÆû‰æãÂàÜÂâ≤ÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåÁâπÂà´ÊòØÂú®ÈïøÂ∞æÁ±ªÂà´‰∏äÔºåËææÂà∞‰∫Ü 36.2 ÁöÑ AP_rÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂêåÁ±ªÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ","title":"ExpAlignÔºöÊó†Ê†áÊ≥®ÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ExpAlign ÊòØ‰∏Ä‰∏™ËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÊ°ÜÊû∂ÔºåÂà©Áî®Â§öÂÆû‰æãÂ≠¶‰π†ÂíåÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÊ±†ÂåñÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂºÄÊîæËØçÊ±áÊ£ÄÊµãÂíåÈõ∂Ê†∑Êú¨ÂÆû‰æãÂàÜÂâ≤ÁöÑÊïàÊûúÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÁöÑÊ†áÊ≥®„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊúüÊúõÂØπÈΩêÂ§¥ÂÆûÁé∞Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËΩØÂ§öÂÆû‰æãÊ±†ÂåñÔºåËÉΩÂ§üÂú®Ê≤°ÊúâÈ¢ùÂ§ñÊ†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÈöêÂºèÁöÑÊ†áËÆ∞ÂíåÂÆû‰æãÈÄâÊã©„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Á®≥ÂÆöÂØπÈΩêÂ≠¶‰π†ÔºåExpAlign ËøòÂºÄÂèë‰∫Ü‰∏ÄÁßçÂü∫‰∫éËÉΩÈáèÁöÑÂ§öÂ∞∫Â∫¶‰∏ÄËá¥ÊÄßÊ≠£ÂàôÂåñÊñπÊ°àÔºåÂåÖÊã¨ Top-K Â§öÊ≠£Ê†∑Êú¨ÂØπÊØîÁõÆÊ†áÂíåÂá†‰ΩïÊÑüÁü•‰∏ÄËá¥ÊÄßÁõÆÊ†á„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåExpAlign Âú®ÂºÄÊîæËØçÊ±áÊ£ÄÊµãÂíåÈõ∂Ê†∑Êú¨ÂÆû‰æãÂàÜÂâ≤ÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåÁâπÂà´ÊòØÂú®ÈïøÂ∞æÁ±ªÂà´‰∏äÔºåËææÂà∞‰∫Ü 36.2 ÁöÑ AP_rÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂêåÁ±ªÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ', title='ExpAlignÔºöÊó†Ê†áÊ≥®ÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÊñ∞ÊñπÊ≥ï'))
[02.02.2026 16:43] Using data from previous issue: {"categories": [], "emoji": "‚è∞", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –µ–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ TAPPA –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ
[02.02.2026 16:43] Querying the API.
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.  					AI-generated summary 				 We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.   KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.   We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.   Code Available at: https://github.com/Leeroo-AI/kapso
[02.02.2026 16:43] Response: ```json
{
  "desc": "KAPSO ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º, –∫–æ—Ç–æ—Ä–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–æ–¥ —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏. –°–∏—Å—Ç–µ–º–∞ —Ä–µ—à–∞–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Å—Ç–æ—Ä–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é git, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –∑–Ω–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å —É—Ä–æ–∫–æ–≤ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–ø—ã—Ç–æ–∫. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: engine –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —Å–∏—Å—Ç–µ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –≤–Ω–µ—à–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –∏ —Å–ª–æ–π —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π. KAPSO –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö ML-—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—è —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –æ—à–∏–±–æ–∫.",
  "emoji": "üß†",
  "title": "–°–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º —á–µ—Ä–µ–∑ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é"
}
```
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.  					AI-generated summary 				 We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.   KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.   We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.   Code Available at: https://github.com/Leeroo-AI/kapso"

[02.02.2026 16:43] Response: ```python
["AGENTS", "PLP", "TRAINING"]
```
[02.02.2026 16:43] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.  					AI-generated summary 				 We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.   KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.   We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.   Code Available at: https://github.com/Leeroo-AI/kapso"

[02.02.2026 16:43] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```
[02.02.2026 16:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"KAPSO is a modular framework designed for autonomous program synthesis that enhances code generation through iterative optimization. It operates by taking a natural language goal and an evaluation method, then cycles through ideation, code synthesis, execution, and learning to refine the output. The framework addresses common long-horizon failures in coding agents by integrating an experimentation engine, a knowledge system, and a cognitive memory layer. This approach allows KAPSO to produce reproducible artifacts, leverage domain expertise, and accelerate learning from past experiments.","title":"KAPSO: Optimizing Code Generation through Iterative Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='KAPSO is a modular framework designed for autonomous program synthesis that enhances code generation through iterative optimization. It operates by taking a natural language goal and an evaluation method, then cycles through ideation, code synthesis, execution, and learning to refine the output. The framework addresses common long-horizon failures in coding agents by integrating an experimentation engine, a knowledge system, and a cognitive memory layer. This approach allows KAPSO to produce reproducible artifacts, leverage domain expertise, and accelerate learning from past experiments.', title='KAPSO: Optimizing Code Generation through Iterative Learning'))
[02.02.2026 16:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"KAPSOÊòØ‰∏Ä‰∏™Ê®°ÂùóÂåñÊ°ÜÊû∂ÔºåÁî®‰∫éËá™‰∏ªÁ®ãÂ∫èÂêàÊàêÂíå‰ºòÂåñ„ÄÇÂÆÉÈÄöËøáËø≠‰ª£‰ºòÂåñÂæ™ÁéØÔºåÁªìÂêàÂÆûÈ™åË∑üË∏™„ÄÅÁü•ËØÜÊï¥ÂêàÂíåËÆ§Áü•ËÆ∞ÂøÜÔºåÊù•ÊèêÈ´ò‰ª£Á†ÅÁîüÊàêÁöÑÊïàÁéá„ÄÇKAPSO‰∏ç‰ªÖÂ∞ÜÂêàÊàêËßÜ‰∏∫ÁªàÁÇπÔºåËÄåÊòØ‰Ωú‰∏∫ÈïøÊó∂Èó¥‰ºòÂåñÂæ™ÁéØ‰∏≠ÁöÑ‰∏Ä‰∏™Êìç‰ΩúÔºåÊó®Âú®Ëß£ÂÜ≥ÁºñÁ†Å‰ª£ÁêÜÂ∏∏ËßÅÁöÑÈïøÊúüÂ§±Ë¥•ÈóÆÈ¢ò„ÄÇÈÄöËøáÈõÜÊàêÂÆûÈ™åÂºïÊìé„ÄÅÁü•ËØÜÁ≥ªÁªüÂíåËÆ§Áü•ËÆ∞ÂøÜÂ±ÇÔºåKAPSOËÉΩÂ§üÊúâÊïàÂú∞ÁÆ°ÁêÜÂÆûÈ™åÁä∂ÊÄÅ„ÄÅÈáçÁî®È¢ÜÂüüÁü•ËØÜÔºåÂπ∂Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇ","title":"KAPSOÔºöÊèêÂçá‰ª£Á†ÅÁîüÊàêÁöÑÊô∫ËÉΩÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='KAPSOÊòØ‰∏Ä‰∏™Ê®°ÂùóÂåñÊ°ÜÊû∂ÔºåÁî®‰∫éËá™‰∏ªÁ®ãÂ∫èÂêàÊàêÂíå‰ºòÂåñ„ÄÇÂÆÉÈÄöËøáËø≠‰ª£‰ºòÂåñÂæ™ÁéØÔºåÁªìÂêàÂÆûÈ™åË∑üË∏™„ÄÅÁü•ËØÜÊï¥ÂêàÂíåËÆ§Áü•ËÆ∞ÂøÜÔºåÊù•ÊèêÈ´ò‰ª£Á†ÅÁîüÊàêÁöÑÊïàÁéá„ÄÇKAPSO‰∏ç‰ªÖÂ∞ÜÂêàÊàêËßÜ‰∏∫ÁªàÁÇπÔºåËÄåÊòØ‰Ωú‰∏∫ÈïøÊó∂Èó¥‰ºòÂåñÂæ™ÁéØ‰∏≠ÁöÑ‰∏Ä‰∏™Êìç‰ΩúÔºåÊó®Âú®Ëß£ÂÜ≥ÁºñÁ†Å‰ª£ÁêÜÂ∏∏ËßÅÁöÑÈïøÊúüÂ§±Ë¥•ÈóÆÈ¢ò„ÄÇÈÄöËøáÈõÜÊàêÂÆûÈ™åÂºïÊìé„ÄÅÁü•ËØÜÁ≥ªÁªüÂíåËÆ§Áü•ËÆ∞ÂøÜÂ±ÇÔºåKAPSOËÉΩÂ§üÊúâÊïàÂú∞ÁÆ°ÁêÜÂÆûÈ™åÁä∂ÊÄÅ„ÄÅÈáçÁî®È¢ÜÂüüÁü•ËØÜÔºåÂπ∂Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇ', title='KAPSOÔºöÊèêÂçá‰ª£Á†ÅÁîüÊàêÁöÑÊô∫ËÉΩÊ°ÜÊû∂'))
[02.02.2026 16:43] Renaming data file.
[02.02.2026 16:43] Renaming previous data. hf_papers.json to ./d/2026-02-02.json
[02.02.2026 16:43] Saving new data file.
[02.02.2026 16:43] Generating page.
[02.02.2026 16:43] Renaming previous page.
[02.02.2026 16:43] Renaming previous data. index.html to ./d/2026-02-02.html
[02.02.2026 16:43] Writing result.
[02.02.2026 16:43] Renaming log file.
[02.02.2026 16:43] Renaming previous data. log.txt to ./logs/2026-02-02_last_log.txt
