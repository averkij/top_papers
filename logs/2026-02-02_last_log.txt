[02.02.2026 02:14] Read previous papers.
[02.02.2026 02:14] Generating top page (month).
[02.02.2026 02:14] Writing top page (month).
[02.02.2026 04:59] Read previous papers.
[02.02.2026 04:59] Get feed.
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.21558
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.23143
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22628
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.21716
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.21468
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.20218
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22642
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.21358
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.23265
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.23184
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22664
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.21525
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22904
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22837
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22491
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.22141
[02.02.2026 04:59] Extract page data from URL. URL: https://huggingface.co/papers/2601.23161
[02.02.2026 04:59] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.02.2026 04:59] Downloading and parsing papers (pdf, html). Total: 17.
[02.02.2026 04:59] Downloading and parsing paper https://huggingface.co/papers/2601.21558.
[02.02.2026 04:59] Downloading paper 2601.21558 from https://arxiv.org/pdf/2601.21558v2...
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas Beike Language and Intelligence "
[02.02.2026 04:59] Response: ```python
[]
```
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas Beike Language and IntelligenceLarge language models (LLMs) are increasingly used as tool-augmented agents for multistep decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed questionanswer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra. 6 2 0 2 0 3 ] . [ 2 8 5 5 1 2 . 1 0 6 2 : r Figure 1: Comparison of Model Performance on BFCL v3 Multi-Turn. 1Beike Language and Intelligence (BLI). For the complete list of authors, please refer to the Contribution section.Large language models (LLMs) are increasingly deployed as tool-augmented agents that interact with external environments, invoke APIs, and perform multi-step decision making. By integrating reasoning with action, such agents enable applications ranging from information retrieval and data analysis to interactive dialogue systems, making tool use core capability of modern language models. Despite rapid progress, training robust and generalizable tool agents remains challenging. Recent work [1, 2] has begun to reduce human intervention by automatically synthesizing tool-use data and environments through model-driven simulation, significantly improving scalability and coverage. However, many of these approaches rely on LLM-simulated environments, where tool executions, state transitions, and feedback are generated through language-model rather than explicit rules or executable backends. That is, their reinforcement learning (RL) setups are not rule-verifiable. This lack of verifiability fundamentally limits stable long-horizon, multi-turn online RL, where deterministic transitions and reliable reward signals are critical. Moreover, several methods [2, 3] generate multi-turn trajectories offline but decompose them into isolated single-step training instances, which limits the agents ability to learn coherent long-horizon, multi-turn decision making. In addition, many existing approaches focus on only single training regimeeither supervised fine-tuning (SFT) or RL [4, 5, 6]. SFT-only methods lack online learning signals from environment interaction, while RL-only approaches are fundamentally constrained by the capability of the original model, limiting their effectiveness when starting from weaker initial policies. To address these challenges, we present ASTRA, fully automated, end-to-end framework for training toolaugmented language model agents via scalable data synthesis and verifiable multi-turn online reinforcement learning. ASTRA removes human intervention throughout both data construction and validation, and is fully open-sourced. ASTRA integrates two complementary components. For SFT, we propose trajectory synthesis pipeline that leverages the static topology of tool-call graphs to construct diverse multi-turn tool-use trajectories grounded in real MCP servers, and automatically scores them for qualityenabling high-quality supervised fine-tuning without manual annotation. For RL, we introduce an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning, converting decomposed questionanswer traces into independent, code-executable, and rule-verifiable environments, thereby supporting multi-turn, long-horizon RL with deterministic rewards. Building on these components, we develop complete training methodology for tool agents.We use SFT to learn stronger initial policy that is better adapted to multi-turn tool interaction and then perform online, multi-turn RL over diverse synthesized environments, incorporating irrelevant-tool mixing and an F1-style trajectory-level reward to jointly optimize task completion and interaction efficiency. This two-stage method first broadens an agents tool-use competence over static tool topology, then deepens its capability by learning within complex semantic topology. As result, ASTRA effectively balances generalization across tools with robustness in realistic, high-complexity scenarios. Our contributions are summarized as follows: We propose fully automated, end-to-end data construction pipeline for tool-agent training, leveraging the static topology of tool-call graphs for multi-turn trajectory synthesis and capturing the rich, compositional topology of human semantic reasoning for QA-derived, rule-verifiable environment construction. We propose complete training methodology consisting of (i) supervised fine-tuning for stronger initial policy that is better adapted to multi-turn tool interaction, and (ii) multi-turn online reinforcement learning over code-executable, rule-verifiable environments across multiple domains, enabling reliable and scalable agent training. ASTRA-trained models achieve state-of-the-art performance among the same scale on multiple agentic tool-use benchmarks, approaching closed-source systems while preserving core reasoning ability, and we make the data synthesis pipelines and trained models publicly available to support reproducibility and future research.We first present the tool-chain-based trajectory synthesis pipeline for SFT, followed by the QA"
[02.02.2026 04:59] Mistral response. {"id": "03773a13006043d2a2f1ef682e0d5e25", "created": 1770008360, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1234, "total_tokens": 1250, "completion_tokens": 16, "num_cached_tokens": 1233}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Beike Language and Intelligence (BLI)\"]\n```"}}]}
[02.02.2026 04:59] Response: ```python
["Beike Language and Intelligence (BLI)"]
```
[02.02.2026 04:59] Deleting PDF ./assets/pdf/2601.21558.pdf.
[02.02.2026 04:59] Success.
[02.02.2026 04:59] Downloading and parsing paper https://huggingface.co/papers/2601.23143.
[02.02.2026 04:59] Downloading paper 2601.23143 from https://arxiv.org/pdf/2601.23143v1...
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"THINKSAFE: Self-Generated Safety Alignment for Reasoning Models Seanie Lee * 1 Sangwoo Park * 1 Yumin Choi 1 Gyeongman Kim 2 Minki Kang 1 Jihun Yun 2 Dongmin Park 2 Jongho Park 3 Sung Ju Hwang 1 4 1KAIST AI 2KRAFTON 3UC Berkeley {lsnfamily02, swgger}@kaist.ac.kr 4DeepAuto.ai 6 2 0 J 0 3 ] . [ 1 3 4 1 3 2 . 1 0 6 2 : r a "
[02.02.2026 04:59] Response: ```python
['KAIST AI', 'KRAFTON', 'UC Berkeley', 'DeepAuto.ai']
```
[02.02.2026 04:59] Deleting PDF ./assets/pdf/2601.23143.pdf.
[02.02.2026 04:59] Success.
[02.02.2026 04:59] Downloading and parsing paper https://huggingface.co/papers/2601.22628.
[02.02.2026 04:59] Downloading paper 2601.22628 from https://arxiv.org/pdf/2601.22628v1...
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 3 ] . [ 1 8 2 6 2 2 . 1 0 6 2 : r TTCS: Test-Time Curriculum Synthesis for Self-Evolving Chengyi Yang1, Zhishang Xiang1, Yunbo Tang1, Zongpei Teng1, Chengsong Huang2, Fei Long1, Yuhan Liu3, Jinsong Su1 1Xiamen University 2Washington University in St. Louis 3Renmin University of China yangchengyi@stu.xmu.edu.cn yuhan.liu@ruc.edu.cn jssu@xmu.edu.cn "
[02.02.2026 04:59] Response: ```python
[
    "Xiamen University",
    "Washington University in St. Louis",
    "Renmin University of China"
]
```
[02.02.2026 04:59] Deleting PDF ./assets/pdf/2601.22628.pdf.
[02.02.2026 04:59] Success.
[02.02.2026 04:59] Downloading and parsing paper https://huggingface.co/papers/2601.21716.
[02.02.2026 04:59] Downloading paper 2601.21716 from https://arxiv.org/pdf/2601.21716v1...
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning Mingshuang Luo123,, Shuang Liang1,, Zhengkun Rong1,, Yuxuan Luo1,, Tianshu Hu1,, Ruibing Hou2,, Hong Chang23, Yong Li4, Yuan Zhang1, Mingyuan Gao1 1ByteDance Intelligent Creation 2Key Lab of Intell. Info. Process., ICT, CAS 3University of Chinese Academy of Sciences 4Southeast University Equal Contribution, Project Lead, Corresponding Authors "
[02.02.2026 04:59] Response: ```python
[
    "ByteDance Intelligent Creation",
    "Key Lab of Intell. Info. Process., ICT, CAS",
    "University of Chinese Academy of Sciences",
    "Southeast University"
]
```
[02.02.2026 04:59] Deleting PDF ./assets/pdf/2601.21716.pdf.
[02.02.2026 04:59] Success.
[02.02.2026 04:59] Downloading and parsing paper https://huggingface.co/papers/2601.21468.
[02.02.2026 04:59] Downloading paper 2601.21468 from https://arxiv.org/pdf/2601.21468v1...
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 8 6 4 1 2 . 1 0 6 2 : r MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Yaorui Shi 1 2 * Shugui Liu 1 Yu Yang 2 Wenyu Mao 1 Yuxin Chen 3 2 * Qi Gu 2 Hui Su 2 Xunliang Cai 2 Xiang Wang 1 An Zhang "
[02.02.2026 04:59] Response: ```python
[]
```
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 8 6 4 1 2 . 1 0 6 2 : r MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning Yaorui Shi 1 2 * Shugui Liu 1 Yu Yang 2 Wenyu Mao 1 Yuxin Chen 3 2 * Qi Gu 2 Hui Su 2 Xunliang Cai 2 Xiang Wang 1 An ZhangLong-horizon agentic reasoning necessitates effectively compressing growing interaction histories into limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets. Our code is available at https:// github.com/syr-cn/MemOCR. 1. Introduction The evolution of large language models (LLMs) has empowered autonomous agents to tackle complex, long-horizon tasks that necessitate robust long-horizon reasoning (Luo et al., 2025; Du et al., 2025; Matarazzo & Torlone, 2025). However, as an agent accumulates extensive interaction history over its lifespan, the sheer volume of data inevitably *Work done during an internship at Meituan 1University of Science and Technology of China, Hefei, China 2Meituan, Beijing, China 3National University of Singapore, School of Computing, Singapore. Correspondence to: An Zhang <an zhang@ustc.edu.cn>, Qi Gu <guqi03@meituan.com>. Preprint. January 30, 2026. overwhelms the hard constraints of the context window, creating fundamental bottleneck (Vaswani et al., 2017; Hsieh et al., 2024; Modarressi et al., 2025). At the core of longhorizon reasoning is memory management under finite working context: agents must continually decide what past information to store and what to retrieve into the context window (Fang et al., 2025; Hu et al., 2025; Zhang et al., 2025b). This essentially constitutes budget allocation problem, where the objective is to maximize the density of task-relevant information within limited number of tokens (i.e., the memory budget) to support the current decision. Leading approaches generally construct the working context using textual forms, which can be categorized into two paradigms. Early works populate the context by retrieving and injecting raw historical segments (e.g., past chats) as uncompressed memory (Zhang et al., 2025a; Jin et al., 2025; Song et al., 2025). Specifically, the agent retrieves relevant passages and inserts the top-k snippets to fill the working context, as demonstrated in Figure 1(a). While this preserves original details, the retrieved snippets can be redundant or noisy, diluting information density and potentially exhausting the context budget (Shi et al., 2025b; Wu et al., 2025). Instead of storing raw information, recent works (Yu et al., 2025; Wang et al., 2025; Chhikara et al., 2025; Shi et al., 2025a) compress past interactions into compact textual summary, and maintain it via incremental updates or overwrites. In principle, summarization distills task-relevant information (i.e., concepts crucial for answering future queries) from noisy history, providing cleaner context to support decision-making. However, the textual memory paradigm suffers from an intrinsic limitation: linear token scaling. Even though summarization alleviates redundancy, representing memory as text tightly couples storage cost to information content retaining more auxiliary details or explanatory context inevitably requires proportionally more tokens (Feng et al., 2026; Fang et al., 2025; Sun et al., 2025). This coupling squanders the limited memory budget on non-critical supporting facts. As conceptually illustrated in Figure 1(b), text imposes uniform information density: to maintain 100 tokens of crucial information, the system is compelled to retain substantial volume of auxiliary details (depicted as 900 tokens), lacking the flexibility to selectively downMemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning sample less important context. 2. Preliminaries We propose paradigm shift from 1D textual memory to 2D visual memory, representing history as an image rather than token stream. The core benefit is adaptive information density: the agent can explicitly allocate the limited budget non-uniformly by controlling visual salience. Crucial evidence is rendered with prominent typography and high-visibility layout (e.g., headers, bold, larger font), while auxiliary details are compressed into visually smaller text. This allows the agent to pack substantially more content into far fewer visual tokens while keeping key evidence readable under aggressive compression (Figure 1(c)). The overall budget can be controlled by resolution manipulation (e.g., downsampling), providing flexible budget-fidelity tradeoff without changing the memory content (Wei et al., 2025). To this end, we introduce MemOCR, multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. As shown in Figure 2, MemOCR formulates and utilizes visual memory with two-stage memory lifecycle. (1) Memory Drafting (Text Domain): upon receiving new experience, the agent incrementally edits persistent rich-text memory, including both updating content and assigning visual priority via structure and formatting. These explicit salience cues determine how memory components compete for limited space, enabling non-uniform budget allocation. (2) Memory Reading (Vision Domain): lightweight renderer compiles the rich text into 2D memory image, which becomes the agents sole working context at query time. The agent then reads this memory image to produce an answer. We train MemOCR via RL with budget-aware training objectives that expose the agent to various memory budgets, forcing it to write"
[02.02.2026 04:59] Mistral response. {"id": "f359b207ae8149539dfac49d1ba28065", "created": 1770008389, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1518, "total_tokens": 1552, "completion_tokens": 34, "num_cached_tokens": 1517}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of Science and Technology of China\",\n    \"Meituan\",\n    \"National University of Singapore, School of Computing\"\n]\n```"}}]}
[02.02.2026 04:59] Response: ```python
[
    "University of Science and Technology of China",
    "Meituan",
    "National University of Singapore, School of Computing"
]
```
[02.02.2026 04:59] Deleting PDF ./assets/pdf/2601.21468.pdf.
[02.02.2026 04:59] Success.
[02.02.2026 04:59] Downloading and parsing paper https://huggingface.co/papers/2601.20218.
[02.02.2026 04:59] Downloading paper 2601.20218 from https://arxiv.org/pdf/2601.20218v1...
[02.02.2026 04:59] Extracting affiliations from text.
[02.02.2026 04:59] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 2 ] . [ 1 8 1 2 0 2 . 1 0 6 2 : r Published as conference paper at ICLR 2026 DENSEGRPO: FROM SPARSE TO DENSE REWARD FOR FLOW MATCHING MODEL ALIGNMENT Haoyou Deng1,2, Keyu Yan2, Chaojie Mao2, Xiang Wang1, Yu Liu2, Changxin Gao1, Nong Sang1 1National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology 2Tongyi Lab, Alibaba Group {haoyoudeng, nsang}@hust.edu.cn yankeyu.yky@alibaba-inc.com "
[02.02.2026 04:59] Response: ```python
[
    "National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology",
    "Tongyi Lab, Alibaba Group"
]
```
[02.02.2026 04:59] Deleting PDF ./assets/pdf/2601.20218.pdf.
[02.02.2026 04:59] Success.
[02.02.2026 04:59] Downloading and parsing paper https://huggingface.co/papers/2601.22642.
[02.02.2026 05:00] Downloading paper 2601.22642 from https://arxiv.org/pdf/2601.22642v1...
[02.02.2026 05:00] Extracting affiliations from text.
[02.02.2026 05:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification in Language Models Chuxue Cao * 1 2 Jinluan Yang * 3 Haoran Li 1 Kunhao Pan 1 Zijian Zhao 1 Zhengyu Chen 3 Yuchen Tian 4 Lijun Wu 2 Conghui He 2 Sirui Han 1 Yike Guo 1 6 2 0 2 0 3 ] . [ 1 2 4 6 2 2 . 1 0 6 2 : r a "
[02.02.2026 05:00] Response: ```python
[]
```
[02.02.2026 05:00] Extracting affiliations from text.
[02.02.2026 05:00] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification in Language Models Chuxue Cao * 1 2 Jinluan Yang * 3 Haoran Li 1 Kunhao Pan 1 Zijian Zhao 1 Zhengyu Chen 3 Yuchen Tian 4 Lijun Wu 2 Conghui He 2 Sirui Han 1 Yike Guo 1 6 2 0 2 0 3 ] . [ 1 2 4 6 2 2 . 1 0 6 2 : r aLarge Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neurosymbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning. We will release our data and models for further exploration soon. 1. Introduction Large Language Models (LLMs) demonstrate impressive proficiency in mathematical and logical reasoning (Ahn et al., 2024; Ji et al., 2025; Liu et al., 2025b; Chen et al., 2025a), yet their probabilistic decoding process lacks inherent mechanisms to ensure consistency (Sheng et al., 2025b; Baker et al., 2025). This tension creates significant risks, in- * Equal contribution. 1Hong Kong University of Science and Technology 2Shanghai Artificial Intelligence Laboratory 3Zhejiang University 4Hong Kong Baptist University. Correspondence to: Sirui Han, Yike Guo. Contact: <ccaoai@connect.ust.hk>. Preprint. February 2, 2026. Figure 1. Comparison between natural language reasoning and formal logic verification-guided reasoning. Formal verification detects logical errors in natural language reasoning and provides corrected reasoning paths. NL means Natural Language. cluding hallucinations (Li & Ng, 2025; Sheng et al., 2025b), safety vulnerabilities (Zhou et al., 2025; Cao et al., 2025b), and reward hacking (Chen et al., 2025b; Baker et al., 2025). Although recent efforts have employed model-based verifiers to offer denser feedback than sparse ground-truth labels (Ma et al., 2025; Liu et al., 2025c; Guo et al., 2025b), they often overlook intermediate reasoning steps. To enforce more rigorous supervision, subsequent research has incorporated formal tools like theorem provers and code interpreters (Ospanov et al., 2025; Kamoi et al., 2025; Liu et al., 2025a) to address this drawback. However, existing formal approaches face critical limitations: they are often restricted to specific domains (e.g., Mathematics) (Ospanov et al., 2025; Liu et al., 2025a), rely on uncertain autoformalization (Ospanov et al., 2025; Feng et al., 2025b), or utilize post-hoc verification that cannot actively prevent error propagation (Kamoi et al., 2025; Feng et al., 2025b). This yields the primary question to be explored: (Q) Can we utilize the formal verification to further enhance the LLM reasoning across diverse domains? Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification To explore this question, we first quantified the logical consistency gap in current LLMs by conducting formal verification analysis of generated reasoning chains. critical finding emerges as shown in Figure 1: even chains that arrive at correct final answers suffer from substantial logical inconsistency, with 39.3% of steps formally disproved, trend consistent with previous research (Sheng et al., 2025b; Leang et al., 2025). For chains leading to incorrect answers, this failure rate rises to 52.4%. The comparative example in Figure 1 illustrates this gap: natural language reasoning incorrectly concludes Diana > Bob from the given constraints, while formal verification identifies the incorrect conclusion and lead to an correct answer. This phenomenon reveals pervasive reward hacking, where models exploit superficial patterns to reach correct labels without establishing sound logical foundations (Skalse et al., 2022). Ultimately, these results expose fundamental limitation of natural language reasoning: without explicit verification mechanisms, models cannot guarantee reasoning validity or global logical coherence across multi-step inference. To bridge this gap, we propose novel framework that synergizes probabilistic natural language reasoning with formal symbolic verification. Distinguished from prior approaches relying on static filtering or narrow domains, our method dynamically interleaves formal verification into the generation process. By incorporating feedback from satisfiability results, counterexamples, and execution outputs, we extend the standard chain-of-thought paradigm to enable real-time error detection and rectification. To effectively operationalize this interleaved reasoning approach, we introduce formal logic verification-guided training framework comprising two synergistic stages: (i) Supervised Fine-tuning (SFT), which employs hierarchical data synthesis pipeline. Crucially, we mitigate the noise of raw autoformalization by enforcing execution-based validation, thereby ensuring high alignment between natural language and formal proofs; and (ii) Reinforcement Learning (RL), which utilizes Group Relative Policy Optimization (GRPO) with composite reward function to enforce structural integrity and penalize logical fallacies. Empirical evaluations across logical, mathematical, and general reasoning domains demonstrate that this framework substantially enhances reasoning accuracy, highlighting the potential of formal verification to push the performance boundaries of LLM reasoning. Our contributions can be concluded as follows: We propose the first framework that dynamically interleaves formal verification into LLM reasoning across diverse domains, utilizing the real-time feedback via symbolic interpreters to transcend the limitations of passive post-hoc filtering a"
[02.02.2026 05:00] Mistral response. {"id": "e366716fae464bd2874fa6a52d487528", "created": 1770008406, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1456, "total_tokens": 1494, "completion_tokens": 38, "num_cached_tokens": 1455}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Hong Kong University of Science and Technology\",\n    \"Shanghai Artificial Intelligence Laboratory\",\n    \"Zhejiang University\",\n    \"Hong Kong Baptist University\"\n]\n```"}}]}
[02.02.2026 05:00] Response: ```python
[
    "Hong Kong University of Science and Technology",
    "Shanghai Artificial Intelligence Laboratory",
    "Zhejiang University",
    "Hong Kong Baptist University"
]
```
[02.02.2026 05:00] Deleting PDF ./assets/pdf/2601.22642.pdf.
[02.02.2026 05:00] Success.
[02.02.2026 05:00] Downloading and parsing paper https://huggingface.co/papers/2601.21358.
[02.02.2026 05:00] Downloading paper 2601.21358 from https://arxiv.org/pdf/2601.21358v1...
[02.02.2026 05:00] Extracting affiliations from text.
[02.02.2026 05:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization Jiecong Wang1, Hao Peng1, Chunyang Liu2 1Beihang University, 2Didi Chuxing {jcwang, penghao}@buaa.edu.cn, liuchunyang@didiglobal.com 6 2 0 2 9 2 ] A . [ 1 8 5 3 1 2 . 1 0 6 2 : r a "
[02.02.2026 05:00] Response: ```python
["Beihang University", "Didi Chuxing"]
```
[02.02.2026 05:00] Deleting PDF ./assets/pdf/2601.21358.pdf.
[02.02.2026 05:00] Success.
[02.02.2026 05:00] Downloading and parsing paper https://huggingface.co/papers/2601.23265.
[02.02.2026 05:00] Downloading paper 2601.23265 from https://arxiv.org/pdf/2601.23265v1...
[02.02.2026 05:00] Extracting affiliations from text.
[02.02.2026 05:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2026-2-2 PaperBanana: Automating Academic Dawei Zhu1 2 *, Rui Meng2, Yale Song2, Xiyu Wei1, Sujian Li1, Tomas Pfister2 and Jinsung Yoon2 1Peking University, 2Google Cloud AI Research https://dwzhu-pku.github.io/PaperBanana/ Despite rapid advances in autonomous AI scientists powered by language models, generating publicationready illustrations remains labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations. 6 2 0 2 0 3 ] . [ 1 5 6 2 3 2 . 1 0 6 2 : r Figure 1 Examples of methodology diagrams and statistical plots generated by PaperBanana, which show the potential of automating the generation of academic illustrations. 1. Introduction Autonomous scientific discovery is long-standing pursuit of artificial general intelligence (Ghahramani, 2015; Langley, 1987, 2024; Schmidhuber, 2010). With the rapid evolution of Large Language Corresponding author(s): dwzhu@pku.edu.cn, lisujian@pku.edu.cn, jinsungyoon@google.com * This work was done while Dawei was student researcher at Google Cloud AI Research. PaperBanana: Automating Academic Illustration for AI Scientis"
[02.02.2026 05:00] Response: ```python
["Peking University", "Google Cloud AI Research"]
```
[02.02.2026 05:00] Deleting PDF ./assets/pdf/2601.23265.pdf.
[02.02.2026 05:00] Success.
[02.02.2026 05:00] Downloading and parsing paper https://huggingface.co/papers/2601.23184.
[02.02.2026 05:00] Downloading paper 2601.23184 from https://arxiv.org/pdf/2601.23184v1...
[02.02.2026 05:00] Extracting affiliations from text.
[02.02.2026 05:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 3 ] . [ 1 4 8 1 3 2 . 1 0 6 2 : r February 2, ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought Fanmeng Wang 1,2 , Haotian Liu 1 , Guojiang Zhao 2 , Hongteng Xu 1,3,4 , Zhifeng Gao 2 1Gaoling School of Artificial Intelligence, Renmin University of China 2DP Technology 3Beijing Key Laboratory of Research on Large Models and Intelligent Governance 4Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE "
[02.02.2026 05:00] Response: ```python
[
    "Gaoling School of Artificial Intelligence, Renmin University of China",
    "DP Technology",
    "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
    "Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE"
]
```
[02.02.2026 05:00] Deleting PDF ./assets/pdf/2601.23184.pdf.
[02.02.2026 05:00] Success.
[02.02.2026 05:00] Downloading and parsing paper https://huggingface.co/papers/2601.22664.
[02.02.2026 05:00] Downloading paper 2601.22664 from https://arxiv.org/pdf/2601.22664v1...
[02.02.2026 05:00] Extracting affiliations from text.
[02.02.2026 05:00] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Real-Time Aligned Reward Model beyond Semantics Zixuan Huang 1 2 Xin Xia 2 Yuxi Ren 2 Jianbin Zheng 2 Xuefeng Xiao 2 Hongyan Xie 1 Li Huaqiu 3 Songshi Liang 4 Zhongxiang Dai 5 Fuzhen Zhuang 1 Jianxin Li 1 Yikun Ban 1 Deqing Wang 1 6 2 0 2 0 3 ] A . [ 1 4 6 6 2 2 . 1 0 6 2 : r Abstract Reinforcement Learning from Human Feedback (RLHF) is pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily rely on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models. 1. Introduction Reinforcement Learning from Human Feedback (RLHF) has become cornerstone technique for aligning large language models (LLMs) with human values and preferences (Vemprala et al., 2023; Huang et al., 2025; Shen & Zhang, 2024; Shen et al., 2025; Hu et al., 2024). However, RLHF faces persistent challenge: reward overoptimization. Instead of faithfully capturing human intent, policy models often exploit spurious reward patterns, such as response 2ByteDance 1Beihang University 3Tsinghua University 4Renmin University of Ch"
[02.02.2026 05:00] Response: ```python
[
    "ByteDance",
    "Beihang University",
    "Tsinghua University",
    "Renmin University of China"
]
```
[02.02.2026 05:00] Deleting PDF ./assets/pdf/2601.22664.pdf.
[02.02.2026 05:00] Success.
[02.02.2026 05:00] Downloading and parsing paper https://huggingface.co/papers/2601.21525.
[02.02.2026 05:00] Downloading paper 2601.21525 from https://arxiv.org/pdf/2601.21525v1...
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LMK > CLS Landmark Pooling for Dense Embeddings Meet Doshi 1 Aashka Trivedi 1 Vishwajeet Kumar 1 Parul Awasthy 1 Yulong Li 1 Jaydeep Sen 1 Radu Florian 1 Sachindra Joshi 1 6 2 0 2 9 2 ] . [ 1 5 2 5 1 2 . 1 0 6 2 : r Abstract Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse variable-length token sequence to single vector using pooling operatormost commonly special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it practical and scalable alternative to existing pooling methods. 1. Introduction Traditionally, document-level representations were constructed using vector space based feature extraction methods such as CBOW and Skip gram, which aggregate wordlevel statistics or embeddings into fixed size representations (Mikolov et al., 2013; Clinchant & Perronnin, 2013; Zuccon et al., 2015). These approaches were later supplanted by neural sequence encoders, including LSTMs and Transformers (Vaswani et al., 2017; Peters et al., 2018), primarily because classical methods fail to preserve rich docum"
[02.02.2026 05:01] Response: ```python
['IBM Research']
```
[02.02.2026 05:01] Deleting PDF ./assets/pdf/2601.21525.pdf.
[02.02.2026 05:01] Success.
[02.02.2026 05:01] Downloading and parsing paper https://huggingface.co/papers/2601.22904.
[02.02.2026 05:01] Downloading paper 2601.22904 from https://arxiv.org/pdf/2601.22904v1...
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation Hun Chang * 1 Byunghee Cha * 1 Jong Chul Ye 1 6 2 0 2 0 3 ] . [ 1 4 0 9 2 2 . 1 0 6 2 : r Figure 1. Generated images from DiTDH model trained on DINO-SAE latents "
[02.02.2026 05:01] Response: ```python
[]
```
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation Hun Chang * 1 Byunghee Cha * 1 Jong Chul Ye 1 6 2 0 2 0 3 ] . [ 1 4 0 9 2 2 . 1 0 6 2 : r Figure 1. Generated images from DiTDH model trained on DINO-SAE latentsRecent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can *Equal contribution 1Graduate School of AI, KAIST. Correspondence to: Hun Chang, Byunghee Cha, Jong Chul Ye <{hun.mark.chang, paulcha1025, jong.ye}@kaist.ac.kr>. Preprint. February 2, 2026. 1 hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSLbased foundation model representations intrinsically lie on hypersphere, we employ Riemannian Flow Matching to train Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving gFID of 3.47 at 80 epochs. DINO-SAE: DINO Spherical Autoencoder for Image Reconstruction and Generation 1. Introduction Diffusion models (Ho et al., 2020; Song et al., 2021; Peebles & Xie, 2023) have revolutionized image generation, with performance largely contingent on the quality of the latent space provided by the underlying autoencoder. Recently, leveraging pretrained Vision Foundation Models (VFMs) like DINOv2 (Oquab et al., 2023) as encodersexemplified by methods like RAE (Zheng et al., 2025b)has shown promise in capturing rich semantic information. However, critical trade-off remains: while these VFM-based approaches excel in semantic generation, they suffer from significant degradation in pixel-level reconstruction fidelity (e.g., low PSNR) compared to standard VAEs. We attribute this limitation to two primary factors: the aggressive downsampling in standard Vision Transformer (ViT) patch embeddings and the rigidity of feature alignment objectives. Existing methods typically employ Mean Squared Error (MSE) to align the student encoder with the VFM teacher. However, strictly enforcing both magnitude and directional alignment creates gradient conflict between semantic preservation and pixel reconstruction. This overconstrained optimization landscape prevents the encoder from learning the subtle high-frequency features necessary for fidelity. To address these challenges, we introduce the DINO Spherical Autoencoder (DINO-SAE), framework designed to reconcile semantic abstraction with high-fidelity reconstruction. Our approach incorporates Hierarchical Convolutional Patch Embedding to preserve local texture information and adopts Directional Feature Alignment strategy using cosine similarity. Unlike MSE, cosine similarity relaxes the magnitude constraint, smoothing the optimization landscape. This flexibility allows the encoder to prioritize semantic alignment via feature direction while utilizing the magnitude degree of freedom to minimize reconstruction error, effectively resolving the conflict between the two objectives. Furthermore, we extend this geometric perspective to the generative stage. We observe that representations from Contrastive Learning (e.g., DINO, CLIP) naturally converge to hyperspherical manifold rather than Euclidean space. Consequently, we argue that aligning the generative dynamics with the underlying hyperspherical manifold via Riemannian Flow Matching (RFM) (Chen & Lipman, 2023) provides more natural and efficient formulation. By constraining the generative process to the hypersphere and modeling geodesics, we eliminate redundant radial variations and focus solely on semantically meaningful directional dynamics. Our contributions can be summarized as follows: We propose Hierarchical Convolutional Stem that mitigates the information bottleneck of standard ViT patchfication, significantly enhancing local detail preservation. We introduce Directional Feature Alignment, which alleviates the optimization conflict inherent in MSEbased distillation. This enables the encoder to simultaneously achieve high semantic alignment and state-ofthe-art reconstruction fidelity (26.2 dB PSNR). We demonstrate that modeling the latent space as spherical manifold via Riemannian Flow Matching is geometrically well-suited for VFM-based latents. This formulation aligns with the intrinsic properties of SSL features and accelerates training convergence compared to Euclidean baselines. 2. Related Work Representation Alignment. Recent work has shown that leveraging semantic representations can significantly improve the training efficiency of diffusion-based generative models. Following the introduction of REPA (Yu et al., 2025), which aligns intermediate features of DiT with pretrained DINOv2 representations to achieve faster convergence, several extensions have been proposed. REG (Wu et al., 2025) further enhances this paradigm by introducing class token whose DINOv2 features are aligned with DiT during generation, leading to even faster convergence. Beyond external representation models, number of selfcontained approaches have demonstrated similar efficiency gains: SRA (Jiang et al., 2026) aligns intermediate features at higher noise levels with later-layer features at lower noise levels, while LSEP (Yun et al., 2025) shows that merely enforcing linear separability of intermediate features via classification probe can improve convergence. Dispersive Loss (Wang & He, 2025) similarly regularizes intermediate representations by encouraging feature dispersion, highlighting that improved"
[02.02.2026 05:01] Mistral response. {"id": "f0b5a5392040491e9d74ef94bf0e3f09", "created": 1770008467, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1388, "total_tokens": 1404, "completion_tokens": 16, "num_cached_tokens": 1387}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Graduate School of AI, KAIST\"]\n```"}}]}
[02.02.2026 05:01] Response: ```python
["Graduate School of AI, KAIST"]
```
[02.02.2026 05:01] Deleting PDF ./assets/pdf/2601.22904.pdf.
[02.02.2026 05:01] Success.
[02.02.2026 05:01] Downloading and parsing paper https://huggingface.co/papers/2601.22837.
[02.02.2026 05:01] Downloading paper 2601.22837 from https://arxiv.org/pdf/2601.22837v1...
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 3 ] . [ 1 7 3 8 2 2 . 1 0 6 2 : r NativeTok: Native Visual Tokenization for Improved Image Generation Bin Wu Mengqi Huang Weinan Jia Zhendong Mao {lilimomotion,jiawn}@mail.ustc.edu.cn, {zdmao,huangmq}@ustc.edu.cn "
[02.02.2026 05:01] Response: ```python
["University of Science and Technology of China"]
```
[02.02.2026 05:01] Deleting PDF ./assets/pdf/2601.22837.pdf.
[02.02.2026 05:01] Success.
[02.02.2026 05:01] Downloading and parsing paper https://huggingface.co/papers/2601.22491.
[02.02.2026 05:01] Downloading paper 2601.22491 from https://arxiv.org/pdf/2601.22491v1...
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Jinyang Wu 1 * Changpeng Yang 2 * Yuhao Shen 3 Fangzhi Xu 4 Bolin Ni 5 Chonghua Liao 1 Yuchen Liu 2 Hongzhen Wang 2 Shuai Nie 2 Shuai Zhang 1 Haoran Luo 4 Jiaming Xu "
[02.02.2026 05:01] Response: ```python
[]
```
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Jinyang Wu 1 * Changpeng Yang 2 * Yuhao Shen 3 Fangzhi Xu 4 Bolin Ni 5 Chonghua Liao 1 Yuchen Liu 2 Hongzhen Wang 2 Shuai Nie 2 Shuai Zhang 1 Haoran Luo 4 Jiaming XuReinforcement learning with verifiable rewards has emerged as powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the sweet spot concept in tennis-the rackets core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), novel framework that provides differentiated guidance for agent optimization. SSL follows simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweetspot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/longterm planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5 sample efficiency gains and effective cross-task transferability. Our work establishes SSL as general principle for training capable and robust agents. 6 2 0 2 0 3 ] . [ 1 1 9 4 2 2 . 1 0 6 2 : r 1. Introduction Reinforcement learning with verifiable rewards (RLVR) has emerged as transformative paradigm for training intelli1Tsinghua University 2Xiaomi Corporation 3Zhejiang University 4Nanyang Technological University 5Institute of Automation, Chinese Academy of Sciences. Correspondence to: Jinyang Wu <wu-jy23@mails.tsinghua.edu.cn>, Changpeng Yang <yangchangpeng@xiaomi.com>. Preprint. February 2, 2026. 1 Figure 1. Performance comparison with Binary (RL-B.) and Continuous (RL-C.) Reward RL. SSL (our method) exhibits remarkable improvements across diverse tasks, including long- /short-term planning and complex reasoning. gent multimodal agents with sophisticated reasoning and planning capabilities (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025; Nguyen et al., 2025; Jiang et al., 2025). Unlike supervised fine-tuning (SFT) that typically depends on costly human-annotated reasoning chains, RLVR directly leverages automatically computable reward signals to optimize agent behaviors. This approach enables agents to autonomously develop complex cognitive abilities, including chain-of-thought reasoning (Wei et al., 2022), problem decomposition (Zhou et al., 2022), self-correction (Gandhi et al., 2025), and multi-step planning (Luo et al., 2025a). However, mainstream RLVR typically adopts binary rewards, grouping all trajectories into success or failure and obscuring finer distinctions among them. Consider two GUI navigation trajectories that both successfully open application settings: one reaches the target in three actions, while another succeeds after eight redundant stepsyet both receive the same reward. Similarly, in maze navigation, multiple paths may reach the goal with vastly different characteristics: some take circuitous detours, while others follow direct corridors. These examples illustrate how SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Optimization Guidance: tiered, proximity-aligned reward signals provide stronger directional feedback during policy updates, enabling more stable and effective optimization; (ii) Enhanced Efficiency: SSL matches or surpasses GRPO using only 40% of its samples, achieving up to 2.5 dataefficiency; (iii) Transferability & Superior Performance: SSL consistently improves performance across diverse tasks, demonstrating strong transferability. Our contributions are: We propose sweet spot learning, unified reward principle that integrates sweet-spot modeling into reinforcement learning. SSL offers tiered, quality-sensitive rewards with theoretical guarantees on preserving optimal solution ordering and improving gradient optimization. We instantiate SSL across diverse agent tasks: distancetiered rewards for visual perception and navigation, and progress-tiered rewards for complex reasoning. This demonstrates SSLs broad applicability. We conduct extensive experiments on 12 benchmarks spanning GUI perception, short-/long-term planning, and complex reasoning, showing consistent gains over strong baselines, enhanced efficiency, and transferability. 2. Related Work RLVR for Agent Optimization. RLVR has become key paradigm for training capable agents by leveraging automatically computable success criteria (Guo et al., 2025; Team et al., 2025; Wu et al., 2025a). Recent work typically applies on-policy algorithms like GRPO with binary terminal rewards across tasks such as math reasoning (Wang et al., 2025), GUI navigation (Nguyen et al., 2025; Zhang et al., 2024), and puzzle solving (Inc., 2025). However, binary rewards treat all successful trajectories equally, ignoring solution quality-e.g., three-step GUI path and an eightstep detour receive the same reward. This leads to sample inefficiency and policy fragility (overfitting to incidental patterns) (Sutton et al., 1998; Xu et al., 2025; Amodei et al., 2016). SSL addresses these issues by providing differentiated guidance through quality-ordered zone partitioning. Reward Shaping in RLVR. Reward shaping techniques augment sparse terminal signals with detailed feedback (Ng et al., 1999; Grzes, 2017; Hu et al., 2020). Classical potential-based methods rely on handcrafted Markovian potentials (Sutton et al., 1998), while modern approaches employ distance rewards for navigation (Ivanitskiy et al., 2023), milestone rewards for reasoning (Luo et al., 2025b), and hindsight relabeling for sparse rewards (Velu et al., 2023). Yet, these methods often require considerable taskspecific designs or lack theoretical guarantees (Ibrahim et al., 2024; Zou et al., 2019). In contrast, SSL introduces unified, task-agnostic principle: partitioning solution space into Figure 2. Sweet Spot Learning Overview. We visualize sweetspot zones and their instantiation across diverse tasks . coarse, undiff"
[02.02.2026 05:01] Mistral response. {"id": "5792a2f8aa6444968e861f507968f416", "created": 1770008488, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1504, "total_tokens": 1553, "completion_tokens": 49, "num_cached_tokens": 1503}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Tsinghua University\",\n    \"Xiaomi Corporation\",\n    \"Zhejiang University\",\n    \"Nanyang Technological University\",\n    \"Institute of Automation, Chinese Academy of Sciences\"\n]\n```"}}]}
[02.02.2026 05:01] Response: ```python
[
    "Tsinghua University",
    "Xiaomi Corporation",
    "Zhejiang University",
    "Nanyang Technological University",
    "Institute of Automation, Chinese Academy of Sciences"
]
```
[02.02.2026 05:01] Deleting PDF ./assets/pdf/2601.22491.pdf.
[02.02.2026 05:01] Success.
[02.02.2026 05:01] Downloading and parsing paper https://huggingface.co/papers/2601.22141.
[02.02.2026 05:01] Downloading paper 2601.22141 from https://arxiv.org/pdf/2601.22141v1...
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Grzegorz Stefa nski 1 Alberto Presta 1 Micha Byra 1 2 6 2 0 2 9 2 ] A . [ 1 1 4 1 2 2 . 1 0 6 2 : r a "
[02.02.2026 05:01] Response: ```python
[]
```
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data Grzegorz Stefa nski 1 Alberto Presta 1 Micha Byra 1 2 6 2 0 2 9 2 ] A . [ 1 1 4 1 2 2 . 1 0 6 2 : r aIn pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms singleand multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, performance drop under aggressive pruning, and introduce subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning. 1. Introduction Despite the remarkable progress deep neural networks have achieved over the past decades, modern models often require billions of parameters and hundreds of gigaflops per inference, making them impractical for deployment in resourceconstrained or real-time settings. This inefficiency stands in stark contrast to biological intelligence, which achieves high performance with extreme parsimony. While specialized hardware continues to advance, the growth in model scale consistently outpaces gains in 1Samsung AI Center Warsaw, Poland 2Institute of Fundamental Technological Research, Polish Academy of SciCorrespondence to: Grzegorz Stefanski ences, Poland. <g.stefanski@samsung.com>. Preprint. January 30, 2026. 1 computational efficiency. Consequently, reducing model complexity is not only crucial for practical deployment but also for understanding the fundamental principles of generalization in deep learning. Pruning has been cornerstone of model efficiency research (Cheng et al., 2024). By removing redundant weights or structures, it aims to uncover smaller subnetworks that retain the performance of their dense counterparts. The Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2019) revitalized this line of work by positing that winning tickets, i.e. sparse subnetworks within large, randomly initialized networks, can be trained in isolation to match full-model accuracy. This reframed pruning as discovery process aiming to reveal the minimal structures responsible for learning. However, nearly all LTH-inspired methods assume universal subnetwork - single sparse mask applied uniformly across all inputs, overlooking real-world data heterogeneity. Different classes, clusters, or environmental conditions often rely on distinct feature representations. one-sizefits-all mask may therefore sacrifice performance by forcing diverse patterns through shared, rigid architecture. Bridging this gap requires moving beyond global sparsity toward adaptive, data-aware pruning, shift that aligns model structure with the intrinsic organization of the data itself. In this work, we introduce Routing the Lottery (RTL), novel adaptive pruning framework that rethinks LTH by discovering multiple specialized subnetworks, dubbed adaptive tickets, each tailored to distinct data subset (e.g., class or semantic cluster). This shift enables the model to allocate representational capacity heterogeneously across the input space, aligning sparsity with data structure rather than enforcing uniform compression. Our framework achieves specialization through pruning alone, without auxiliary routing networks or additional parameters. It discovers multiple stable, sparse subnetworks, each tied to class or cluster, and selects them via simple context-based routing (e.g., label or environment), offering lightweight and interpretable alternative to Mixture-of-Experts (MoE) architectures that prioritizes structural efficiency over dynamic routing flexibility. This work serves as precursor, aiming to guide pruning from static tool into dynamic mechanism for building modular and semantically grounded models. The major Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data contributions of this paper are: RTL jointly learns multiple sparse subnetworks from shared dense initialization, with masks adapted to data subsets while preserving parameter sharing, maintaining single compact backbone and employing mask-based routing to enable context-aware inference. We show that class-specific subnetworks on CIFAR-10 outperform both single-mask and multi-model pruning baselines while using up to an order of magnitude fewer parameters than independent models, and that on CIFAR-100, RTL scales naturally to enable effective specialization across larger number of classes. We validate RTL on real-world speech enhancement task, where subnetworks specialized for acoustic environments achieve higher SI-SNRi than universal or independent baselines. 2. Related Work Pruning has rich history, beginning with sensitivity-based methods like Optimal Brain Damage (LeCun et al., 1989) and Optimal Brain Surgeon (Hassibi et al., 1993). In particular, unstructured pruning removes non-relevant weights without imposing structural constraints, i.e., without removing entire layers. (Han et al., 2015) performed pruning by learning meaningful connections, while (Yang et al., 2017; 2018) exploit energy consumption as metric for removing elements. (Sreenivasan et al., 2022; Tartaglione et al., 2022) introduced regularization terms to constrain the magnitude of non-relevant parameters during iterative training, while (Benbaki et al., 2023) considered the combined effect of pruning and updating multiple weights under sparsity constraint. Furthermore, (Tartaglione et al., 2020) compared and analyze two different pruning approaches, i.e. one-shot and gradual, highlighting that the latter allows for better generalization. (Zhang et al., 2024) analyzed fundamental aspects of pruning, identifying two key factors that determine the pruning ratio limit, i.e., weight magnitude and network sharpness, while (Liao et al., 2023; Hur & Ka"
[02.02.2026 05:01] Mistral response. {"id": "60d87530434142e2b94f9e55a7a7288d", "created": 1770008496, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1397, "total_tokens": 1432, "completion_tokens": 35, "num_cached_tokens": 1396}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Samsung AI Center Warsaw, Poland\",\n    \"Institute of Fundamental Technological Research, Polish Academy of Sciences, Poland\"\n]\n```"}}]}
[02.02.2026 05:01] Response: ```python
[
    "Samsung AI Center Warsaw, Poland",
    "Institute of Fundamental Technological Research, Polish Academy of Sciences, Poland"
]
```
[02.02.2026 05:01] Deleting PDF ./assets/pdf/2601.22141.pdf.
[02.02.2026 05:01] Success.
[02.02.2026 05:01] Downloading and parsing paper https://huggingface.co/papers/2601.23161.
[02.02.2026 05:01] Downloading paper 2601.23161 from https://arxiv.org/pdf/2601.23161v1...
[02.02.2026 05:01] Extracting affiliations from text.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 3 ] . [ 1 1 6 1 3 2 . 1 0 6 2 : r DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Jiaming Zhou1,2 Xuxin Cheng2 Shiwan Zhao1 Yuhang Jia1,2 Cao Liu2 Ke Zeng2 Xunliang Cai2 Yong Qin1 1College of Computer Science, Nankai University 2Meituan LongCat Interaction Team https://github.com/NKU-HLT/DIFFA "
[02.02.2026 05:01] Response: ```python
[
    "College of Computer Science, Nankai University",
    "Meituan LongCat Interaction Team"
]
```
[02.02.2026 05:01] Deleting PDF ./assets/pdf/2601.23161.pdf.
[02.02.2026 05:01] Success.
[02.02.2026 05:01] Enriching papers with extra data.
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 0. ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for mu...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 1. ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) a...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 2. TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the ...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 3. DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character ima...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 4. MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 5. DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in huma...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 6. A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their s...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 7. PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but re...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 8. _paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generatin...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 9. ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performa...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 10. RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 11. Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many d...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 12. A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 13. NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically ...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 14. Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with ver...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 15. Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis...
[02.02.2026 05:01] ********************************************************************************
[02.02.2026 05:01] Abstract 16. DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio languag...
[02.02.2026 05:01] Read previous papers.
[02.02.2026 05:01] Generating reviews via LLM API.
[02.02.2026 05:01] Querying the API.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.
[02.02.2026 05:01] Response: ```json
{
  "desc": "ASTRA        ,  ,         .                  -    .   supervised fine-tuning  online RL,             .   ,  ,    ASTRA,  state-of-the-art     ,   .",
  "emoji": "",
  "title": "          "
}
```
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra."

[02.02.2026 05:01] Response: ```python
["AGENTS", "RL", "TRAINING", "DATA", "BENCHMARK"]
```
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra."

[02.02.2026 05:01] Response: ```python
['SYNTHETIC', 'OPTIMIZATION', 'REASONING', 'OPEN_SOURCE']
```
[02.02.2026 05:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ASTRA is an innovative framework designed to enhance the training of tool-augmented language models for complex decision-making tasks. It automates the process by using synthetic data and verifiable reinforcement learning, addressing the limitations of existing methods that often require manual input and lack reliable environments. The framework consists of two main components: a pipeline for generating diverse tool-use trajectories and an environment synthesis framework that creates verifiable scenarios for training. As a result, ASTRA enables models to achieve superior performance in multi-step decision-making while maintaining their reasoning capabilities.","title":"ASTRA: Automating Tool-Augmented Learning for Better Decision-Making"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ASTRA is an innovative framework designed to enhance the training of tool-augmented language models for complex decision-making tasks. It automates the process by using synthetic data and verifiable reinforcement learning, addressing the limitations of existing methods that often require manual input and lack reliable environments. The framework consists of two main components: a pipeline for generating diverse tool-use trajectories and an environment synthesis framework that creates verifiable scenarios for training. As a result, ASTRA enables models to achieve superior performance in multi-step decision-making while maintaining their reasoning capabilities.', title='ASTRA: Automating Tool-Augmented Learning for Better Decision-Making'))
[02.02.2026 05:01] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ASTRAASTRA-ASTRA","title":"ASTRA"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ASTRAASTRA-ASTRA', title='ASTRA'))
[02.02.2026 05:01] Querying the API.
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.
[02.02.2026 05:01] Response: ```json
{
  "desc": "ThinkSafe    ,          .       (refusal steering),         ,    .       ,       .                .",
  "emoji": "",
  "title": "  :     "
}
```
[02.02.2026 05:01] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git."

[02.02.2026 05:02] Response: ```python
["RL", "TRAINING", "RLHF"]
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git."

[02.02.2026 05:02] Response: ```python
['ALIGNMENT', 'REASONING', 'OPEN_SOURCE']
```
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ThinkSafe is a novel framework designed to enhance the safety of large reasoning models (LRMs) while maintaining their reasoning capabilities. It employs lightweight refusal steering and fine-tuning on responses generated by the model itself, avoiding the need for external teacher distillation which can introduce discrepancies. This approach allows the model to leverage its inherent knowledge to identify harmful prompts and generate safer responses. Experiments demonstrate that ThinkSafe significantly improves safety without compromising reasoning performance and does so with lower computational costs.","title":"Enhancing Safety in Reasoning Models with ThinkSafe"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ThinkSafe is a novel framework designed to enhance the safety of large reasoning models (LRMs) while maintaining their reasoning capabilities. It employs lightweight refusal steering and fine-tuning on responses generated by the model itself, avoiding the need for external teacher distillation which can introduce discrepancies. This approach allows the model to leverage its inherent knowledge to identify harmful prompts and generate safer responses. Experiments demonstrate that ThinkSafe significantly improves safety without compromising reasoning performance and does so with lower computational costs.', title='Enhancing Safety in Reasoning Models with ThinkSafe'))
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ThinkSafeThinkSafeThinkSafe","title":"ThinkSafe"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ThinkSafeThinkSafeThinkSafe', title='ThinkSafe'))
[02.02.2026 05:02] Querying the API.
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.
[02.02.2026 05:02] Response: ```json
{
  "desc": "   TTCS           ,     :     .       ,     ,     .  ,   ,        ,     .   ,  TTCS              .",
  "emoji": "",
  "title": "  LLM     "
}
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS."

[02.02.2026 05:02] Response: ```python
['TRAINING', 'MATH', 'BENCHMARK']
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS."

[02.02.2026 05:02] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TTCS is a novel framework designed to enhance the reasoning capabilities of large language models (LLMs) during test-time training. It operates by co-evolving two components: a question synthesizer that generates increasingly difficult question variants and a reasoning solver that improves its performance based on feedback from these questions. This iterative process allows the synthesizer to tailor questions to the solver\'s current abilities, creating a dynamic learning environment. The results demonstrate that TTCS effectively boosts reasoning skills on complex tasks and can be applied across various LLM architectures, paving the way for adaptive test-time learning.","title":"Dynamic Questioning for Enhanced Reasoning in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="TTCS is a novel framework designed to enhance the reasoning capabilities of large language models (LLMs) during test-time training. It operates by co-evolving two components: a question synthesizer that generates increasingly difficult question variants and a reasoning solver that improves its performance based on feedback from these questions. This iterative process allows the synthesizer to tailor questions to the solver's current abilities, creating a dynamic learning environment. The results demonstrate that TTCS effectively boosts reasoning skills on complex tasks and can be applied across various LLM architectures, paving the way for adaptive test-time learning.", title='Dynamic Questioning for Enhanced Reasoning in LLMs'))
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TTCSLLMTTCSLLM","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TTCSLLMTTCSLLM', title=''))
[02.02.2026 05:02] Querying the API.
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/
[02.02.2026 05:02] Response: ```json
{
  "desc": "DreamActor-M2        ,          .          ,           .              ,           RGB- .             .",
  "emoji": "",
  "title": "        "
}
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/"

[02.02.2026 05:02] Response: ```python
['VIDEO', 'MULTIMODAL', 'BENCHMARK', 'DATASET']
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/"

[02.02.2026 05:02] Response: ```python
['SYNTHETIC']
```

The paper discusses a self-bootstrapped data synthesis pipeline that generates pseudo cross-identity training pairs, which directly relates to using synthetic data for training and leveraging artificial data for improved model generalization.
[02.02.2026 05:02] Error. Failed to parse JSON from LLM. ["SYNTHETIC"]


The paper discusses a self-bootstrapped data synthesis pipeline that generates pseudo cross-identity training pairs, which directly relates to using synthetic data for training and leveraging artificial data for improved model generalization.
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DreamActor-M2 is a novel framework for character animation that improves how motion is applied to static images. It tackles two main issues: the balance between keeping a character\'s identity and ensuring smooth motion, and the limitations of using fixed pose references like skeletons. By using in-context learning, it combines visual and motion information into a single representation, allowing for better understanding of both identity and movement. Additionally, it employs a self-bootstrapped data synthesis method to create diverse training examples, enhancing its ability to animate various character types effectively.","title":"Revolutionizing Character Animation with DreamActor-M2"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="DreamActor-M2 is a novel framework for character animation that improves how motion is applied to static images. It tackles two main issues: the balance between keeping a character's identity and ensuring smooth motion, and the limitations of using fixed pose references like skeletons. By using in-context learning, it combines visual and motion information into a single representation, allowing for better understanding of both identity and movement. Additionally, it employs a self-bootstrapped data synthesis method to create diverse training examples, enhancing its ability to animate various character types effectively.", title='Revolutionizing Character Animation with DreamActor-M2'))
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DreamActor-M2DreamActor-M2RGBDreamActor-M2","title":"M2"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DreamActor-M2DreamActor-M2RGBDreamActor-M2', title='M2'))
[02.02.2026 05:02] Querying the API.
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.
[02.02.2026 05:02] Response: ```json
{
  "desc": "MemOCR       ,          .     ,       ,    ,     .           ,     ,       .           ,  MemOCR             .",
  "emoji": "",
  "title": "      "
}
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets."

[02.02.2026 05:02] Response: ```python
["AGENTS", "MULTIMODAL", "RL", "BENCHMARK"]
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets."

[02.02.2026 05:02] Response: ```python
['LONG_CONTEXT', 'REASONING']
```
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MemOCR is a novel multimodal memory agent designed to enhance reasoning over long interactions by compressing historical data into visual formats. Unlike traditional memory systems that use text serialization, MemOCR adapts its memory allocation based on the importance of information, allowing for efficient use of limited context space. It employs a structured rich-text memory that is transformed into images, prioritizing essential details while minimizing less critical information. By utilizing reinforcement learning with budget-aware objectives, MemOCR demonstrates superior performance in question-answering tasks, effectively managing context under strict budget constraints.","title":"MemOCR: Smart Memory for Efficient Long-Horizon Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MemOCR is a novel multimodal memory agent designed to enhance reasoning over long interactions by compressing historical data into visual formats. Unlike traditional memory systems that use text serialization, MemOCR adapts its memory allocation based on the importance of information, allowing for efficient use of limited context space. It employs a structured rich-text memory that is transformed into images, prioritizing essential details while minimizing less critical information. By utilizing reinforcement learning with budget-aware objectives, MemOCR demonstrates superior performance in question-answering tasks, effectively managing context under strict budget constraints.', title='MemOCR: Smart Memory for Efficient Long-Horizon Reasoning'))
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MemOCRMemOCRMemOCR","title":"MemOCR"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MemOCRMemOCRMemOCR', title='MemOCR'))
[02.02.2026 05:02] Querying the API.
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.
[02.02.2026 05:02] Response: ```json
{
  "desc": "DenseGRPO       flow matching         .         ,             .         ,     ,         .                    .",
  "emoji": "",
  "title": "       "
}
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment."

[02.02.2026 05:02] Response: ```python
["RLHF", "TRAINING", "BENCHMARK", "MULTIMODAL"]
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment."

[02.02.2026 05:02] Response: ```python
['ALIGNMENT', 'OPTIMIZATION', 'DIFFUSION']
```
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DenseGRPO is a new framework designed to tackle the sparse reward problem in flow matching models used for text-to-image generation. It introduces dense rewards for each intermediate denoising step, allowing for a more accurate evaluation of contributions at each stage. By predicting step-wise reward gains and applying a reward model to intermediate clean images, DenseGRPO aligns feedback signals with individual contributions effectively. Additionally, it calibrates the exploration space by adjusting stochasticity based on the noise intensity, ensuring optimal exploration throughout the denoising process.","title":"Dense Rewards for Better Flow Matching"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DenseGRPO is a new framework designed to tackle the sparse reward problem in flow matching models used for text-to-image generation. It introduces dense rewards for each intermediate denoising step, allowing for a more accurate evaluation of contributions at each stage. By predicting step-wise reward gains and applying a reward model to intermediate clean images, DenseGRPO aligns feedback signals with individual contributions effectively. Additionally, it calibrates the exploration space by adjusting stochasticity based on the noise intensity, ensuring optimal exploration throughout the denoising process.', title='Dense Rewards for Better Flow Matching'))
[02.02.2026 05:02] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DenseGRPODenseGRPO","title":"DenseGRPO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DenseGRPODenseGRPO', title='DenseGRPO'))
[02.02.2026 05:02] Querying the API.
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.
[02.02.2026 05:02] Response: ```json
{
  "desc": "    ,                .   ,        ,                .      ,    ,         .      ,  supervised fine-tuning    policy optimization.",
  "emoji": "",
  "title": "        LLM"
}
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning."

[02.02.2026 05:02] Response: ```python
["TRAINING", "BENCHMARK", "RLHF"]
```
[02.02.2026 05:02] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning."

[02.02.2026 05:03] Response: ```python
["REASONING", "ALIGNMENT"]
```

**Justification:**

- **REASONING**: The paper explicitly focuses on improving reasoning accuracy in LLMs through formal logic verification. It addresses logical inconsistencies, error detection/rectification during reasoning chains, and evaluates performance on mathematical, logical, and general reasoning benchmarks.

- **ALIGNMENT**: The framework aims to align LLM behavior with formal logical correctness by penalizing intermediate fallacies and reducing errors/reward hacking, which represents alignment with intended logical behavior and human-defined formal verification standards.
[02.02.2026 05:03] Error. Failed to parse JSON from LLM. ["REASONING", "ALIGNMENT"]


**Justification:**

- **REASONING**: The paper explicitly focuses on improving reasoning accuracy in LLMs through formal logic verification. It addresses logical inconsistencies, error detection/rectification during reasoning chains, and evaluates performance on mathematical, logical, and general reasoning benchmarks.

- **ALIGNMENT**: The framework aims to align LLM behavior with formal logical correctness by penalizing intermediate fallacies and reducing errors/reward hacking, which represents alignment with intended logical behavior and human-defined formal verification standards.
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework that combines formal logic verification with natural language generation to enhance the reasoning capabilities of large language models (LLMs). By integrating symbolic verification into the generation process, the framework provides immediate feedback to identify and correct logical errors in real-time. Unlike previous methods that only validate reasoning after it has occurred, this approach actively penalizes mistakes during the reasoning process. The authors demonstrate that their models, trained with this framework, significantly outperform existing models on various reasoning tasks, showcasing the effectiveness of formal verification in improving LLM performance.","title":"Enhancing LLM Reasoning with Real-Time Logic Verification"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework that combines formal logic verification with natural language generation to enhance the reasoning capabilities of large language models (LLMs). By integrating symbolic verification into the generation process, the framework provides immediate feedback to identify and correct logical errors in real-time. Unlike previous methods that only validate reasoning after it has occurred, this approach actively penalizes mistakes during the reasoning process. The authors demonstrate that their models, trained with this framework, significantly outperform existing models on various reasoning tasks, showcasing the effectiveness of formal verification in improving LLM performance.', title='Enhancing LLM Reasoning with Real-Time Logic Verification'))
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LLMsLLM","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LLMsLLM', title=''))
[02.02.2026 05:03] Querying the API.
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.
[02.02.2026 05:03] Response: ```json
{
  "desc": "PLaT      ,      .        ,           .       ,   ,      .        ,         .",
  "emoji": "",
  "title": "   :      "
}
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search."

[02.02.2026 05:03] Response: ```python
["ARCHITECTURE", "TRAINING", "INFERENCE"]
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search."

[02.02.2026 05:03] Response: ```python
["REASONING", "INTERPRETABILITY"]
```
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PLaT introduces a novel framework for latent reasoning that separates the reasoning process from the generation of text. This allows the model to dynamically decide when to stop reasoning, improving efficiency compared to traditional methods that rely on fixed steps. By modeling reasoning as a series of latent planning states, PLaT enhances scalability and diversity in problem-solving. Empirical results show that while it may have lower immediate accuracy, it offers a more flexible and robust approach to reasoning in complex tasks.","title":"Decoupling Reasoning for Scalable AI Solutions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PLaT introduces a novel framework for latent reasoning that separates the reasoning process from the generation of text. This allows the model to dynamically decide when to stop reasoning, improving efficiency compared to traditional methods that rely on fixed steps. By modeling reasoning as a series of latent planning states, PLaT enhances scalability and diversity in problem-solving. Empirical results show that while it may have lower immediate accuracy, it offers a more flexible and robust approach to reasoning in complex tasks.', title='Decoupling Reasoning for Scalable AI Solutions'))
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PLaTPLaT","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PLaTPLaT', title=''))
[02.02.2026 05:03] Querying the API.
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.
[02.02.2026 05:03] Response: ```json
{
  "desc": "PaperBanana    ,            -     .        ,    ,       .   PaperBananaBench    292       NeurIPS 2025,       .   ,       , ,   ,        .",
  "emoji": "",
  "title": "      "
}
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations."

[02.02.2026 05:03] Response: ```python
["AGENTS", "MULTIMODAL", "DATASET", "BENCHMARK", "CV"]
```

**Justification:**
- **AGENTS**: The paper explicitly describes "PaperBanana" as an "agentic framework" that "orchestrates specialized agents" for different tasks
- **MULTIMODAL**: The framework uses "vision-language models and image generation techniques," combining visual and language modalities
- **DATASET**: The paper introduces "PaperBananaBench," a new dataset "comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications"
- **BENCHMARK**: PaperBananaBench is explicitly described as a benchmark for evaluating the framework
- **CV**: The paper focuses on image generation and visual processing of academic illustrations
[02.02.2026 05:03] Error. Failed to parse JSON from LLM. ["AGENTS", "MULTIMODAL", "DATASET", "BENCHMARK", "CV"]


**Justification:**
- **AGENTS**: The paper explicitly describes "PaperBanana" as an "agentic framework" that "orchestrates specialized agents" for different tasks
- **MULTIMODAL**: The framework uses "vision-language models and image generation techniques," combining visual and language modalities
- **DATASET**: The paper introduces "PaperBananaBench," a new dataset "comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications"
- **BENCHMARK**: PaperBananaBench is explicitly described as a benchmark for evaluating the framework
- **CV**: The paper focuses on image generation and visual processing of academic illustrations
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations."

[02.02.2026 05:03] Response: ```python
['SCIENCE', 'OPEN_SOURCE']
```
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PaperBanana is an innovative framework designed to automate the creation of high-quality academic illustrations using advanced vision-language models (VLMs) and image generation techniques. It addresses the challenge of generating publication-ready visuals, which is often a time-consuming task for researchers. The framework utilizes specialized agents to gather references, plan the design, create images, and refine them through self-critique. Evaluation through PaperBananaBench shows that this system outperforms existing methods in terms of accuracy, clarity, and visual appeal, making it a significant advancement in the research workflow.","title":"Automating Academic Illustrations with PaperBanana"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PaperBanana is an innovative framework designed to automate the creation of high-quality academic illustrations using advanced vision-language models (VLMs) and image generation techniques. It addresses the challenge of generating publication-ready visuals, which is often a time-consuming task for researchers. The framework utilizes specialized agents to gather references, plan the design, create images, and refine them through self-critique. Evaluation through PaperBananaBench shows that this system outperforms existing methods in terms of accuracy, clarity, and visual appeal, making it a significant advancement in the research workflow.', title='Automating Academic Illustrations with PaperBanana'))
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PaperBanana-PaperBananaBench292PaperBanana","title":"PaperBanana"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PaperBanana-PaperBananaBench292PaperBanana', title='PaperBanana'))
[02.02.2026 05:03] Querying the API.
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.
[02.02.2026 05:03] Response: ```json
{
  "desc": "ReGuLaR          ,   .      ,     -     .         ,         .   ,  ReGuLaR           ,    Chain-of-Thought .",
  "emoji": "",
  "title": "       "
}
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR."

[02.02.2026 05:03] Response: ```python
['TRAINING', 'MULTIMODAL', 'ARCHITECTURE']
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR."

[02.02.2026 05:03] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReGuLaR presents a new approach to latent reasoning by integrating a variational auto-encoding framework that compresses reasoning processes into a latent space. It utilizes rendered images of explicit reasoning chains to guide the compression, ensuring that performance is maintained while reducing computational redundancy. This method allows for efficient sampling of latent states based on previous reasoning, enhancing the model\'s ability to learn effectively. Experimental results show that ReGuLaR outperforms existing methods in both efficiency and reasoning capabilities, even exceeding traditional Chain-of-Thought techniques.","title":"Efficient Latent Reasoning with Visual Guidance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="ReGuLaR presents a new approach to latent reasoning by integrating a variational auto-encoding framework that compresses reasoning processes into a latent space. It utilizes rendered images of explicit reasoning chains to guide the compression, ensuring that performance is maintained while reducing computational redundancy. This method allows for efficient sampling of latent states based on previous reasoning, enhancing the model's ability to learn effectively. Experimental results show that ReGuLaR outperforms existing methods in both efficiency and reasoning capabilities, even exceeding traditional Chain-of-Thought techniques.", title='Efficient Latent Reasoning with Visual Guidance'))
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReGuLaR-ReGuLaRCoT","title":"ReGuLaR"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ReGuLaR-ReGuLaRCoT', title='ReGuLaR'))
[02.02.2026 05:03] Querying the API.
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.
[02.02.2026 05:03] Response: ```json
{
  "desc": "        RLHF,   -      .   R2M   ,                   .                    .                .",
  "emoji": "",
  "title": "       "
}
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models."

[02.02.2026 05:03] Response: ```python
["RLHF", "TRAINING"]
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models."

[02.02.2026 05:03] Response: ```python
["ALIGNMENT"]
```
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses a problem in Reinforcement Learning from Human Feedback (RLHF) where reward models can lead to overoptimization, causing policy models to misalign with human preferences. The authors introduce R2M, a new framework that incorporates real-time feedback from policy models to improve reward modeling during training. By using the evolving hidden states of the policy, R2M dynamically adjusts the reward model to better match the current policy distribution. This approach aims to reduce the reward discrepancy and enhance the alignment between the reward model and the policy model, ultimately improving the performance of large language models.","title":"R2M: Real-Time Feedback for Better Reward Alignment"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses a problem in Reinforcement Learning from Human Feedback (RLHF) where reward models can lead to overoptimization, causing policy models to misalign with human preferences. The authors introduce R2M, a new framework that incorporates real-time feedback from policy models to improve reward modeling during training. By using the evolving hidden states of the policy, R2M dynamically adjusts the reward model to better match the current policy distribution. This approach aims to reduce the reward discrepancy and enhance the alignment between the reward model and the policy model, ultimately improving the performance of large language models.', title='R2M: Real-Time Feedback for Better Reward Alignment'))
[02.02.2026 05:03] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RLHFLLMRLHFR2MR2M","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RLHFLLMRLHFR2MR2M', title=''))
[02.02.2026 05:03] Querying the API.
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.
[02.02.2026 05:03] Response: ```json
{
  "desc": "         Landmark pooling,        (pooling)    .  ,    [CLS]      ,      . Landmark pooling             ,    .  ,         ,      .",
  "emoji": "",
  "title": "     "
}
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods."

[02.02.2026 05:03] Response: ```python
["ARCHITECTURE", "TRAINING"]
```
[02.02.2026 05:03] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods."

[02.02.2026 05:03] Response: ```python
["LONG_CONTEXT"]
```
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Landmark (LMK) pooling, a new method for improving representation learning in long-context sequences. Traditional pooling methods, like [CLS] tokens and mean pooling, often fail to capture important information from both local and global contexts. LMK pooling addresses these weaknesses by dividing sequences into chunks and using landmark tokens to maintain critical information. The results show that LMK pooling performs well on short-context tasks and significantly enhances performance on long-context tasks, making it a valuable alternative to existing methods.","title":"Enhancing Long-Context Learning with Landmark Pooling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Landmark (LMK) pooling, a new method for improving representation learning in long-context sequences. Traditional pooling methods, like [CLS] tokens and mean pooling, often fail to capture important information from both local and global contexts. LMK pooling addresses these weaknesses by dividing sequences into chunks and using landmark tokens to maintain critical information. The results show that LMK pooling performs well on short-context tasks and significantly enhances performance on long-context tasks, making it a valuable alternative to existing methods.', title='Enhancing Long-Context Learning with Landmark Pooling'))
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Landmark pooling","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Landmark pooling', title=''))
[02.02.2026 05:04] Querying the API.
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.
[02.02.2026 05:04] Response: ```json
{
  "desc": "    DINO-SAE  ,       Vision Foundation Models     .     ,          ,         ,    .         Riemannian Flow Matching  Diffusion Transformer,      .       (0.37 rFID, 26.2 dB PSNR)        .",
  "emoji": "",
  "title": " :     "
}
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs."

[02.02.2026 05:04] Response: ```python
["CV", "ARCHITECTURE", "TRAINING"]
```

**Justification:**

- **CV**: The paper develops computer vision methods, specifically a vision autoencoder framework for image reconstruction and generation tasks on ImageNet-1K.

- **ARCHITECTURE**: The paper proposes novel neural architecture components including the Hierarchical Convolutional Patch Embedding module, Cosine Similarity Alignment objective, and Riemannian Flow Matching approach with Diffusion Transformer.

- **TRAINING**: The paper presents improved training methodologies, including the Riemannian Flow Matching training approach for the Diffusion Transformer on spherical latent manifolds, and the Cosine Similarity Alignment training objective.
[02.02.2026 05:04] Error. Failed to parse JSON from LLM. ["CV", "ARCHITECTURE", "TRAINING"]


**Justification:**

- **CV**: The paper develops computer vision methods, specifically a vision autoencoder framework for image reconstruction and generation tasks on ImageNet-1K.

- **ARCHITECTURE**: The paper proposes novel neural architecture components including the Hierarchical Convolutional Patch Embedding module, Cosine Similarity Alignment objective, and Riemannian Flow Matching approach with Diffusion Transformer.

- **TRAINING**: The paper presents improved training methodologies, including the Riemannian Flow Matching training approach for the Diffusion Transformer on spherical latent manifolds, and the Cosine Similarity Alignment training objective.
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs."

[02.02.2026 05:04] Response: ```python
['DIFFUSION', 'OPTIMIZATION']
```

**Justification:**

1. **DIFFUSION**: The paper explicitly discusses "Riemannian Flow Matching to train a Diffusion Transformer (DiT)" and uses diffusion-based generative models as a core component of their framework.

2. **OPTIMIZATION**: The paper addresses training efficiency and optimization, noting that "our Riemannian Flow Matching-based DiT exhibits efficient convergence" and discusses improvements in reconstruction fidelity and efficiency through novel training objectives and architectural choices.
[02.02.2026 05:04] Error. Failed to parse JSON from LLM. ["DIFFUSION", "OPTIMIZATION"]


**Justification:**

1. **DIFFUSION**: The paper explicitly discusses "Riemannian Flow Matching to train a Diffusion Transformer (DiT)" and uses diffusion-based generative models as a core component of their framework.

2. **OPTIMIZATION**: The paper addresses training efficiency and optimization, noting that "our Riemannian Flow Matching-based DiT exhibits efficient convergence" and discusses improvements in reconstruction fidelity and efficiency through novel training objectives and architectural choices.
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the DINO Spherical Autoencoder (DINO-SAE), a new framework that enhances image reconstruction by combining semantic representation with pixel-level details. It addresses the common issue of losing high-frequency details in existing generative models by using a Hierarchical Convolutional Patch Embedding module and a Cosine Similarity Alignment objective. The framework leverages Riemannian Flow Matching to train a Diffusion Transformer on a spherical latent space, which improves both fidelity and efficiency in reconstruction tasks. Experimental results show that DINO-SAE achieves state-of-the-art performance on ImageNet-1K, demonstrating superior reconstruction quality and semantic alignment with pretrained Vision Foundation Models.","title":"Bridging Semantic and Pixel-Level Reconstruction with DINO-SAE"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the DINO Spherical Autoencoder (DINO-SAE), a new framework that enhances image reconstruction by combining semantic representation with pixel-level details. It addresses the common issue of losing high-frequency details in existing generative models by using a Hierarchical Convolutional Patch Embedding module and a Cosine Similarity Alignment objective. The framework leverages Riemannian Flow Matching to train a Diffusion Transformer on a spherical latent space, which improves both fidelity and efficiency in reconstruction tasks. Experimental results show that DINO-SAE achieves state-of-the-art performance on ImageNet-1K, demonstrating superior reconstruction quality and semantic alignment with pretrained Vision Foundation Models.', title='Bridging Semantic and Pixel-Level Reconstruction with DINO-SAE'))
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DINODINO-SAERiemannianDiTDINO-SAEImageNet-1K","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DINODINO-SAERiemannianDiTDINO-SAEImageNet-1K', title=''))
[02.02.2026 05:04] Querying the API.
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.
[02.02.2026 05:04] Response: ```json
{
  "desc": "NativeTok       ,         .    Meta Image Transformer      Mixture of Causal Expert Transformer,           .          ,         .                 .",
  "emoji": "",
  "title": "      "
}
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok."

[02.02.2026 05:04] Response: ```python
['CV', 'ARCHITECTURE', 'TRAINING']
```

**Justification:**

- **CV**: The paper focuses on visual tokenization and image generation/reconstruction, which are core computer vision tasks.
- **ARCHITECTURE**: The paper proposes novel neural architectures including the Meta Image Transformer (MIT) and Mixture of Causal Expert Transformer (MoCET).
- **TRAINING**: The paper introduces a "Hierarchical Native Training strategy" that is specifically designed to improve training efficiency.
[02.02.2026 05:04] Error. Failed to parse JSON from LLM. ["CV", "ARCHITECTURE", "TRAINING"]


**Justification:**

- **CV**: The paper focuses on visual tokenization and image generation/reconstruction, which are core computer vision tasks.
- **ARCHITECTURE**: The paper proposes novel neural architectures including the Meta Image Transformer (MIT) and Mixture of Causal Expert Transformer (MoCET).
- **TRAINING**: The paper introduces a "Hierarchical Native Training strategy" that is specifically designed to improve training efficiency.
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok."

[02.02.2026 05:04] Response: ```python
['OPTIMIZATION']
```

The paper focuses on improving the efficiency and coherence of image tokenization and generation through novel architectural designs (Meta Image Transformer and Mixture of Causal Expert Transformer) and training strategies (Hierarchical Native Training). This represents an advancement in training optimization methods for visual generation tasks.
[02.02.2026 05:04] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on improving the efficiency and coherence of image tokenization and generation through novel architectural designs (Meta Image Transformer and Mixture of Causal Expert Transformer) and training strategies (Hierarchical Native Training). This represents an advancement in training optimization methods for visual generation tasks.
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NativeTok presents a new method for visual tokenization that maintains causal relationships during the encoding of images. This approach utilizes a Meta Image Transformer for modeling latent images and a Mixture of Causal Expert Transformer to generate tokens based on previous tokens and features. By enforcing these dependencies, NativeTok improves the coherence and quality of generated images compared to traditional methods that treat tokens as unordered. The framework also incorporates a Hierarchical Native Training strategy to enhance training efficiency by updating only new expert blocks.","title":"Enhancing Image Generation with Causal Tokenization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NativeTok presents a new method for visual tokenization that maintains causal relationships during the encoding of images. This approach utilizes a Meta Image Transformer for modeling latent images and a Mixture of Causal Expert Transformer to generate tokens based on previous tokens and features. By enforcing these dependencies, NativeTok improves the coherence and quality of generated images compared to traditional methods that treat tokens as unordered. The framework also incorporates a Hierarchical Native Training strategy to enhance training efficiency by updating only new expert blocks.', title='Enhancing Image Generation with Causal Tokenization'))
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NativeTokMeta Image TransformerMixture of Causal Expert TransformerVQNativeTok","title":"NativeTok"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NativeTokMeta Image TransformerMixture of Causal Expert TransformerVQNativeTok', title='NativeTok'))
[02.02.2026 05:04] Querying the API.
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.
[02.02.2026 05:04] Response: ```json
{
  "desc": "Sweet Spot Learning (SSL)       ,            .        , SSL    ,       .  ,              .        2.5x       .",
  "emoji": "",
  "title": "    :      "
}
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents."

[02.02.2026 05:04] Response: ```python
['RL', 'AGENTS', 'BENCHMARK']
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents."

[02.02.2026 05:04] Response: ```python
['REASONING', 'OPTIMIZATION', 'TRANSFER_LEARNING']
```
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Sweet Spot Learning (SSL) is a new approach in reinforcement learning that uses tiered rewards to help agents find the best solutions more efficiently. Instead of just giving a simple yes or no reward, SSL provides different levels of rewards based on how close the agent\'s actions are to the optimal solution. This method not only improves how quickly agents learn but also helps them apply what they\'ve learned to different tasks. Experiments show that SSL significantly boosts performance, making agents smarter and more adaptable across various challenges.","title":"Unlocking Optimal Learning with Tiered Rewards"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="Sweet Spot Learning (SSL) is a new approach in reinforcement learning that uses tiered rewards to help agents find the best solutions more efficiently. Instead of just giving a simple yes or no reward, SSL provides different levels of rewards based on how close the agent's actions are to the optimal solution. This method not only improves how quickly agents learn but also helps them apply what they've learned to different tasks. Experiments show that SSL significantly boosts performance, making agents smarter and more adaptable across various challenges.", title='Unlocking Optimal Learning with Tiered Rewards'))
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SSLSSLSSL","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SSLSSLSSL', title=''))
[02.02.2026 05:04] Querying the API.
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.
[02.02.2026 05:04] Response: ```json
{
  "desc": "        Routing the Lottery,     ,          .     ,        ,      .         ,   10   ,         .            ,      -  .",
  "emoji": "",
  "title": "      "
}
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning."

[02.02.2026 05:04] Response: ```python
["INFERENCE", "TRAINING"]
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning."

[02.02.2026 05:04] Response: ```python
["OPTIMIZATION"]
```

The paper focuses on pruning and optimization of neural networks through the Lottery Ticket Hypothesis framework, proposing an adaptive pruning method (Routing the Lottery) that discovers specialized subnetworks while reducing parameters. This is directly related to training optimization methods.
[02.02.2026 05:04] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on pruning and optimization of neural networks through the Lottery Ticket Hypothesis framework, proposing an adaptive pruning method (Routing the Lottery) that discovers specialized subnetworks while reducing parameters. This is directly related to training optimization methods.
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Routing the Lottery (RTL) framework enhances traditional pruning methods by identifying multiple specialized subnetworks, known as adaptive tickets, that are optimized for different data conditions. Unlike previous approaches that rely on a single winning ticket, RTL acknowledges the diversity in real-world data by tailoring subnetworks to specific classes or environments. This method not only improves performance metrics like balanced accuracy and recall but also significantly reduces the number of parameters needed, making models more efficient. Additionally, RTL addresses the issue of subnetwork collapse, providing a new similarity score to diagnose oversparsification without requiring labels.","title":"Unlocking Adaptive Subnetworks for Diverse Data Conditions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The Routing the Lottery (RTL) framework enhances traditional pruning methods by identifying multiple specialized subnetworks, known as adaptive tickets, that are optimized for different data conditions. Unlike previous approaches that rely on a single winning ticket, RTL acknowledges the diversity in real-world data by tailoring subnetworks to specific classes or environments. This method not only improves performance metrics like balanced accuracy and recall but also significantly reduces the number of parameters needed, making models more efficient. Additionally, RTL addresses the issue of subnetwork collapse, providing a new similarity score to diagnose oversparsification without requiring labels.', title='Unlocking Adaptive Subnetworks for Diverse Data Conditions'))
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Routing the LotteryRTLRTL10","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Routing the LotteryRTLRTL10', title=''))
[02.02.2026 05:04] Querying the API.
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.
[02.02.2026 05:04] Response: ```json
{
  "desc": "DIFFA-2         ,           .     ,       ,          .         ,       .  ,  DIFFA-2                 .",
  "emoji": "",
  "title": "        "
}
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git."

[02.02.2026 05:04] Response: ```python
["AUDIO", "TRAINING", "ARCHITECTURE", "RLHF"]
```
[02.02.2026 05:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git."

[02.02.2026 05:04] Response: ```python
['DIFFUSION', 'OPEN_SOURCE']
```
[02.02.2026 05:04] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DIFFA-2 is a diffusion-based large audio language model that enhances audio understanding while being more efficient than traditional autoregressive models. It incorporates an upgraded speech encoder and dual adapters to better capture both semantic and acoustic features. The model is trained using a four-stage curriculum that optimizes performance through a combination of alignment and fine-tuning techniques. Experiments demonstrate that DIFFA-2 outperforms its predecessor and competes effectively with leading autoregressive models, showcasing the potential of diffusion-based approaches in audio processing.","title":"Revolutionizing Audio Understanding with Diffusion Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DIFFA-2 is a diffusion-based large audio language model that enhances audio understanding while being more efficient than traditional autoregressive models. It incorporates an upgraded speech encoder and dual adapters to better capture both semantic and acoustic features. The model is trained using a four-stage curriculum that optimizes performance through a combination of alignment and fine-tuning techniques. Experiments demonstrate that DIFFA-2 outperforms its predecessor and competes effectively with leading autoregressive models, showcasing the potential of diffusion-based approaches in audio processing.', title='Revolutionizing Audio Understanding with Diffusion Models'))
[02.02.2026 05:05] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DIFFA-2DIFFA-2DIFFA-2DIFFA","title":"DIFFA-2"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DIFFA-2DIFFA-2DIFFA-2DIFFA', title='DIFFA-2'))
[02.02.2026 05:05] Renaming data file.
[02.02.2026 05:05] Renaming previous data. hf_papers.json to ./d/2026-02-02.json
[02.02.2026 05:05] Saving new data file.
[02.02.2026 05:05] Generating page.
[02.02.2026 05:05] Renaming previous page.
[02.02.2026 05:05] Renaming previous data. index.html to ./d/2026-02-02.html
[02.02.2026 05:05] Writing result.
[02.02.2026 05:05] Renaming log file.
[02.02.2026 05:05] Renaming previous data. log.txt to ./logs/2026-02-02_last_log.txt
