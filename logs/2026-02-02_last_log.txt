[02.02.2026 14:42] Read previous papers.
[02.02.2026 14:42] Generating top page (month).
[02.02.2026 14:42] Writing top page (month).
[02.02.2026 15:39] Read previous papers.
[02.02.2026 15:39] Get feed.
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21558
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23143
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22628
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22975
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23184
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23265
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23182
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22491
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20218
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21716
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22642
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21957
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21468
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22636
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13097
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21358
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22904
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22837
[02.02.2026 15:39] Extract page data from URL. URL: https://huggingface.co/papers/2601.22813
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18241
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23188
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23161
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21419
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20732
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15625
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22664
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22141
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21525
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23228
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23134
[02.02.2026 15:39] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21709
[02.02.2026 15:39] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.02.2026 15:39] No deleted papers detected.
[02.02.2026 15:39] Downloading and parsing papers (pdf, html). Total: 31.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21558.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21558.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21558.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23143.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23143.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23143.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22628.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22628.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22628.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22975.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22975.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22975.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23184.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23184.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23184.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23265.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23265.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23265.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23182.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23182.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23182.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22491.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22491.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22491.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.20218.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.20218.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.20218.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21716.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21716.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21716.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22642.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22642.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22642.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21957.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21957.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21957.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21468.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21468.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21468.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22636.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22636.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22636.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.13097.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.13097.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.13097.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21358.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21358.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21358.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22904.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22904.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22904.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22837.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22837.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22837.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22813.
[02.02.2026 15:39] Downloading paper 2601.22813 from https://arxiv.org/pdf/2601.22813v1...
[02.02.2026 15:39] Extracting affiliations from text.
[02.02.2026 15:39] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation Andrei Panferov 1 Erik Schultheis 1 Soroush Tabesh 1 Dan Alistarh 1 2 6 2 0 2 0 3 ] . [ 1 3 1 8 2 2 . 1 0 6 2 : r a "
[02.02.2026 15:39] Response: ```python
[]
```
[02.02.2026 15:39] Extracting affiliations from text.
[02.02.2026 15:39] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation Andrei Panferov 1 Erik Schultheis 1 Soroush Tabesh 1 Dan Alistarh 1 2 6 2 0 2 0 3 ] . [ 1 3 1 8 2 2 . 1 0 6 2 : r aThe NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fullyquantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github. com/IST-DASLab/Quartet-II. 1. Introduction The computational cost of training state-of-the-art foundation models has been increasing at roughly exponential pace, putting into question the sustainability of the area, e.g. (Amodei & Hernandez, 2018; Sevilla et al., 2022). Pretraining modern Transformer-based foundation values is dominated by dense matrix multiplications (GEMMs), e.g. the linear projections in attention and MLPs, and so, reduc1Institute of Science and Technology Austria 2Red Hat AI. Correspondence to: Dan Alistarh <Dan.Alistarh@ist.ac.at>. Preprint. February 2, 2026. 1 ing the precision of these GEMMs is one of the most direct levers for lowering end-to-end training costs. This motivation has driven steady progression of mixedprecision training recipes, from FP16/BF16 to FP8 (Micikevicius et al., 2022), and now toward 4-bit microscaling floating point formats such as MXFP and NVFP. In these formats, values are stored in 4-bit floating-point encoding, but each small block is accompanied by higher-precision, e.g. FP8, scale, preserving dynamic range while enabling tensor-core acceleration. Recent GPU accelerators provide native support for such formats, with 2-4x throughput gains over FP8 for individual matmuls (NVIDIA, 2024). The key challenge is to retain FP16/FP8-quality optimization while performing most operations at 4-bit precision (Xi et al., 2023; Chmiel et al., 2024). At this scale, naive quantization leads to divergence over long pre-training runs. Emerging work on stable FP4 native training (Tseng et al., 2025; Castro et al., 2025; Chmiel et al., 2025) has converged on two guiding principles. First, the forward pass should seek to maximize representation capacity by minimizing the quantization error of activations and weights, typically measured via mean-square error (MSE). Second, the backward pass is especially sensitive to bias: as such, biased gradient estimators can accumulate systematic error over many steps, making unbiased (or carefully controlled) gradient quantization essential for stable convergence. These insights underpinned NVIDIAs first end-to-end NVFP4 pre-training recipe (NVIDIA et al., 2025) and subsequent refinements, including forward-pass scale selection heuristics (Cook et al., 2025) and improved NVFP4 stability mechanisms (Chen et al., 2025b). Yet, current state-of-the-art FP4 recipes still drop significant accuracy relative to FP8 and FP16. Contributions. In this paper, we improve the current state of the art for NVFP4 native training by revisiting the question of unbiased gradient estimation for the NVFP4 microscaling format. Surprisingly, we show that the prevailing prior solution, element-wise FP4 stochastic rounding (SR), can be significantly improved. We do so by introducing new unbiased quantization routine for microscaling formats, called MicroScaling EDEN (MS-EDEN), that reduces quantization error by moving the stochasticity from individual FP4 values to the microscale factors, while retaining provable unbiasedness in expectation. Based on MS-EDEN, we Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation build Quartet II, fully-NVFP4 linear-layer computation graph that combines (i) high-capacity forward pass using native NVFP4 scaling augmented with the Four-over-Six scale selection heuristic (Cook et al., 2025), with (ii) an unbiased backward pass based on MS-EDEN and efficient inner-dimension randomized block rotations. We provide an analytic comparison showing that Quartet II yields consistently improved gradient estimation across the major matrix multiplications in transformer training, and we validate these improvements in end-to-end LLM pre-training. Finally, we provide kernels enabling efficient execution on NVIDIA Blackwell GPUs, making the proposed recipe practical at scale. In summary, our contributions are as follows: new unbiased quantization primitive called MSEDEN tailored to microscaling FP4 formats that substantially reduces quantization error relative to FP4 stochastic rounding while remaining hardwarecompatible and efficient. fully-NVFP4 linear-layer training graph called Quartet II that combines improved forward-pass quantization with improved unbiased backward-pass quantization (MS-EDEN), yielding better gradient estimates. Empirical validation: we perform extensive ablations and end-to-end accuracy validation via training runs showing consistent accuracy improvements over prior NVFP4 recipes. Efficient kernels: we show that our scheme is efficiently implementable on the NVIDIA Blackwell generation of GPUs, with up to 4.2x speedup vs BF16. 2. Related Work Lower-precision training. Low-precision training is longstanding direction in deep learning, e.g. (Courbariaux et al., 2015; Esser et al., 2019; Panferov et al., 2025a; Micikevicius et al., 2022; Hernandez-Cano et al., 2025). Early demonstrations of 4-bit training and 4-bit matrix multiplications focused on INT4, and established that careful handling of sc"
[02.02.2026 15:39] Mistral response. {"id": "1b42e086938d420787efbceb3e289476", "created": 1770046764, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1579, "total_tokens": 1598, "completion_tokens": 19, "num_cached_tokens": 1578}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Institute of Science and Technology Austria\", \"Red Hat AI\"]\n```"}}]}
[02.02.2026 15:39] Response: ```python
["Institute of Science and Technology Austria", "Red Hat AI"]
```
[02.02.2026 15:39] Deleting PDF ./assets/pdf/2601.22813.pdf.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.18241.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.18241.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.18241.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23188.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23188.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23188.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23161.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23161.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23161.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21419.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21419.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21419.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.20732.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.20732.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.20732.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.15625.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.15625.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.15625.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22664.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22664.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22664.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.22141.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.22141.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.22141.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21525.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21525.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21525.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23228.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23228.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23228.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.23134.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.23134.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.23134.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Downloading and parsing paper https://huggingface.co/papers/2601.21709.
[02.02.2026 15:39] Extra JSON file exists (./assets/json/2601.21709.json), skip PDF parsing.
[02.02.2026 15:39] Paper image links file exists (./assets/img_data/2601.21709.json), skip HTML parsing.
[02.02.2026 15:39] Success.
[02.02.2026 15:39] Enriching papers with extra data.
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 0. ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for mu...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 1. ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) a...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 2. TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the ...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 3. Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.  					AI-generated summary 				 ...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 4. ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performa...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 5. _paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generatin...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 6. Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.  					AI-generated summ...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 7. Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with ver...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 8. DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in huma...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 9. DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character ima...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 10. A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their s...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 11. A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.  					AI-generated summary 				 We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-a...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 12. MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 13. A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.  					AI-generated summary 				 Large Language Models (LLMs) are typica...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 14. RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.  					AI-generated summary 				 We present RM-RF, a lightweight reward model for run-free evaluation of automatica...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 15. PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but re...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 16. A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 17. NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically ...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 18. Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  					AI-generated summary 				 The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to al...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 19. TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.  					AI-generated summary 				 While Large Language Models (LLMs) have shown promise in software e...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 20. Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.  					AI-generated summary 				 Deep search agents powered by large language models have demonstrated strong capab...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 21. DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio languag...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 22. Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.  					AI-generated summary 				 Recent advances in diffusion and flow matching mod...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 23. Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.  					AI-generated summary 				 As digital environments (data distributi...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 24. A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.  					AI-generated summary 				 Large language models (LLMs) can call tools effectively, yet ...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 25. RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 26. Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 27. Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many d...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 28. Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.  					AI-generated summary 				 While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning mult...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 29. A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.  					AI-generated summary 				 In the post-Dennard era, optimizing embedded systems requi...
[02.02.2026 15:39] ********************************************************************************
[02.02.2026 15:39] Abstract 30. Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-s...
[02.02.2026 15:39] Read previous papers.
[02.02.2026 15:39] Generating reviews via LLM API.
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#data", "#benchmark", "#open_source", "#rl", "#agents", "#optimization", "#synthetic", "#training", "#reasoning"], "emoji": "üõ†Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "ASTRA ‚Äî —ç—Ç–æ –∞–≤—Ç
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#open_source", "#rl", "#training", "#reasoning"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–µ–∑ —É—á–∏—Ç–µ–ª–µ–π: —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "ThinkSafe ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#benchmark", "#math", "#open_source", "#optimization", "#training", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è TTCS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ–ø–æ—Å—Ä
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#rl", "#data", "#reasoning", "#small_models", "#dataset", "#training"], "emoji": "ü™ø", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è –∏–∑ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Golden Goose –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#multimodal", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–°–∂–∞—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "ReGuLaR –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã", "desc": "PaperBanana ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –≤
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üåä", "ru": {"title": "–û—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ –¥–µ—Ç–∞–ª—è–º: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–µ–¥—ë–Ω –∞–Ω–∞–ª–∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —á—Ç–æ –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ–¥–∏—Ä—É
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#rl", "#optimization", "#agents", "#reasoning"], "emoji": "üéæ", "ru": {"title": "–¢–æ—á–Ω—ã–π —É–¥–∞—Ä –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–µ—à–µ–Ω–∏–π: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "Sweet Spot Learning (SSL) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#benchmark", "#rlhf", "#diffusion", "#alignment", "#optimization", "#multimodal", "#training"], "emoji": "üéØ", "ru": {"title": "–ü–ª–æ—Ç–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –¥–µ–Ω–æ–π–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "DenseGRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow match
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–µ–∑ —è–≤–Ω—ã—Ö –ø–æ–∑ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "DreamActor-M2 ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#benchmark", "#training", "#rlhf"], "emoji": "‚öôÔ∏è", "ru": {"title": "–§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∫ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–∞—è —Å–∏–ª–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –ª–æ–≥–∏—á–µ—Å–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#small_models", "#cv", "#multimodal", "#open_source", "#dataset", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç PaddleOCR-VL-1.5, –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —Ä–∞–∑–º–µ—Ä
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#rl", "#agents", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–ª–≥–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "MemOCR ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–∂–∞—Ç–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#alignment", "#security"], "emoji": "üîì", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Å–ø–æ—Å–æ–± –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SABER –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ adversarial –∞—Ç–∞–∫–∞–º –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º sampling. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#optimization", "#training", "#science", "#plp", "#dataset", "#multilingual", "#open_source", "#small_models", "#data"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤ –±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç–∫–æ–Ω–æ–º–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ RM-RF ‚Äî –ª—ë–≥–∫–∞—è –º–æ–¥–µ–ª—å-–Ω–∞–≥—Ä–∞–¥–∞, –∫
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#training", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–û—Ç–¥–µ–ª–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –æ—Ç —Å–ª–æ–≤: –≥–∏–±–∫–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —à–∞–≥–æ–≤", "desc": "PLaT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, —Ä–∞–∑–¥–µ–ª—è—è –ø—Ä–æ—Ü–µ—Å—Å
[02.02.2026 15:39] Using data from previous issue: {"categories": [], "emoji": "üåê", "ru": {"title": "–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä: –æ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DINO-SAE ‚Äî –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Vision Foundation Models —Å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–∏–∫—Å–µ–ª–µ–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
[02.02.2026 15:39] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "NativeTok –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤–≤–æ–¥–∏—Ç –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏
[02.02.2026 15:39] Querying the API.
[02.02.2026 15:39] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  					AI-generated summary 				 The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .
[02.02.2026 15:39] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ Quartet II –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ NVFP4 –Ω–∞ GPU NVIDIA Blackwell. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É MS-EDEN –¥–ª—è –Ω–µ–∏—Å–∫–∞–∂–µ–Ω–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–Ω–∏–∂–∞–µ—Ç –æ—à–∏–±–∫—É –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª–µ–µ —á–µ–º –≤ 2 —Ä–∞–∑–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–º –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫–∞–∫ –Ω–∞ –ø—Ä—è–º–æ–º, —Ç–∞–∫ –∏ –Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ Quartet II –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 4.2x –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ BF16 –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ 1.9B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.",
  "emoji": "‚ö°",
  "title": "Quartet II: –ø–æ–ª–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è NVFP4 –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π"
}
```
[02.02.2026 15:39] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  					AI-generated summary 				 The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II ."

[02.02.2026 15:39] Response: ```python
["TRAINING", "INFERENCE"]
```

**Justification:**
- **TRAINING**: The paper focuses on improving model training methods, specifically introducing "Quartet II," a quantized training method that enhances gradient estimation and training efficiency for large language models during pre-training.
- **INFERENCE**: The paper addresses model deployment optimization through quantization (NVFP4 format) and GPU execution efficiency, which are core inference optimization concerns.
[02.02.2026 15:39] Error. Failed to parse JSON from LLM. ["TRAINING", "INFERENCE"]


**Justification:**
- **TRAINING**: The paper focuses on improving model training methods, specifically introducing "Quartet II," a quantized training method that enhances gradient estimation and training efficiency for large language models during pre-training.
- **INFERENCE**: The paper addresses model deployment optimization through quantization (NVFP4 format) and GPU execution efficiency, which are core inference optimization concerns.
[02.02.2026 15:39] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  					AI-generated summary 				 The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II ."

[02.02.2026 15:39] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on improving training optimization methods for large language models through quantized training (Quartet II), enhanced gradient estimation, and faster GPU execution. This directly addresses advancing training optimization methods.

- **OPEN_SOURCE**: The paper explicitly states "Our code is available at https://github.com/IST-DASLab/Quartet-II", indicating the authors are releasing their code/framework to the public.
[02.02.2026 15:39] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on improving training optimization methods for large language models through quantized training (Quartet II), enhanced gradient estimation, and faster GPU execution. This directly addresses advancing training optimization methods.

- **OPEN_SOURCE**: The paper explicitly states "Our code is available at https://github.com/IST-DASLab/Quartet-II", indicating the authors are releasing their code/framework to the public.
[02.02.2026 15:39] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents Quartet II, a quantized training method that enhances the utilization of the NVFP4 format for pre-training large language models (LLMs). It introduces a new unbiased quantization routine called MS-EDEN, which significantly reduces quantization error compared to traditional stochastic rounding methods. This approach allows for improved gradient estimation during both forward and backward passes in matrix multiplications. The authors demonstrate that Quartet II not only improves accuracy but also accelerates training on NVIDIA Blackwell GPUs, achieving up to 4.2x speedup over BF16.","title":"Boosting LLM Training with Quartet II: Precision Meets Speed"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents Quartet II, a quantized training method that enhances the utilization of the NVFP4 format for pre-training large language models (LLMs). It introduces a new unbiased quantization routine called MS-EDEN, which significantly reduces quantization error compared to traditional stochastic rounding methods. This approach allows for improved gradient estimation during both forward and backward passes in matrix multiplications. The authors demonstrate that Quartet II not only improves accuracy but also accelerates training on NVIDIA Blackwell GPUs, achieving up to 4.2x speedup over BF16.', title='Boosting LLM Training with Quartet II: Precision Meets Speed'))
[02.02.2026 15:39] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈáèÂåñËÆ≠ÁªÉÊñπÊ≥ïQuartet IIÔºåÊó®Âú®ÊèêÈ´òNVFP4Ê†ºÂºèÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÂà©Áî®Áéá„ÄÇÈÄöËøáÊîπËøõÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÂíåÊõ¥Âø´ÁöÑGPUÊâßË°åÔºåQuartet IIÂÆûÁé∞‰∫ÜÊØîÁé∞ÊúâÊñπÊ≥ïÊõ¥‰ΩéÁöÑÈáèÂåñËØØÂ∑Æ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ÂÅèÈáèÂåñ‰æãÁ®ãMS-EDENÔºå‰ΩøÂæóÂú®ÂæÆÂ∞∫Â∫¶Ê†ºÂºè‰∏ãÁöÑÈáèÂåñËØØÂ∑ÆÈôç‰ΩéË∂ÖËøá2ÂÄç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQuartet IIÂú®ÊâÄÊúâ‰∏ªË¶ÅÁü©Èòµ‰πòÊ≥ïÁöÑÂâçÂêëÂíåÂèçÂêë‰º†Êí≠‰∏≠ÈÉΩËÉΩÂÆûÁé∞Êõ¥Â•ΩÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°„ÄÇ","title":"ÊèêÂçáNVFP4Ê†ºÂºèÁöÑÈáèÂåñËÆ≠ÁªÉÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈáèÂåñËÆ≠ÁªÉÊñπÊ≥ïQuartet IIÔºåÊó®Âú®ÊèêÈ´òNVFP4Ê†ºÂºèÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÂà©Áî®Áéá„ÄÇÈÄöËøáÊîπËøõÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÂíåÊõ¥Âø´ÁöÑGPUÊâßË°åÔºåQuartet IIÂÆûÁé∞‰∫ÜÊØîÁé∞ÊúâÊñπÊ≥ïÊõ¥‰ΩéÁöÑÈáèÂåñËØØÂ∑Æ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ÂÅèÈáèÂåñ‰æãÁ®ãMS-EDENÔºå‰ΩøÂæóÂú®ÂæÆÂ∞∫Â∫¶Ê†ºÂºè‰∏ãÁöÑÈáèÂåñËØØÂ∑ÆÈôç‰ΩéË∂ÖËøá2ÂÄç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQuartet IIÂú®ÊâÄÊúâ‰∏ªË¶ÅÁü©Èòµ‰πòÊ≥ïÁöÑÂâçÂêëÂíåÂèçÂêë‰º†Êí≠‰∏≠ÈÉΩËÉΩÂÆûÁé∞Êõ¥Â•ΩÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°„ÄÇ', title='ÊèêÂçáNVFP4Ê†ºÂºèÁöÑÈáèÂåñËÆ≠ÁªÉÊïàÁéá'))
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#plp", "#agents", "#dataset", "#open_source", "#benchmark"], "emoji": "üß™", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ –ø–æ–ª–Ω–æ–º—É –∂–∏–∑–Ω–µ–Ω–Ω–æ–º—É —Ü–∏–∫–ª—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "TAM-Eval ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–¥
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#agents"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Deep Search with Meta-Cognitive Monitoring (DS-MCM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#rlhf", "#diffusion", "#audio", "#training", "#open_source", "#architecture"], "emoji": "üéµ", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ", "desc": "DIFFA-2 ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–Ω
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Å –ø—Ä–µ–¥—Å–∫–∞
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üéØ", "ru": {"title": "–Ø–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø–æ—Ç–æ–∫–µ: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–¥–≤–∏–≥–∏ –≤ –¥–æ–º–µ–Ω–∞—Ö –∏ —Ä–∞–∑—Ä
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "üîß", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫–∏ –≤ –∑–Ω–∞–Ω–∏—è: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–µ–µ—Å—è –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è tool-use –º–æ–¥–µ–ª–µ–π", "desc": "–§ission-GRPO ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –û—Å–Ω–æ–≤–Ω–∞
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#training", "#rlhf", "#alignment"], "emoji": "üéØ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã –≤ –º–µ—Ç–æ–¥–µ RLHF, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –Ω–∞–≥—Ä–∞–¥—ã –∏ –ø–æ–ª–∏—Ç–∏
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üé´", "ru": {"title": "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–ª–µ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π ¬´Routing the Lottery¬ª, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Å–µ—Ç–µ
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context"], "emoji": "üß©", "ru": {"title": "–û—Ä–∏–µ–Ω—Ç–∏—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Landmark pooling, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Å–≤–µ—Ä—Ç
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#training", "#math"], "emoji": "ü§ù", "ru": {"title": "–ü–æ—à–∞–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –∫ —É—Å–ø–µ—Ö—É —Å–∏—Å—Ç–µ–º—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ MAPPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –æ—Ç AI –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á
[02.02.2026 15:39] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –≥–µ—Ç–µ—Ä–æ
[02.02.2026 15:39] Using data from previous issue: {"categories": [], "emoji": "‚è∞", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –µ–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ TAPPA –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ
[02.02.2026 15:39] Renaming data file.
[02.02.2026 15:39] Renaming previous data. hf_papers.json to ./d/2026-02-02.json
[02.02.2026 15:39] Saving new data file.
[02.02.2026 15:39] Generating page.
[02.02.2026 15:39] Renaming previous page.
[02.02.2026 15:39] Renaming previous data. index.html to ./d/2026-02-02.html
[02.02.2026 15:39] Writing result.
[02.02.2026 15:39] Renaming log file.
[02.02.2026 15:39] Renaming previous data. log.txt to ./logs/2026-02-02_last_log.txt
