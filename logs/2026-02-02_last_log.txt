[02.02.2026 16:43] Read previous papers.
[02.02.2026 16:43] Generating top page (month).
[02.02.2026 16:43] Writing top page (month).
[02.02.2026 17:40] Read previous papers.
[02.02.2026 17:40] Get feed.
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21558
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23143
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22813
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22628
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22975
[02.02.2026 17:40] Extract page data from URL. URL: https://huggingface.co/papers/2601.21192
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23265
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23184
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23182
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22491
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20218
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21716
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13097
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22642
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22636
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21957
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21468
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18241
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21358
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23228
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23161
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22904
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22837
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23188
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21419
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20732
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15625
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22664
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22141
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21525
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23134
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22666
[02.02.2026 17:40] Extract page data from URL. URL: https://huggingface.co/papers/2601.22108
[02.02.2026 17:40] Extract page data from URL. URL: https://huggingface.co/papers/2601.21998
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21709
[02.02.2026 17:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21526
[02.02.2026 17:40] Extract page data from URL. URL: https://huggingface.co/papers/2601.22680
[02.02.2026 17:40] Extract page data from URL. URL: https://huggingface.co/papers/2601.21666
[02.02.2026 17:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.02.2026 17:40] No deleted papers detected.
[02.02.2026 17:40] Downloading and parsing papers (pdf, html). Total: 38.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21558.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21558.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21558.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23143.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23143.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23143.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22813.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22813.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22813.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22628.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22628.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22628.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22975.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22975.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22975.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21192.
[02.02.2026 17:40] Downloading paper 2601.21192 from https://arxiv.org/pdf/2601.21192v1...
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 2 9 1 1 2 . 1 0 6 2 : r Do Reasoning Models Enhance Embedding Models? Wun Yu Chan1, Shaojin Chen1, Huihao Jing1, Kwun Hang Lau1, Elton Chun-Chai Li1, Zihao Wang1, Haoran Li1, Yangqiu Song1 1CSE, HKUST Correspondance: wychanbu@connect.ust.hk Reasoning-Embedding Reasoning-Embedding January 30, Abstract State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals null effect: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce Hierarchical Representation Similarity Analysis (HRSA), framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifolds local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between baseand reasoning-initialized models, phenomenon we term Manifold Realignment. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself. Vector representations of text, known as text embeddings, are core abstraction in modern natural language processing (NLP) (Mikolov et al., 2013). As Large Language Models (LLMs) continue to evolve, embedding models have now been built by adapting decoder-only LLMs (Lee et al., 2"
[02.02.2026 17:40] Response: ```python
["CSE, HKUST"]
```
[02.02.2026 17:40] Deleting PDF ./assets/pdf/2601.21192.pdf.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23265.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23265.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23265.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23184.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23184.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23184.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23182.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23182.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23182.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22491.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22491.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22491.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.20218.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.20218.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.20218.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21716.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21716.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21716.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.13097.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.13097.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.13097.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22642.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22642.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22642.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22636.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22636.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22636.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21957.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21957.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21957.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21468.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21468.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21468.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.18241.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.18241.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.18241.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21358.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21358.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21358.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23228.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23228.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23228.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23161.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23161.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23161.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22904.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22904.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22904.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22837.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22837.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22837.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23188.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23188.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23188.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21419.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21419.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21419.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.20732.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.20732.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.20732.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.15625.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.15625.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.15625.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22664.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22664.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22664.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22141.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22141.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22141.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21525.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21525.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21525.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.23134.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.23134.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.23134.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22666.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.22666.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.22666.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22108.
[02.02.2026 17:40] Downloading paper 2601.22108 from https://arxiv.org/pdf/2601.22108v1...
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Value-Based Pre-Training with Downstream Feedback Shuqi Ke 1 Giulia Fanti "
[02.02.2026 17:40] Response: ```python
[]
```
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Value-Based Pre-Training with Downstream Feedback Shuqi Ke 1 Giulia FantiCan small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: value-based, modalityagnostic method for controlled continued pretraining in which lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider selfsupervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with gradient computed over downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining. 6 2 0 2 9 2 ] . [ 1 8 0 1 2 2 . 1 0 6 2 : r 1. Introduction The era of blind scaling that improves models primarily by scaling proxy-objective pretraining is showing signs of diminishing returns (Lin et al., 2025; Kaplan et al., 2020; Hoffmann et al., 2022). Yet foundation models are still trained in remarkably undirected way: we minimize 1Carnegie Mellon University. Correspondence to: Shuqi Ke <shuqik@andrew.cmu.edu>. Preprint. 1 Figure 1. Value-Based Pretraining with Downstream Feedback. Today, the learner θ trains on unlabeled data using proxy objective Lpre, for frozen pretraining task. In V-Pretraining, small task designer ϕ is trained on small feedback set of verifiable downstream tasks with predefined value functions, but never updates the learner on downstream labels. ϕ thus reshapes the pretraining target (or views) so that the induced SSL update aligns with downstream improvement, calculated via the value function. Relative to current pretraining methods, V-Pretraining adds the components in the left blue box. static self-supervised proxy loss on massive, weakly curated data, and hope the capabilities we care about (reasoning, dense perception, tool use, world modeling) emerge as byproduct. In language, the proxy is next-token prediction (Brown et al., 2020; OpenAI et al., 2024; Yang et al., 2025); in vision, it is self-supervised reconstruction or representation learning under augmentations (Chen et al., 2020b; He et al., 2022; Assran et al., 2023; Simeoni et al., 2025). While this recipe scales, it functions as an open-loop system and learns from static world: the optimization trajectory is fixed at the start, ignoring whether intermediate steps actually align with complex human goals. This open-loop nature can lead to sample inefficiency in pretraining. Unlike humans, who utilize closed-loop feedback to rapidly correct errors and master tasks, models blindly consume trillions of tokens without corrective guidance. Current pipelines inject feedback mostly after pretraining via supervised fine-tuning or preference optimization (Christiano et al., 2017; Ouyang et al., 2022; Rafailov et al., 2023). These stages are effective, but they arrive late. By the time downstream feedback is applied, the representation has already been shaped by millions of proxy-gradient steps that were agnostic to the target behavior. To break the ceiling of blind scaling, we ask: can we introduce scalable supervision into pretraining, turning an open-loop process into controlled trajectory toward what we actually want? Value-Based Pre-Training with Downstream Feedback We introduce V-Pretraining: Value-based Pre-Training with downstream feedback, framework for controlled pretraining. Standard pretraining fixes the unlabeled stream and proxy task construction (e.g., one-hot next-token targets in language, or fixed augmentation pipeline in vision) and optimizes the resulting proxy loss. We keep the unlabeled stream and learner training budget fixed, but add lightweight task designer trained on small labeled verification (value) set for the capability of interest (e.g., GSM8K for reasoning, ADE20K/NYUv2 for dense vision). Crucially, the verification set is used only as an evaluator: the learner is never updated on verification labels. Instead, the task designer reshapes the pretraining target (the supervision signal inside predictive learning) so that the learners next unlabeled update is predicted to be more valuable for the target capability. In language, the designer replaces onehot next-token labels with adaptive soft targets supported on the learners top-K candidates. In vision SSL, it replaces fixed augmentation pipeline with instance-wise learned views optimized for transfer, especially dense prediction. Directly optimizing the task designer for downstream performance is computationally prohibitive: it is bilevel problem that would require differentiating through long pretraining trajectories (Maclaurin et al., 2015; Franceschi et al., 2018). As result, prior efforts to design task-aware SSL methods were largely tailored to specific domains and tasks, which allowed them to avoid this computational bottleneck (Zhang et al., 2019; Tian et al., 2020; Shi et al., 2022). key insight of our work is showing how to efficiently generalize task-aware pretraining (including SSL) to different tasks and modalities by defining the value of pretraining step via an influence-style first-order estimate: the alignment between proxy and downstream gradients (Koh & Liang, 2017; Pruthi et al., 2020a). This yields differentiable meta-updates for the task designer while leaving the learners pretraining loop essentially unchanged. Because V-Pretraining intervenes only through target/view construction, it can be layered on top of diverse pretraining objectives (e.g., next-token prediction, masked modeling, and joint-embedding SSL) without changing the learner architecture or optimizer. This makes V-Pretraining largely orthogonal to advances i"
[02.02.2026 17:40] Mistral response. {"id": "0f93be68aea247219d83604e2df341e8", "created": 1770054016, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1486, "total_tokens": 1499, "completion_tokens": 13, "num_cached_tokens": 1485}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Carnegie Mellon University\"]\n```"}}]}
[02.02.2026 17:40] Response: ```python
["Carnegie Mellon University"]
```
[02.02.2026 17:40] Deleting PDF ./assets/pdf/2601.22108.pdf.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21998.
[02.02.2026 17:40] Downloading paper 2601.21998 from https://arxiv.org/pdf/2601.21998v1...
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 8 9 9 1 2 . 1 0 6 2 : r a Lin Li Qihang Zhang Yiming Luo Zelin Gao Nan Xue Xing Zhu Yujun Shen Yinghao Xu Equal Contribution Project Lead Corresponding Author This work highlights that video world modeling, alongside vision-language pre-training, establishes fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) shared latent space, integrating vision and action tokens, driven by Mixture-of-Transformers (MoT) architecture, (2) closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community. Website: https://technology.robbyant.com/lingbot-va Github: https://github.com/robbyant/lingbot-va Checkpoints: https://huggingface.co/robbyant/lingbot-va Vision-Language-Action (VLA) models have emerged as promising paradigm for general-purpose robotic manipulation [7, 11, 12, 34], demonstrating impressive capabilities in grounding linguistic instructions into visual perceptions across diverse objects and unstructured environments. However, beneath their apparent success lies significant challenge: representation entanglement. Most existing VLAs adopt feedforward paradigm that maps current observations to action sequences [17, 90], requiring "
[02.02.2026 17:40] Response: ```python
[]
```
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 2 ] . [ 1 8 9 9 1 2 . 1 0 6 2 : r aLin Li Qihang Zhang Yiming LuoZelin Gao Nan Xue Xing Zhu Yujun Shen Yinghao Xu Equal Contribution Project Lead Corresponding Author This work highlights that video world modeling, alongside vision-language pre-training, establishes fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) shared latent space, integrating vision and action tokens, driven by Mixture-of-Transformers (MoT) architecture, (2) closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community. Website: https://technology.robbyant.com/lingbot-va Github: https://github.com/robbyant/lingbot-va Checkpoints: https://huggingface.co/robbyant/lingbot-vaVision-Language-Action (VLA) models have emerged as promising paradigm for general-purpose robotic manipulation [7, 11, 12, 34], demonstrating impressive capabilities in grounding linguistic instructions into visual perceptions across diverse objects and unstructured environments. However, beneath their apparent success lies significant challenge: representation entanglement. Most existing VLAs adopt feedforward paradigm that maps current observations to action sequences [17, 90], requiring single neural network to simultaneously learn visual scene understanding, physical dynamics, and motor control from unified supervision signal. This entanglement can create bottleneckthe model must compress heterogeneous knowledge, ranging from high-dimensional visual semantics to low-dimensional motor commands, into shared representation space. This often leads to limited sample efficiency and suboptimal generalization. Without explicit modeling of environmental evolution [25, 26, 81], reactive policies may rely on pattern matching rather than principled understanding of physical dynamics. Recent attempts to bring world modeling into robotic policies span interactive neural simulators (e.g., UniSim [85]), chunk-based video-action diffusion models (e.g., UVA [40] and UWM [96]), and offline video generators for subgoal synthesis (e.g. Gen2Act [4], Act2Goal [94]). While conceptually appealing, these approaches face three primary limitations for effective closed-loop control. First, the reactivity gap: chunk/open-loop generation often rolls out long segments without incorporating real-time feedback, making it hard to adapt to disturbances. Second, limited long-term memory: chunk-wise generation can introduce inconsistencies over long horizons when history is not persistently cached. Third, causality: bidirectional attention within segment allows future tokens to influence past predictions, which diverges from the causal nature of physical reality where the present depends only on the past. These observations motivate an autoregressive formulation for robust closed-loop reasoning. We propose LingBot-VA, an autoregressive diffusion world model that addresses these limitations through unified 1 Figure 1. LingBot-VA : An Autoregressive World Model for Robotic Manipulation. (1) Pretraining: LingBot-VA is pretrained on diverse in-the-wild videos and robot action data, enabling strong generalization across scenes and objects. (2) Comprehensive Evaluation: We conduct extensive experiments on real-world tasks (long-horizon, deformable objects, and precision manipulation) and simulation benchmarks, significantly outperforming state-of-the-art methods including π0.5. (3) Versatile Capabilities: Beyond policy learning, our model supports visual dynamics prediction and inverse dynamics inference from robot videos. (4) Emergent Properties: Our causal world modeling approach exhibits long-range temporal memory and strong few-shot adaptation ability. video-action framework. Unlike autoregressive language models that predict discrete tokens, our model operates in continuous latent space via flow matching [46, 50], autoregressively generating chunks of video and action representations through iterative denoising. While our approach conceptually separates visual dynamics prediction and action decoding [22, 27], the key architectural insight is to interleave video and action tokens into single autoregressive sequence. Both modalities are jointly processed through Mixture-of-Transformers (MoT) architecture [43] with shared attention. Within this unified autoregressive generation process, latent imagination and action inference occur jointly: at each autoregressive step, the model generates predicted future visual states through iterative denoising while simultaneously decoding the corresponding actions, allowing both streams to mutually condition on one another. This integration, built upon large-scale pretrained video diffusion backbone [78], offers several advantages: (i) Reactive AR loop: because video and action tokens form unified sequence, each autoregressive step allows the system to recalibrate based on the latest real-world observation, enabling timely adjustments to both the predicted future and motor commands; (ii) Persistent context through KV-cache: the cached key-value pairs preserve the interleaved video-action trajectory, providing rich context that helps mitigate temporal drift; (iii) Causal consistency: causal attention masking over the unified sequence ensures that both predicted visual states and action commands are governed by preceding states, respecting the temporal arrow of physical dynamics. By incorporating real-world observations at each step, this formulation helps mitigate the distribution drift that often affects open-loop methods in long-horizon tasks. primary challenge in deploying large-scale autoregressive video-action models is inference latency; generating high-fidelity video tokens through iterative denoisi"
[02.02.2026 17:40] Mistral response. {"id": "1f338e2e5bcd44d6a3c15d31931508b5", "created": 1770054026, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1299, "total_tokens": 1305, "completion_tokens": 6, "num_cached_tokens": 1298}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[02.02.2026 17:40] Response: ```python
[]
```
[02.02.2026 17:40] Deleting PDF ./assets/pdf/2601.21998.pdf.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21709.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21709.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21709.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21526.
[02.02.2026 17:40] Extra JSON file exists (./assets/json/2601.21526.json), skip PDF parsing.
[02.02.2026 17:40] Paper image links file exists (./assets/img_data/2601.21526.json), skip HTML parsing.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.22680.
[02.02.2026 17:40] Downloading paper 2601.22680 from https://arxiv.org/pdf/2601.22680v1...
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Kuan-Chieh Jackson Wang Snap Research https://snap-research.github.io/vptt 6 2 0 2 0 3 ] . [ 1 0 8 6 2 2 . 1 0 6 2 : r a "
[02.02.2026 17:40] Response: ```python
["Snap Research"]
```
[02.02.2026 17:40] Deleting PDF ./assets/pdf/2601.22680.pdf.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Downloading and parsing paper https://huggingface.co/papers/2601.21666.
[02.02.2026 17:40] Downloading paper 2601.21666 from https://arxiv.org/pdf/2601.21666v1...
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding Ahmed Y. Radwan 1 Christos Emmanouildis 2 Hina Tabassum 3 Deval Pandya 1 Shaina Raza 1 6 2 0 2 9 2 ] A . [ 1 6 6 6 1 2 . 1 0 6 2 : r a "
[02.02.2026 17:40] Response: ```python
[]
```
[02.02.2026 17:40] Extracting affiliations from text.
[02.02.2026 17:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding Ahmed Y. Radwan 1 Christos Emmanouildis 2 Hina Tabassum 3 Deval Pandya 1 Shaina Raza 1 6 2 0 2 9 2 ] A . [ 1 6 6 6 1 2 . 1 0 6 2 : r aMultimodal Large Language Models (MLLMs) are major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audiovideo data remains underexplored. This gap highlights the need for high-quality benchmark to systematically evaluate MLLM performance in real-world setting. We introduce SONIC-O1, comprehensive, fully humanverified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closedand open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: (cid:140) Project page 3 Leaderboard1Vector Institute Intelligence, MaRS for Artificial Centre, Toronto, ON M5G 1L7, Canada 2University of Groningen, Nijenborgh 4, 9747 AG Groningen, Netherlands 3York University, 4700 Keele Street, Toronto, ON Correspondence to: Ahmed Y. RadM3J 1P3, Canada. Raza <ahmed.radwan@vectorinstitute.ai, wan, shaina.raza@vectorinstitute.ai>. Shaina Preprint. January 30, 2026. Figure 1. Performance comparison across 13 conversational domains. We compare closed-source and open-source MLLMs across 13 conversational domains using LLM-judge scores (010) for video summarization task. Gemini 3.0 Pro consistently outperforms open-source models, and high-stakes domains (e.g., Emergency Response, Mental Health) remain more challenging. 1. Introduction MLLMs have advanced from static image captioning to general-purpose perception-and-reasoning systems capable of processing video and audio inputs (Fu et al., 2024b). These models are increasingly deployed to assist real-world decisions through conversational interactions. As MLLMs move into high-stakes such as healthcare, education, and public safety capability alone is insufficient. We must also evaluate whether these systems behave accurately, fairly across demographics, and transparently across diverse users and situations. Real-world interactions rely fundamentally on both audio and video modalities. For example, speech conveys affect, emphasis, hesitation, and social intent that are essential for understanding communication. However, existing audiovideo benchmarks exhibit two critical gaps. First, audio is frequently treated as optional or replaced with transcripts and subtitles (Ataallah et al., 2025; Wang et al., 2025), SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Figure 2. Overview of SONIC-O1 tasks and evaluation format: (Top) video summarization, (Middle) evidence-grounded MCQ, (Bottom) temporal localization (event timing). Each example shows the input clip (frames) and the expected output format; demographic attributes shown beneath each clip represent associated metadata, enabling group-wise evaluation across 13 domains. under-exploring paralinguistic cues that shape interpretation of speaker meaning and their emotional state. Second, even when native audio-video inputs are evaluated, groupwise analysis across demographic groups remains largely absent (Chen et al., 2024; Li et al., 2025a), precluding assessment of whether model performance varies systematically with user characteristics. Together, these limitations obscure whether current MLLMs can operate accurately and fairly in real-world deployment scenarios. We present comparison of seminal audio-video benchmarks in Table 1. To this end, we introduce SONIC-O1, SOcial Natural Interaction Corpus (Omnimodal, v1), the first open-source, fully human-verified benchmark of 4,958 audio-video question-answer (QA) instances derived from 60 hours of real-world audio-video interactions. As shown in Figure 3, the dataset spans 13 high-impact topics across 5 domains (including legal/civic, educational, and public health), covering real-world scenarios. SONIC-O1 covers videos that range from 30 seconds to 60 minuts (covering both short, medium, long duration) and tagged with metadata covering 6 racial groups, genders, and age groups. SONIC-O1 evaluates three important evaluation tasks on state-of-the-art MLLMs: (i) summarization for global comprehension, (ii) multiple-choice QA for fine-grained reasoning, and (iii) temporal localization for identifying when events occur. Unlike prior work that treats audio as optional (as shown in Table 1) , SONIC-O1 requires native audiovideo reasoning in an omnimodal (audio, video, and text) setting, where models use the audiovideo input together with text to evaluate MLLMs responses. Our goal with this work is to reveal where omnimodal MLLMs succeed, fail, and diverge across demographic groups in conversational contexts. Contributions. (1) We introduce SONIC-O1, an opensource benchmark with human-verified domain-expert annotations and demographic metadata for evaluating MLLM performance and group-wise analysis on real-world interactions. (2) We benchmark omnimodal MLLMs, revealing substantial performance gaps and systematic disparities across demographic groups. (3) We release the full evaluation suite (dataset, scripts, leaderboard) under research license. Our evaluation reveals several key findings. (1) Closedsource models consistently outperform open-source alternatives, (2) temporal localization remains the most challenging task, and (3) systematic demographic disparities persist across models. 2. Related Work Growth in MLLMs. MLLMs have rapidly evolved from static image understanding (Radford et al., 2021; Li et al., 2022) to video comprehension (Sun et al., 2019; Tong et al., 2 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table 1. Comparison of video-QA benchmarks. S"
[02.02.2026 17:40] Mistral response. {"id": "88129f2e5f634a10821207c4078c5f20", "created": 1770054044, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1535, "total_tokens": 1617, "completion_tokens": 82, "num_cached_tokens": 1534}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Vector Institute Intelligence, MaRS for Artificial Centre, Toronto, ON M5G 1L7, Canada\",\n    \"University of Groningen, Nijenborgh 4, 9747 AG Groningen, Netherlands\",\n    \"York University, 4700 Keele Street, Toronto, ON M3J 1P3, Canada\"\n]\n```"}}]}
[02.02.2026 17:40] Response: ```python
[
    "Vector Institute Intelligence, MaRS for Artificial Centre, Toronto, ON M5G 1L7, Canada",
    "University of Groningen, Nijenborgh 4, 9747 AG Groningen, Netherlands",
    "York University, 4700 Keele Street, Toronto, ON M3J 1P3, Canada"
]
```
[02.02.2026 17:40] Deleting PDF ./assets/pdf/2601.21666.pdf.
[02.02.2026 17:40] Success.
[02.02.2026 17:40] Enriching papers with extra data.
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 0. ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as tool-augmented agents for mu...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 1. ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  					AI-generated summary 				 Large reasoning models (LRMs) a...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 2. Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  					AI-generated summary 				 The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to al...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 3. TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  					AI-generated summary 				 Test-Time Training offers a promising way to improve the ...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 4. Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.  					AI-generated summary 				 ...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 5. Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.  					AI-generated summary 				 State-of-the-art embedding models are increasingly de...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 6. _paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  					AI-generated summary 				 Despite rapid advances in autonomous AI scientists powered by language models, generatin...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 7. ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  					AI-generated summary 				 While Chain-of-Thought (CoT) significantly enhances the performa...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 8. Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.  					AI-generated summ...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 9. Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  					AI-generated summary 				 Reinforcement learning with ver...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 10. DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  					AI-generated summary 				 Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in huma...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 11. DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  					AI-generated summary 				 Character ima...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 12. RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.  					AI-generated summary 				 We present RM-RF, a lightweight reward model for run-free evaluation of automatica...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 13. A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  					AI-generated summary 				 Large Language Models (LLMs) show remarkable capabilities, yet their s...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 14. A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.  					AI-generated summary 				 Large Language Models (LLMs) are typica...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 15. A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.  					AI-generated summary 				 We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-a...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 16. MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  					AI-generated summary 				 Long-horizon agentic reasoning necessitates effectively...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 17. TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.  					AI-generated summary 				 While Large Language Models (LLMs) have shown promise in software e...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 18. PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  					AI-generated summary 				 Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but re...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 19. Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.  					AI-generated summary 				 While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning mult...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 20. DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  					AI-generated summary 				 Autoregressive (AR) large audio languag...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 21. A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  					AI-generated summary 				 Recent studies have explored using pretrained Vision Foundation Models...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 22. NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  					AI-generated summary 				 VQ-based image generation typically ...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 23. Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.  					AI-generated summary 				 Deep search agents powered by large language models have demonstrated strong capab...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 24. Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.  					AI-generated summary 				 Recent advances in diffusion and flow matching mod...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 25. Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.  					AI-generated summary 				 As digital environments (data distributi...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 26. A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.  					AI-generated summary 				 Large language models (LLMs) can call tools effectively, yet ...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 27. RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  					AI-generated summary 				 Reinforcement Learning from Human Feedback (RLHF...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 28. Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  					AI-generated summary 				 In pruning, the Lottery Ticket Hypothesis...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 29. Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  					AI-generated summary 				 Representation learning is central to many d...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 30. A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.  					AI-generated summary 				 In the post-Dennard era, optimizing embedded systems requi...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 31. ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.  					AI-generated summary 				 Open-vocabulary grounding requires accurate vi...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 32. V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.  					AI-generated summary 				 Can a small amount of verified goal information steer the expensive self-supervised pretraining of f...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 33. Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.  					AI-generated summary 				 This work highlights that video world modeling, alongside vision-language...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 34. Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-s...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 35. KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.  					AI-generated summary 				 We introduce KAPSO, a modular framework for ...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 36. A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.  					AI-generated summary 				 We introduce the Visual Person...
[02.02.2026 17:40] ********************************************************************************
[02.02.2026 17:40] Abstract 37. A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are a major focus of re...
[02.02.2026 17:40] Read previous papers.
[02.02.2026 17:40] Generating reviews via LLM API.
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#data", "#benchmark", "#open_source", "#rl", "#agents", "#optimization", "#synthetic", "#training", "#reasoning"], "emoji": "🛠️", "ru": {"title": "Автоматическое обучение агентов использовать инструменты через синтетические данные и верифицируемое обучение", "desc": "ASTRA — это авт
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#open_source", "#rl", "#training", "#reasoning"], "emoji": "🛡️", "ru": {"title": "Безопасность без учителей: самовыравнивание через внутренние знания модели", "desc": "ThinkSafe — это фреймворк самовыравнивания, который улучшает безопасность больших моделей ра
[02.02.2026 17:40] Using data from previous issue: {"categories": [], "emoji": "⚡", "ru": {"title": "Quartet II: полная квантизация NVFP4 для эффективного предобучения больших моделей", "desc": "В работе предложен метод квантизации Quartet II для обучения больших языковых моделей с использованием формата NVFP4 на GPU NVIDIA Blackwell. Авторы разрабо
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#benchmark", "#math", "#open_source", "#optimization", "#training", "#reasoning"], "emoji": "🔄", "ru": {"title": "Самоэволюционирующее обучение LLM через совместную генерацию и оптимизацию", "desc": "В статье предлагается TTCS — фреймворк для обучения больших языковых моделей непоср
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#rl", "#data", "#reasoning", "#small_models", "#dataset", "#training"], "emoji": "🪿", "ru": {"title": "Синтез неограниченных задач обучения из неверифицируемых текстов", "desc": "В статье представлен метод Golden Goose для синтеза неограниченного колич
[02.02.2026 17:40] Querying the API.
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.  					AI-generated summary 				 State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.
[02.02.2026 17:40] Response: ```json
{
  "desc": "Исследование показало, что модели встраивания (embedding models), инициализированные из LLM, обученных с использованием обучения с подкреплением и проверяемыми вознаграждениями (RLVR), не демонстрируют преимуществ в производительности по сравнению с базовыми моделями. Авторы предложили метод Hierarchical Representation Similarity Analysis (HRSA), который анализирует геометрию представлений на разных уровнях и выявил, что RLVR сохраняет глобальную геометрию многообразия, несмотря на локальное переупорядочивание. Контрастивное обучение приводит к выравниванию представлений базовых и инициализированных моделей, явление названное Manifold Realignment. Результаты свидетельствуют, что RLVR, в отличие от supervised fine-tuning, оптимизирует траектории в существующем семантическом пространстве, не переструктурируя его фундаментально.",
  "emoji": "🔄",
  "title": "Улучшенное рассуждение не гарантирует лучшие семантические представления"
}
```
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.  					AI-generated summary 				 State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself."

[02.02.2026 17:40] Response: ```python
["RLHF", "BENCHMARK", "TRAINING", "ARCHITECTURE"]
```
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.  					AI-generated summary 				 State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself."

[02.02.2026 17:40] Response: ```python
['INTERPRETABILITY', 'REASONING']
```
[02.02.2026 17:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates whether embedding models that are initialized from Reinforcement Learning with Verifiable Rewards (RLVR) reasoning models perform better than those from base models. The study finds that there is no significant performance improvement, as shown by evaluations on MTEB and BRIGHT datasets. To analyze this, the authors introduce Hierarchical Representation Similarity Analysis (HRSA), which shows that while RLVR changes local geometry, it maintains the overall structure of the representation space. The results indicate that RLVR enhances the alignment of models without altering the fundamental semantic landscape, a phenomenon termed Manifold Realignment.","title":"No Performance Boost from RLVR-Tuned Embeddings"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates whether embedding models that are initialized from Reinforcement Learning with Verifiable Rewards (RLVR) reasoning models perform better than those from base models. The study finds that there is no significant performance improvement, as shown by evaluations on MTEB and BRIGHT datasets. To analyze this, the authors introduce Hierarchical Representation Similarity Analysis (HRSA), which shows that while RLVR changes local geometry, it maintains the overall structure of the representation space. The results indicate that RLVR enhances the alignment of models without altering the fundamental semantic landscape, a phenomenon termed Manifold Realignment.', title='No Performance Boost from RLVR-Tuned Embeddings'))
[02.02.2026 17:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文探讨了通过强化学习与可验证奖励（RLVR）训练的推理模型初始化的嵌入模型是否能提供更好的语义表示。研究发现，这些模型在性能上并没有显著优于基础模型，尽管局部几何结构发生了重组，但全局几何结构和线性输出保持不变。为了解释这一现象，作者提出了层次表示相似性分析（HRSA）框架，揭示了基础模型与推理初始化模型之间的强对齐现象。结果表明，RLVR优化了现有语义空间中的轨迹，而不是根本重构语义空间。","title":"推理模型未能提升嵌入性能的奥秘"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文探讨了通过强化学习与可验证奖励（RLVR）训练的推理模型初始化的嵌入模型是否能提供更好的语义表示。研究发现，这些模型在性能上并没有显著优于基础模型，尽管局部几何结构发生了重组，但全局几何结构和线性输出保持不变。为了解释这一现象，作者提出了层次表示相似性分析（HRSA）框架，揭示了基础模型与推理初始化模型之间的强对齐现象。结果表明，RLVR优化了现有语义空间中的轨迹，而不是根本重构语义空间。', title='推理模型未能提升嵌入性能的奥秘'))
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#open_source", "#science"], "emoji": "🎨", "ru": {"title": "Автоматизация создания научных иллюстраций через агентские системы", "desc": "PaperBanana — это агентский фреймворк, который автоматизирует создание готовых к публикации академических иллюстраций с использованием передовых в
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#multimodal", "#training", "#reasoning", "#architecture"], "emoji": "🧠", "ru": {"title": "Сжатое рассуждение через визуальную регуляризацию в скрытом пространстве", "desc": "ReGuLaR предлагает вариационный автокодировщик для сжатия процессов рассужде
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "🌊", "ru": {"title": "От структуры к деталям: частотный анализ для улучшения диффузионных языковых моделей", "desc": "В работе проведён анализ диффузионных языковых моделей в частотной области, показывающий что низкочастотные компоненты кодиру
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#rl", "#optimization", "#agents", "#reasoning"], "emoji": "🎾", "ru": {"title": "Точный удар в пространстве решений: многоуровневые награды для умного обучения агентов", "desc": "Sweet Spot Learning (SSL) представляет собой новый фреймворк обучения
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#benchmark", "#rlhf", "#diffusion", "#alignment", "#optimization", "#multimodal", "#training"], "emoji": "🎯", "ru": {"title": "Плотные вознаграждения для точной настройки каждого шага денойзирования", "desc": "DenseGRPO решает проблему разреженных вознаграждений в моделях flow match
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "🎬", "ru": {"title": "Универсальная анимация персонажей без явных поз через контекстное обучение", "desc": "DreamActor-M2 — это универсальная система для синтеза анимации персонажей, которая решает проблему баланса между со
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#optimization", "#training", "#science", "#plp", "#dataset", "#multilingual", "#open_source", "#small_models", "#data"], "emoji": "⚡", "ru": {"title": "Предсказание качества тестов без компиляции — скорость и экономия", "desc": "В статье представлена RM-RF — лёгкая модель-награда, к
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#benchmark", "#training", "#rlhf"], "emoji": "⚙️", "ru": {"title": "Формальная верификация как направляющая сила для точных рассуждений LLM", "desc": "В статье предлагается гибридный подход, объединяющий формальную логическую верификацию с генерацией естественного языка для повышени
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#alignment", "#security"], "emoji": "🔓", "ru": {"title": "Масштабируемый способ предсказать настоящую уязвимость языковых моделей", "desc": "В работе предложен метод SABER для оценки уязвимости языковых моделей к adversarial атакам при масштабном параллельном sampling. Авторы исполь
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#small_models", "#cv", "#multimodal", "#open_source", "#dataset", "#benchmark"], "emoji": "📄", "ru": {"title": "Ультракомпактная модель для понимания документов с лучшей точностью и надёжностью", "desc": "Авторы представляют PaddleOCR-VL-1.5, компактную модель видения и языка размер
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#rl", "#agents", "#multimodal", "#reasoning"], "emoji": "🧠", "ru": {"title": "Визуальная память для эффективного долгого рассуждения агентов", "desc": "MemOCR — это многомодальный агент с адаптивной памятью, который решает проблему сжатия истории взаим
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#plp", "#agents", "#dataset", "#open_source", "#benchmark"], "emoji": "🧪", "ru": {"title": "Оценка способности LLM к полному жизненному циклу поддержки тестовых наборов", "desc": "TAM-Eval — это фреймворк и бенчмарк для оценки больших языковых моделей на задачах автоматического подд
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#training", "#reasoning", "#architecture"], "emoji": "🧠", "ru": {"title": "Отделение мышления от слов: гибкое латентное рассуждение без фиксированных шагов", "desc": "PLaT представляет инновационный подход к латентному рассуждению, разделяя процесс
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#training", "#math"], "emoji": "🤝", "ru": {"title": "Пошаговое обучение: от действий агентов к успеху системы", "desc": "В работе предложен метод MAPPA для улучшения обучения многоагентных систем через процессные награды от AI обратной связи. Основная идея заключ
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#rlhf", "#diffusion", "#audio", "#training", "#open_source", "#architecture"], "emoji": "🎵", "ru": {"title": "Диффузионные модели как эффективная альтернатива авторегрессии в обработке аудио", "desc": "DIFFA-2 — это диффузионная модель большого языка для понимания аудио, которая кон
[02.02.2026 17:40] Using data from previous issue: {"categories": [], "emoji": "🌐", "ru": {"title": "Сферический автоэнкодер: от семантики к пиксельной точности", "desc": "В этой работе представлен DINO-SAE — автоэнкодер, который объединяет семантическую информацию из предобученных Vision Foundation Models с реконструкцией пикселей высокого качества
[02.02.2026 17:40] Using data from previous issue: {"categories": [], "emoji": "🎨", "ru": {"title": "Причинная токенизация для более согласованной генерации изображений", "desc": "NativeTok предлагает инновационный подход к визуальной токенизации изображений, который вводит причинные зависимости между токенами на этапе кодирования. Система состоит и
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#agents"], "emoji": "🧠", "ru": {"title": "Самопроверка рассуждений через иерархический метакогнитивный мониторинг", "desc": "Работа предлагает фреймворк Deep Search with Meta-Cognitive Monitoring (DS-MCM) для улучшения способности больших языковых моделей к мно
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "🎯", "ru": {"title": "Адаптивное предсказание в диффузионных моделях через обучение оптимальной геометрии", "desc": "Статья исследует, почему модели диффузии, предсказывающие исходные данные напрямую, превосходят традиционные подходы с предска
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "🎯", "ru": {"title": "Якорение в потоке: стабилизация обучения GUI-агентов в динамичной цифровой среде", "desc": "Работа посвящена проблеме деградации производительности GUI-агентов при изменении цифровой среды, включая сдвиги в доменах и разр
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#training", "#agents", "#rl"], "emoji": "🔧", "ru": {"title": "Превращаем ошибки в знания: самокорректирующееся обучение для tool-use моделей", "desc": "Фission-GRPO — это фреймворк для улучшения работы больших языковых моделей при многошаговом взаимодействии с инструментами. Основна
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#training", "#rlhf", "#alignment"], "emoji": "🎯", "ru": {"title": "Синхронная адаптация награды через обратную связь от политики", "desc": "В статье рассматривается проблема переоптимизации награды в методе RLHF, которая возникает из-за рассогласования между моделью награды и полити
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "🎫", "ru": {"title": "Множественные специализированные билеты для адаптивного сжатия нейросетей", "desc": "В работе предложен адаптивный фреймворк сжатия нейронных сетей «Routing the Lottery», который открывает несколько специализированных подсете
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context"], "emoji": "🧩", "ru": {"title": "Ориентиры для лучшего понимания длинных текстов", "desc": "В работе предложен новый метод агрегации последовательностей под названием Landmark pooling, который решает проблемы традиционных подходов к сверт
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#optimization"], "emoji": "⚡", "ru": {"title": "Умная автоматизация энергетического баланса многоядерных систем", "desc": "В работе предлагается фреймворк байесовской оптимизации на основе гауссовских процессов для автоматизации поиска оптимальных конфигураций планирования на гетеро
[02.02.2026 17:40] Using data from previous issue: {"categories": ["#cv", "#architecture", "#alignment", "#multimodal"], "emoji": "🔗", "ru": {"title": "Выравнивание видения и языка через внимательный выбор без аннотаций", "desc": "ExpAlign — это фреймворк для выравнивания визуальных и языковых представлений, основанный на методе множественного обуче
[02.02.2026 17:40] Querying the API.
[02.02.2026 17:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.  					AI-generated summary 				 Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.
[02.02.2026 17:41] Response: ```json
{
  "desc": "V-Pretraining — это метод, который использует градиенты целевых задач для переориентации процесса предварительной подготовки моделей, позволяя эффективнее использовать вычислительные ресурсы. Вместо оптимизации фиксированной прокси-функции потерь, легковесный проектировщик задач переформатирует задачи предварительной подготовки так, чтобы каждый шаг градиентного спуска был направлен на развитие нужных способностей модели. Метод не требует обновления модели на примерах целевых задач — градиенты целевых задач используются только для управления процессом предварительной подготовки. Экспериментальные результаты показывают значительные улучшения в рассуждениях языковых моделей и точности семантической сегментации в компьютерном зрении при сокращении требуемых вычислений.",
  "emoji": "🎯",
  "title": "Целевая предварительная подготовка: направляем предобучение через градиенты целевых задач"
}
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.  					AI-generated summary 				 Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining."

[02.02.2026 17:41] Response: ```python
["TRAINING", "SMALL_MODELS", "MULTIMODAL"]
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.  					AI-generated summary 				 Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining."

[02.02.2026 17:41] Response: ```python
['OPTIMIZATION', 'REASONING', 'TRANSFER_LEARNING']
```
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"V-Pretraining is a novel approach that enhances the pretraining of machine learning models by using gradients from downstream tasks to refine the pretraining objectives. This method allows for more efficient use of limited labeled data and reduces the computational resources needed for training. By aligning the pretraining tasks with the goals of downstream tasks, V-Pretraining ensures that each gradient step contributes more effectively to the model\'s capabilities. The results show significant improvements in reasoning and vision tasks, demonstrating the effectiveness of this value-based, modality-agnostic strategy.","title":"Steering Pretraining with Downstream Task Insights"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="V-Pretraining is a novel approach that enhances the pretraining of machine learning models by using gradients from downstream tasks to refine the pretraining objectives. This method allows for more efficient use of limited labeled data and reduces the computational resources needed for training. By aligning the pretraining tasks with the goals of downstream tasks, V-Pretraining ensures that each gradient step contributes more effectively to the model's capabilities. The results show significant improvements in reasoning and vision tasks, demonstrating the effectiveness of this value-based, modality-agnostic strategy.", title='Steering Pretraining with Downstream Task Insights'))
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"V-预训练是一种利用下游任务梯度来重塑预训练目标的方法，旨在以最少的标记数据和降低计算成本的方式提升模型能力。该方法通过轻量级任务设计器选择与下游任务梯度一致的预训练任务，从而引导预训练朝向相关的下游能力。与传统的固定代理目标不同，V-预训练能够动态调整预训练任务，以最大化每一步梯度的价值。实验结果表明，V-预训练在语言模型和视觉自监督学习中均显著提升了性能。","title":"V-预训练：用少量数据提升模型能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='V-预训练是一种利用下游任务梯度来重塑预训练目标的方法，旨在以最少的标记数据和降低计算成本的方式提升模型能力。该方法通过轻量级任务设计器选择与下游任务梯度一致的预训练任务，从而引导预训练朝向相关的下游能力。与传统的固定代理目标不同，V-预训练能够动态调整预训练任务，以最大化每一步梯度的价值。实验结果表明，V-预训练在语言模型和视觉自监督学习中均显著提升了性能。', title='V-预训练：用少量数据提升模型能力'))
[02.02.2026 17:41] Querying the API.
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.  					AI-generated summary 				 This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.
[02.02.2026 17:41] Response: ```json
{
  "desc": "В статье предлагается единая архитектура для обучения робототехнических систем, которая одновременно предсказывает будущие кадры видео и исполняет управляющие команды, используя общее латентное пространство. Авторы развивают идею видео-моделей мира, которые позволяют роботам предугадывать последствия своих действий на основе понимания причинно-следственных связей между действиями и визуальной динамикой окружающей среды. Предложенная модель LingBot-VA использует диффузионный фреймворк с архитектурой Mixture-of-Transformers для интеграции видео и управляющих токенов в едином пространстве, механизм замкнутого цикла для накопления обратной связи и асинхронный конвейер вывода для эффективного управления. Экспериментальные результаты демонстрируют высокую эффективность на задачах манипуляции, хорошую обобщаемость к новым конфигурациям и улучшенную выборку-эффективность при дообучении модели.",
  "emoji": "🤖",
  "title": "Видео-модели мира как фундамент для единого обучения робототехники"
}
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.  					AI-generated summary 				 This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community."

[02.02.2026 17:41] Response: ```python
['VIDEO', 'ROBOTICS', 'ARCHITECTURE', 'TRAINING']
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.  					AI-generated summary 				 This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community."

[02.02.2026 17:41] Response: ```python
['DIFFUSION', 'OPEN_SOURCE']
```
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach to robot learning through video world modeling, which predicts future frames and executes actions in a unified framework. The proposed model, LingBot-VA, utilizes an autoregressive diffusion method that integrates vision and action in a shared latent space, enhancing the robot\'s ability to understand and anticipate the consequences of its actions. Key innovations include a closed-loop feedback mechanism for real-time environmental interaction and an asynchronous inference pipeline that optimizes action prediction and execution. The model demonstrates strong performance in both simulated and real-world tasks, showcasing its effectiveness in long-horizon manipulation and adaptability to new situations.","title":"Revolutionizing Robot Learning with Video World Modeling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a novel approach to robot learning through video world modeling, which predicts future frames and executes actions in a unified framework. The proposed model, LingBot-VA, utilizes an autoregressive diffusion method that integrates vision and action in a shared latent space, enhancing the robot's ability to understand and anticipate the consequences of its actions. Key innovations include a closed-loop feedback mechanism for real-time environmental interaction and an asynchronous inference pipeline that optimizes action prediction and execution. The model demonstrates strong performance in both simulated and real-world tasks, showcasing its effectiveness in long-horizon manipulation and adaptability to new situations.", title='Revolutionizing Robot Learning with Video World Modeling'))
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"视频世界建模通过统一框架使机器人学习成为可能，该框架同时预测帧和执行策略，利用共享的潜在空间和闭环反馈机制。本文提出的LingBot-VA是一种自回归扩散框架，能够同时学习帧预测和策略执行。模型设计包括共享潜在空间、闭环回滚机制和异步推理管道，支持高效控制。实验结果表明，该模型在长时间操作、后训练的数据效率和对新配置的强泛化能力方面表现出色。","title":"视频世界建模：机器人学习的新基础"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='视频世界建模通过统一框架使机器人学习成为可能，该框架同时预测帧和执行策略，利用共享的潜在空间和闭环反馈机制。本文提出的LingBot-VA是一种自回归扩散框架，能够同时学习帧预测和策略执行。模型设计包括共享潜在空间、闭环回滚机制和异步推理管道，支持高效控制。实验结果表明，该模型在长时间操作、后训练的数据效率和对新配置的强泛化能力方面表现出色。', title='视频世界建模：机器人学习的新基础'))
[02.02.2026 17:41] Using data from previous issue: {"categories": [], "emoji": "⏰", "ru": {"title": "Понимание внимания через временные закономерности и предсказуемость", "desc": "В статье предложена единая структура TAPPA для анализа паттернов внимания в больших языковых моделях с временной перспективы. Авторы различают предсказуемые и непредсказуе
[02.02.2026 17:41] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#plp", "#agents"], "emoji": "🧠", "ru": {"title": "Синтез программ через долгосрочную оптимизацию с когнитивной памятью", "desc": "KAPSO — это модульная система для автоматического синтеза и оптимизации программ, которая итеративно улучша
[02.02.2026 17:41] Querying the API.
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.  					AI-generated summary 				 We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.
[02.02.2026 17:41] Response: ```json
{
  "desc": "В работе представлена новая парадигма Visual Personalization Turing Test (VPTT) для оценки персонализации визуального контента на основе перцептивной неразличимости от контента, создаваемого людьми. Фреймворк включает бенчмарк из 10 тысяч персон, генератор на основе retrieval-augmented generation и калиброванную метрику VPTT Score, которая работает только с текстом. Модель проходит тест, если её вывод (изображение, видео, 3D-ассет) невозможно отличить от контента, который реально мог бы создать конкретный человек. Экспериментально показано, что предложенный подход достигает лучшего баланса между соответствием контексту и оригинальностью, обеспечивая масштабируемое и приватное решение для персонализованной генеративной AI.",
  "emoji": "🎭",
  "title": "Когда персонализованный контент неотличим от человеческого"
}
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.  					AI-generated summary 				 We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI."

[02.02.2026 17:41] Response: ```python
["BENCHMARK", "RAG", "DATASET", "MULTIMODAL", "VIDEO", "3D"]
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.  					AI-generated summary 				 We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI."

[02.02.2026 17:41] Response: ```python
["ALIGNMENT"]
```

The paper discusses aligning AI-generated visual content with human preferences and intended behavior through the concept of perceptual indistinguishability from human-created content, which relates to alignment with human values and preferences.
[02.02.2026 17:41] Error. Failed to parse JSON from LLM. ["ALIGNMENT"]


The paper discusses aligning AI-generated visual content with human preferences and intended behavior through the concept of perceptual indistinguishability from human-created content, which relates to alignment with human values and preferences.
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces the Visual Personalization Turing Test (VPTT), a new method for evaluating how well AI can create personalized visual content that feels human-made. Instead of just copying existing images, the VPTT focuses on whether the generated content is indistinguishable from what a real person might create. The framework includes a large benchmark of 10,000 personas, a visual retrieval-augmented generator, and a scoring system that aligns with human and AI judgments. Results show that this new evaluation method effectively measures the quality of personalized AI outputs while maintaining privacy and originality.","title":"Evaluating AI\'s Human-Like Visual Personalization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces the Visual Personalization Turing Test (VPTT), a new method for evaluating how well AI can create personalized visual content that feels human-made. Instead of just copying existing images, the VPTT focuses on whether the generated content is indistinguishable from what a real person might create. The framework includes a large benchmark of 10,000 personas, a visual retrieval-augmented generator, and a scoring system that aligns with human and AI judgments. Results show that this new evaluation method effectively measures the quality of personalized AI outputs while maintaining privacy and originality.', title="Evaluating AI's Human-Like Visual Personalization"))
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种新的评估框架，称为视觉个性化图灵测试（VPTT），用于评估上下文视觉个性化。该框架通过感知不可区分性来判断生成内容是否与人类创作的内容相似，而不是简单的身份复制。VPTT框架包括一个包含1万个人物的基准（VPTT-Bench）、一个视觉检索增强生成器（VPRAG）和一个基于文本的VPTT评分指标。实验结果表明，VPRAG在对齐性和原创性之间达到了最佳平衡，为个性化生成AI提供了可扩展且安全的基础。","title":"视觉个性化的新评估标准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种新的评估框架，称为视觉个性化图灵测试（VPTT），用于评估上下文视觉个性化。该框架通过感知不可区分性来判断生成内容是否与人类创作的内容相似，而不是简单的身份复制。VPTT框架包括一个包含1万个人物的基准（VPTT-Bench）、一个视觉检索增强生成器（VPRAG）和一个基于文本的VPTT评分指标。实验结果表明，VPRAG在对齐性和原创性之间达到了最佳平衡，为个性化生成AI提供了可扩展且安全的基础。', title='视觉个性化的新评估标准'))
[02.02.2026 17:41] Querying the API.
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard
[02.02.2026 17:41] Response: ```json
{
  "desc": "Авторы представляют SONIC-O1, комплексный бенчмарк для оценки мультимодальных большых языковых моделей на последовательных аудио-видео данных из реальных разговорных диалогов. Бенчмарк содержит 4958 полностью проверенных человеком аннотаций с демографическими метаданными и охватывает 13 реальных областей общения. Модели оценивались на ключевых задачах: открытое резюмирование, ответы на вопросы с множественным выбором и локализация во времени с обоснованием. Эксперименты выявили значительные проблемы: разница в производительности между открытыми и закрытыми моделями достигает 22,6% для задачи локализации во времени, а также обнаружены существенные различия в качестве работы модели среди разных демографических групп.",
  "emoji": "🎬",
  "title": "Оценка мультимодальных моделей на реальных видео-аудио данных с проверкой справедливости"
}
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard"

[02.02.2026 17:41] Response: ```python
["DATASET", "BENCHMARK", "MULTIMODAL", "AUDIO", "VIDEO"]
```
[02.02.2026 17:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard"

[02.02.2026 17:41] Response: ```python
["SURVEY", "ETHICS", "OPEN_SOURCE"]
```

**Justification:**

1. **SURVEY**: The paper introduces "a comprehensive benchmark" for evaluating multimodal large language models, which is a systematic review and evaluation framework across multiple domains and tasks.

2. **ETHICS**: The paper explicitly addresses fairness and bias concerns, noting "Performance further degrades across demographic groups, indicating persistent disparities in model behavior" and emphasizes "socially robust multimodal understanding."

3. **OPEN_SOURCE**: The paper releases multiple resources to the public: the dataset on Hugging Face, code on GitHub, and a leaderboard, making it a contribution to open-source research infrastructure.
[02.02.2026 17:41] Error. Failed to parse JSON from LLM. ["SURVEY", "ETHICS", "OPEN_SOURCE"]


**Justification:**

1. **SURVEY**: The paper introduces "a comprehensive benchmark" for evaluating multimodal large language models, which is a systematic review and evaluation framework across multiple domains and tasks.

2. **ETHICS**: The paper explicitly addresses fairness and bias concerns, noting "Performance further degrades across demographic groups, indicating persistent disparities in model behavior" and emphasizes "socially robust multimodal understanding."

3. **OPEN_SOURCE**: The paper releases multiple resources to the public: the dataset on Hugging Face, code on GitHub, and a leaderboard, making it a contribution to open-source research infrastructure.
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents SONIC-O1, a benchmark designed to evaluate multimodal large language models (MLLMs) on sequential audio-video data in real-world conversational contexts. Unlike previous studies that primarily focused on static images, SONIC-O1 includes human-verified annotations across 13 domains, allowing for a comprehensive assessment of MLLM capabilities. The benchmark tests various tasks such as open-ended summarization and multiple-choice question answering, revealing significant performance gaps, particularly in temporal localization. Additionally, the findings highlight disparities in model performance across different demographic groups, emphasizing the need for socially robust AI systems.","title":"SONIC-O1: Bridging the Gap in Multimodal Language Understanding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents SONIC-O1, a benchmark designed to evaluate multimodal large language models (MLLMs) on sequential audio-video data in real-world conversational contexts. Unlike previous studies that primarily focused on static images, SONIC-O1 includes human-verified annotations across 13 domains, allowing for a comprehensive assessment of MLLM capabilities. The benchmark tests various tasks such as open-ended summarization and multiple-choice question answering, revealing significant performance gaps, particularly in temporal localization. Additionally, the findings highlight disparities in model performance across different demographic groups, emphasizing the need for socially robust AI systems.', title='SONIC-O1: Bridging the Gap in Multimodal Language Understanding'))
[02.02.2026 17:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文介绍了SONIC-O1，这是一个用于评估多模态大型语言模型（MLLMs）在顺序音频-视频数据上的基准测试。该基准涵盖了13个真实世界的对话领域，包含4,958个经过人工验证的注释和人口统计元数据。研究表明，尽管在多项选择题（MCQ）准确性上，闭源和开源模型之间的差距较小，但在时间定位任务上，闭源模型的表现明显优于开源模型，差距达到22.6%。此外，模型在不同人口群体中的表现差异也表明，模型行为存在持续的不平等现象。","title":"SONIC-O1：多模态语言模型评估的新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文介绍了SONIC-O1，这是一个用于评估多模态大型语言模型（MLLMs）在顺序音频-视频数据上的基准测试。该基准涵盖了13个真实世界的对话领域，包含4,958个经过人工验证的注释和人口统计元数据。研究表明，尽管在多项选择题（MCQ）准确性上，闭源和开源模型之间的差距较小，但在时间定位任务上，闭源模型的表现明显优于开源模型，差距达到22.6%。此外，模型在不同人口群体中的表现差异也表明，模型行为存在持续的不平等现象。', title='SONIC-O1：多模态语言模型评估的新基准'))
[02.02.2026 17:41] Renaming data file.
[02.02.2026 17:41] Renaming previous data. hf_papers.json to ./d/2026-02-02.json
[02.02.2026 17:41] Saving new data file.
[02.02.2026 17:41] Generating page.
[02.02.2026 17:41] Renaming previous page.
[02.02.2026 17:41] Renaming previous data. index.html to ./d/2026-02-02.html
[02.02.2026 17:41] Writing result.
[02.02.2026 17:41] Renaming log file.
[02.02.2026 17:41] Renaming previous data. log.txt to ./logs/2026-02-02_last_log.txt
