[27.03.2025 06:15] Read previous papers.
[27.03.2025 06:15] Generating top page (month).
[27.03.2025 06:15] Writing top page (month).
[27.03.2025 07:11] Read previous papers.
[27.03.2025 07:11] Get feed.
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19757
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19990
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20240
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20314
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20201
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19480
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20757
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20672
[27.03.2025 07:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.20215
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19462
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20756
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20020
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19950
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20198
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19846
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20220
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19953
[27.03.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16870
[27.03.2025 07:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.03.2025 07:11] No deleted papers detected.
[27.03.2025 07:11] Downloading and parsing papers (pdf, html). Total: 18.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.19757.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.19757.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.19757.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.19990.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.19990.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.19990.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20240.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20240.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20240.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20314.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20314.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20314.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20201.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20201.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20201.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.19480.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.19480.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.19480.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20757.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20757.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20757.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20672.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20672.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20672.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20215.
[27.03.2025 07:11] Downloading paper 2503.20215 from http://arxiv.org/pdf/2503.20215v1...
[27.03.2025 07:11] Extracting affiliations from text.
[27.03.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-03-27 Qwen2.5-Omni Technical Report Qwen Team https://huggingface.co/Qwen https://modelscope.cn/organization/qwen https://github.com/QwenLM/Qwen2.5-Omni "
[27.03.2025 07:11] Response: []
[27.03.2025 07:11] Extracting affiliations from text.
[27.03.2025 07:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-03-27 Qwen2.5-Omni Technical Report Qwen Team https://huggingface.co/Qwen https://modelscope.cn/organization/qwen https://github.com/QwenLM/Qwen2.5-OmniIn this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to large language model. Such division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as large language model tasked with text generation, while Talker is dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in streaming manner, we introduce sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omnis streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness. 5 2 0 2 6 2 ] . [ 1 5 1 2 0 2 . 3 0 5 2 : r Figure 1: Qwen2.5-Omni is unified end-to-end model capable of processing multiple modalities, such as text, audio, image and video, and generating real-time text or speech response. Based on these features, Qwen2.5-Omni supports wide range of tasks, including but not limited to voice dialogue, video dialogue, and video reasoning.In daily life, humans are capable of simultaneously perceiving the visual and auditory information around them. After processing this information through the brain, they express feedback through writing, vocalization, or using tools (and physical actions), thereby engaging in information exchange In recent years, general artificial with various organisms in the world and exhibiting intelligence. intelligence has become increasingly visible, largely due to advancements in Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; 2024; Gemini Team, 2024; Anthropic, 2023a;b; 2024; Bai et al., 2023a; Yang et al., 2024a; Touvron et al., 2023a;b; Dubey et al., 2024a). These models, trained on vast amounts of textual data, represent high-level discrete representation created by humans, showcasing the ability to solve complex problems and learn rapidly. Furthermore, in the realm of understanding, Language-Audio-Language Models (LALMs) (OpenAI, 2024; Tang et al., 2024; Chu et al., 2023b; 2024b) and Language-Visual-Language Models (LVLMs) (Li et al., 2023; Liu et al., 2023b; Dai et al., 2023; Zhu et al., 2023; Huang et al., 2023; Bai et al., 2023b; Liu et al., 2023a; Wang et al., 2023b; OpenAI., 2023; Gemini Team, 2024) have helped LLMs to further extend auditory and visual capabilities in an end-to-end manner. However, efficiently unifying all these different understanding modalities in an end-to-end fashion, utilizing as much data as possible, and providing responses in both text and speech streams akin to human communication still presents significant challenge. The development of unified and intelligent omni-model requires careful consideration of several key factors. First, it is crucial to implement systematic method for the joint training of various modalities, including text, images, videos, and audio, to foster mutual enhancement among them. This alignment is particularly important for video content, where synchronization of the temporal aspects of audio and visual signals is necessary. Second, it is essential to manage potential interference among outputs from different modalities, ensuring that the training processes for outputs such as text and voice tokens do not disrupt each other. Finally, there is need to explore architectural designs that enable realtime understanding of multimodal information and allow for efficient audio output streaming, thereby reducing initial latency. In this report, we introduce Qwen2.5-Omni, unified single model capable of processing multiple modalities and generating text and natural speech responses simultaneously in streaming format. To tackle the first challenge, we propose novel position embedding approach, named TMRoPE (Timealigned Multimodal RoPE). We organize these audio and video frames in an interleaved structure to represent video sequences in time order. For the second challenge, we present Thinker-Talker architecture, wherein Thinker is tasked with text generation while the Talker focuses on generating streaming speech tokens. Talker receives high-level representations directly from Thinker. This design is inspired by the way humans utilize different organs to produce various signals, which are simultaneously coordinated through the same neural networks. As result, Thinker-Talker architecture is end-to-end jointly trained, with each component dedicated to generating distinct signals. To address the challenges associated with streaming and to facilitate the pre-filling necessary for real-time comprehension of multimodal signals, we propose modifications to all multimodal encoders by adopting block-wise streaming processing approach. In order to support streaming speech generation, we implement dual-track autoregressive model that generates speech tokens, alongside DiT model which converts these tokens into waveforms, thereby enabling streaming audio generatio"
[27.03.2025 07:11] Mistral response. {"id": "868b4fa6b6134869a872aea59ee659c2", "object": "chat.completion", "created": 1743059476, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1643, "total_tokens": 1644, "completion_tokens": 1}}
[27.03.2025 07:11] Response: []
[27.03.2025 07:11] Deleting PDF ./assets/pdf/2503.20215.pdf.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.19462.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.19462.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.19462.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20756.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20756.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20756.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20020.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20020.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20020.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.19950.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.19950.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.19950.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20198.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20198.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20198.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.19846.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.19846.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.19846.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.20220.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.20220.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.20220.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.19953.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.19953.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.19953.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.16870.
[27.03.2025 07:11] Extra JSON file exists (./assets/json/2503.16870.json), skip PDF parsing.
[27.03.2025 07:11] Paper image links file exists (./assets/img_data/2503.16870.json), skip HTML parsing.
[27.03.2025 07:11] Success.
[27.03.2025 07:11] Enriching papers with extra data.
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 0. While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We prese...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 1. Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multim...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 2. Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 3. This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, includi...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 4. We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 5. The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models ...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 6. We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically int...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 7. Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating h...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 8. In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 9. Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challeng...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 10. Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these cha...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 11. Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robot...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 12. We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important ...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 13. Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We pr...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 14. Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We ...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 15. Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspecti...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 16. Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these ...
[27.03.2025 07:11] ********************************************************************************
[27.03.2025 07:11] Abstract 17. Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse...
[27.03.2025 07:11] Read previous papers.
[27.03.2025 07:11] Generating reviews via LLM API.
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#cv", "#open_source", "#multimodal", "#architecture", "#diffusion", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Dita - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#multimodal", "#reasoning"], "emoji": "üß©", "ru": {"title": "LEGO-Puzzles: –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LEGO-Puzzles - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —É
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#video"], "emoji": "üîÄ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ CFG: –∑–∞–º–µ–Ω–∞ –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–≥–æ —à—É–º–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç–æ–¥–∞ Classifier-Free Guidance (CFG) –≤ –æ–±—É—á–µ–Ω–∏–∏ —É—Å–ª–æ–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ 
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#video", "#open_source", "#multimodal", "#architecture", "#diffusion", "#benchmark", "#dataset"], "emoji": "üé¨", "ru": {"title": "Wan: –û—Ç–∫—Ä—ã—Ç—ã–π –Ω–∞–±–æ—Ä –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Wan - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#agents", "#benchmark", "#reasoning"], "emoji": "üîç", "ru": {"title": "ODS: –æ—Ç–∫—Ä—ã—Ç—ã–π –ò–ò-–ø–æ–∏—Å–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π", "desc": "Open Deep Search (ODS) - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –≤–µ–±-–ø
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#cv", "#open_source", "#multimodal", "#architecture", "#optimization", "#benchmark", "#training"], "emoji": "üî¨", "ru": {"title": "GenHancer: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π —á–µ—Ä–µ–∑ —Å–∏–Ω–µ—Ä–≥–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–∏–Ω–µ—Ä–≥–∏—é –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#rag", "#small_models"], "emoji": "üß†", "ru": {"title": "MCTS-RAG: –£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MCTS-RAG - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π retrieval-augmented generation (RAG) –∏ Monte Carlo Tree Search (MCTS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#long_context", "#cv", "#synthetic", "#dataset", "#rag"], "emoji": "üìä", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç–∞: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫—É –∏ —Å–ª–∞–π–¥—ã, –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑
[27.03.2025 07:11] Querying the API.
[27.03.2025 07:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.
[27.03.2025 07:11] Response: {
  "desc": "Qwen2.5-Omni - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –ø—Ä–∏ —ç—Ç–æ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ —Ä–µ—á–µ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–ª–æ—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º—É –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é TMRoPE –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Thinker-Talker –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ —Ä–µ—á—å –±–µ–∑ –≤–∑–∞–∏–º–Ω—ã—Ö –ø–æ–º–µ—Ö. Qwen2.5-Omni –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≤–∫–ª—é—á–∞—è Omni-Bench –∏ MMLU.",

  "emoji": "ü§ñ",

  "title": "Qwen2.5-Omni: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è"
}
[27.03.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness."

[27.03.2025 07:11] Response: ```python
['MULTIMODAL', 'AUDIO', 'VIDEO', 'ARCHITECTURE', 'BENCHMARK']
```
[27.03.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness."

[27.03.2025 07:11] Response: ```python
["AGI", "GAMES"]
```
[27.03.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Qwen2.5-Omni is a cutting-edge multimodal model that can process and generate responses across various formats, including text, images, audio, and video. It employs a unique block-wise processing method for audio and visual data to facilitate real-time streaming. The model features a dual architecture, where the Thinker generates text and the Talker produces audio, ensuring smooth interaction between the two. With innovative techniques like TMRoPE for timestamp alignment and a sliding-window DiT for audio decoding, Qwen2.5-Omni sets new benchmarks in multimodal performance and speech generation.","title":"Streamlining Multimodal Interaction with Qwen2.5-Omni"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Qwen2.5-Omni is a cutting-edge multimodal model that can process and generate responses across various formats, including text, images, audio, and video. It employs a unique block-wise processing method for audio and visual data to facilitate real-time streaming. The model features a dual architecture, where the Thinker generates text and the Talker produces audio, ensuring smooth interaction between the two. With innovative techniques like TMRoPE for timestamp alignment and a sliding-window DiT for audio decoding, Qwen2.5-Omni sets new benchmarks in multimodal performance and speech generation.', title='Streamlining Multimodal Interaction with Qwen2.5-Omni'))
[27.03.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜQwen2.5-OmniÔºåËøôÊòØ‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåËÉΩÂ§üÂ§ÑÁêÜÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçËæìÂÖ•ÔºåÂêåÊó∂ÁîüÊàêÊñáÊú¨ÂíåËá™ÁÑ∂ËØ≠Èü≥ÂìçÂ∫î„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Â§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÊµÅÂºèÂ§ÑÁêÜÔºåÈü≥È¢ëÂíåËßÜËßâÁºñÁ†ÅÂô®ÈááÁî®‰∫ÜÂùóÂ§ÑÁêÜÁöÑÊñπÊ≥ïÔºåÂπ∂ÈÄöËøá‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰ΩçÁΩÆÂµåÂÖ•ÊñπÊ≥ïTMRoPEÊù•ÂêåÊ≠•Èü≥È¢ëÂíåËßÜÈ¢ëÁöÑÊó∂Èó¥Êà≥„ÄÇËØ•Ê®°ÂûãÁöÑThinker-TalkerÊû∂ÊûÑ‰ΩøÂæóÊñáÊú¨ÁîüÊàêÂíåËØ≠Èü≥ÁîüÊàêÂèØ‰ª•Âπ∂Ë°åËøõË°åÔºåÈÅøÂÖç‰∫Ü‰∏§ËÄÖ‰πãÈó¥ÁöÑÂπ≤Êâ∞„ÄÇQwen2.5-OmniÂú®Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®ÊµÅÂºèËØ≠Èü≥ÁîüÊàêÊñπÈù¢ÔºåÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂ§ßÂ§öÊï∞Áé∞ÊúâÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ","title":"Â§öÊ®°ÊÄÅÊµÅÂºèÁîüÊàêÁöÑÊú™Êù•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜQwen2.5-OmniÔºåËøôÊòØ‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåËÉΩÂ§üÂ§ÑÁêÜÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçËæìÂÖ•ÔºåÂêåÊó∂ÁîüÊàêÊñáÊú¨ÂíåËá™ÁÑ∂ËØ≠Èü≥ÂìçÂ∫î„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Â§öÊ®°ÊÄÅ‰ø°ÊÅØÁöÑÊµÅÂºèÂ§ÑÁêÜÔºåÈü≥È¢ëÂíåËßÜËßâÁºñÁ†ÅÂô®ÈááÁî®‰∫ÜÂùóÂ§ÑÁêÜÁöÑÊñπÊ≥ïÔºåÂπ∂ÈÄöËøá‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰ΩçÁΩÆÂµåÂÖ•ÊñπÊ≥ïTMRoPEÊù•ÂêåÊ≠•Èü≥È¢ëÂíåËßÜÈ¢ëÁöÑÊó∂Èó¥Êà≥„ÄÇËØ•Ê®°ÂûãÁöÑThinker-TalkerÊû∂ÊûÑ‰ΩøÂæóÊñáÊú¨ÁîüÊàêÂíåËØ≠Èü≥ÁîüÊàêÂèØ‰ª•Âπ∂Ë°åËøõË°åÔºåÈÅøÂÖç‰∫Ü‰∏§ËÄÖ‰πãÈó¥ÁöÑÂπ≤Êâ∞„ÄÇQwen2.5-OmniÂú®Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®ÊµÅÂºèËØ≠Èü≥ÁîüÊàêÊñπÈù¢ÔºåÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂ§ßÂ§öÊï∞Áé∞ÊúâÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ', title='Â§öÊ®°ÊÄÅÊµÅÂºèÁîüÊàêÁöÑÊú™Êù•'))
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#inference", "#video", "#dataset", "#synthetic"], "emoji": "üé¨", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ AccVideo –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#dataset"], "emoji": "üöó", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è (ADS). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#agents", "#agi", "#ethics", "#games", "#reasoning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "Gemini Robotics: –ò–ò –≤—ã—Ö–æ–¥–∏—Ç –≤ —Ä–µ–∞–ª—å–Ω—ã–π –º–∏—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Gemini Robotics, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ Gemini 2.0 –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LogQuant - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ 2-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è KV-–∫—ç—à–∞ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π 
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#long_context", "#cv", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "–ù–æ–≤–∞—è —ç—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ autoregressive –∏ diffusion –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#cv", "#dataset", "#interpretability"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è - Attention-IoU. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç 
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#transfer_learning", "#3d", "#robotics", "#optimization"], "emoji": "üöó", "ru": {"title": "–¢–æ—á–Ω–∞—è 3D-–ø–æ–∑–∞ –±–µ–∑ 3D-—Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "DINeMo - —ç—Ç–æ –Ω–æ–≤–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 3D/6D –ø–æ–∑—ã –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 3D-—Ä–∞–∑–º–µ—Ç–∫–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Å–µ–≤–¥–æ-
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#cv", "#video"], "emoji": "üé•", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞–µ–º–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Opt-CWM - —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ—Ç–æ–∫–∞ –∏ –æ–∫–∫–ª—é–∑–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–¥—Ä–∞. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫
[27.03.2025 07:11] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è
[27.03.2025 07:11] Loading Chinese text from previous data.
[27.03.2025 07:11] Renaming data file.
[27.03.2025 07:11] Renaming previous data. hf_papers.json to ./d/2025-03-27.json
[27.03.2025 07:11] Saving new data file.
[27.03.2025 07:11] Generating page.
[27.03.2025 07:11] Renaming previous page.
[27.03.2025 07:11] Renaming previous data. index.html to ./d/2025-03-27.html
[27.03.2025 07:11] [Experimental] Generating Chinese page for reading.
[27.03.2025 07:11] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Èïø‰∏ä‰∏ãÊñá', 'pinyin': 'ch√°ng sh√†ng xi√† w√©n', 'trans': 'long context'}, {'word': 'Ëá™ÂõûÂΩí', 'pinyin': 'z√¨ hu√≠ guƒ´', 'trans': 'autoregressive'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'ËøõÊ≠•', 'pinyin': 'j√¨n b√π', 'trans': 'progress'}, {'word': 'Èù¢‰∏¥', 'pinyin': 'mi√†n l√≠n', 'trans': 'face'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Frame', 'pinyin': 'Frame', 'trans': 'Frame'}, {'word': 'AutoRegressive', 'pinyin': 'AutoRegressive', 'trans': 'AutoRegressive'}, {'word': 'FAR', 'pinyin': 'FAR', 'trans': 'FAR'}, {'word': 'Áî®‰∫é', 'pinyin': 'y√≤ng y√∫', 'trans': 'used for'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': 'Ëá™ÂõûÂΩíÂª∫Ê®°', 'pinyin': 'z√¨ hu√≠ guƒ´ ji√†n m√≥', 'trans': 'autoregressive modeling'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ng d√†', 'trans': 'powerful'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÈÄöËøá', 'pinyin': 't≈çng gu√≤', 'trans': 'through'}, {'word': 'Âª∫Ê®°', 'pinyin': 'ji√†n m√≥', 'trans': 'modeling'}, {'word': 'ËøûÁª≠', 'pinyin': 'li√°n x√π', 'trans': 'continuous'}, {'word': 'Â∏ß', 'pinyin': 'zh√®n', 'trans': 'frame'}, {'word': '‰πãÈó¥', 'pinyin': 'zhƒ´ jiƒÅn', 'trans': 'between'}, {'word': 'Êó∂Èó¥', 'pinyin': 'sh√≠ jiƒÅn', 'trans': 'time'}, {'word': 'Âõ†Êûú', 'pinyin': 'yƒ´n gu«í', 'trans': 'causal'}, {'word': '‰æùËµñÊÄß', 'pinyin': 'yƒ´ l√†i x√¨ng', 'trans': 'dependency'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Êî∂Êïõ', 'pinyin': 'sh≈çu li«én', 'trans': 'convergence'}, {'word': 'Token', 'pinyin': 'Token', 'trans': 'Token'}, {'word': 'AR', 'pinyin': 'AR', 'trans': 'AR'}, {'word': 'ËßÜÈ¢ëÊâ©Êï£ÂèòÂéãÂô®', 'pinyin': 'sh√¨ p√≠n ku√≤ s√†n bi√†n yƒÅ q√¨', 'trans': 'video diffusion transformer'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ÂÜó‰Ωô', 'pinyin': 'r√≥ng y√∫', 'trans': 'redundancy'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨ su√†n', 'trans': 'computation'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ng bƒõn', 'trans': 'cost'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'FlexRoPE', 'pinyin': 'FlexRoPE', 'trans': 'FlexRoPE'}, {'word': 'ÈïøÁü≠Êúü', 'pinyin': 'ch√°ng du«én qƒ´', 'trans': 'long short-term'}, {'word': '‰∏ä‰∏ãÊñáÂª∫Ê®°ÊäÄÊúØ', 'pinyin': 'sh√†ng xi√† w√©n ji√†n m√≥ j√¨ sh√π', 'trans': 'context modeling techniques'}, {'word': '‰ΩøÂæó', 'pinyin': 'sh«ê d√©', 'trans': 'make'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅo xi√†o', 'trans': 'efficient'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'training'}, {'word': 'Â∫èÂàó', 'pinyin': 'x√π li√®', 'trans': 'sequence'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'}, {'word': 'ÊúÄ‰Ω≥', 'pinyin': 'zu√¨ jiƒÅ', 'trans': 'optimal'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}]
[27.03.2025 07:11] Renaming previous Chinese page.
[27.03.2025 07:11] Renaming previous data. zh.html to ./d/2025-03-26_zh_reading_task.html
[27.03.2025 07:11] Writing Chinese reading task.
[27.03.2025 07:11] Writing result.
[27.03.2025 07:11] Renaming log file.
[27.03.2025 07:11] Renaming previous data. log.txt to ./logs/2025-03-27_last_log.txt
