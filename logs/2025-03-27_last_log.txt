[27.03.2025 20:11] Read previous papers.
[27.03.2025 20:11] Generating top page (month).
[27.03.2025 20:11] Writing top page (month).
[27.03.2025 21:10] Read previous papers.
[27.03.2025 21:10] Get feed.
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20215
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19757
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20314
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19990
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20201
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20240
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19480
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20020
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19786
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20672
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20757
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19950
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20271
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19462
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19846
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20756
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20220
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20198
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17358
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20641
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19953
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16870
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18929
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.15893
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10997
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20731
[27.03.2025 21:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.17970
[27.03.2025 21:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.03.2025 21:10] No deleted papers detected.
[27.03.2025 21:10] Downloading and parsing papers (pdf, html). Total: 27.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20215.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20215.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20215.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19757.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19757.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19757.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20314.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20314.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20314.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19990.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19990.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19990.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20201.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20201.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20201.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20240.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20240.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20240.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19480.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19480.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19480.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20020.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20020.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20020.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19786.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19786.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19786.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20672.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20672.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20672.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20757.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20757.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20757.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19950.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19950.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19950.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20271.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20271.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20271.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19462.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19462.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19462.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19846.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19846.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19846.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20756.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20756.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20756.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20220.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20220.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20220.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20198.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20198.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20198.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.17358.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.17358.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.17358.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20641.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20641.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20641.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.19953.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.19953.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.19953.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.16870.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.16870.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.16870.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.18929.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.18929.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.18929.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.15893.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.15893.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.15893.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.10997.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.10997.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.10997.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.20731.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.20731.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.20731.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Downloading and parsing paper https://huggingface.co/papers/2503.17970.
[27.03.2025 21:10] Extra JSON file exists (./assets/json/2503.17970.json), skip PDF parsing.
[27.03.2025 21:10] Paper image links file exists (./assets/img_data/2503.17970.json), skip HTML parsing.
[27.03.2025 21:10] Success.
[27.03.2025 21:10] Enriching papers with extra data.
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 0. In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 1. While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We prese...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 2. This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, includi...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 3. Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multim...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 4. We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 5. Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 6. The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models ...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 7. Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robot...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 8. We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architectu...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 9. Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating h...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 10. We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically int...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 11. We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important ...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 12. Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 13. Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challeng...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 14. Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We ...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 15. Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these cha...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 16. Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspecti...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 17. Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We pr...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 18. In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted ar...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 19. The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant rea...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 20. Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these ...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 21. Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 22. Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to en...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 23. Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to ...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 24. Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 25. Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-...
[27.03.2025 21:10] ********************************************************************************
[27.03.2025 21:10] Abstract 26. Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract represen...
[27.03.2025 21:10] Read previous papers.
[27.03.2025 21:10] Generating reviews via LLM API.
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#architecture", "#agi", "#benchmark", "#multimodal", "#video", "#games", "#audio"], "emoji": "ü§ñ", "ru": {"title": "Qwen2.5-Omni: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "Qwen2.5-Omni - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ, –≥–µ–Ω–µ—Ä–∏
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#open_source", "#multimodal", "#architecture", "#diffusion", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Dita - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#video", "#open_source", "#multimodal", "#architecture", "#diffusion", "#benchmark", "#dataset"], "emoji": "üé¨", "ru": {"title": "Wan: –û—Ç–∫—Ä—ã—Ç—ã–π –Ω–∞–±–æ—Ä –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Wan - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#multimodal", "#reasoning"], "emoji": "üß©", "ru": {"title": "LEGO-Puzzles: –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LEGO-Puzzles - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —É
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#agents", "#benchmark", "#reasoning"], "emoji": "üîç", "ru": {"title": "ODS: –æ—Ç–∫—Ä—ã—Ç—ã–π –ò–ò-–ø–æ–∏—Å–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π", "desc": "Open Deep Search (ODS) - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –≤–µ–±-–ø
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#video"], "emoji": "üîÄ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ CFG: –∑–∞–º–µ–Ω–∞ –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–≥–æ —à—É–º–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç–æ–¥–∞ Classifier-Free Guidance (CFG) –≤ –æ–±—É—á–µ–Ω–∏–∏ —É—Å–ª–æ–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ 
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#open_source", "#multimodal", "#architecture", "#optimization", "#benchmark", "#training"], "emoji": "üî¨", "ru": {"title": "GenHancer: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π —á–µ—Ä–µ–∑ —Å–∏–Ω–µ—Ä–≥–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–∏–Ω–µ—Ä–≥–∏—é –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#agents", "#agi", "#ethics", "#games", "#reasoning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "Gemini Robotics: –ò–ò –≤—ã—Ö–æ–¥–∏—Ç –≤ —Ä–µ–∞–ª—å–Ω—ã–π –º–∏—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Gemini Robotics, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ Gemini 2.0 –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#training", "#multimodal", "#architecture", "#open_source", "#multilingual", "#long_context"], "emoji": "üß†", "ru": {"title": "Gemma 3: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Gemma 3 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–µ–º–µ–π—Å—Ç–≤–∞ –ª–µ–≥–∫–æ
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#long_context", "#cv", "#synthetic", "#dataset", "#rag"], "emoji": "üìä", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç–∞: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫—É –∏ —Å–ª–∞–π–¥—ã, –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#rag", "#small_models"], "emoji": "üß†", "ru": {"title": "MCTS-RAG: –£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MCTS-RAG - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π retrieval-augmented generation (RAG) –∏ Monte Carlo Tree Search (MCTS) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LogQuant - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ 2-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è KV-–∫—ç—à–∞ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π 
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#benchmark", "#optimization", "#multimodal", "#dataset"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ—Ü–µ–Ω–∫–µ –∏ —É–ª—É—á—à–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–º (PRM
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#inference", "#video", "#dataset", "#synthetic"], "emoji": "üé¨", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ AccVideo –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#cv", "#dataset", "#interpretability"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –º–æ–¥–µ–ª—è—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è - Attention-IoU. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç 
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#dataset"], "emoji": "üöó", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è (ADS). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#transfer_learning", "#3d", "#robotics", "#optimization"], "emoji": "üöó", "ru": {"title": "–¢–æ—á–Ω–∞—è 3D-–ø–æ–∑–∞ –±–µ–∑ 3D-—Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "DINeMo - —ç—Ç–æ –Ω–æ–≤–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 3D/6D –ø–æ–∑—ã –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 3D-—Ä–∞–∑–º–µ—Ç–∫–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Å–µ–≤–¥–æ-
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#long_context", "#cv", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "–ù–æ–≤–∞—è —ç—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ autoregressive –∏ diffusion –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#dataset", "#robotics", "#training", "#cv", "#benchmark"], "emoji": "üì∑", "ru": {"title": "–†–∞–∑–º—ã—Ç–∏–µ –≤ –¥–≤–∏–∂–µ–Ω–∏–∏ –∫–∞–∫ –∫–ª—é—á –∫ —Ç–æ—á–Ω–æ–º—É –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –∫–∞–º–µ—Ä—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã –≤ —É—Å–ª–æ–≤–∏—è—Ö —Å–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º—ã—Ç–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –¥–≤–∏–∂–µ–Ω–∏
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#architecture"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ LLM: –æ—Ç –¥–ª–∏–Ω–Ω–æ–≥–æ –∫ –∫–æ—Ä–æ—Ç–∫–æ–º—É —á–µ—Ä–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –æ—Ç –±—ã—Å
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#video"], "emoji": "üé•", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞–µ–º–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Opt-CWM - —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ—Ç–æ–∫–∞ –∏ –æ–∫–∫–ª—é–∑–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–¥—Ä–∞. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#optimization", "#rl", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "TBA: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Trajectory B
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#multimodal", "#benchmark"], "emoji": "üìÑ", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (HDSA) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º UniHDSA. –≠—Ç–æ—Ç –º
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#open_source", "#story_generation", "#multimodal", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "RONA: –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ RONA –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#dataset", "#optimization", "#diffusion", "#training"], "emoji": "üìä", "ru": {"title": "RecTable: –±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç RecTable - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏
[27.03.2025 21:10] Using data from previous issue: {"categories": ["#cv", "#healthcare", "#training"], "emoji": "üî¨", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ä–∞–∫–∞ –≥—Ä—É–¥–∏ —Å –ø–æ–º–æ—â—å—é —É–ª—É—á—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "PathoHR - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–∫–µ –º–æ–ª–æ—á–Ω–æ–π –∂–µ–ª–µ–∑—ã, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ
[27.03.2025 21:10] Loading Chinese text from previous data.
[27.03.2025 21:10] Renaming data file.
[27.03.2025 21:10] Renaming previous data. hf_papers.json to ./d/2025-03-27.json
[27.03.2025 21:10] Saving new data file.
[27.03.2025 21:10] Generating page.
[27.03.2025 21:10] Renaming previous page.
[27.03.2025 21:10] Renaming previous data. index.html to ./d/2025-03-27.html
[27.03.2025 21:10] [Experimental] Generating Chinese page for reading.
[27.03.2025 21:10] Chinese vocab [{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'vision'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'}, {'word': 'Âä®‰Ωú', 'pinyin': 'd√≤ngzu√≤', 'trans': 'action'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìngw√©i', 'trans': 'called'}, {'word': 'Transformer', 'pinyin': 'Tu≈çnghu√†nzh√®', 'trans': 'Transformer'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√†g√≤u', 'trans': 'architecture'}, {'word': 'Áõ¥Êé•', 'pinyin': 'zh√≠jiƒì', 'trans': 'directly'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íngyƒ´', 'trans': 'unified'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈çm√≥shu√†i', 'trans': 'multimodal'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤s√†n', 'trans': 'diffusion'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ch√©ng', 'trans': 'process'}, {'word': 'ÂéªÂô™', 'pinyin': 'q√πz√†o', 'trans': 'denoise'}, {'word': 'ËøûÁª≠', 'pinyin': 'li√°nx√π', 'trans': 'continuous'}, {'word': 'Â∫èÂàó', 'pinyin': 'x√πli√®', 'trans': 'sequence'}, {'word': '‰∏ä‰∏ãÊñá', 'pinyin': 'sh√†ngxi√†w√©n', 'trans': 'context'}, {'word': 'Êù°‰ª∂', 'pinyin': 'ti√°oji√†n', 'trans': 'condition'}, {'word': 'ËÆæÁΩÆ', 'pinyin': 'sh√®zh√¨', 'trans': 'setting'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠xi√†n', 'trans': 'achieve'}, {'word': 'ÁªÜÁ≤íÂ∫¶', 'pinyin': 'x√¨l√¨d√π', 'trans': 'fine-grained'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨q√≠', 'trans': 'alignment'}, {'word': 'ÂéüÂßã', 'pinyin': 'yu√°nsh«ê', 'trans': 'original'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅoj√¨', 'trans': 'marker'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√®j√¨', 'trans': 'design'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'ÈÄÇÂ∫îÊÄß', 'pinyin': 'sh√¨y√¨ngx√¨ng', 'trans': 'adaptability'}, {'word': 'È≤ÅÊ£íÊÄß', 'pinyin': 'l«îbƒÅngx√¨ng', 'trans': 'robustness'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°nj√¨ng', 'trans': 'environment'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éom√≠ng', 'trans': 'indicate'}, {'word': 'Ê®°Êãü', 'pinyin': 'm√≥n«ê', 'trans': 'simulation'}, {'word': 'ÂÆûÈôÖ', 'pinyin': 'sh√≠j√¨', 'trans': 'actual'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´s√®', 'trans': 'outstanding'}, {'word': 'ÁâπÂà´', 'pinyin': 't√®bi√©', 'trans': 'especially'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√πz√°', 'trans': 'complex'}, {'word': 'ÈïøÊó∂Èó¥', 'pinyin': 'ch√°ngsh√≠jiƒÅn', 'trans': 'long-term'}]
[27.03.2025 21:10] Renaming previous Chinese page.
[27.03.2025 21:10] Renaming previous data. zh.html to ./d/2025-03-26_zh_reading_task.html
[27.03.2025 21:10] Writing Chinese reading task.
[27.03.2025 21:10] Writing result.
[27.03.2025 21:10] Renaming log file.
[27.03.2025 21:10] Renaming previous data. log.txt to ./logs/2025-03-27_last_log.txt
