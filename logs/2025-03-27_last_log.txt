[27.03.2025 08:15] Read previous papers.
[27.03.2025 08:15] Generating top page (month).
[27.03.2025 08:15] Writing top page (month).
[27.03.2025 09:12] Read previous papers.
[27.03.2025 09:12] Get feed.
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19757
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19990
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20240
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20314
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20215
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20201
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19480
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20757
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20672
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20020
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19950
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19462
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20756
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20271
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20198
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19846
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20220
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.19953
[27.03.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.16870
[27.03.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.03.2025 09:12] No deleted papers detected.
[27.03.2025 09:12] Downloading and parsing papers (pdf, html). Total: 19.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.19757.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.19757.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.19757.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.19990.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.19990.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.19990.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20240.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20240.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20240.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20314.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20314.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20314.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20215.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20215.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20215.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20201.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20201.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20201.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.19480.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.19480.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.19480.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20757.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20757.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20757.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20672.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20672.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20672.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20020.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20020.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20020.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.19950.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.19950.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.19950.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.19462.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.19462.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.19462.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20756.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20756.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20756.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20271.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20271.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20271.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20198.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20198.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20198.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.19846.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.19846.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.19846.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.20220.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.20220.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.20220.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.19953.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.19953.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.19953.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.16870.
[27.03.2025 09:12] Extra JSON file exists (./assets/json/2503.16870.json), skip PDF parsing.
[27.03.2025 09:12] Paper image links file exists (./assets/img_data/2503.16870.json), skip HTML parsing.
[27.03.2025 09:12] Success.
[27.03.2025 09:12] Enriching papers with extra data.
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 0. While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We prese...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 1. Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multim...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 2. Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 3. This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, includi...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 4. In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 5. We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 6. The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models ...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 7. We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically int...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 8. Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating h...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 9. Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robot...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 10. We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important ...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 11. Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challeng...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 12. Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these cha...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 13. Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 14. Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We pr...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 15. Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We ...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 16. Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspecti...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 17. Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these ...
[27.03.2025 09:12] ********************************************************************************
[27.03.2025 09:12] Abstract 18. Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse...
[27.03.2025 09:12] Read previous papers.
[27.03.2025 09:12] Generating reviews via LLM API.
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#open_source", "#multimodal", "#architecture", "#diffusion", "#agents", "#training"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dita - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#multimodal", "#reasoning"], "emoji": "ğŸ§©", "ru": {"title": "LEGO-Puzzles: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LEGO-Puzzles - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#video"], "emoji": "ğŸ”€", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ CFG: Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Classifier-Free Guidance (CFG) Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ 
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#video", "#open_source", "#multimodal", "#architecture", "#diffusion", "#benchmark", "#dataset"], "emoji": "ğŸ¬", "ru": {"title": "Wan: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Wan - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#agi", "#benchmark", "#multimodal", "#video", "#games", "#audio"], "emoji": "ğŸ¤–", "ru": {"title": "Qwen2.5-Omni: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ", "desc": "Qwen2.5-Omni - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#agents", "#benchmark", "#reasoning"], "emoji": "ğŸ”", "ru": {"title": "ODS: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹", "desc": "Open Deep Search (ODS) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²ĞµĞ±-Ğ¿
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#open_source", "#multimodal", "#architecture", "#optimization", "#benchmark", "#training"], "emoji": "ğŸ”¬", "ru": {"title": "GenHancer: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#hallucinations", "#reasoning", "#rag", "#small_models"], "emoji": "ğŸ§ ", "ru": {"title": "MCTS-RAG: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "MCTS-RAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ retrieval-augmented generation (RAG) Ğ¸ Monte Carlo Tree Search (MCTS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ñ
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#long_context", "#cv", "#synthetic", "#dataset", "#rag"], "emoji": "ğŸ“Š", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºÑƒ Ğ¸ ÑĞ»Ğ°Ğ¹Ğ´Ñ‹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#agents", "#agi", "#ethics", "#games", "#reasoning", "#robotics"], "emoji": "ğŸ¤–", "ru": {"title": "Gemini Robotics: Ğ˜Ğ˜ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemini Robotics, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Gemini 2.0 Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "LogQuant - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ KV-ĞºÑÑˆĞ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ 
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#inference", "#video", "#dataset", "#synthetic"], "emoji": "ğŸ¬", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AccVideo Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#dataset"], "emoji": "ğŸš—", "ru": {"title": "Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (ADS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#benchmark", "#optimization", "#multimodal", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ (PRM
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#long_context", "#cv", "#multimodal"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸", "desc": "Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ autoregressive Ğ¸ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğµ
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#cv", "#dataset", "#interpretability"], "emoji": "ğŸ‘ï¸", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ - Attention-IoU. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ 
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#transfer_learning", "#3d", "#robotics", "#optimization"], "emoji": "ğŸš—", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ 3D-Ğ¿Ğ¾Ğ·Ğ° Ğ±ĞµĞ· 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸", "desc": "DINeMo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D/6D Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#video"], "emoji": "ğŸ¥", "ru": {"title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Opt-CWM - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞĞ½ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°Ğº
[27.03.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ñ
[27.03.2025 09:12] Trying to get texts in Chinese.
[27.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.
[27.03.2025 09:12] Mistral response. {"id": "16fb70e1c41745bc81a204f0e77f09b4", "object": "chat.completion", "created": 1743066757, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u79f0\u4e3aDita\u3002\u5b83\u4f7f\u7528Transformer\u67b6\u6784\u76f4\u63a5\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6269\u6563\u8fc7\u7a0b\u53bb\u566a\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002Dita\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u6761\u4ef6\u8bbe\u7f6e\uff0c\u5b9e\u73b0\u4e86\u53bb\u566a\u52a8\u4f5c\u4e0e\u539f\u59cb\u89c6\u89c9\u6807\u8bb0\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002\u8fd9\u79cd\u8bbe\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDita\u5728\u6a21\u62df\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 341, "total_tokens": 517, "completion_tokens": 176}}
[27.03.2025 09:12] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¡†æ¶ï¼Œç§°ä¸ºDitaã€‚å®ƒä½¿ç”¨Transformeræ¶æ„ç›´æ¥é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹å»å™ªè¿ç»­åŠ¨ä½œåºåˆ—ã€‚Ditaé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå®ç°äº†å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDitaåœ¨æ¨¡æ‹Ÿå’Œå®é™…ç¯å¢ƒä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­ã€‚
[27.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¡†æ¶ï¼Œç§°ä¸ºDitaã€‚å®ƒä½¿ç”¨Transformeræ¶æ„ç›´æ¥é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹å»å™ªè¿ç»­åŠ¨ä½œåºåˆ—ã€‚Ditaé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå®ç°äº†å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDitaåœ¨æ¨¡æ‹Ÿå’Œå®é™…ç¯å¢ƒä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­ã€‚
[27.03.2025 09:12] Mistral response. {"id": "dce0302eca524446b211d4c267437408", "object": "chat.completion", "created": 1743066761, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u79f0\u4e3aDita\u3002\u5b83\u4f7f\u7528Transformer\u67b6\u6784\u76f4\u63a5\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6269\u6563\u8fc7\u7a0b\u53bb\u566a\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002Dita\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u6761\u4ef6\u8bbe\u7f6e\uff0c\u5b9e\u73b0\u4e86\u53bb\u566a\u52a8\u4f5c\u4e0e\u539f\u59cb\u89c6\u89c9\u6807\u8bb0\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002\u8fd9\u79cd\u8bbe\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDita\u5728\u6a21\u62df\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012b zh\u01d2ng x\u012bn de sh\u00ecju\u00e9-y\u01d4y\u00e1n-d\u00f2ngzu\u00f2 m\u00f3x\u00edng ku\u00e0ngji\u00e0, ch\u0113ngw\u00e9i Dita. T\u0101 sh\u01d0y\u00f2ng Transformer ji\u00e0g\u00f2u zh\u00edji\u0113 t\u014dnggu\u00f2 t\u01d2ngy\u012b de du\u014d m\u00f3sh\u00ec ku\u00f2s\u00e0n gu\u00f2ch\u00e9ng q\u00f9z\u00e0o li\u00e1nx\u00f9 d\u00f2ngzu\u00f2 x\u00f9li\u00e8. Dita t\u014dnggu\u00f2 z\u00e0i sh\u00e0ngxi\u00e0t\u01d0 zh\u014dng j\u00ecnx\u00edng ti\u00e1oji\u00e0n sh\u00e8zh\u00ec, sh\u00edxi\u00e0n le q\u00f9z\u00e0o d\u00f2ngzu\u00f2 y\u01d4 yu\u00e1nsh\u01d0 sh\u00ecju\u00e9 bi\u0101oj\u00ec zh\u012bji\u0101n de x\u00ecl\u00ecd\u00f9 du\u00ecq\u00ed. Zh\u00e8 zh\u01d2ng sh\u00e8j\u00ec xi\u01cenzh\u00f9 t\u00edg\u0101o le m\u00f3x\u00edng z\u00e0i b\u00f9t\u00f3ng r\u00e8nw\u00f9 h\u00e9 hu\u00e1nj\u00ecng zh\u014dng de sh\u00ecy\u00ecngx\u00ecng h\u00e9 l\u01d4b\u014dx\u00ecng. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 bi\u01ceom\u00edng, Dita z\u00e0i m\u00f3n\u01d0 h\u00e9 sh\u00edj\u00ec hu\u00e1nj\u00ecng zh\u014dng d\u014du bi\u01ceoxi\u00e0n ch\u016bs\u00e8, t\u00e8bi\u00e9 sh\u00ec z\u00e0i f\u00f9z\u00e1 de ch\u00e1ng sh\u00edji\u0101n r\u00e8nw\u00f9 zh\u014dng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 191, "total_tokens": 737, "completion_tokens": 546}}
[27.03.2025 09:12] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¡†æ¶ï¼Œç§°ä¸ºDitaã€‚å®ƒä½¿ç”¨Transformeræ¶æ„ç›´æ¥é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹å»å™ªè¿ç»­åŠ¨ä½œåºåˆ—ã€‚Ditaé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå®ç°äº†å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDitaåœ¨æ¨¡æ‹Ÿå’Œå®é™…ç¯å¢ƒä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­ã€‚

ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de shÃ¬juÃ©-yÇ”yÃ¡n-dÃ²ngzuÃ² mÃ³xÃ­ng kuÃ ngjiÃ , chÄ“ngwÃ©i Dita. TÄ shÇyÃ²ng Transformer jiÃ gÃ²u zhÃ­jiÄ“ tÅngguÃ² tÇ’ngyÄ« de duÅ mÃ³shÃ¬ kuÃ²sÃ n guÃ²chÃ©ng qÃ¹zÃ o liÃ¡nxÃ¹ dÃ²ngzuÃ² xÃ¹liÃ¨. Dita tÅngguÃ² zÃ i shÃ ngxiÃ tÇ zhÅng jÃ¬nxÃ­ng tiÃ¡ojiÃ n shÃ¨zhÃ¬, shÃ­xiÃ n le qÃ¹zÃ o dÃ²ngzuÃ² yÇ” yuÃ¡nshÇ shÃ¬juÃ© biÄojÃ¬ zhÄ«jiÄn de xÃ¬lÃ¬dÃ¹ duÃ¬qÃ­. ZhÃ¨ zhÇ’ng shÃ¨jÃ¬ xiÇnzhÃ¹ tÃ­gÄo le mÃ³xÃ­ng zÃ i bÃ¹tÃ³ng rÃ¨nwÃ¹ hÃ© huÃ¡njÃ¬ng zhÅng de shÃ¬yÃ¬ngxÃ¬ng hÃ© lÇ”bÅxÃ¬ng. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, Dita zÃ i mÃ³nÇ hÃ© shÃ­jÃ¬ huÃ¡njÃ¬ng zhÅng dÅu biÇoxiÃ n chÅ«sÃ¨, tÃ¨biÃ© shÃ¬ zÃ i fÃ¹zÃ¡ de chÃ¡ng shÃ­jiÄn rÃ¨nwÃ¹ zhÅng.
[27.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¡†æ¶ï¼Œç§°ä¸ºDitaã€‚å®ƒä½¿ç”¨Transformeræ¶æ„ç›´æ¥é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹å»å™ªè¿ç»­åŠ¨ä½œåºåˆ—ã€‚Ditaé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå®ç°äº†å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDitaåœ¨æ¨¡æ‹Ÿå’Œå®é™…ç¯å¢ƒä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­ã€‚
[27.03.2025 09:13] Mistral response. {"id": "39033249d8ef4f47b3a3b8c265180ec4", "object": "chat.completion", "created": 1743066772, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u89c6\u89c9\", \"pinyin\": \"sh\u00ecju\u00e9\", \"trans\": \"vision\"},\n    {\"word\": \"\u8bed\u8a00\", \"pinyin\": \"y\u01d4y\u00e1n\", \"trans\": \"language\"},\n    {\"word\": \"\u52a8\u4f5c\", \"pinyin\": \"d\u00f2ngzu\u00f2\", \"trans\": \"action\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u6846\u67b6\", \"pinyin\": \"ku\u00e0ngji\u00e0\", \"trans\": \"framework\"},\n    {\"word\": \"\u79f0\u4e3a\", \"pinyin\": \"ch\u0113ngw\u00e9i\", \"trans\": \"called\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Tu\u014dnghu\u00e0nzh\u00e8\", \"trans\": \"Transformer\"},\n    {\"word\": \"\u67b6\u6784\", \"pinyin\": \"ji\u00e0g\u00f2u\", \"trans\": \"architecture\"},\n    {\"word\": \"\u76f4\u63a5\", \"pinyin\": \"zh\u00edji\u0113\", \"trans\": \"directly\"},\n    {\"word\": \"\u7edf\u4e00\", \"pinyin\": \"t\u01d2ngy\u012b\", \"trans\": \"unified\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014dm\u00f3shu\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u6269\u6563\", \"pinyin\": \"ku\u00f2s\u00e0n\", \"trans\": \"diffusion\"},\n    {\"word\": \"\u8fc7\u7a0b\", \"pinyin\": \"gu\u00f2ch\u00e9ng\", \"trans\": \"process\"},\n    {\"word\": \"\u53bb\u566a\", \"pinyin\": \"q\u00f9z\u00e0o\", \"trans\": \"denoise\"},\n    {\"word\": \"\u8fde\u7eed\", \"pinyin\": \"li\u00e1nx\u00f9\", \"trans\": \"continuous\"},\n    {\"word\": \"\u5e8f\u5217\", \"pinyin\": \"x\u00f9li\u00e8\", \"trans\": \"sequence\"},\n    {\"word\": \"\u4e0a\u4e0b\u6587\", \"pinyin\": \"sh\u00e0ngxi\u00e0w\u00e9n\", \"trans\": \"context\"},\n    {\"word\": \"\u6761\u4ef6\", \"pinyin\": \"ti\u00e1oji\u00e0n\", \"trans\": \"condition\"},\n    {\"word\": \"\u8bbe\u7f6e\", \"pinyin\": \"sh\u00e8zh\u00ec\", \"trans\": \"setting\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00edxi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u7ec6\u7c92\u5ea6\", \"pinyin\": \"x\u00ecl\u00ecd\u00f9\", \"trans\": \"fine-grained\"},\n    {\"word\": \"\u5bf9\u9f50\", \"pinyin\": \"du\u00ecq\u00ed\", \"trans\": \"alignment\"},\n    {\"word\": \"\u539f\u59cb\", \"pinyin\": \"yu\u00e1nsh\u01d0\", \"trans\": \"original\"},\n    {\"word\": \"\u6807\u8bb0\", \"pinyin\": \"bi\u0101oj\u00ec\", \"trans\": \"marker\"},\n    {\"word\": \"\u8bbe\u8ba1\", \"pinyin\": \"sh\u00e8j\u00ec\", \"trans\": \"design\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cenzh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00edg\u0101o\", \"trans\": \"improve\"},\n    {\"word\": \"\u9002\u5e94\u6027\", \"pinyin\": \"sh\u00ecy\u00ecngx\u00ecng\", \"trans\": \"adaptability\"},\n    {\"word\": \"\u9c81\u68d2\u6027\", \"pinyin\": \"l\u01d4b\u0101ngx\u00ecng\", \"trans\": \"robustness\"},\n    {\"word\": \"\u4efb\u52a1\", \"pinyin\": \"r\u00e8nw\u00f9\", \"trans\": \"task\"},\n    {\"word\": \"\u73af\u5883\", \"pinyin\": \"hu\u00e1nj\u00ecng\", \"trans\": \"environment\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00edy\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u8868\u660e\", \"pinyin\": \"bi\u01ceom\u00edng\", \"trans\": \"indicate\"},\n    {\"word\": \"\u6a21\u62df\", \"pinyin\": \"m\u00f3n\u01d0\", \"trans\": \"simulation\"},\n    {\"word\": \"\u5b9e\u9645\", \"pinyin\": \"sh\u00edj\u00ec\", \"trans\": \"actual\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceoxi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016bs\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u7279\u522b\", \"pinyin\": \"t\u00e8bi\u00e9\", \"trans\": \"especially\"},\n    {\"word\": \"\u590d\u6742\", \"pinyin\": \"f\u00f9z\u00e1\", \"trans\": \"complex\"},\n    {\"word\": \"\u957f\u65f6\u95f4\", \"pinyin\": \"ch\u00e1ngsh\u00edji\u0101n\", \"trans\": \"long-term\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 223, "total_tokens": 1381, "completion_tokens": 1158}}
[27.03.2025 09:13] Response: [
    {"word": "è§†è§‰", "pinyin": "shÃ¬juÃ©", "trans": "vision"},
    {"word": "è¯­è¨€", "pinyin": "yÇ”yÃ¡n", "trans": "language"},
    {"word": "åŠ¨ä½œ", "pinyin": "dÃ²ngzuÃ²", "trans": "action"},
    {"word": "æ¨¡å‹", "pinyin": "mÃ³xÃ­ng", "trans": "model"},
    {"word": "æ¡†æ¶", "pinyin": "kuÃ ngjiÃ ", "trans": "framework"},
    {"word": "ç§°ä¸º", "pinyin": "chÄ“ngwÃ©i", "trans": "called"},
    {"word": "Transformer", "pinyin": "TuÅnghuÃ nzhÃ¨", "trans": "Transformer"},
    {"word": "æ¶æ„", "pinyin": "jiÃ gÃ²u", "trans": "architecture"},
    {"word": "ç›´æ¥", "pinyin": "zhÃ­jiÄ“", "trans": "directly"},
    {"word": "ç»Ÿä¸€", "pinyin": "tÇ’ngyÄ«", "trans": "unified"},
    {"word": "å¤šæ¨¡æ€", "pinyin": "duÅmÃ³shuÃ i", "trans": "multimodal"},
    {"word": "æ‰©æ•£", "pinyin": "kuÃ²sÃ n", "trans": "diffusion"},
    {"word": "è¿‡ç¨‹", "pinyin": "guÃ²chÃ©ng", "trans": "process"},
    {"word": "å»å™ª", "pinyin": "qÃ¹zÃ o", "trans": "denoise"},
    {"word": "è¿ç»­", "pinyin": "liÃ¡nxÃ¹", "trans": "continuous"},
    {"word": "åºåˆ—", "pinyin": "xÃ¹liÃ¨", "trans": "sequence"},
    {"word": "ä¸Šä¸‹æ–‡", "pinyin": "shÃ ngxiÃ wÃ©n", "trans": "context"},
    {"word": "æ¡ä»¶", "pinyin": "tiÃ¡ojiÃ n", "trans": "condition"},
    {"word": "è®¾ç½®", "pinyin": "shÃ¨zhÃ¬", "trans": "setting"},
    {"word": "å®ç°", "pinyin": "shÃ­xiÃ n", "trans": "achieve"},
    {"word": "ç»†ç²’åº¦", "pinyin": "xÃ¬lÃ¬dÃ¹", "trans": "fine-grained"},
    {"word": "å¯¹é½", "pinyin": "duÃ¬qÃ­", "trans": "alignment"},
    {"word": "åŸå§‹", "pinyin": "yuÃ¡nshÇ", "trans": "original"},
    {"word": "æ ‡è®°", "pinyin": "biÄojÃ¬", "trans": "marker"},
    {"word": "è®¾è®¡", "pinyin": "shÃ¨jÃ¬", "trans": "design"},
    {"word": "æ˜¾è‘—", "pinyin": "xiÇnzhÃ¹", "trans": "significant"},
    {"word": "æé«˜", "pinyin": "tÃ­gÄo", "trans": "improve"},
    {"word": "é€‚åº”æ€§", "pinyin": "shÃ¬yÃ¬ngxÃ¬ng", "trans": "adaptability"},
    {"word": "é²æ£’æ€§", "pinyin": "lÇ”bÄngxÃ¬ng", "trans": "robustness"},
    {"word": "ä»»åŠ¡", "pinyin": "rÃ¨nwÃ¹", "trans": "task"},
    {"word": "ç¯å¢ƒ", "pinyin": "huÃ¡njÃ¬ng", "trans": "environment"},
    {"word": "å®éªŒ", "pinyin": "shÃ­yÃ n", "trans": "experiment"},
    {"word": "ç»“æœ", "pinyin": "jiÃ©guÇ’", "trans": "result"},
    {"word": "è¡¨æ˜", "pinyin": "biÇomÃ­ng", "trans": "indicate"},
    {"word": "æ¨¡æ‹Ÿ", "pinyin": "mÃ³nÇ", "trans": "simulation"},
    {"word": "å®é™…", "pinyin": "shÃ­jÃ¬", "trans": "actual"},
    {"word": "è¡¨ç°", "pinyin": "biÇoxiÃ n", "trans": "performance"},
    {"word": "å‡ºè‰²", "pinyin": "chÅ«sÃ¨", "trans": "outstanding"},
    {"word": "ç‰¹åˆ«", "pinyin": "tÃ¨biÃ©", "trans": "especially"},
    {"word": "å¤æ‚", "pinyin": "fÃ¹zÃ¡", "trans": "complex"},
    {"word": "é•¿æ—¶é—´", "pinyin": "chÃ¡ngshÃ­jiÄn", "trans": "long-term"}
]
[27.03.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¡†æ¶ï¼Œç§°ä¸ºDitaã€‚å®ƒä½¿ç”¨Transformeræ¶æ„ç›´æ¥é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ‰©æ•£è¿‡ç¨‹å»å™ªè¿ç»­åŠ¨ä½œåºåˆ—ã€‚Ditaé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå®ç°äº†å»å™ªåŠ¨ä½œä¸åŸå§‹è§†è§‰æ ‡è®°ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDitaåœ¨æ¨¡æ‹Ÿå’Œå®é™…ç¯å¢ƒä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ä¸­ã€‚
[27.03.2025 09:13] Mistral response. {"id": "07a195b8a9b84441a3c25e85cc9cd9fe", "object": "chat.completion", "created": 1743066798, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a new vision-language-action model framework called Dita. It employs a Transformer architecture to directly denoise continuous action sequences through a unified multimodal diffusion process. Dita achieves fine-grained alignment between denoised actions and original visual tokens by conditioning within the context. This design significantly enhances the model's adaptability and robustness across different tasks and environments. Experimental results demonstrate that Dita performs excellently in both simulated and real-world settings, particularly in complex, long-duration tasks."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 189, "total_tokens": 306, "completion_tokens": 117}}
[27.03.2025 09:13] Response: This article introduces a new vision-language-action model framework called Dita. It employs a Transformer architecture to directly denoise continuous action sequences through a unified multimodal diffusion process. Dita achieves fine-grained alignment between denoised actions and original visual tokens by conditioning within the context. This design significantly enhances the model's adaptability and robustness across different tasks and environments. Experimental results demonstrate that Dita performs excellently in both simulated and real-world settings, particularly in complex, long-duration tasks.
[27.03.2025 09:13] Renaming data file.
[27.03.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-03-27.json
[27.03.2025 09:13] Saving new data file.
[27.03.2025 09:13] Generating page.
[27.03.2025 09:13] Renaming previous page.
[27.03.2025 09:13] Renaming previous data. index.html to ./d/2025-03-27.html
[27.03.2025 09:13] [Experimental] Generating Chinese page for reading.
[27.03.2025 09:13] Chinese vocab [{'word': 'è§†è§‰', 'pinyin': 'shÃ¬juÃ©', 'trans': 'vision'}, {'word': 'è¯­è¨€', 'pinyin': 'yÇ”yÃ¡n', 'trans': 'language'}, {'word': 'åŠ¨ä½œ', 'pinyin': 'dÃ²ngzuÃ²', 'trans': 'action'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'}, {'word': 'ç§°ä¸º', 'pinyin': 'chÄ“ngwÃ©i', 'trans': 'called'}, {'word': 'Transformer', 'pinyin': 'TuÅnghuÃ nzhÃ¨', 'trans': 'Transformer'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ gÃ²u', 'trans': 'architecture'}, {'word': 'ç›´æ¥', 'pinyin': 'zhÃ­jiÄ“', 'trans': 'directly'}, {'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ngyÄ«', 'trans': 'unified'}, {'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅmÃ³shuÃ i', 'trans': 'multimodal'}, {'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'}, {'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ²chÃ©ng', 'trans': 'process'}, {'word': 'å»å™ª', 'pinyin': 'qÃ¹zÃ o', 'trans': 'denoise'}, {'word': 'è¿ç»­', 'pinyin': 'liÃ¡nxÃ¹', 'trans': 'continuous'}, {'word': 'åºåˆ—', 'pinyin': 'xÃ¹liÃ¨', 'trans': 'sequence'}, {'word': 'ä¸Šä¸‹æ–‡', 'pinyin': 'shÃ ngxiÃ wÃ©n', 'trans': 'context'}, {'word': 'æ¡ä»¶', 'pinyin': 'tiÃ¡ojiÃ n', 'trans': 'condition'}, {'word': 'è®¾ç½®', 'pinyin': 'shÃ¨zhÃ¬', 'trans': 'setting'}, {'word': 'å®ç°', 'pinyin': 'shÃ­xiÃ n', 'trans': 'achieve'}, {'word': 'ç»†ç²’åº¦', 'pinyin': 'xÃ¬lÃ¬dÃ¹', 'trans': 'fine-grained'}, {'word': 'å¯¹é½', 'pinyin': 'duÃ¬qÃ­', 'trans': 'alignment'}, {'word': 'åŸå§‹', 'pinyin': 'yuÃ¡nshÇ', 'trans': 'original'}, {'word': 'æ ‡è®°', 'pinyin': 'biÄojÃ¬', 'trans': 'marker'}, {'word': 'è®¾è®¡', 'pinyin': 'shÃ¨jÃ¬', 'trans': 'design'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'}, {'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'}, {'word': 'é€‚åº”æ€§', 'pinyin': 'shÃ¬yÃ¬ngxÃ¬ng', 'trans': 'adaptability'}, {'word': 'é²æ£’æ€§', 'pinyin': 'lÇ”bÄngxÃ¬ng', 'trans': 'robustness'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡njÃ¬ng', 'trans': 'environment'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇomÃ­ng', 'trans': 'indicate'}, {'word': 'æ¨¡æ‹Ÿ', 'pinyin': 'mÃ³nÇ', 'trans': 'simulation'}, {'word': 'å®é™…', 'pinyin': 'shÃ­jÃ¬', 'trans': 'actual'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ«sÃ¨', 'trans': 'outstanding'}, {'word': 'ç‰¹åˆ«', 'pinyin': 'tÃ¨biÃ©', 'trans': 'especially'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹zÃ¡', 'trans': 'complex'}, {'word': 'é•¿æ—¶é—´', 'pinyin': 'chÃ¡ngshÃ­jiÄn', 'trans': 'long-term'}]
[27.03.2025 09:13] Renaming previous Chinese page.
[27.03.2025 09:13] Renaming previous data. zh.html to ./d/2025-03-26_zh_reading_task.html
[27.03.2025 09:13] Writing Chinese reading task.
[27.03.2025 09:13] Writing result.
[27.03.2025 09:13] Renaming log file.
[27.03.2025 09:13] Renaming previous data. log.txt to ./logs/2025-03-27_last_log.txt
