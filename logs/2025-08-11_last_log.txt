[11.08.2025 03:29] Read previous papers.
[11.08.2025 03:29] Generating top page (month).
[11.08.2025 03:29] Writing top page (month).
[11.08.2025 04:36] Read previous papers.
[11.08.2025 04:36] Get feed.
[11.08.2025 04:36] Get page data from previous paper. URL: https://huggingface.co/papers/2508.05731
[11.08.2025 04:36] Get page data from previous paper. URL: https://huggingface.co/papers/2508.05988
[11.08.2025 04:36] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04825
[11.08.2025 04:36] Extract page data from URL. URL: https://huggingface.co/papers/2508.06471
[11.08.2025 04:36] Get page data from previous paper. URL: https://huggingface.co/papers/2508.05502
[11.08.2025 04:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.08.2025 04:36] No deleted papers detected.
[11.08.2025 04:36] Downloading and parsing papers (pdf, html). Total: 5.
[11.08.2025 04:36] Downloading and parsing paper https://huggingface.co/papers/2508.05731.
[11.08.2025 04:36] Extra JSON file exists (./assets/json/2508.05731.json), skip PDF parsing.
[11.08.2025 04:36] Paper image links file exists (./assets/img_data/2508.05731.json), skip HTML parsing.
[11.08.2025 04:36] Success.
[11.08.2025 04:36] Downloading and parsing paper https://huggingface.co/papers/2508.05988.
[11.08.2025 04:36] Extra JSON file exists (./assets/json/2508.05988.json), skip PDF parsing.
[11.08.2025 04:36] Paper image links file exists (./assets/img_data/2508.05988.json), skip HTML parsing.
[11.08.2025 04:36] Success.
[11.08.2025 04:36] Downloading and parsing paper https://huggingface.co/papers/2508.04825.
[11.08.2025 04:36] Extra JSON file exists (./assets/json/2508.04825.json), skip PDF parsing.
[11.08.2025 04:36] Paper image links file exists (./assets/img_data/2508.04825.json), skip HTML parsing.
[11.08.2025 04:36] Success.
[11.08.2025 04:36] Downloading and parsing paper https://huggingface.co/papers/2508.06471.
[11.08.2025 04:36] Downloading paper 2508.06471 from http://arxiv.org/pdf/2508.06471v1...
[11.08.2025 04:36] Extracting affiliations from text.
[11.08.2025 04:36] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models GLM-4.5 Team Zhipu AI & Tsinghua University (For the complete list of authors, please refer to the Contribution section) "
[11.08.2025 04:36] Response: ```python
["Zhipu AI", "Tsinghua University"]
```
[11.08.2025 04:36] Deleting PDF ./assets/pdf/2508.06471.pdf.
[11.08.2025 04:36] Success.
[11.08.2025 04:36] Downloading and parsing paper https://huggingface.co/papers/2508.05502.
[11.08.2025 04:36] Extra JSON file exists (./assets/json/2508.05502.json), skip PDF parsing.
[11.08.2025 04:36] Paper image links file exists (./assets/img_data/2508.05502.json), skip HTML parsing.
[11.08.2025 04:36] Success.
[11.08.2025 04:36] Enriching papers with extra data.
[11.08.2025 04:36] ********************************************************************************
[11.08.2025 04:36] Abstract 0. Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.  					AI-generated summary 				 The emergence of Multimodal Large Language Models (MLLMs) h...
[11.08.2025 04:36] ********************************************************************************
[11.08.2025 04:36] Abstract 1. ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.  					AI-generated summary 				 Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code r...
[11.08.2025 04:36] ********************************************************************************
[11.08.2025 04:36] Abstract 2. Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.  					AI-generated summary 				 Virtual try-on aims to synthesize a realistic image of a person wearing a targe...
[11.08.2025 04:36] ********************************************************************************
[11.08.2025 04:36] Abstract 3. GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  					AI-generated summary 				 We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large la...
[11.08.2025 04:36] ********************************************************************************
[11.08.2025 04:36] Abstract 4. MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have shown remarkable per...
[11.08.2025 04:36] Read previous papers.
[11.08.2025 04:36] Generating reviews via LLM API.
[11.08.2025 04:36] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#rl", "#benchmark", "#agents", "#multimodal"], "emoji": "üîç", "ru": {"title": "AEPO: –ü—Ä–æ—Ä—ã–≤ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
[11.08.2025 04:36] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#architecture", "#data", "#benchmark"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º", "desc": "ASAP - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏
[11.08.2025 04:36] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#optimization", "#inference", "#architecture", "#benchmark", "#multimodal"], "emoji": "üëö", "ru": {"title": "Voost: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–µ –æ–¥–µ–∂–¥—ã", "desc": "Voost - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–±—É—á
[11.08.2025 04:36] Querying the API.
[11.08.2025 04:36] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  					AI-generated summary 				 We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.
[11.08.2025 04:36] Response: {
  "desc": "GLM-4.5 - —ç—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Experts, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è 355 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 23 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. GLM-4.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –º–Ω–æ–≥–∏–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤—ã–ø—É—Å—Ç–∏–ª–∏ –∫–∞–∫ –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é GLM-4.5, —Ç–∞–∫ –∏ –∫–æ–º–ø–∞–∫—Ç–Ω—É—é GLM-4.5-Air –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö –∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.",
  "emoji": "üß†",
  "title": "GLM-4.5: –ú–æ—â–Ω–∞—è MoE-–º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò"
}
[11.08.2025 04:36] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  					AI-generated summary 				 We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5."

[11.08.2025 04:36] Response: ```python
['RL', 'AGENTS', 'TRAINING', 'ARCHITECTURE']
```
[11.08.2025 04:36] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  					AI-generated summary 				 We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5."

[11.08.2025 04:36] Response: ```python
['AGI', 'REASONING', 'OPEN_SOURCE']
```
[11.08.2025 04:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GLM-4.5 is a large language model that uses a Mixture-of-Experts (MoE) architecture with 355 billion parameters, of which 32 billion are activated during operation. It employs a hybrid reasoning approach that allows it to perform tasks in both thinking and direct response modes. The model has undergone extensive multi-stage training on 23 trillion tokens and has been fine-tuned using reinforcement learning, resulting in impressive performance on various benchmarks. Notably, GLM-4.5 ranks highly among its peers, demonstrating strong capabilities in agentic, reasoning, and coding tasks while being more parameter-efficient than many competitors.","title":"GLM-4.5: Powerful Reasoning with Fewer Parameters"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GLM-4.5 is a large language model that uses a Mixture-of-Experts (MoE) architecture with 355 billion parameters, of which 32 billion are activated during operation. It employs a hybrid reasoning approach that allows it to perform tasks in both thinking and direct response modes. The model has undergone extensive multi-stage training on 23 trillion tokens and has been fine-tuned using reinforcement learning, resulting in impressive performance on various benchmarks. Notably, GLM-4.5 ranks highly among its peers, demonstrating strong capabilities in agentic, reasoning, and coding tasks while being more parameter-efficient than many competitors.', title='GLM-4.5: Powerful Reasoning with Fewer Parameters'))
[11.08.2025 04:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GLM-4.5ÊòØ‰∏ÄÁßçÂÖ∑Êúâ3550‰∫øÂèÇÊï∞ÁöÑÊ∑∑Âêà‰∏ìÂÆ∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåË°®Áé∞Âá∫Ëâ≤ÔºåÁâπÂà´ÊòØÂú®‰ª£ÁêÜ„ÄÅÊé®ÁêÜÂíåÁºñÁ†Å‰ªªÂä°‰∏ä„ÄÇÂÆÉÈááÁî®Â§öÈò∂ÊÆµËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÁªèËøá23‰∏á‰∫ø‰∏™Ê†áËÆ∞ÁöÑËÆ≠ÁªÉÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇGLM-4.5Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊàêÁª©ÔºåÂ∞§ÂÖ∂Âú®‰ª£ÁêÜÂü∫ÂáÜ‰∏≠ÊéíÂêçÁ¨¨‰∫å„ÄÇËØ•Ê®°ÂûãÁöÑÂºÄÊ∫êÁâàÊú¨ÂíåÁ¥ßÂáëÁâàÔºàGLM-4.5-AirÔºâÈÉΩÂ∑≤ÂèëÂ∏ÉÔºåÊó®Âú®Êé®Âä®Êé®ÁêÜÂíå‰ª£ÁêÜ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂„ÄÇ","title":"GLM-4.5ÔºöÂº∫Â§ßÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËØ≠Ë®ÄÊ®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GLM-4.5ÊòØ‰∏ÄÁßçÂÖ∑Êúâ3550‰∫øÂèÇÊï∞ÁöÑÊ∑∑Âêà‰∏ìÂÆ∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåË°®Áé∞Âá∫Ëâ≤ÔºåÁâπÂà´ÊòØÂú®‰ª£ÁêÜ„ÄÅÊé®ÁêÜÂíåÁºñÁ†Å‰ªªÂä°‰∏ä„ÄÇÂÆÉÈááÁî®Â§öÈò∂ÊÆµËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÁªèËøá23‰∏á‰∫ø‰∏™Ê†áËÆ∞ÁöÑËÆ≠ÁªÉÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇGLM-4.5Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊàêÁª©ÔºåÂ∞§ÂÖ∂Âú®‰ª£ÁêÜÂü∫ÂáÜ‰∏≠ÊéíÂêçÁ¨¨‰∫å„ÄÇËØ•Ê®°ÂûãÁöÑÂºÄÊ∫êÁâàÊú¨ÂíåÁ¥ßÂáëÁâàÔºàGLM-4.5-AirÔºâÈÉΩÂ∑≤ÂèëÂ∏ÉÔºåÊó®Âú®Êé®Âä®Êé®ÁêÜÂíå‰ª£ÁêÜ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂„ÄÇ', title='GLM-4.5ÔºöÂº∫Â§ßÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËØ≠Ë®ÄÊ®°Âûã'))
[11.08.2025 04:36] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#multilingual", "#training", "#data", "#multimodal"], "emoji": "üåç", "ru": {"title": "MELLA: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ MLLM –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤", "desc": "–î–∞—Ç–∞—Å–µ—Ç MELLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É–ª—É—á—à–µ–Ω
[11.08.2025 04:36] Renaming data file.
[11.08.2025 04:36] Renaming previous data. hf_papers.json to ./d/2025-08-11.json
[11.08.2025 04:36] Saving new data file.
[11.08.2025 04:36] Generating page.
[11.08.2025 04:36] Renaming previous page.
[11.08.2025 04:36] Renaming previous data. index.html to ./d/2025-08-11.html
[11.08.2025 04:36] Writing result.
[11.08.2025 04:36] Renaming log file.
[11.08.2025 04:36] Renaming previous data. log.txt to ./logs/2025-08-11_last_log.txt
