[24.12.2025 04:38] Read previous papers.
[24.12.2025 04:38] Generating top page (month).
[24.12.2025 04:38] Writing top page (month).
[24.12.2025 05:25] Read previous papers.
[24.12.2025 05:25] Get feed.
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20619
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19673
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20617
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20491
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18099
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17102
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19526
[24.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20615
[24.12.2025 05:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.12.2025 05:25] No deleted papers detected.
[24.12.2025 05:25] Downloading and parsing papers (pdf, html). Total: 8.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.20619.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.20619.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.20619.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19673.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19673.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19673.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.20617.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.20617.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.20617.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.20491.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.20491.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.20491.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.18099.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.18099.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.18099.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.17102.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.17102.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.17102.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.19526.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.19526.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.19526.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.20615.
[24.12.2025 05:25] Extra JSON file exists (./assets/json/2512.20615.json), skip PDF parsing.
[24.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.20615.json), skip HTML parsing.
[24.12.2025 05:25] Success.
[24.12.2025 05:25] Enriching papers with extra data.
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 0. SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.  					AI-generated summary 				 State-of-the-art video...
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 1. The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.  					AI-generated summary 				 Existing reinforcement...
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 2. SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.  					AI-generated summary 				 Cognitive science suggests that spat...
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 3. Step-DeepResearch, an end-to-end agent enhanced with a data synthesis strategy and progressive training, achieves expert-level capabilities in deep research scenarios, outperforming established models.  					AI-generated summary 				 As LLMs shift toward autonomous agents, Deep Research has emerged ...
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 4. SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types.  					AI-generated summary 				 General audio source separation is a key capability for multimodal AI...
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 5. A novel RL framework, SAGE, enhances LLM-based agents' self-improvement capabilities by systematically incorporating skills from a skill library, leading to better performance and efficiency in new environments.  					AI-generated summary 				 Large Language Model (LLM)-based agents have demonstrate...
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 6. QuantiPhy is a benchmark that quantitatively assesses state-of-the-art vision perception models' ability to reason about physical properties such as size, velocity, and acceleration from video observations, revealing gaps between qualitative plausibility and numerical correctness.  					AI-generated...
[24.12.2025 05:25] ********************************************************************************
[24.12.2025 05:25] Abstract 7. ORCA, a framework for goal-directed planning in video avatars, uses an internal world model and dual-system architecture to enable autonomous task completion in stochastic environments.  					AI-generated summary 				 Current video avatar generation methods excel at identity preservation and motion ...
[24.12.2025 05:25] Read previous papers.
[24.12.2025 05:25] Generating reviews via LLM API.
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#training", "#video", "#diffusion", "#optimization", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "SemanticGen –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#optimization", "#interpretability", "#open_source", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–Ω–∏–∑—É –≤–≤–µ—Ä—Ö —á–µ—Ä–µ–∑ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Å–ª–æ—ë–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#transfer_learning", "#benchmark", "#multimodal", "#reasoning", "#optimization", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –∞–≥–µ–Ω—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è SpatialTree ‚Äî –∏–µ—Ä–∞—Ä—Ö–∏—á–µ
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#synthetic", "#rlhf", "#benchmark", "#dataset", "#optimization", "#open_source", "#multilingual", "#science", "#agents", "#training"], "emoji": "üî¨", "ru": {"title": "–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞ —Å—Ä–µ–¥–Ω–µ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–∏–Ω—Ç–µ–∑ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#benchmark", "#multimodal", "#open_source", "#audio", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏", "desc": "SAM Audio ‚Äî —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è 
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#agents", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—èÊ°ÜÊû∂ SAGE, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#benchmark", "#video", "#multimodal", "#reasoning", "#cv", "#science"], "emoji": "üìπ", "ru": {"title": "–û—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ñ–∏–∑–∏–∫–∏", "desc": "QuantiPhy ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ –±–æ–ª—å—à–∏—Ö –≤–∏–¥–µ–æ—è–∑
[24.12.2025 05:25] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–û—Ç –ø–∞—Å—Å–∏–≤–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –∫ –∞–∫—Ç–∏–≤–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É: –≤–∏–¥–µ–æ-–∞–≤–∞—Ç–∞—Ä—ã —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –º–æ–¥–µ–ª—å—é –º–∏—Ä–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç ORCA ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–∞–≤–∞—Ç–∞—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –º–æ–¥–µ
[24.12.2025 05:25] Renaming data file.
[24.12.2025 05:25] Renaming previous data. hf_papers.json to ./d/2025-12-24.json
[24.12.2025 05:25] Saving new data file.
[24.12.2025 05:25] Generating page.
[24.12.2025 05:25] Renaming previous page.
[24.12.2025 05:25] Renaming previous data. index.html to ./d/2025-12-24.html
[24.12.2025 05:25] Writing result.
[24.12.2025 05:25] Renaming log file.
[24.12.2025 05:25] Renaming previous data. log.txt to ./logs/2025-12-24_last_log.txt
