[24.12.2025 21:20] Read previous papers.
[24.12.2025 21:20] Generating top page (month).
[24.12.2025 21:20] Writing top page (month).
[24.12.2025 22:21] Read previous papers.
[24.12.2025 22:21] Get feed.
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20619
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19673
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20618
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20617
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18746
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20491
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17102
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.18099
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16144
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20182
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13472
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17648
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20615
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20352
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20092
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19526
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15031
[24.12.2025 22:21] Get page data from previous paper. URL: https://huggingface.co/papers/2512.19823
[24.12.2025 22:21] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.12.2025 22:21] No deleted papers detected.
[24.12.2025 22:21] Downloading and parsing papers (pdf, html). Total: 18.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20619.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20619.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20619.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.19673.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.19673.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.19673.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20618.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20618.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20618.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20617.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20617.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20617.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.18746.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.18746.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.18746.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20491.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20491.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20491.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.17102.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.17102.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.17102.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.18099.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.18099.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.18099.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.16144.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.16144.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.16144.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20182.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20182.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20182.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.13472.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.13472.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.13472.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.17648.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.17648.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.17648.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20615.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20615.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20615.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20352.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20352.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20352.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.20092.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.20092.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.20092.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.19526.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.19526.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.19526.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.15031.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.15031.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.15031.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Downloading and parsing paper https://huggingface.co/papers/2512.19823.
[24.12.2025 22:21] Extra JSON file exists (./assets/json/2512.19823.json), skip PDF parsing.
[24.12.2025 22:21] Paper image links file exists (./assets/img_data/2512.19823.json), skip HTML parsing.
[24.12.2025 22:21] Success.
[24.12.2025 22:21] Enriching papers with extra data.
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 0. SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.  					AI-generated summary 				 State-of-the-art video...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 1. The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.  					AI-generated summary 				 Existing reinforcement...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 2. A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.  					AI-generated summary 				 Recent advances in multimodal LLMs and systems that use tools for long-video QA point to ...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 3. SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.  					AI-generated summary 				 Cognitive science suggests that spat...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 4. MemEvolve, a meta-evolutionary framework, enhances self-evolving memory systems by jointly evolving agents' experiential knowledge and memory architecture, leading to improved performance and generalization.  					AI-generated summary 				 Self-evolving memory systems are unprecedentedly reshaping t...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 5. Step-DeepResearch, an end-to-end agent enhanced with a data synthesis strategy and progressive training, achieves expert-level capabilities in deep research scenarios, outperforming established models.  					AI-generated summary 				 As LLMs shift toward autonomous agents, Deep Research has emerged ...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 6. A novel RL framework, SAGE, enhances LLM-based agents' self-improvement capabilities by systematically incorporating skills from a skill library, leading to better performance and efficiency in new environments.  					AI-generated summary 				 Large Language Model (LLM)-based agents have demonstrate...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 7. SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types.  					AI-generated summary 				 General audio source separation is a key capability for multimodal AI...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 8. INTELLECT-3, a large Mixture-of-Experts model trained with reinforcement learning, achieves top performance across various benchmarks and is supported by an open-source RL infrastructure framework.  					AI-generated summary 				 We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 9. FaithLens, a cost-efficient faithfulness hallucination detection model using advanced LLMs for training data synthesis and rule-based reinforcement learning, outperforms models like GPT-4.1 and o3 on 12 tasks with high-quality explanations.  					AI-generated summary 				 Recognizing whether outputs...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 10. Systematic exploration of scaling laws for multilingual code pre-training reveals language-specific benefits and proposes a strategy for optimal token allocation across programming languages.  					AI-generated summary 				 Code large language models (Code LLMs) are powerful but costly to train, wit...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 11. A new open-source framework, simulstream, is introduced for evaluating and demonstrating streaming speech-to-text translation systems, supporting both incremental decoding and re-translation methods with a focus on long-form audio processing.  					AI-generated summary 				 Streaming Speech-to-Text ...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 12. ORCA, a framework for goal-directed planning in video avatars, uses an internal world model and dual-system architecture to enable autonomous task completion in stochastic environments.  					AI-generated summary 				 Current video avatar generation methods excel at identity preservation and motion ...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 13. A multi-perspective validation framework using LLMs for thematic analysis combines ensemble validation with Cohen's Kappa and cosine similarity to enhance reliability and extract consensus themes from qualitative data.  					AI-generated summary 				 Qualitative research faces a critical reliability...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 14. Memory-T1, a framework using reinforcement learning, enhances temporal reasoning in long dialogues by selecting relevant sessions and ensuring temporal consistency, achieving state-of-the-art performance on the Time-Dialog benchmark.  					AI-generated summary 				 Temporal reasoning over long, mult...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 15. QuantiPhy is a benchmark that quantitatively assesses state-of-the-art vision perception models' ability to reason about physical properties such as size, velocity, and acceleration from video observations, revealing gaps between qualitative plausibility and numerical correctness.  					AI-generated...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 16. A novel framework using Large Language Models and a two-step prompting pipeline effectively predicts and prevents conversational derailment in open source software communities on GitHub.  					AI-generated summary 				 Toxic interactions in Open Source Software (OSS) communities reduce contributor e...
[24.12.2025 22:21] ********************************************************************************
[24.12.2025 22:21] Abstract 17. Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approac...
[24.12.2025 22:21] Read previous papers.
[24.12.2025 22:21] Generating reviews via LLM API.
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#training", "#video", "#diffusion", "#optimization", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "SemanticGen –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#optimization", "#interpretability", "#open_source", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–Ω–∏–∑—É –≤–≤–µ—Ä—Ö —á–µ—Ä–µ–∑ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Å–ª–æ—ë–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#rl", "#video", "#reasoning", "#agents", "#dataset"], "emoji": "üé¨", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –≥
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#transfer_learning", "#benchmark", "#multimodal", "#reasoning", "#optimization", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –∞–≥–µ–Ω—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è SpatialTree ‚Äî –∏–µ—Ä–∞—Ä—Ö–∏—á–µ
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#open_source", "#training", "#transfer_learning", "#agents"], "emoji": "üß†", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–µ —Å –∏—Ö –æ–ø—ã—Ç–æ–º", "desc": "MemEvolve ‚Äî —ç—Ç–æ –º–µ—Ç–∞-—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∑–Ω–∞–Ω–∏—è –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#synthetic", "#rlhf", "#benchmark", "#dataset", "#optimization", "#open_source", "#multilingual", "#science", "#agents", "#training"], "emoji": "üî¨", "ru": {"title": "–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞ —Å—Ä–µ–¥–Ω–µ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–∏–Ω—Ç–µ–∑ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#agents", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—èÊ°ÜÊû∂ SAGE, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#benchmark", "#multimodal", "#open_source", "#audio", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏", "desc": "SAM Audio ‚Äî —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è 
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#open_source", "#rl", "#math", "#plp", "#science", "#training", "#agents", "#architecture", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–æ–≥—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–º–µ—à–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ INTEL
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#optimization", "#data", "#synthetic", "#rag", "#rlhf", "#training", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–ú–∞–ª–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–æ–π —á–µ—Å—Ç–Ω–æ—Å—Ç—å—é: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –±–µ–∑ –±–æ–ª—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "FaithLens ‚Äî —ç—Ç–æ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –≤—ã—Ö–æ–¥–∞—Ö
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#optimization", "#plp", "#training", "#low_resource", "#architecture", "#multilingual", "#transfer_learning"], "emoji": "üêç", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫–æ–¥–∞", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#open_source", "#machine_translation", "#multimodal", "#benchmark", "#audio", "#long_context", "#dataset"], "emoji": "üéôÔ∏è", "ru": {"title": "–ï–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ —Ä–µ—á–µ–≤–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è open-source —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ simulstream 
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–û—Ç –ø–∞—Å—Å–∏–≤–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –∫ –∞–∫—Ç–∏–≤–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É: –≤–∏–¥–µ–æ-–∞–≤–∞—Ç–∞—Ä—ã —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –º–æ–¥–µ–ª—å—é –º–∏—Ä–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç ORCA ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–∞–≤–∞—Ç–∞—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –º–æ–¥–µ
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#science", "#open_source"], "emoji": "ü§ù", "ru": {"title": "–ù–∞–¥—ë–∂–Ω–∞—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞–Ω—Å–∞–º–±–ª—å LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∞–Ω—Å–∞–º–±–ª–µ–≤—É—é –≤–∞–ª–∏–¥–∞—Ü
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#optimization", "#open_source", "#reasoning", "#rl", "#long_context"], "emoji": "‚è∞", "ru": {"title": "–£–º–Ω—ã–π –æ—Ç–±–æ—Ä –ø–∞–º—è—Ç–∏: RL –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –¥–ª–∏–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö", "desc": "Memory-T1 ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reinforcement learning –¥–ª—è —É–ª—É
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#benchmark", "#video", "#multimodal", "#reasoning", "#cv", "#science"], "emoji": "üìπ", "ru": {"title": "–û—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∫ —á–∏—Å–ª–æ–≤–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ñ–∏–∑–∏–∫–∏", "desc": "QuantiPhy ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ –±–æ–ª—å—à–∏—Ö –≤–∏–¥–µ–æ—è–∑
[24.12.2025 22:21] Using data from previous issue: {"categories": ["#open_source", "#ethics"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–æ–º–ø—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∏—Å–∫—É—Å—Å–∏–π –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤
[24.12.2025 22:21] Using data from previous issue: {"categories": [], "emoji": "üì∏", "ru": {"title": "–ü–µ—Ä–µ–¥–µ–ª–∫–∞ —Ñ–æ–∫—É—Å–∞ –ø–æ—Å–ª–µ —Å—ä—ë–º–∫–∏ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–æ–∫—É—Å–∞ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ø–æ—Å–ª–µ –µ—ë —Å—ä—ë–º–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ò–∑ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º—ã—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏—Ö –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 
[24.12.2025 22:21] Renaming data file.
[24.12.2025 22:21] Renaming previous data. hf_papers.json to ./d/2025-12-24.json
[24.12.2025 22:21] Saving new data file.
[24.12.2025 22:21] Generating page.
[24.12.2025 22:21] Renaming previous page.
[24.12.2025 22:21] Renaming previous data. index.html to ./d/2025-12-24.html
[24.12.2025 22:21] Writing result.
[24.12.2025 22:21] Renaming log file.
[24.12.2025 22:21] Renaming previous data. log.txt to ./logs/2025-12-24_last_log.txt
