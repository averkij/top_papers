[14.02.2025 03:15] Read previous papers.
[14.02.2025 03:15] Generating top page (month).
[14.02.2025 03:15] Writing top page (month).
[14.02.2025 04:12] Read previous papers.
[14.02.2025 04:12] Get feed.
[14.02.2025 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2502.08910
[14.02.2025 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09604
[14.02.2025 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09056
[14.02.2025 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08946
[14.02.2025 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2502.06608
[14.02.2025 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2502.08690
[14.02.2025 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09100
[14.02.2025 04:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.02.2025 04:12] No deleted papers detected.
[14.02.2025 04:12] Downloading and parsing papers (pdf, html). Total: 7.
[14.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.08910.
[14.02.2025 04:12] Downloading paper 2502.08910 from http://arxiv.org/pdf/2502.08910v1...
[14.02.2025 04:12] Extracting affiliations from text.
[14.02.2025 04:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 0 1 9 8 0 . 2 0 5 2 : r Extending Language Model Context Up to 3 Million Tokens on Single GPU Heejun Lee * 1 2 Geon Park * 1 2 Jaduk Suh 1 Sung Ju Hwang "
[14.02.2025 04:12] Response: []
[14.02.2025 04:12] Extracting affiliations from text.
[14.02.2025 04:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 0 1 9 8 0 . 2 0 5 2 : r Extending Language Model Context Up to 3 Million Tokens on Single GPU Heejun Lee * 1 2 Geon Park * 1 2 Jaduk Suh 1 Sung Ju HwangIn modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, novel and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As result, InfiniteHiP enables the processing of up to 3 million tokens on single L40s 48GB GPU 3x larger without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations. 1. Introduction In modern Transformer-based generative large language models (LLMs), extending the context length is essential for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. However, achieving this poses significant challenges, primarily due to the attention mechanism (Vaswani et al., 2017), fundamental component of these models. The attention mechanism computes relationships between each input to- *Equal contribution 1Graduate School of AI, KAIST, Seoul, Korea 2DeepAuto.ai, Seoul, Korea. Correspondence to: Sung Ju Hwang <sungju.hwang@kaist.ac.kr>. Preprint, Copyright 2025 by the author(s). ken and all preceding tokens, causing computational and memory costs to scale quadratically as the input sequence length increases. Another problem arising from the attention mechanism is the key-value (KV) cache. During generation, previously computed attention keys and values are cached on GPU memory for reuse. However, the KV cache size scales linearly with context length, creating challenge for long context inference. Various methods have been proposed to reduce the high costs of the attention mechanism. FlashAttention (FA2) (Dao et al., 2022) significantly reduces memory consumption and bandwidth utilization by avoiding writing the entire attention score matrix to global GPU memory. However, it does not reduce the arithmetic computation cost. Other approaches (Xiao et al., 2024b; Lee et al., 2024b) selectively attend to fixed number of key tokens, either statically or dynamically, during attention inference. Many efforts have also been made to mitigate the memory burden of the KV cache. KV cache eviction methods selectively forget past contexts to conserve GPU memory (Zhang et al., 2023; Oren et al., 2024). However, these methods permanently erase past contexts, which may be needed again later. HiP attention (Lee et al., 2024b) offloads infrequently accessed cold tokens to larger and cheaper host memory, dynamically fetching them back to GPU during generation only when needed while keeping only frequently accessed hot tokens on the GPU. Despite these optimizations, another problem with context extension still remains: pre-trained LLMs cannot handle inputs longer than their trained context length. Since the attention mechanism is permutation invariant, they utilize positional embedding methods such as Rotary Positional Embeddings (RoPE) (Su et al., 2023) to model the temporal order of tokens. However, as LLMs are typically pre-trained on sequences truncated to fixed length, they fail to adapt to unseen positions when prompted with longer contexts. One option for overcoming this problem is long context fine-tuning (Rozi`ere et al., 2024), i.e., fine-tuning the model on set of longer inputs. However, fine-tuning, especially on long sequences, requires exorbitant training costs and high-quality training data. Thus, out-of-length (OOL) generalization, i.e., the capability for pre-trained models to perform well beyond their pre-trained limits without trainInfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on Single GPU Figure 1. Overview of InfiniteHiP. (a) Infinitely growing KV cache: In InfiniteHiP, the context keys and values are stored in unified memory space, where some of the keys and values are loaded on GPU memory. (b) Configurable modular pruning: Each pruning stage narrows down the candidate key indices based on the current query block. During pruning, if cache miss is encountered, the missing tokens are dynamically loaded and the GPU cache is updated. (c) Paged block sparse attention: The selected key indices are used to perform efficient paged block sparse attention. ing, becomes increasingly important. Self-Extend (Jin et al., 2024) proposes training-free way of scaling the RoPE embeddings beyond the pre-trained limit. In this paper, we propose InfiniteHiP, long-context LLM framework that combines the strengths of all the above methods. To alleviate the computational burden of attention, InfiniteHiP proposes novel modular sparse attention scheme that minimizes computation for less important contexts. For optimizing KV cache offloading, InfiniteHiP enhances HiP attention (Lee et al., 2024b)s offloading strategy with sophisticated LRU-based cache policy. Finally, InfiniteHiP achieves OOL generalization by carefully applying various RoPE adjustment strategies within different components of LLMs according to their internal attention patterns. By providing unified solution to all the aforementioned problems as whole, InfiniteHiP demonstrates strong practicality and is well suited for real-world deployment. What sets InfiniteHiP apart is its innovative use of pruning modules, as illustrated in Figure 1. These modules employ novel modular hierarchical pruning algorithm to selectively discard less important input tokens. The algorithm leverages common patterns observed in attention matrices of popular LLMs namely, their sparsity and the spatial locality of nonzero entries within sequence to prune "
[14.02.2025 04:12] Mistral response. {"id": "2915751401384c26a2f56c3673c9aac2", "object": "chat.completion", "created": 1739506376, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\"Graduate School of AI, KAIST, Seoul, Korea\", \"DeepAuto.ai, Seoul, Korea\"]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1560, "total_tokens": 1587, "completion_tokens": 27}}
[14.02.2025 04:12] Response: ["Graduate School of AI, KAIST, Seoul, Korea", "DeepAuto.ai, Seoul, Korea"]
[14.02.2025 04:12] Deleting PDF ./assets/pdf/2502.08910.pdf.
[14.02.2025 04:12] Success.
[14.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.09604.
[14.02.2025 04:12] Extra JSON file exists (./assets/json/2502.09604.json), skip PDF parsing.
[14.02.2025 04:12] Paper image links file exists (./assets/img_data/2502.09604.json), skip HTML parsing.
[14.02.2025 04:12] Success.
[14.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.09056.
[14.02.2025 04:12] Extra JSON file exists (./assets/json/2502.09056.json), skip PDF parsing.
[14.02.2025 04:12] Paper image links file exists (./assets/img_data/2502.09056.json), skip HTML parsing.
[14.02.2025 04:12] Success.
[14.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.08946.
[14.02.2025 04:12] Extra JSON file exists (./assets/json/2502.08946.json), skip PDF parsing.
[14.02.2025 04:12] Paper image links file exists (./assets/img_data/2502.08946.json), skip HTML parsing.
[14.02.2025 04:12] Success.
[14.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.06608.
[14.02.2025 04:12] Downloading paper 2502.06608 from http://arxiv.org/pdf/2502.06608v1...
[14.02.2025 04:13] Extracting affiliations from text.
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models Yangguang Li1 * Zi-Xin Zou1 * Zexiang Liu1 Dehu Wang1 Yuan Liang1 Zhipeng Yu1 Xingchao Liu3 Yuan-Chen Guo1 Ding Liang1 Wanli Ouyang2,4 Yan-Pei Cao1 5 2 0 2 0 1 ] . [ 1 8 0 6 6 0 . 2 0 5 2 : r Figure 1. High-quality 3D shape samples from our largest TripoSG model. Covering various complex structures, diverse styles, imaginative designs, multi-object compositions, and richly detailed outputs, demonstrates its powerful generation capabilities. "
[14.02.2025 04:13] Response: ```python
[]
```
[14.02.2025 04:13] Extracting affiliations from text.
[14.02.2025 04:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models Yangguang Li1 * Zi-Xin Zou1 * Zexiang Liu1 Dehu Wang1 Yuan Liang1 Zhipeng Yu1 Xingchao Liu3 Yuan-Chen Guo1 Ding Liang1 Wanli Ouyang2,4 Yan-Pei Cao1 5 2 0 2 0 1 ] . [ 1 8 0 6 6 0 . 2 0 5 2 : r Figure 1. High-quality 3D shape samples from our largest TripoSG model. Covering various complex structures, diverse styles, imaginative designs, multi-object compositions, and richly detailed outputs, demonstrates its powerful generation capabilities.Recent advancements in diffusion techniques have propelled image and video generation to unprecedented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has 1VAST 2The Chinese University of Hong Kong 3The University of Texas at Austin 4Shanghai AI Laboratory : Equal Contribution : Corresponding Author . so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data processing, and insufficient exploration of advanced techniques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capability, and alignment with input conditions. We present TripoSG, new streamlined shape diffusion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) hybrid super1 TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models vised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving highquality 3D reconstruction performance. 3) data processing pipeline to generate 2 million highquality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D generative models. Through comprehensive experiments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit enhanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input images. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong generalization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available. Figure 2. The overview of our method consists of two main components: (i) Data-Building System and (ii) TripoSG Model. The databuilding system processes the 3D models from various datasets (e.g., Objaverse and ShapeNet) through series of data processing steps to create the training data. Our TripoSG model is then trained on this curated dataset for high-fidelity shape generation from single input image. Key words: 3D Generation, Rectified Flow, Image-to-3D 1. Introduction Recent advancements in large-scale visual datasets (Bain et al., 2021; Schuhmann et al., 2022; Wang et al., 2023c) have propelled remarkable progress in generative models. These models effectively compress high-dimensional visual data, such as images and videos, into latent spaces, enabling the generation of high-quality visual content conditioned on various input modalities. State-of-the-art generative AI models, including SD3 (Esser et al., 2024), FLUX (blackforestlabs, 2024), and Sora (Brooks et al., 2024), exemplify this capability, producing strikingly realistic images and videos from diverse conditional inputs. This breakthrough has revolutionized human visual creation, opening new avenues for artistic expression and content generation. In the domain of 3D content creation, the pursuit of highquality, production-ready 3D generation remains primary objective for researchers, artists, and designers. Substantial progress has been made in generating 3D models from single images, with approaches broadly categorized into two paradigms: large-scale reconstruction-based methods (Hong et al., 2023; Li et al., 2023; Tochilkin et al., 2024; Wang et al., 2023b; 2024; Wei et al., 2024; Xu et al., 2024; 2023; Zhang et al., 2024a; Zou et al., 2024b) and diffusion-based methods (Li et al., 2024c; Wu et al., 2024b; Zhang et al., 2023; 2024b; Zhao et al., 2024). Large-scale reconstructionbased methods primarily utilize network to regress the 3D model in deterministic way. While effective, these approaches often struggle with inconsistencies in overlapping regions from multiple input views (which can be generated from single view by multi-view diffusion models (Liu et al., 2023d; Shi et al., 2023b; Wang & Shi, 2023b)) and exhibit artifacts in occluded areas. Conversely, diffusion-based methods train on 3D representations or latent representations compressed by Variational AutoEncoders (VAEs). As generative rather than regression methods, they circumvent some challenges inherent to reconstruction approaches. However, current methods predominantly rely on occupancy representations, often necessitating additional post-processing to mitigate aliasing artifacts and lacking fine-grained geometric details. Moreover, vanilla diffusion architectures and sampling strategies yield suboptimal 3D model quality, resulting in significant alignment gap between generated models and input images. common limitation across both approaches is their heavy reliance on the Objaverse dataset (Deitke et al., 2023). The necessity for rigorous data filtering often reduces the usable data samples by nearly half, presenting substantial challenge in scaling data compared to the image and video domains. Given these challenges in 3D generation and the substantial successes observed in image and video synthesis, we posit critical question: What is the optimal paradigm for generating high-fidelity 3D models with precise alignment to input conditions? In response to this inquiry, we present TripoSG, highfidelity 3D generative model that leverages rectified flow transformer trained on meticulously curated, largescale data. Our approach achieves unprecedented quality in image-to-3D generation, characterized by finer details and superior input condition alignment. Inspired by 3DShape2VecSet (Zhang et al., 2023), we train our genera2 TripoSG: High-Fidelity 3D Shape Synthesis using Large-Sca"
[14.02.2025 04:13] Mistral response. {"id": "41ab406f1b5447afa9b80ce5ab825437", "object": "chat.completion", "created": 1739506385, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['VAST', 'The Chinese University of Hong Kong', 'The University of Texas at Austin', 'Shanghai AI Laboratory']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1707, "total_tokens": 1741, "completion_tokens": 34}}
[14.02.2025 04:13] Response: ```python
['VAST', 'The Chinese University of Hong Kong', 'The University of Texas at Austin', 'Shanghai AI Laboratory']
```
[14.02.2025 04:13] Deleting PDF ./assets/pdf/2502.06608.pdf.
[14.02.2025 04:13] Success.
[14.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.08690.
[14.02.2025 04:13] Downloading paper 2502.08690 from http://arxiv.org/pdf/2502.08690v1...
[14.02.2025 04:13] Extracting affiliations from text.
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Hoigi Seo * 1 Wongi Jeong * 1 Jae-sun Seo 2 Se Young Chun 1 3 5 2 0 2 2 1 ] . [ 1 0 9 6 8 0 . 2 0 5 2 : r a "
[14.02.2025 04:13] Response: ```python
[]
```
[14.02.2025 04:13] Extracting affiliations from text.
[14.02.2025 04:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Hoigi Seo * 1 Wongi Jeong * 1 Jae-sun Seo 2 Se Young Chun 1 3 5 2 0 2 2 1 ] . [ 1 0 9 6 8 0 . 2 0 5 2 : r aLarge-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores. 1. Introduction Diffusion generative models excel at tasks such as text-toimage (T2I) synthesis (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024; Chen et al., 2024), editing (Brooks et al., 2023; Cao et al., 2023; Kawar et al., 2023), video generation (Liu et al., 2024; Polyak et al., 2024), and 3D creation (Poole et al., 2022; Cao et al., 2023; Seo et al., *Equal contribution 1Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea 2School of Electrical and Computer Engineering, Cornell Tech, USA 3INMC & IPAI, Seoul National University, Republic of Korea. Correspondence to: Se Young Chun <sychun@snu.ac.kr>. 1 (a) FLOPs ratio. (b) Parameter ratio. Figure 1. (a) FLOPs distribution during image generation in Stable Diffusion 3 (SD3) (Esser et al., 2024). (b) Parameter distribution across modules in SD3. The text encoders contributes less than 0.5% to the overall FLOPs but account for over 70% of the total model parameters. For VAE, only the decoder was considered. 2023; Wang et al., 2024c). With modern architecture and large-scale text encoders, they produce high-quality images that closely match text prompts. Despite these successes, they require significant computational resources, especially memory, making deployment and scalability challenging. To address these issues, research suggests enhancing the efficiency of the T2I diffusion model through strategies such as knowledge distillation (KD) (Castells et al., 2024b; Li et al., 2024b; Song et al., 2024b; Kim et al., 2024; Zhao et al., 2024), which transfers knowledge from larger models to smaller ones; pruning (Castells et al., 2024a; Ganjdanesh et al., 2024; Lee et al., 2024; Wang et al., 2024b), which eliminates superfluous weights; and quantization (Li et al., 2023; He et al., 2024; Ryu et al., 2025), which reduces precision by utilizing fewer bits. While effective, these methods target the denoising module. As shown in Fig. 1, the text encoders account for over 70% of the total parameters, but only 0.5% of floating-point operations (FLOPs), causing disproportionate memory usage. Despite this imbalance, efforts to reduce the text encoder size have been limited. Large language models (LLMs) also face similar challenges, where model sizes result in significant computational and memory overhead, hindering their practical use. To address this problem, studies such as KD (Hsieh et al., 2023; Huang et al., 2023; Ko et al., 2024), quantization (Xiao et al., 2023; Ashkboos et al., 2024b; Lin et al., 2024), and pruning (Sun et al., 2023; Ashkboos et al., 2024a; Men et al., 2024; Song Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation et al., 2024a; Yang et al., 2024; Zhang et al., 2024) have been proposed. While KD reduces the size of the model, it requires costly training. Quantization reduces memory usage by reducing precision, but it requires specific hardware support. In contrast, pruning offers parameter reduction with minimal performance loss, making it an efficient solution. Among pruning techniques, structured pruning has been actively studied to remove rows and columns (van der Ouderaa et al., 2023; Ashkboos et al., 2024a) of the model weights, or entire layers or blocks (Gromov et al., 2024; Men et al., 2024; Yang et al., 2024; Zhang et al., 2024) of the model to reduce the size of the model and improve the inference speed. However, these methods are designed for autoregressive LLMs and face challenges when applied to T2I diffusion models, limiting their effectiveness in this context. We propose Skip and reuse the layers (Skrr), blockwise pruning technique for T2I diffusion models. Skrr effectively reduces text encoder size, alleviating memory overhead while preserving image quality and text alignment. Skrr involves two primary stages: the Skip to detect layers for pruning, followed by the Re-use to recycle the remaining layers to mitigate performance degradation. To the best of our knowledge, this is the first work to tackle the challenge of constructing lightweight text encoder for T2I tasks. In the Skip phase, sub-blocks of the text encoder transformer are pruned using T2I diffusion-tailored discrepancy metric to align dense and pruned models. Prior methods greedily remove blocks to reduce computational costs, but often overlook block interactions, leading to suboptimal pruning. To mitigate this, we propose beam search (Freitag & Al-Onaizan, 2017)-based approach that explores multiple pruning paths simultaneously, balancing the performance of exhaustive and efficiency of greedy strategies. In the Re-use phase, we assess the discrepancy from reusing adjacent unskipped blocks to identify those that can be effectively reutilized. Additionally, we provide theoretical support showing that Re-use can enhance performance beyond mere skipping. To improve discrepancy measurement in both phases, we employ projection module identical to the one used for conditioning text embeddings in the denoising module. We co"
[14.02.2025 04:13] Mistral response. {"id": "af0f9149f196492c91520d6477b767dc", "object": "chat.completion", "created": 1739506398, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea\",\n    \"School of Electrical and Computer Engineering, Cornell Tech, USA\",\n    \"INMC & IPAI, Seoul National University, Republic of Korea\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1844, "total_tokens": 1911, "completion_tokens": 67}}
[14.02.2025 04:13] Response: ```python
[
    "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
    "School of Electrical and Computer Engineering, Cornell Tech, USA",
    "INMC & IPAI, Seoul National University, Republic of Korea"
]
```
[14.02.2025 04:13] Deleting PDF ./assets/pdf/2502.08690.pdf.
[14.02.2025 04:13] Success.
[14.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.09100.
[14.02.2025 04:13] Extra JSON file exists (./assets/json/2502.09100.json), skip PDF parsing.
[14.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.09100.json), skip HTML parsing.
[14.02.2025 04:13] Success.
[14.02.2025 04:13] Enriching papers with extra data.
[14.02.2025 04:13] ********************************************************************************
[14.02.2025 04:13] Abstract 0. In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable effic...
[14.02.2025 04:13] ********************************************************************************
[14.02.2025 04:13] Abstract 1. We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by...
[14.02.2025 04:13] ********************************************************************************
[14.02.2025 04:13] Abstract 2. This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities o...
[14.02.2025 04:13] ********************************************************************************
[14.02.2025 04:13] Abstract 3. In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates...
[14.02.2025 04:13] ********************************************************************************
[14.02.2025 04:13] Abstract 4. Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D d...
[14.02.2025 04:13] ********************************************************************************
[14.02.2025 04:13] Abstract 5. Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddi...
[14.02.2025 04:13] ********************************************************************************
[14.02.2025 04:13] Abstract 6. With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logi...
[14.02.2025 04:13] Read previous papers.
[14.02.2025 04:13] Generating reviews via LLM API.
[14.02.2025 04:13] Querying the API.
[14.02.2025 04:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.
[14.02.2025 04:13] Response: {
  "desc": "InfiniteHiP - это новая система для обработки больших языковых моделей (БЯМ) с длинным контекстом. Она ускоряет обработку, удаляя неважные токены контекста с помощью иерархического алгоритма. Система позволяет обобщать на более длинные последовательности, применяя методы настройки RoPE. InfiniteHiP достигает 18.95-кратного ускорения в декодировании внимания для контекста в 1 миллион токенов без дополнительного обучения.",
  "emoji": "🚀",
  "title": "Преодоление барьера длинного контекста в БЯМ"
}
[14.02.2025 04:13] Renaming some terms.
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations."

[14.02.2025 04:13] Response: ```python
["INFERENCE", "TRAINING", "ARCHITECTURE"]
```
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations."

[14.02.2025 04:13] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[14.02.2025 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents InfiniteHiP, a new framework designed to improve the efficiency of large language models (LLMs) when processing very long contexts. It addresses the issues of slow inference speeds and high memory costs by using a hierarchical token pruning algorithm to remove irrelevant tokens dynamically. Additionally, InfiniteHiP enhances the model\'s ability to generalize to longer sequences by adjusting the relative positional encodings based on attention patterns. The framework allows for processing up to 3 million tokens on a single GPU while achieving significant speed improvements without the need for extra training.","title":"InfiniteHiP: Unlocking Long Contexts in Language Models Efficiently"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents InfiniteHiP, a new framework designed to improve the efficiency of large language models (LLMs) when processing very long contexts. It addresses the issues of slow inference speeds and high memory costs by using a hierarchical token pruning algorithm to remove irrelevant tokens dynamically. Additionally, InfiniteHiP enhances the model's ability to generalize to longer sequences by adjusting the relative positional encodings based on attention patterns. The framework allows for processing up to 3 million tokens on a single GPU while achieving significant speed improvements without the need for extra training.", title='InfiniteHiP: Unlocking Long Contexts in Language Models Efficiently'))
[14.02.2025 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"在现代大型语言模型（LLMs）中，处理非常长的上下文长度面临显著挑战，导致推理速度变慢和内存成本增加。现有的大多数预训练LLMs无法超出其原始训练序列长度进行泛化。为了解决这一问题，我们提出了InfiniteHiP，这是一种新颖且实用的LLM推理框架，通过模块化的分层令牌修剪算法动态消除无关的上下文令牌，从而加速处理。我们的框架能够在不需要额外训练的情况下，实现对长达300万令牌的处理，并在1百万令牌上下文中实现18.95倍的注意力解码加速。","title":"InfiniteHiP：高效处理超长上下文的LLM框架"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='在现代大型语言模型（LLMs）中，处理非常长的上下文长度面临显著挑战，导致推理速度变慢和内存成本增加。现有的大多数预训练LLMs无法超出其原始训练序列长度进行泛化。为了解决这一问题，我们提出了InfiniteHiP，这是一种新颖且实用的LLM推理框架，通过模块化的分层令牌修剪算法动态消除无关的上下文令牌，从而加速处理。我们的框架能够在不需要额外训练的情况下，实现对长达300万令牌的处理，并在1百万令牌上下文中实现18.95倍的注意力解码加速。', title='InfiniteHiP：高效处理超长上下文的LLM框架'))
[14.02.2025 04:13] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#training", "#alignment", "#rlhf", "#benchmark"], "emoji": "📚", "ru": {"title": "SelfCite: Самообучение ИИ искусству цитирования", "desc": "SelfCite - это новый подход к самообучению больших языковых моделей (LLM) для генерации качественных цитат на
[14.02.2025 04:13] Using data from previous issue: {"categories": ["#data", "#dataset", "#low_resource", "#multilingual", "#training", "#reasoning"], "emoji": "🧠", "ru": {"title": "Усиление логики локальных языковых моделей", "desc": "Данное исследование посвящено методам улучшения способностей к рассуждению у языково-специфичных больших языковых мо
[14.02.2025 04:13] Using data from previous issue: {"categories": ["#hallucinations", "#dataset", "#training", "#reasoning", "#interpretability"], "emoji": "🦜", "ru": {"title": "Языковые модели: понимание или имитация?", "desc": "Исследователи изучают вопрос о действительном понимании языковыми моделями (LLM) того, что они говорят. Они разработали з
[14.02.2025 04:13] Querying the API.
[14.02.2025 04:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.
[14.02.2025 04:13] Response: {
  "desc": "TripoSG - это новая парадигма генерации трехмерных форм, способная создавать высококачественные 3D-модели с точным соответствием входным изображениям. Она использует масштабный трансформер с выпрямленным потоком для генерации 3D-форм, обученный на обширных высококачественных данных. TripoSG применяет гибридную стратегию обучения, сочетающую потери SDF, нормалей и эйконала для 3D VAE. Благодаря интеграции этих компонентов, TripoSG достигает передового уровня в генерации 3D-форм с улучшенной детализацией и верностью входным изображениям.",
  "emoji": "🧊",
  "title": "TripoSG: Революция в генерации трехмерных форм"
}
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available."

[14.02.2025 04:13] Response: ```python
["3D", "DATASET", "TRAINING"]
```
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available."

[14.02.2025 04:13] Response: ```python
["DIFFUSION", "OPEN_SOURCE"]
```
[14.02.2025 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces TripoSG, a new method for generating high-quality 3D shapes using diffusion techniques. It addresses the challenges of 3D shape generation by employing a large-scale rectified flow transformer and a hybrid supervised training strategy that enhances reconstruction performance. The model is trained on a vast dataset, producing 2 million high-quality 3D samples, which improves the fidelity and detail of the generated shapes. TripoSG not only achieves state-of-the-art results but also shows strong generalization across various input images, making it a significant advancement in 3D generative models.","title":"TripoSG: Revolutionizing 3D Shape Generation with Diffusion Techniques"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces TripoSG, a new method for generating high-quality 3D shapes using diffusion techniques. It addresses the challenges of 3D shape generation by employing a large-scale rectified flow transformer and a hybrid supervised training strategy that enhances reconstruction performance. The model is trained on a vast dataset, producing 2 million high-quality 3D samples, which improves the fidelity and detail of the generated shapes. TripoSG not only achieves state-of-the-art results but also shows strong generalization across various input images, making it a significant advancement in 3D generative models.', title='TripoSG: Revolutionizing 3D Shape Generation with Diffusion Techniques'))
[14.02.2025 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文介绍了一种新的3D形状生成方法TripoSG，旨在提高3D网格的生成质量。我们提出了一种大规模的流动变换器，能够在大量高质量数据上进行训练，从而实现高保真度的3D形状生成。此外，我们采用了一种混合监督训练策略，结合了SDF、法线和Eikonal损失，以提高3D重建性能。通过全面的实验验证，我们的框架在3D形状生成方面达到了最先进的性能，展现了对输入图像的高保真度和多样化的生成能力。","title":"TripoSG：高保真3D形状生成的新范式"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文介绍了一种新的3D形状生成方法TripoSG，旨在提高3D网格的生成质量。我们提出了一种大规模的流动变换器，能够在大量高质量数据上进行训练，从而实现高保真度的3D形状生成。此外，我们采用了一种混合监督训练策略，结合了SDF、法线和Eikonal损失，以提高3D重建性能。通过全面的实验验证，我们的框架在3D形状生成方面达到了最先进的性能，展现了对输入图像的高保真度和多样化的生成能力。', title='TripoSG：高保真3D形状生成的新范式'))
[14.02.2025 04:13] Querying the API.
[14.02.2025 04:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.
[14.02.2025 04:13] Response: {
  "desc": "Статья представляет новый метод оптимизации текстовых энкодеров в моделях диффузии текст-в-изображение (T2I). Авторы предлагают стратегию прунинга под названием Skip and Re-use layers (Skrr), которая позволяет значительно сократить потребление памяти без ущерба для качества генерируемых изображений. Метод Skrr избирательно пропускает или повторно использует определенные слои в трансформерных блоках, учитывая специфику задач T2I. Эксперименты показывают, что Skrr превосходит существующие методы поблочного прунинга и достигает наилучших показателей эффективности использования памяти при сохранении производительности по различным метрикам оценки.",

  "emoji": "✂️",

  "title": "Эффективное сжатие текстовых энкодеров для генерации изображений без потери качества"
}
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores."

[14.02.2025 04:13] Response: ```python
["INFERENCE", "ARCHITECTURE", "TRAINING"]
```
[14.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores."

[14.02.2025 04:13] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[14.02.2025 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new method called Skip and Re-use layers (Skrr) to improve the efficiency of text encoders in text-to-image diffusion models. Text encoders are crucial for generating images from text but use a lot of memory compared to denoising modules. Skrr reduces memory usage by selectively skipping or reusing layers in the transformer architecture, which helps maintain performance while lowering resource demands. The results show that Skrr achieves high image quality and outperforms other pruning methods, making it a significant advancement in memory efficiency for T2I tasks.","title":"Efficient Memory Use in Text-to-Image Models with Skrr"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new method called Skip and Re-use layers (Skrr) to improve the efficiency of text encoders in text-to-image diffusion models. Text encoders are crucial for generating images from text but use a lot of memory compared to denoising modules. Skrr reduces memory usage by selectively skipping or reusing layers in the transformer architecture, which helps maintain performance while lowering resource demands. The results show that Skrr achieves high image quality and outperforms other pruning methods, making it a significant advancement in memory efficiency for T2I tasks.', title='Efficient Memory Use in Text-to-Image Models with Skrr'))
[14.02.2025 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为Skip and Re-use layers（Skrr）的新策略，旨在提高文本到图像扩散模型中文本编码器的内存效率。尽管文本编码器在推理时间和浮点运算方面的贡献较小，但它们的内存需求却高达去噪模块的八倍。Skrr通过选择性跳过或重用某些变换器层，利用了变换器块中的冗余性，从而在不影响性能的情况下减少内存消耗。实验结果表明，Skrr在高稀疏度下仍能保持与原始模型相当的图像质量，并在多个评估指标上实现了最先进的内存效率。","title":"提升文本编码器的内存效率"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为Skip and Re-use layers（Skrr）的新策略，旨在提高文本到图像扩散模型中文本编码器的内存效率。尽管文本编码器在推理时间和浮点运算方面的贡献较小，但它们的内存需求却高达去噪模块的八倍。Skrr通过选择性跳过或重用某些变换器层，利用了变换器块中的冗余性，从而在不影响性能的情况下减少内存消耗。实验结果表明，Skrr在高稀疏度下仍能保持与原始模型相当的图像质量，并在多个评估指标上实现了最先进的内存效率。', title='提升文本编码器的内存效率'))
[14.02.2025 04:13] Using data from previous issue: {"categories": ["#training", "#survey", "#reasoning", "#rl", "#benchmark"], "emoji": "🧠", "ru": {"title": "Логическое мышление в больших языковых моделях: прогресс и перспективы", "desc": "Этот обзор посвящен последним достижениям в области логического рассуждения в больших языковых моделях (LLM). В
[14.02.2025 04:13] Loading Chinese text from previous data.
[14.02.2025 04:13] Renaming data file.
[14.02.2025 04:13] Renaming previous data. hf_papers.json to ./d/2025-02-14.json
[14.02.2025 04:13] Saving new data file.
[14.02.2025 04:13] Generating page.
[14.02.2025 04:13] Renaming previous page.
[14.02.2025 04:13] Renaming previous data. index.html to ./d/2025-02-14.html
[14.02.2025 04:13] [Experimental] Generating Chinese page for reading.
[14.02.2025 04:13] Chinese vocab [{'word': '重光', 'pinyin': 'chóng guāng', 'trans': 'relighting'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}, {'word': '一致', 'pinyin': 'yī zhì', 'trans': 'consistent'}, {'word': '光照', 'pinyin': 'guāng zhào', 'trans': 'illumination'}, {'word': '滞后', 'pinyin': 'zhì hòu', 'trans': 'lag behind'}, {'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'mainly'}, {'word': '由于', 'pinyin': 'yóu yú', 'trans': 'due to'}, {'word': '高昂', 'pinyin': 'gāo áng', 'trans': 'high'}, {'word': '缺乏', 'pinyin': 'quē fá', 'trans': 'lack'}, {'word': '多样化', 'pinyin': 'duō yàng huà', 'trans': 'diversified'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}, {'word': '逐帧', 'pinyin': 'zhú zhēn', 'trans': 'frame-by-frame'}, {'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'}, {'word': '外观', 'pinyin': 'wài guǎn', 'trans': 'appearance'}, {'word': '闪烁', 'pinyin': 'shǎn shuò', 'trans': 'flicker'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '无需', 'pinyin': 'wú xū', 'trans': 'without needing'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '增强', 'pinyin': 'zēng qiáng', 'trans': 'enhance'}, {'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'}, {'word': '连贯性', 'pinyin': 'lián guàn xìng', 'trans': 'coherence'}]
[14.02.2025 04:13] Renaming previous Chinese page.
[14.02.2025 04:13] Renaming previous data. zh.html to ./d/2025-02-13_zh_reading_task.html
[14.02.2025 04:13] Writing Chinese reading task.
[14.02.2025 04:13] Writing result.
[14.02.2025 04:13] Renaming log file.
[14.02.2025 04:13] Renaming previous data. log.txt to ./logs/2025-02-14_last_log.txt
