[17.02.2025 08:15] Read previous papers.
[17.02.2025 08:15] Generating top page (month).
[17.02.2025 08:15] Writing top page (month).
[17.02.2025 09:12] Read previous papers.
[17.02.2025 09:12] Get feed.
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10389
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10248
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09696
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09992
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10391
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09955
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09741
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09935
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10177
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07856
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09638
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09980
[17.02.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.02.2025 09:12] No deleted papers detected.
[17.02.2025 09:12] Downloading and parsing papers (pdf, html). Total: 12.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10389.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10389.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10389.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10248.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10248.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10248.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09696.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09696.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09696.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09992.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09992.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09992.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10391.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10391.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10391.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09955.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09955.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09955.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09741.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09741.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09741.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09935.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09935.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09935.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10177.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10177.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10177.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.07856.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.07856.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.07856.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09638.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09638.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09638.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09980.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09980.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09980.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Enriching papers with extra data.
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 0. Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps o...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 1. We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal comp...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 2. Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model pro...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 3. Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward da...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 4. Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), w...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 5. Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) ...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 6. Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks....
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 7. Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content wi...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 8. A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning a...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 9. In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of ...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 10. Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or oth...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 11. Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V)...
[17.02.2025 09:12] Read previous papers.
[17.02.2025 09:12] Generating reviews via LLM API.
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#diffusion", "#architecture", "#dataset"], "emoji": "ğŸš€", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RAS. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#training", "#open_source", "#diffusion", "#video", "#architecture"], "emoji": "ğŸ¬", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ»Ğ¸ĞºĞ°Ğ¼", "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Step-Video-T2V - Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#interpretability", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "ZeroBench: Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ZeroBench, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#benchmark", "#diffusion", "#training"], "emoji": "ğŸŒŠ", "ru": {"title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ€Ğ¾ÑĞ°ÑÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ·Ñ‹ĞºĞ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLaDA - Ğ½Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#alignment", "#training", "#interpretability", "#open_source", "#rlhf", "#dataset"], "emoji": "ğŸ¤–", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MM-RLHF - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 120 
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#agents", "#rl", "#training", "#inference"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#data", "#optimization"], "emoji": "ğŸ”¢", "ru": {"title": "FoNE: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‡Ğ¸ÑĞµĞ» Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ¸ÑĞµĞ» Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Fourier Number Embedding (FoNE). Fo
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#training", "#synthetic", "#architecture"], "emoji": "ğŸ”", "ru": {"title": "Ğ›Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#games", "#agents", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Spatio-Temporal Memory Agent (STMA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#training", "#cv", "#data", "#diffusion", "#optimization"], "emoji": "ğŸš€", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ MRS (MR Sampler) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Mean Reverting (MR) Diffus
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#ethics", "#multimodal", "#training", "#security", "#rlhf"], "emoji": "ğŸ•µï¸", "ru": {"title": "LLM Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² LLM: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ°Ğ¼Ğ¸ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ 'ĞºÑ€Ğ°ÑĞ½Ğ¾
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#reasoning", "#science", "#agi", "#games", "#optimization", "#dataset", "#agents"], "emoji": "ğŸš—", "ru": {"title": "LLM Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿
[17.02.2025 09:12] Trying to get texts in Chinese.
[17.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.
[17.02.2025 09:12] Mistral response. {"id": "7d474964ad7a4ada9d51d71f259e5527", "object": "chat.completion", "created": 1739783545, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u5404\u79cd\u9886\u57df\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u6210\u4e3a\u9996\u9009\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4f9d\u8d56\u591a\u6b21\u987a\u5e8f\u524d\u5411\u4f20\u9012\uff0c\u663e\u8457\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\u3002\u4ee5\u524d\u7684\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u6216\u91cd\u7528\u4e2d\u95f4\u7ed3\u679c\uff0c\u672a\u80fd\u5229\u7528\u56fe\u50cf\u5185\u90e8\u7a7a\u95f4\u533a\u57df\u7684\u53d8\u5316\u3002\u901a\u8fc7\u5229\u7528\u6269\u6563\u53d8\u538b\u5668\uff08DiTs\uff09\u5904\u7406\u53ef\u53d8\u6570\u91cf\u7684\u6807\u8bb0\u7684\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86RAS\uff0c\u4e00\u79cd\u65b0\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u7b56\u7565\uff0c\u6839\u636eDiT\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u52a8\u6001\u5206\u914d\u56fe\u50cf\u5185\u4e0d\u540c\u533a\u57df\u7684\u91c7\u6837\u6bd4\u7387\u3002\u6211\u4eec\u7684\u5173\u952e\u89c2\u5bdf\u662f\uff0c\u5728\u6bcf\u4e2a\u91c7\u6837\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u96c6\u4e2d\u5728\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u5173\u6ce8\u533a\u57df\u5728\u8fde\u7eed\u6b65\u9aa4\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fde\u7eed\u6027\u3002\u5229\u7528\u8fd9\u4e00\u6d1e\u5bdf\uff0cRAS\u4ec5\u66f4\u65b0\u5f53\u524d\u5173\u6ce8\u7684\u533a\u57df\uff0c\u800c\u5176\u4ed6\u533a\u57df\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7f13\u5b58\u566a\u58f0\u66f4\u65b0\u3002\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u6839\u636e\u524d\u4e00\u6b65\u7684\u8f93\u51fa\u786e\u5b9a\uff0c\u5229\u7528\u4e86\u6211\u4eec\u89c2\u5bdf\u5230\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5728Stable Diffusion 3\u548cLumina-Next-T2I\u4e0a\u8bc4\u4f30RAS\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad82.36\u500d\u548c2.51\u500d\u7684\u52a0\u901f\uff0c\u751f\u6210\u8d28\u91cf\u4ec5\u8f7b\u5fae\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cRAS\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e0b\u63d0\u4f9b\u4e86\u76f8\u4f3c\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e861.6\u500d\u7684\u52a0\u901f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u66f4\u9ad8\u6548\u7684\u6269\u6563\u53d8\u538b\u5668\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u589e\u5f3a\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 362, "total_tokens": 808, "completion_tokens": 446}}
[17.02.2025 09:12] Response: æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
[17.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
[17.02.2025 09:13] Mistral response. {"id": "057f2ebfe09c43808c5e972a2ca3fc1f", "object": "chat.completion", "created": 1739783553, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u5404\u79cd\u9886\u57df\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u6210\u4e3a\u9996\u9009\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4f9d\u8d56\u591a\u6b21\u987a\u5e8f\u524d\u5411\u4f20\u9012\uff0c\u663e\u8457\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\u3002\u4ee5\u524d\u7684\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u6216\u91cd\u7528\u4e2d\u95f4\u7ed3\u679c\uff0c\u672a\u80fd\u5229\u7528\u56fe\u50cf\u5185\u90e8\u7a7a\u95f4\u533a\u57df\u7684\u53d8\u5316\u3002\u901a\u8fc7\u5229\u7528\u6269\u6563\u53d8\u538b\u5668\uff08DiTs\uff09\u5904\u7406\u53ef\u53d8\u6570\u91cf\u7684\u6807\u8bb0\u7684\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86RAS\uff0c\u4e00\u79cd\u65b0\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u7b56\u7565\uff0c\u6839\u636eDiT\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u52a8\u6001\u5206\u914d\u56fe\u50cf\u5185\u4e0d\u540c\u533a\u57df\u7684\u91c7\u6837\u6bd4\u7387\u3002\u6211\u4eec\u7684\u5173\u952e\u89c2\u5bdf\u662f\uff0c\u5728\u6bcf\u4e2a\u91c7\u6837\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u96c6\u4e2d\u5728\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u5173\u6ce8\u533a\u57df\u5728\u8fde\u7eed\u6b65\u9aa4\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fde\u7eed\u6027\u3002\u5229\u7528\u8fd9\u4e00\u6d1e\u5bdf\uff0cRAS\u4ec5\u66f4\u65b0\u5f53\u524d\u5173\u6ce8\u7684\u533a\u57df\uff0c\u800c\u5176\u4ed6\u533a\u57df\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7f13\u5b58\u566a\u58f0\u66f4\u65b0\u3002\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u6839\u636e\u524d\u4e00\u6b65\u7684\u8f93\u51fa\u786e\u5b9a\uff0c\u5229\u7528\u4e86\u6211\u4eec\u89c2\u5bdf\u5230\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5728Stable Diffusion 3\u548cLumina-Next-T2I\u4e0a\u8bc4\u4f30RAS\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad82.36\u500d\u548c2.51\u500d\u7684\u52a0\u901f\uff0c\u751f\u6210\u8d28\u91cf\u4ec5\u8f7b\u5fae\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cRAS\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e0b\u63d0\u4f9b\u4e86\u76f8\u4f3c\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e861.6\u500d\u7684\u52a0\u901f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u66f4\u9ad8\u6548\u7684\u6269\u6563\u53d8\u538b\u5668\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u589e\u5f3a\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002\n\nku\u00f2 s\u00e0n m\u00f3 x\u00edng (DMs) z\u00e0i g\u00e8 zh\u01d2ng l\u01d0ng y\u00f9 de sh\u0113ng ch\u00e9ng r\u00e8n w\u00f9 zh\u014dng ch\u00e9ng w\u00e9i sh\u01d2u xu\u01cen. r\u00e1n \u00e9r, t\u0101 men y\u012b l\u00e0i du\u014d c\u00ec sh\u00f9n x\u00f9 qi\u00e1n xi\u0101ng chu\u00e1n d\u00ec, xi\u01cen zh\u00f9 xi\u00e0n zh\u00ec le sh\u00ed sh\u00ed x\u00ecng n\u00e9ng. y\u01d0 qi\u00e1n de ji\u0101 s\u00f9 f\u0101ng f\u01ce zh\u01d4 y\u00e0o j\u012b zh\u014dng z\u00e0i ji\u01cen sh\u01ceo c\u01cei y\u00e0ng b\u00f9 zh\u00f2u hu\u00f2 ch\u00f3ng y\u00f2ng zh\u014dng ji\u0101n ji\u00e9 gu\u01d2, w\u00e8i n\u00e9ng l\u00ec y\u00f2ng t\u00fa xi\u00e0ng n\u00e8i b\u00f9 k\u014dng ji\u0101n q\u016b y\u00f9 de bi\u00e0n hu\u00e0. t\u014dng gu\u00f2 l\u00ec y\u00f2ng ku\u00f2 s\u00e0n bi\u00e0n sh\u016b zh\u01d4 (DiTs) ch\u01d4 l\u01d0 k\u011b bi\u00e0n sh\u00f9 li\u00e0ng de bi\u0101o j\u00ec de l\u00edng hu\u00f3 x\u00ecng, w\u01d2 men y\u01d0n r\u00f9 le RAS, y\u012b zh\u01d2ng x\u012bn de w\u00fa x\u016b x\u00f9n li\u00e0n de c\u01cei y\u00e0ng c\u00e8 l\u00fc\u00e8, g\u0113n j\u00f9 DiT m\u00f3 x\u00edng de gu\u0101n zh\u00f9 di\u01cen d\u00f2ng t\u00e0i f\u0113n p\u00e8i t\u00fa xi\u00e0ng n\u00e8i b\u00f9 t\u014dng q\u016b y\u00f9 de c\u01cei y\u00e0ng b\u01d0 l\u01dc. w\u01d2 men de gu\u01cen ji\u00e0n gu\u0101n ch\u00e1 sh\u00ec, z\u00e0i m\u011bi g\u00e8 c\u01cei y\u00e0ng b\u00f9 zh\u00f2u zh\u014dng, m\u00f3 x\u00edng j\u00ed zh\u014dng z\u00e0i y\u01d4 y\u00ec sh\u00e0ng y\u01d2u y\u00ec y\u00ec de q\u016b y\u00f9, zh\u00e8 xi\u0113 gu\u0101n zh\u00f9 q\u016b y\u00f9 z\u00e0i li\u00e1n x\u00f9 b\u00f9 zh\u00f2u zh\u014dng bi\u01ceo xi\u00e0n ch\u016b qi\u00e1ng d\u00e0 de li\u00e1n x\u00f9 x\u00ecng. l\u00ec y\u00f2ng zh\u00e8 y\u012b d\u00f2ng ch\u00e1, RAS j\u01d0n g\u0113ng x\u012bn sh\u01d0 d\u0101ng qi\u00e1n gu\u0101n zh\u00f9 de q\u016b y\u00f9, \u00e9r q\u00ed t\u0101 q\u016b y\u00f9 sh\u01d0 y\u00f2ng sh\u00e0ng y\u012b b\u00f9 de hu\u01cen c\u00f9n z\u00e0o sh\u0113ng g\u0113ng x\u012bn. m\u00f3 x\u00edng de gu\u0101n zh\u00f9 di\u01cen g\u0113n j\u00f9 qi\u00e1n y\u012b b\u00f9 de sh\u016b ch\u016b qu\u00e8 d\u00ecng, l\u00ec y\u00f2ng le w\u01d2 men gu\u0101n ch\u00e1 d\u00e0o de sh\u00ed ji\u0101n y\u012b zh\u00ec x\u00ecng. w\u01d2 men z\u00e0i Stable Diffusion 3 h\u00e9 Lumina-Next-T2I sh\u00e0ng p\u00edng gu\u01ce RAS, f\u0113n bi\u00e9 sh\u00ed xi\u00e0n le zu\u00ec g\u0101o 2.36 b\u00e8i h\u00e9 2.51 b\u00e8i de ji\u0101 s\u00f9, sh\u0113ng ch\u00e9ng zh\u00ec li\u00e0ng j\u01d0n q\u012bng w\u0113i xi\u00e0 ji\u00e0ng. c\u01d0 w\u00e0i, y\u00f2ng h\u00f9 y\u00e1n ji\u016b bi\u01ceo m\u00edng, RAS z\u00e0i r\u00e9n l\u00e8i p\u00edng ji\u00e0 xi\u00e0 t\u00ed g\u014dng le xi\u0101ng s\u00ec de zh\u00ec li\u00e0ng, t\u00f3ng sh\u00ed sh\u00ed xi\u00e0n le 1.6 b\u00e8i de ji\u0101 s\u00f9. w\u01d2 men de f\u0101ng f\u01ce z\u00e0i g\u00e8ng g\u0101o xi\u00e0o de ku\u00f2 s\u00e0n bi\u00e0n sh\u016b zh\u01d4 f\u0101ng mi\u00e0n zh\u01d4 d\u00e9 d\u00e0o le zh\u00f2ng y\u00e0o j\u00ecn zh\u01cen, z\u0113ng qi\u00e1ng le t\u0101 men z\u00e0i sh\u00ed sh\u00ed y\u00ecng y\u00f2ng zh\u014dng de qi\u00e1n l\u00ec."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 461, "total_tokens": 1886, "completion_tokens": 1425}}
[17.02.2025 09:13] Response: æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚

kuÃ² sÃ n mÃ³ xÃ­ng (DMs) zÃ i gÃ¨ zhÇ’ng lÇng yÃ¹ de shÄ“ng chÃ©ng rÃ¨n wÃ¹ zhÅng chÃ©ng wÃ©i shÇ’u xuÇn. rÃ¡n Ã©r, tÄ men yÄ« lÃ i duÅ cÃ¬ shÃ¹n xÃ¹ qiÃ¡n xiÄng chuÃ¡n dÃ¬, xiÇn zhÃ¹ xiÃ n zhÃ¬ le shÃ­ shÃ­ xÃ¬ng nÃ©ng. yÇ qiÃ¡n de jiÄ sÃ¹ fÄng fÇ zhÇ” yÃ o jÄ« zhÅng zÃ i jiÇn shÇo cÇi yÃ ng bÃ¹ zhÃ²u huÃ² chÃ³ng yÃ²ng zhÅng jiÄn jiÃ© guÇ’, wÃ¨i nÃ©ng lÃ¬ yÃ²ng tÃº xiÃ ng nÃ¨i bÃ¹ kÅng jiÄn qÅ« yÃ¹ de biÃ n huÃ . tÅng guÃ² lÃ¬ yÃ²ng kuÃ² sÃ n biÃ n shÅ« zhÇ” (DiTs) chÇ” lÇ kÄ› biÃ n shÃ¹ liÃ ng de biÄo jÃ¬ de lÃ­ng huÃ³ xÃ¬ng, wÇ’ men yÇn rÃ¹ le RAS, yÄ« zhÇ’ng xÄ«n de wÃº xÅ« xÃ¹n liÃ n de cÇi yÃ ng cÃ¨ lÃ¼Ã¨, gÄ“n jÃ¹ DiT mÃ³ xÃ­ng de guÄn zhÃ¹ diÇn dÃ²ng tÃ i fÄ“n pÃ¨i tÃº xiÃ ng nÃ¨i bÃ¹ tÅng qÅ« yÃ¹ de cÇi yÃ ng bÇ lÇœ. wÇ’ men de guÇn jiÃ n guÄn chÃ¡ shÃ¬, zÃ i mÄ›i gÃ¨ cÇi yÃ ng bÃ¹ zhÃ²u zhÅng, mÃ³ xÃ­ng jÃ­ zhÅng zÃ i yÇ” yÃ¬ shÃ ng yÇ’u yÃ¬ yÃ¬ de qÅ« yÃ¹, zhÃ¨ xiÄ“ guÄn zhÃ¹ qÅ« yÃ¹ zÃ i liÃ¡n xÃ¹ bÃ¹ zhÃ²u zhÅng biÇo xiÃ n chÅ« qiÃ¡ng dÃ  de liÃ¡n xÃ¹ xÃ¬ng. lÃ¬ yÃ²ng zhÃ¨ yÄ« dÃ²ng chÃ¡, RAS jÇn gÄ“ng xÄ«n shÇ dÄng qiÃ¡n guÄn zhÃ¹ de qÅ« yÃ¹, Ã©r qÃ­ tÄ qÅ« yÃ¹ shÇ yÃ²ng shÃ ng yÄ« bÃ¹ de huÇn cÃ¹n zÃ o shÄ“ng gÄ“ng xÄ«n. mÃ³ xÃ­ng de guÄn zhÃ¹ diÇn gÄ“n jÃ¹ qiÃ¡n yÄ« bÃ¹ de shÅ« chÅ« quÃ¨ dÃ¬ng, lÃ¬ yÃ²ng le wÇ’ men guÄn chÃ¡ dÃ o de shÃ­ jiÄn yÄ« zhÃ¬ xÃ¬ng. wÇ’ men zÃ i Stable Diffusion 3 hÃ© Lumina-Next-T2I shÃ ng pÃ­ng guÇ RAS, fÄ“n biÃ© shÃ­ xiÃ n le zuÃ¬ gÄo 2.36 bÃ¨i hÃ© 2.51 bÃ¨i de jiÄ sÃ¹, shÄ“ng chÃ©ng zhÃ¬ liÃ ng jÇn qÄ«ng wÄ“i xiÃ  jiÃ ng. cÇ wÃ i, yÃ²ng hÃ¹ yÃ¡n jiÅ« biÇo mÃ­ng, RAS zÃ i rÃ©n lÃ¨i pÃ­ng jiÃ  xiÃ  tÃ­ gÅng le xiÄng sÃ¬ de zhÃ¬ liÃ ng, tÃ³ng shÃ­ shÃ­ xiÃ n le 1.6 bÃ¨i de jiÄ sÃ¹. wÇ’ men de fÄng fÇ zÃ i gÃ¨ng gÄo xiÃ o de kuÃ² sÃ n biÃ n shÅ« zhÇ” fÄng miÃ n zhÇ” dÃ© dÃ o le zhÃ²ng yÃ o jÃ¬n zhÇn, zÄ“ng qiÃ¡ng le tÄ men zÃ i shÃ­ shÃ­ yÃ¬ng yÃ²ng zhÅng de qiÃ¡n lÃ¬.
[17.02.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
[17.02.2025 09:13] Mistral response. {"id": "a65a28ecfc50480a9d4ef7a5a1411e57", "object": "chat.completion", "created": 1739783582, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u6269\u6563\u6a21\u578b\", \"pinyin\": \"ku\u00f2 s\u00e0n m\u00f3 x\u00edng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"\u9996\u9009\", \"pinyin\": \"sh\u01d2u xu\u01cen\", \"trans\": \"preferred choice\"},\n    {\"word\": \"\u4f9d\u8d56\", \"pinyin\": \"y\u012b l\u00e0i\", \"trans\": \"depend on\"},\n    {\"word\": \"\u987a\u5e8f\", \"pinyin\": \"sh\u00f9n x\u00f9\", \"trans\": \"sequential\"},\n    {\"word\": \"\u524d\u5411\u4f20\u9012\", \"pinyin\": \"qi\u00e1n xi\u00e0ng chu\u00e1n d\u00ec\", \"trans\": \"forward pass\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u9650\u5236\", \"pinyin\": \"xi\u00e0n zh\u00ec\", \"trans\": \"limit\"},\n    {\"word\": \"\u5b9e\u65f6\u6027\u80fd\", \"pinyin\": \"sh\u00ed sh\u00ed x\u00ecng n\u00e9ng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"\u52a0\u901f\", \"pinyin\": \"ji\u0101 s\u00f9\", \"trans\": \"accelerate\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ng f\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u96c6\u4e2d\", \"pinyin\": \"j\u00ed zh\u014dng\", \"trans\": \"focus on\"},\n    {\"word\": \"\u51cf\u5c11\", \"pinyin\": \"ji\u01cen sh\u01ceo\", \"trans\": \"reduce\"},\n    {\"word\": \"\u91c7\u6837\u6b65\u9aa4\", \"pinyin\": \"c\u01cei y\u00e0ng b\u00f9 zh\u00f2u\", \"trans\": \"sampling steps\"},\n    {\"word\": \"\u91cd\u7528\", \"pinyin\": \"ch\u00f3ng y\u00f2ng\", \"trans\": \"reuse\"},\n    {\"word\": \"\u4e2d\u95f4\u7ed3\u679c\", \"pinyin\": \"zh\u014dng ji\u0101n ji\u00e9 gu\u01d2\", \"trans\": \"intermediate results\"},\n    {\"word\": \"\u5229\u7528\", \"pinyin\": \"l\u00ec y\u00f2ng\", \"trans\": \"utilize\"},\n    {\"word\": \"\u56fe\u50cf\", \"pinyin\": \"t\u00fa xi\u00e0ng\", \"trans\": \"image\"},\n    {\"word\": \"\u5185\u90e8\u7a7a\u95f4\u533a\u57df\", \"pinyin\": \"n\u00e8i b\u00f9 k\u014dng ji\u0101n q\u016b y\u00f9\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"\u53d8\u5316\", \"pinyin\": \"bi\u00e0n hu\u00e0\", \"trans\": \"change\"},\n    {\"word\": \"\u6269\u6563\u53d8\u538b\u5668\", \"pinyin\": \"ku\u00f2 s\u00e0n bi\u00e0n y\u0101 q\u00ec\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"\u7075\u6d3b\u6027\", \"pinyin\": \"l\u00edng hu\u00f3 x\u00ecng\", \"trans\": \"flexibility\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"\u91c7\u6837\u7b56\u7565\", \"pinyin\": \"c\u01cei y\u00e0ng c\u00e8 l\u00fc\u00e8\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"\u52a8\u6001\u5206\u914d\", \"pinyin\": \"d\u00f2ng t\u00e0i f\u0113n p\u00e8i\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"\u5173\u6ce8\u70b9\", \"pinyin\": \"gu\u0101n zh\u00f9 di\u01cen\", \"trans\": \"focus points\"},\n    {\"word\": \"\u5173\u952e\u89c2\u5bdf\", \"pinyin\": \"gu\u01cen ji\u00e0n gu\u0101n ch\u00e1\", \"trans\": \"key observation\"},\n    {\"word\": \"\u8bed\u4e49\", \"pinyin\": \"y\u01d4 y\u00ec\", \"trans\": \"semantic\"},\n    {\"word\": \"\u6709\u610f\u4e49\", \"pinyin\": \"y\u01d2u y\u00ec y\u00ec\", \"trans\": \"meaningful\"},\n    {\"word\": \"\u8fde\u7eed\u6b65\u9aa4\", \"pinyin\": \"li\u00e1n x\u00f9 b\u00f9 zh\u00f2u\", \"trans\": \"continuous steps\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u8fde\u7eed\u6027\", \"pinyin\": \"li\u00e1n x\u00f9 x\u00ecng\", \"trans\": \"continuity\"},\n    {\"word\": \"\u6d1e\u5bdf\", \"pinyin\": \"d\u00f2ng ch\u00e1\", \"trans\": \"insight\"},\n    {\"word\": \"\u66f4\u65b0\", \"pinyin\": \"g\u0113ng x\u012bn\", \"trans\": \"update\"},\n    {\"word\": \"\u7f13\u5b58\u566a\u58f0\", \"pinyin\": \"hu\u01cen c\u00fan z\u00e0o sh\u0113ng\", \"trans\": \"cached noise\"},\n    {\"word\": \"\u786e\u5b9a\", \"pinyin\": \"qu\u00e8 d\u00ecng\", \"trans\": \"determine\"},\n    {\"word\": \"\u65f6\u95f4\u4e00\u81f4\u6027\", \"pinyin\": \"sh\u00ed ji\u0101n y\u012b zh\u00ec x\u00ecng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u52a0\u901f\", \"pinyin\": \"ji\u0101 s\u00f9\", \"trans\": \"acceleration\"},\n    {\"word\": \"\u751f\u6210\u8d28\u91cf\", \"pinyin\": \"sh\u0113ng ch\u00e9ng zh\u00ec li\u00e0ng\", \"trans\": \"generation quality\"},\n    {\"word\": \"\u8f7b\u5fae\u4e0b\u964d\", \"pinyin\": \"q\u012bng w\u0113i xi\u00e0 ji\u00e0ng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"\u7528\u6237\u7814\u7a76\", \"pinyin\": \"y\u00f2ng h\u00f9 y\u00e1n ji\u016b\", \"trans\": \"user study\"},\n    {\"word\": \"\u4eba\u7c7b\u8bc4\u4f30\", \"pinyin\": \"r\u00e9n l\u00e8i p\u00edng g\u016b\", \"trans\": \"human evaluation\"},\n    {\"word\": \"\u76f8\u4f3c\", \"pinyin\": \"xi\u0101ng s\u00ec\", \"trans\": \"similar\"},\n    {\"word\": \"\u6f5c\u529b\", \"pinyin\": \"qi\u00e1n l\u00ec\", \"trans\": \"potential\"},\n    {\"word\": \"\u91cd\u8981\u8fdb\u5c55\", \"pinyin\": \"zh\u00f2ng y\u00e0o j\u00ecn zh\u01cen\", \"trans\": \"significant progress\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 493, "total_tokens": 2101, "completion_tokens": 1608}}
[17.02.2025 09:13] Response: [
    {"word": "æ‰©æ•£æ¨¡å‹", "pinyin": "kuÃ² sÃ n mÃ³ xÃ­ng", "trans": "diffusion model"},
    {"word": "é¦–é€‰", "pinyin": "shÇ’u xuÇn", "trans": "preferred choice"},
    {"word": "ä¾èµ–", "pinyin": "yÄ« lÃ i", "trans": "depend on"},
    {"word": "é¡ºåº", "pinyin": "shÃ¹n xÃ¹", "trans": "sequential"},
    {"word": "å‰å‘ä¼ é€’", "pinyin": "qiÃ¡n xiÃ ng chuÃ¡n dÃ¬", "trans": "forward pass"},
    {"word": "æ˜¾è‘—", "pinyin": "xiÇn zhÃ¹", "trans": "significant"},
    {"word": "é™åˆ¶", "pinyin": "xiÃ n zhÃ¬", "trans": "limit"},
    {"word": "å®æ—¶æ€§èƒ½", "pinyin": "shÃ­ shÃ­ xÃ¬ng nÃ©ng", "trans": "real-time performance"},
    {"word": "åŠ é€Ÿ", "pinyin": "jiÄ sÃ¹", "trans": "accelerate"},
    {"word": "æ–¹æ³•", "pinyin": "fÄng fÇ", "trans": "method"},
    {"word": "é›†ä¸­", "pinyin": "jÃ­ zhÅng", "trans": "focus on"},
    {"word": "å‡å°‘", "pinyin": "jiÇn shÇo", "trans": "reduce"},
    {"word": "é‡‡æ ·æ­¥éª¤", "pinyin": "cÇi yÃ ng bÃ¹ zhÃ²u", "trans": "sampling steps"},
    {"word": "é‡ç”¨", "pinyin": "chÃ³ng yÃ²ng", "trans": "reuse"},
    {"word": "ä¸­é—´ç»“æœ", "pinyin": "zhÅng jiÄn jiÃ© guÇ’", "trans": "intermediate results"},
    {"word": "åˆ©ç”¨", "pinyin": "lÃ¬ yÃ²ng", "trans": "utilize"},
    {"word": "å›¾åƒ", "pinyin": "tÃº xiÃ ng", "trans": "image"},
    {"word": "å†…éƒ¨ç©ºé—´åŒºåŸŸ", "pinyin": "nÃ¨i bÃ¹ kÅng jiÄn qÅ« yÃ¹", "trans": "internal spatial regions"},
    {"word": "å˜åŒ–", "pinyin": "biÃ n huÃ ", "trans": "change"},
    {"word": "æ‰©æ•£å˜å‹å™¨", "pinyin": "kuÃ² sÃ n biÃ n yÄ qÃ¬", "trans": "diffusion transformer"},
    {"word": "çµæ´»æ€§", "pinyin": "lÃ­ng huÃ³ xÃ¬ng", "trans": "flexibility"},
    {"word": "å¼•å…¥", "pinyin": "yÇn rÃ¹", "trans": "introduce"},
    {"word": "RAS", "pinyin": "RAS", "trans": "RAS"},
    {"word": "é‡‡æ ·ç­–ç•¥", "pinyin": "cÇi yÃ ng cÃ¨ lÃ¼Ã¨", "trans": "sampling strategy"},
    {"word": "åŠ¨æ€åˆ†é…", "pinyin": "dÃ²ng tÃ i fÄ“n pÃ¨i", "trans": "dynamic allocation"},
    {"word": "å…³æ³¨ç‚¹", "pinyin": "guÄn zhÃ¹ diÇn", "trans": "focus points"},
    {"word": "å…³é”®è§‚å¯Ÿ", "pinyin": "guÇn jiÃ n guÄn chÃ¡", "trans": "key observation"},
    {"word": "è¯­ä¹‰", "pinyin": "yÇ” yÃ¬", "trans": "semantic"},
    {"word": "æœ‰æ„ä¹‰", "pinyin": "yÇ’u yÃ¬ yÃ¬", "trans": "meaningful"},
    {"word": "è¿ç»­æ­¥éª¤", "pinyin": "liÃ¡n xÃ¹ bÃ¹ zhÃ²u", "trans": "continuous steps"},
    {"word": "è¡¨ç°", "pinyin": "biÇo xiÃ n", "trans": "performance"},
    {"word": "è¿ç»­æ€§", "pinyin": "liÃ¡n xÃ¹ xÃ¬ng", "trans": "continuity"},
    {"word": "æ´å¯Ÿ", "pinyin": "dÃ²ng chÃ¡", "trans": "insight"},
    {"word": "æ›´æ–°", "pinyin": "gÄ“ng xÄ«n", "trans": "update"},
    {"word": "ç¼“å­˜å™ªå£°", "pinyin": "huÇn cÃºn zÃ o shÄ“ng", "trans": "cached noise"},
    {"word": "ç¡®å®š", "pinyin": "quÃ¨ dÃ¬ng", "trans": "determine"},
    {"word": "æ—¶é—´ä¸€è‡´æ€§", "pinyin": "shÃ­ jiÄn yÄ« zhÃ¬ xÃ¬ng", "trans": "temporal consistency"},
    {"word": "è¯„ä¼°", "pinyin": "pÃ­ng gÅ«", "trans": "evaluate"},
    {"word": "Stable Diffusion 3", "pinyin": "Stable Diffusion 3", "trans": "Stable Diffusion 3"},
    {"word": "Lumina-Next-T2I", "pinyin": "Lumina-Next-T2I", "trans": "Lumina-Next-T2I"},
    {"word": "å®ç°", "pinyin": "shÃ­ xiÃ n", "trans": "achieve"},
    {"word": "åŠ é€Ÿ", "pinyin": "jiÄ sÃ¹", "trans": "acceleration"},
    {"word": "ç”Ÿæˆè´¨é‡", "pinyin": "shÄ“ng chÃ©ng zhÃ¬ liÃ ng", "trans": "generation quality"},
    {"word": "è½»å¾®ä¸‹é™", "pinyin": "qÄ«ng wÄ“i xiÃ  jiÃ ng", "trans": "slight decrease"},
    {"word": "ç”¨æˆ·ç ”ç©¶", "pinyin": "yÃ²ng hÃ¹ yÃ¡n jiÅ«", "trans": "user study"},
    {"word": "äººç±»è¯„ä¼°", "pinyin": "rÃ©n lÃ¨i pÃ­ng gÅ«", "trans": "human evaluation"},
    {"word": "ç›¸ä¼¼", "pinyin": "xiÄng sÃ¬", "trans": "similar"},
    {"word": "æ½œåŠ›", "pinyin": "qiÃ¡n lÃ¬", "trans": "potential"},
    {"word": "é‡è¦è¿›å±•", "pinyin": "zhÃ²ng yÃ o jÃ¬n zhÇn", "trans": "significant progress"}
]
[17.02.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å„ç§é¢†åŸŸçš„ç”Ÿæˆä»»åŠ¡ä¸­æˆä¸ºé¦–é€‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–å¤šæ¬¡é¡ºåºå‰å‘ä¼ é€’ï¼Œæ˜¾è‘—é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚ä»¥å‰çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æˆ–é‡ç”¨ä¸­é—´ç»“æœï¼Œæœªèƒ½åˆ©ç”¨å›¾åƒå†…éƒ¨ç©ºé—´åŒºåŸŸçš„å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰å¤„ç†å¯å˜æ•°é‡çš„æ ‡è®°çš„çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RASï¼Œä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®DiTæ¨¡å‹çš„å…³æ³¨ç‚¹åŠ¨æ€åˆ†é…å›¾åƒå†…ä¸åŒåŒºåŸŸçš„é‡‡æ ·æ¯”ç‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæ¨¡å‹é›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œè¿™äº›å…³æ³¨åŒºåŸŸåœ¨è¿ç»­æ­¥éª¤ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç»­æ€§ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒRASä»…æ›´æ–°å½“å‰å…³æ³¨çš„åŒºåŸŸï¼Œè€Œå…¶ä»–åŒºåŸŸä½¿ç”¨ä¸Šä¸€æ­¥çš„ç¼“å­˜å™ªå£°æ›´æ–°ã€‚æ¨¡å‹çš„å…³æ³¨ç‚¹æ ¹æ®å‰ä¸€æ­¥çš„è¾“å‡ºç¡®å®šï¼Œåˆ©ç”¨äº†æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨Stable Diffusion 3å’ŒLumina-Next-T2Iä¸Šè¯„ä¼°RASï¼Œåˆ†åˆ«å®ç°äº†æœ€é«˜2.36å€å’Œ2.51å€çš„åŠ é€Ÿï¼Œç”Ÿæˆè´¨é‡ä»…è½»å¾®ä¸‹é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒRASåœ¨äººç±»è¯„ä¼°ä¸‹æä¾›äº†ç›¸ä¼¼çš„è´¨é‡ï¼ŒåŒæ—¶å®ç°äº†1.6å€çš„åŠ é€Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ›´é«˜æ•ˆçš„æ‰©æ•£å˜å‹å™¨æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œå¢å¼ºäº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚
[17.02.2025 09:13] Mistral response. {"id": "7747d507e314491aaf87c1c5ece9cd98", "object": "chat.completion", "created": 1739783612, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 459, "total_tokens": 771, "completion_tokens": 312}}
[17.02.2025 09:13] Response: Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.
[17.02.2025 09:13] Renaming data file.
[17.02.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-02-17.json
[17.02.2025 09:13] Saving new data file.
[17.02.2025 09:13] Generating page.
[17.02.2025 09:13] Renaming previous page.
[17.02.2025 09:13] Renaming previous data. index.html to ./d/2025-02-17.html
[17.02.2025 09:13] [Experimental] Generating Chinese page for reading.
[17.02.2025 09:13] Chinese vocab [{'word': 'æ‰©æ•£æ¨¡å‹', 'pinyin': 'kuÃ² sÃ n mÃ³ xÃ­ng', 'trans': 'diffusion model'}, {'word': 'é¦–é€‰', 'pinyin': 'shÇ’u xuÇn', 'trans': 'preferred choice'}, {'word': 'ä¾èµ–', 'pinyin': 'yÄ« lÃ i', 'trans': 'depend on'}, {'word': 'é¡ºåº', 'pinyin': 'shÃ¹n xÃ¹', 'trans': 'sequential'}, {'word': 'å‰å‘ä¼ é€’', 'pinyin': 'qiÃ¡n xiÃ ng chuÃ¡n dÃ¬', 'trans': 'forward pass'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'é™åˆ¶', 'pinyin': 'xiÃ n zhÃ¬', 'trans': 'limit'}, {'word': 'å®æ—¶æ€§èƒ½', 'pinyin': 'shÃ­ shÃ­ xÃ¬ng nÃ©ng', 'trans': 'real-time performance'}, {'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'accelerate'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'é›†ä¸­', 'pinyin': 'jÃ­ zhÅng', 'trans': 'focus on'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'}, {'word': 'é‡‡æ ·æ­¥éª¤', 'pinyin': 'cÇi yÃ ng bÃ¹ zhÃ²u', 'trans': 'sampling steps'}, {'word': 'é‡ç”¨', 'pinyin': 'chÃ³ng yÃ²ng', 'trans': 'reuse'}, {'word': 'ä¸­é—´ç»“æœ', 'pinyin': 'zhÅng jiÄn jiÃ© guÇ’', 'trans': 'intermediate results'}, {'word': 'åˆ©ç”¨', 'pinyin': 'lÃ¬ yÃ²ng', 'trans': 'utilize'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃº xiÃ ng', 'trans': 'image'}, {'word': 'å†…éƒ¨ç©ºé—´åŒºåŸŸ', 'pinyin': 'nÃ¨i bÃ¹ kÅng jiÄn qÅ« yÃ¹', 'trans': 'internal spatial regions'}, {'word': 'å˜åŒ–', 'pinyin': 'biÃ n huÃ ', 'trans': 'change'}, {'word': 'æ‰©æ•£å˜å‹å™¨', 'pinyin': 'kuÃ² sÃ n biÃ n yÄ qÃ¬', 'trans': 'diffusion transformer'}, {'word': 'çµæ´»æ€§', 'pinyin': 'lÃ­ng huÃ³ xÃ¬ng', 'trans': 'flexibility'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇn rÃ¹', 'trans': 'introduce'}, {'word': 'RAS', 'pinyin': 'RAS', 'trans': 'RAS'}, {'word': 'é‡‡æ ·ç­–ç•¥', 'pinyin': 'cÇi yÃ ng cÃ¨ lÃ¼Ã¨', 'trans': 'sampling strategy'}, {'word': 'åŠ¨æ€åˆ†é…', 'pinyin': 'dÃ²ng tÃ i fÄ“n pÃ¨i', 'trans': 'dynamic allocation'}, {'word': 'å…³æ³¨ç‚¹', 'pinyin': 'guÄn zhÃ¹ diÇn', 'trans': 'focus points'}, {'word': 'å…³é”®è§‚å¯Ÿ', 'pinyin': 'guÇn jiÃ n guÄn chÃ¡', 'trans': 'key observation'}, {'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantic'}, {'word': 'æœ‰æ„ä¹‰', 'pinyin': 'yÇ’u yÃ¬ yÃ¬', 'trans': 'meaningful'}, {'word': 'è¿ç»­æ­¥éª¤', 'pinyin': 'liÃ¡n xÃ¹ bÃ¹ zhÃ²u', 'trans': 'continuous steps'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'è¿ç»­æ€§', 'pinyin': 'liÃ¡n xÃ¹ xÃ¬ng', 'trans': 'continuity'}, {'word': 'æ´å¯Ÿ', 'pinyin': 'dÃ²ng chÃ¡', 'trans': 'insight'}, {'word': 'æ›´æ–°', 'pinyin': 'gÄ“ng xÄ«n', 'trans': 'update'}, {'word': 'ç¼“å­˜å™ªå£°', 'pinyin': 'huÇn cÃºn zÃ o shÄ“ng', 'trans': 'cached noise'}, {'word': 'ç¡®å®š', 'pinyin': 'quÃ¨ dÃ¬ng', 'trans': 'determine'}, {'word': 'æ—¶é—´ä¸€è‡´æ€§', 'pinyin': 'shÃ­ jiÄn yÄ« zhÃ¬ xÃ¬ng', 'trans': 'temporal consistency'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'}, {'word': 'Stable Diffusion 3', 'pinyin': 'Stable Diffusion 3', 'trans': 'Stable Diffusion 3'}, {'word': 'Lumina-Next-T2I', 'pinyin': 'Lumina-Next-T2I', 'trans': 'Lumina-Next-T2I'}, {'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'}, {'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'acceleration'}, {'word': 'ç”Ÿæˆè´¨é‡', 'pinyin': 'shÄ“ng chÃ©ng zhÃ¬ liÃ ng', 'trans': 'generation quality'}, {'word': 'è½»å¾®ä¸‹é™', 'pinyin': 'qÄ«ng wÄ“i xiÃ  jiÃ ng', 'trans': 'slight decrease'}, {'word': 'ç”¨æˆ·ç ”ç©¶', 'pinyin': 'yÃ²ng hÃ¹ yÃ¡n jiÅ«', 'trans': 'user study'}, {'word': 'äººç±»è¯„ä¼°', 'pinyin': 'rÃ©n lÃ¨i pÃ­ng gÅ«', 'trans': 'human evaluation'}, {'word': 'ç›¸ä¼¼', 'pinyin': 'xiÄng sÃ¬', 'trans': 'similar'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡n lÃ¬', 'trans': 'potential'}, {'word': 'é‡è¦è¿›å±•', 'pinyin': 'zhÃ²ng yÃ o jÃ¬n zhÇn', 'trans': 'significant progress'}]
[17.02.2025 09:13] Renaming previous Chinese page.
[17.02.2025 09:13] Renaming previous data. zh.html to ./d/2025-02-16_zh_reading_task.html
[17.02.2025 09:13] Writing Chinese reading task.
[17.02.2025 09:13] Writing result.
[17.02.2025 09:13] Renaming log file.
[17.02.2025 09:13] Renaming previous data. log.txt to ./logs/2025-02-17_last_log.txt
