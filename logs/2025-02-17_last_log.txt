[17.02.2025 10:12] Read previous papers.
[17.02.2025 10:12] Generating top page (month).
[17.02.2025 10:12] Writing top page (month).
[17.02.2025 11:09] Read previous papers.
[17.02.2025 11:09] Get feed.
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10389
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10248
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09696
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09992
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10391
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09955
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09935
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07586
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09741
[17.02.2025 11:09] Extract page data from URL. URL: https://huggingface.co/papers/2502.10235
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10177
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07856
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09638
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09980
[17.02.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10173
[17.02.2025 11:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.02.2025 11:09] No deleted papers detected.
[17.02.2025 11:09] Downloading and parsing papers (pdf, html). Total: 15.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.10389.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.10389.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.10389.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.10248.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.10248.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.10248.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.09696.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.09696.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.09696.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.09992.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.09992.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.09992.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.10391.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.10391.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.10391.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.09955.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.09955.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.09955.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.09935.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.09935.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.09935.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.07586.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.07586.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.07586.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.09741.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.09741.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.09741.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.10235.
[17.02.2025 11:09] Downloading paper 2502.10235 from http://arxiv.org/pdf/2502.10235v1...
[17.02.2025 11:09] Extracting affiliations from text.
[17.02.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting Abdelhakim Benechehab 1 2 Vasilii Feofanov 1 Giuseppe Paolo 1 Albert Thomas 1 Maurizio Filippone 3 Balazs Kegl 1 5 2 0 2 4 1 ] . s [ 1 5 3 2 0 1 . 2 0 5 2 : r a "
[17.02.2025 11:09] Response: ```python
[]
```
[17.02.2025 11:09] Extracting affiliations from text.
[17.02.2025 11:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting Abdelhakim Benechehab 1 2 Vasilii Feofanov 1 Giuseppe Paolo 1 Albert Thomas 1 Maurizio Filippone 3 Balazs Kegl 1 5 2 0 2 4 1 ] . s [ 1 5 3 2 0 1 . 2 0 5 2 : r aPre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adaptersfeature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS. 1. Introduction Time series forecasting is well-established machine learning problem that involves analyzing sequential data to predict future trends based on historical patterns. Two key challenges frequently arise in this context: (a) time series are often multivariate, incorporating multiple descriptive 1Huawei Noahs Ark Lab, Paris, France 2Department of Data Science, EURECOM 3Statistics Program, KAUST. Abdelhakim Benechehab <abdelCorrespondence hakim.benechehab@gmail.com>. to: Preprint, under review. Copyright 2025 by the author(s). 1 (a) (b) Figure 1: (a) Augmenting Moment time series foundation model with the AdaPTS framework provides probabilistic and more accurate predictions. (b) The AdaPTS framework: The input time series is transformed through feature space transformation φ that maps into stochastic latent space. The prediction is then conducted using pre-trained FM before transforming back the predicted, now distribution, to the original feature space. The fire symbol indicate trainable weights while the snowflake implicates that the parameters of the FM are kept frozen. features (Wei, 2019), and (b) estimating the uncertainty of forecast is equally important, requiring probabilistic model outputs (Gneiting & Katzfuss, 2014). These challenges are particularly relevant in real-world applications where risk assessment depends on reliable forecasts, such as healthcare (Jones & Spiegelhalter, 2012), finance (Groen et al., 2013), energy management (Zhang et al., 2014; Nowotarski & Weron, 2018), and weather prediction (Palmer, 2012; Bi et al., 2023). Existing foundation models (FMs) for time series forecasting, such as Chronos (Ansari et al., 2024), are typically trained for univariate forecasting tasks due to tractability constraints, as the wide range of real world time series problems typically have different numbers of features. Even AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting without discretization, handling multivariate time series directly within these models (Moment (Goswami et al., 2024), Moirai (Liu et al., 2024)) remains computationally challenging due to the high-dimensional dependencies among features. This limitation raises fundamental question: how can we leverage existing pre-trained univariate FMs to enable probabilistic forecasting for multivariate time series? To address this, we introduce AdaPTS, novel framework designed to augment FMs with probabilistic adapters. As illustrated in Figure 1, AdaPTS applies stochastic feature transformation that maps the input time series into latent space, where predictions are made using the frozen FM. Our framework sets itself apart from existing literature by enforcing an invertibility constraint on the adapter, allowing predictions to be transformed back into the original feature space. Beyond enhancing forecasting accuracy, the integration of stochasticity into the adapters latent representation ensures that the model captures uncertainty, thereby improving both calibration and robustness. Our approach leads to several novel insights and contributions, which we summarize as follows: 1. Multivariate FM adaptation. We introduce principled methodology for adapting existing pre-trained univariate FMs to multivariate probabilistic forecasting, resulting in the AdaPTS framework. 2. Theoretical foundations of adapters. We provide theoretical analysis to support the necessity of adapters, starting with the analytically tractable case of linear adapters and linear FMs. We then build on the literature on partially stochastic Bayesian neural networks to introduce probabilistic adapters. 3. Empirical validation. We conduct extensive experiments on multivariate time series forecasting benchmarks, demonstrating that our approach improves forecasting accuracy baseline methods. We also analyze the interpretability of the learned latent representation and show that adapters enable cost-effective adaptation by reducing the dimensionality of the feature space. The rest of this paper is organized as follows: Section 2 discusses related work, Section 3 details the problem setup and the theoretical analysis on linear adapters. Section 4 extends our framework to probabilistic adapters, and Section 5 showcases experimental results. Finally, we conclude with limitations and future directions in Section 6. 2. Related work Time Series Foundational Models. Over the past two years, plethora of foundation models have been proposed with particular focus on time series forecasting. Some of these models like GPT4TS (Tian et al., 2023) and Time-LLM (Jin et al., 2024) reprogram Large Language Model to the forecasting setting by freezing most of its layers and fine-tuning additional time series-specific modules to new downstream task. The majority of these time series FMs including Lag-Llama (Rasul et al., 2024), Chronos (Ansari et al., 2024), Moirai (Liu et al., 2024), TimesFM (Das et al., 2024"
[17.02.2025 11:09] Mistral response. {"id": "664f9d8ee98e4889803bd61531c27a13", "object": "chat.completion", "created": 1739790577, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Huawei Noahs Ark Lab, Paris, France\", \"Department of Data Science, EURECOM\", \"Statistics Program, KAUST\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1633, "total_tokens": 1671, "completion_tokens": 38}}
[17.02.2025 11:09] Response: ```python
["Huawei Noahs Ark Lab, Paris, France", "Department of Data Science, EURECOM", "Statistics Program, KAUST"]
```
[17.02.2025 11:09] Deleting PDF ./assets/pdf/2502.10235.pdf.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.10177.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.10177.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.10177.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.07856.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.07856.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.07856.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.09638.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.09638.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.09638.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.09980.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.09980.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.09980.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2502.10173.
[17.02.2025 11:09] Extra JSON file exists (./assets/json/2502.10173.json), skip PDF parsing.
[17.02.2025 11:09] Paper image links file exists (./assets/img_data/2502.10173.json), skip HTML parsing.
[17.02.2025 11:09] Success.
[17.02.2025 11:09] Enriching papers with extra data.
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 0. Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps o...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 1. We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal comp...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 2. Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model pro...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 3. Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward da...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 4. Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), w...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 5. Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) ...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 6. Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content wi...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 7. This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start f...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 8. Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks....
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 9. Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these crit...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 10. A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning a...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 11. In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of ...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 12. Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or oth...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 13. Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V)...
[17.02.2025 11:09] ********************************************************************************
[17.02.2025 11:09] Abstract 14. Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerat...
[17.02.2025 11:09] Read previous papers.
[17.02.2025 11:09] Generating reviews via LLM API.
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#diffusion", "#architecture", "#dataset"], "emoji": "🚀", "ru": {"title": "Ускорение диффузионных трансформеров без потери качества", "desc": "Статья представляет новый метод ускорения диффузионных моделей под названием RAS. Этот метод осно
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#training", "#open_source", "#diffusion", "#video", "#architecture"], "emoji": "🎬", "ru": {"title": "Революция в генерации видео: от текста к реалистичным роликам", "desc": "Представлена модель Step-Video-T2V - предобученная нейросеть для генерации вид
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#interpretability", "#reasoning"], "emoji": "🧠", "ru": {"title": "ZeroBench: невозможный визуальный тест для мультимодальных моделей", "desc": "В статье представлен новый визуальный бенчмарк ZeroBench, который полностью невозможно решить для современных мультимо
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#benchmark", "#diffusion", "#training"], "emoji": "🌊", "ru": {"title": "Диффузионные модели бросают вызов авторегрессии в обработке языка", "desc": "Статья представляет LLaDA - новую диффузионную модель для обработки естественного языка, альтернатив
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#benchmark", "#alignment", "#training", "#interpretability", "#open_source", "#rlhf", "#dataset"], "emoji": "🤖", "ru": {"title": "Новый подход к согласованию мультимодальных ИИ-моделей с человеческими предпочтениями", "desc": "Исследователи представили MM-RLHF - набор данных из 120 
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#agents", "#rl", "#training", "#inference"], "emoji": "🧠", "ru": {"title": "Усиление LLM для решения сложных задач: многомодельный подход", "desc": "Статья описывает подход к улучшению производительности языковых моделей (LLM) в решении сложны
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#training", "#synthetic", "#architecture"], "emoji": "🔍", "ru": {"title": "Локализация текста в диффузионных моделях для повышения эффективности генерации", "desc": "Статья представляет новый подход к улучшению генерации текста в диффузионных моделях для создани
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#interpretability"], "emoji": "🗣️", "ru": {"title": "Неологизмы как мост между человеком и ИИ", "desc": "В статье утверждается, что для понимания искусственного интеллекта недостаточно существующего человеческого словаря. Авторы предлагают разрабатывать 
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#data", "#optimization"], "emoji": "🔢", "ru": {"title": "FoNE: революция в обработке чисел для языковых моделей", "desc": "Исследователи предложили новый метод кодирования чисел для больших языковых моделей, названный Fourier Number Embedding (FoNE). Fo
[17.02.2025 11:09] Querying the API.
[17.02.2025 11:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.
[17.02.2025 11:09] Response: {
  "desc": "Исследование представляет AdaPTS - фреймворк, использующий адаптеры для применения предобученных моделей-основ (foundation models) в задачах многомерного прогнозирования временных рядов. Адаптеры проецируют многомерные входные данные в латентное пространство, позволяя эффективно использовать одномерные модели для многомерных задач. Эксперименты на синтетических и реальных данных показали значительное улучшение точности прогнозирования и количественной оценки неопределенности по сравнению с базовыми методами. Фреймворк AdaPTS представляет собой модульное и масштабируемое решение для использования моделей временных рядов в многомерных контекстах.",

  "emoji": "📊",

  "title": "AdaPTS: Адаптация одномерных моделей для многомерного прогнозирования временных рядов"
}
[17.02.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS."

[17.02.2025 11:09] Response: ```python
['DATASET', 'DATA', 'INFERENCE', 'TRAINING']
```
[17.02.2025 11:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS."

[17.02.2025 11:09] Response: ```python
["OPTIMIZATION", "SYNTHETIC"]
```
[17.02.2025 11:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new approach called adapters to improve the use of pre-trained foundation models (FMs) for multivariate time series forecasting. Adapters transform multivariate inputs into a latent space, allowing the FM to process each feature independently while managing complex dependencies. The study also explores various optimization and inference strategies inspired by representation learning and Bayesian neural networks. Experiments show that this method significantly enhances forecasting accuracy and uncertainty quantification, making it a valuable tool for real-world applications.","title":"Enhancing Multivariate Time Series Forecasting with Adapters"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new approach called adapters to improve the use of pre-trained foundation models (FMs) for multivariate time series forecasting. Adapters transform multivariate inputs into a latent space, allowing the FM to process each feature independently while managing complex dependencies. The study also explores various optimization and inference strategies inspired by representation learning and Bayesian neural networks. Experiments show that this method significantly enhances forecasting accuracy and uncertainty quantification, making it a valuable tool for real-world applications.', title='Enhancing Multivariate Time Series Forecasting with Adapters'))
[17.02.2025 11:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"预训练基础模型在单变量时间序列预测任务中表现出色，但在处理特征间复杂依赖关系和量化预测不确定性方面仍面临挑战。本研究通过引入适配器来解决这些关键限制，适配器是一种特征空间转换，能够有效利用预训练的单变量时间序列模型进行多变量任务。适配器通过将多变量输入投影到合适的潜在空间，并独立地对每个维度应用基础模型，从而实现功能。实验结果表明，适配器在预测准确性和不确定性量化方面显著优于基线方法，展示了其在多变量时间序列应用中的有效性。","title":"适配器：多变量时间序列预测的新解决方案"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='预训练基础模型在单变量时间序列预测任务中表现出色，但在处理特征间复杂依赖关系和量化预测不确定性方面仍面临挑战。本研究通过引入适配器来解决这些关键限制，适配器是一种特征空间转换，能够有效利用预训练的单变量时间序列模型进行多变量任务。适配器通过将多变量输入投影到合适的潜在空间，并独立地对每个维度应用基础模型，从而实现功能。实验结果表明，适配器在预测准确性和不确定性量化方面显著优于基线方法，展示了其在多变量时间序列应用中的有效性。', title='适配器：多变量时间序列预测的新解决方案'))
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#games", "#agents", "#reasoning"], "emoji": "🧠", "ru": {"title": "Пространственно-временная память повышает эффективность воплощенных агентов", "desc": "Статья представляет новый фреймворк под названием Spatio-Temporal Memory Agent (STMA) для улучшения планирования и выполнения зада
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#training", "#cv", "#data", "#diffusion", "#optimization"], "emoji": "🚀", "ru": {"title": "Ускорение управляемой генерации изображений без потери качества", "desc": "Статья представляет новый алгоритм MRS (MR Sampler) для ускорения процесса семплирования в Mean Reverting (MR) Diffus
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#ethics", "#multimodal", "#training", "#security", "#rlhf"], "emoji": "🕵️", "ru": {"title": "LLM против LLM: новый фронт в безопасности ИИ", "desc": "Статья представляет новый подход к тестированию безопасности больших языковых моделей (LLM), использующий сами LLM в качестве 'красно
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#reasoning", "#science", "#agi", "#games", "#optimization", "#dataset", "#agents"], "emoji": "🚗", "ru": {"title": "LLM на службе кооперативного автономного вождения", "desc": "Статья представляет новый подход к кооперативному автономному вождению с исп
[17.02.2025 11:09] Using data from previous issue: {"categories": ["#architecture", "#agents", "#dataset"], "emoji": "🧬", "ru": {"title": "VibeGen: ИИ-дизайн белков с заданной динамикой", "desc": "VibeGen - это генеративная ИИ-система для проектирования белков с заданными динамическими свойствами. Она использует двойную архитектуру с моделью-дизайне
[17.02.2025 11:09] Loading Chinese text from previous data.
[17.02.2025 11:09] Renaming data file.
[17.02.2025 11:09] Renaming previous data. hf_papers.json to ./d/2025-02-17.json
[17.02.2025 11:09] Saving new data file.
[17.02.2025 11:09] Generating page.
[17.02.2025 11:09] Renaming previous page.
[17.02.2025 11:09] Renaming previous data. index.html to ./d/2025-02-17.html
[17.02.2025 11:09] [Experimental] Generating Chinese page for reading.
[17.02.2025 11:09] Chinese vocab [{'word': '扩散模型', 'pinyin': 'kuò sàn mó xíng', 'trans': 'diffusion model'}, {'word': '首选', 'pinyin': 'shǒu xuǎn', 'trans': 'preferred choice'}, {'word': '依赖', 'pinyin': 'yī lài', 'trans': 'depend on'}, {'word': '顺序', 'pinyin': 'shùn xù', 'trans': 'sequential'}, {'word': '前向传递', 'pinyin': 'qián xiàng chuán dì', 'trans': 'forward pass'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '限制', 'pinyin': 'xiàn zhì', 'trans': 'limit'}, {'word': '实时性能', 'pinyin': 'shí shí xìng néng', 'trans': 'real-time performance'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '集中', 'pinyin': 'jí zhōng', 'trans': 'focus on'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '采样步骤', 'pinyin': 'cǎi yàng bù zhòu', 'trans': 'sampling steps'}, {'word': '重用', 'pinyin': 'chóng yòng', 'trans': 'reuse'}, {'word': '中间结果', 'pinyin': 'zhōng jiān jié guǒ', 'trans': 'intermediate results'}, {'word': '利用', 'pinyin': 'lì yòng', 'trans': 'utilize'}, {'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '内部空间区域', 'pinyin': 'nèi bù kōng jiān qū yù', 'trans': 'internal spatial regions'}, {'word': '变化', 'pinyin': 'biàn huà', 'trans': 'change'}, {'word': '扩散变压器', 'pinyin': 'kuò sàn biàn yā qì', 'trans': 'diffusion transformer'}, {'word': '灵活性', 'pinyin': 'líng huó xìng', 'trans': 'flexibility'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': 'RAS', 'pinyin': 'RAS', 'trans': 'RAS'}, {'word': '采样策略', 'pinyin': 'cǎi yàng cè lüè', 'trans': 'sampling strategy'}, {'word': '动态分配', 'pinyin': 'dòng tài fēn pèi', 'trans': 'dynamic allocation'}, {'word': '关注点', 'pinyin': 'guān zhù diǎn', 'trans': 'focus points'}, {'word': '关键观察', 'pinyin': 'guǎn jiàn guān chá', 'trans': 'key observation'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '有意义', 'pinyin': 'yǒu yì yì', 'trans': 'meaningful'}, {'word': '连续步骤', 'pinyin': 'lián xù bù zhòu', 'trans': 'continuous steps'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '连续性', 'pinyin': 'lián xù xìng', 'trans': 'continuity'}, {'word': '洞察', 'pinyin': 'dòng chá', 'trans': 'insight'}, {'word': '更新', 'pinyin': 'gēng xīn', 'trans': 'update'}, {'word': '缓存噪声', 'pinyin': 'huǎn cún zào shēng', 'trans': 'cached noise'}, {'word': '确定', 'pinyin': 'què dìng', 'trans': 'determine'}, {'word': '时间一致性', 'pinyin': 'shí jiān yī zhì xìng', 'trans': 'temporal consistency'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': 'Stable Diffusion 3', 'pinyin': 'Stable Diffusion 3', 'trans': 'Stable Diffusion 3'}, {'word': 'Lumina-Next-T2I', 'pinyin': 'Lumina-Next-T2I', 'trans': 'Lumina-Next-T2I'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'acceleration'}, {'word': '生成质量', 'pinyin': 'shēng chéng zhì liàng', 'trans': 'generation quality'}, {'word': '轻微下降', 'pinyin': 'qīng wēi xià jiàng', 'trans': 'slight decrease'}, {'word': '用户研究', 'pinyin': 'yòng hù yán jiū', 'trans': 'user study'}, {'word': '人类评估', 'pinyin': 'rén lèi píng gū', 'trans': 'human evaluation'}, {'word': '相似', 'pinyin': 'xiāng sì', 'trans': 'similar'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '重要进展', 'pinyin': 'zhòng yào jìn zhǎn', 'trans': 'significant progress'}]
[17.02.2025 11:09] Renaming previous Chinese page.
[17.02.2025 11:09] Renaming previous data. zh.html to ./d/2025-02-16_zh_reading_task.html
[17.02.2025 11:09] Writing Chinese reading task.
[17.02.2025 11:09] Writing result.
[17.02.2025 11:09] Renaming log file.
[17.02.2025 11:09] Renaming previous data. log.txt to ./logs/2025-02-17_last_log.txt
