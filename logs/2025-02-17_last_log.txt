[17.02.2025 08:15] Read previous papers.
[17.02.2025 08:15] Generating top page (month).
[17.02.2025 08:15] Writing top page (month).
[17.02.2025 09:12] Read previous papers.
[17.02.2025 09:12] Get feed.
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10389
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10248
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09696
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09992
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10391
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09955
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09741
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09935
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10177
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07856
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09638
[17.02.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09980
[17.02.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.02.2025 09:12] No deleted papers detected.
[17.02.2025 09:12] Downloading and parsing papers (pdf, html). Total: 12.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10389.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10389.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10389.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10248.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10248.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10248.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09696.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09696.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09696.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09992.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09992.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09992.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10391.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10391.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10391.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09955.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09955.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09955.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09741.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09741.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09741.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09935.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09935.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09935.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.10177.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.10177.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.10177.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.07856.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.07856.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.07856.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09638.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09638.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09638.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2502.09980.
[17.02.2025 09:12] Extra JSON file exists (./assets/json/2502.09980.json), skip PDF parsing.
[17.02.2025 09:12] Paper image links file exists (./assets/img_data/2502.09980.json), skip HTML parsing.
[17.02.2025 09:12] Success.
[17.02.2025 09:12] Enriching papers with extra data.
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 0. Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps o...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 1. We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal comp...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 2. Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model pro...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 3. Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward da...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 4. Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), w...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 5. Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) ...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 6. Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks....
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 7. Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content wi...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 8. A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning a...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 9. In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of ...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 10. Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or oth...
[17.02.2025 09:12] ********************************************************************************
[17.02.2025 09:12] Abstract 11. Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V)...
[17.02.2025 09:12] Read previous papers.
[17.02.2025 09:12] Generating reviews via LLM API.
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#diffusion", "#architecture", "#dataset"], "emoji": "🚀", "ru": {"title": "Ускорение диффузионных трансформеров без потери качества", "desc": "Статья представляет новый метод ускорения диффузионных моделей под названием RAS. Этот метод осно
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#training", "#open_source", "#diffusion", "#video", "#architecture"], "emoji": "🎬", "ru": {"title": "Революция в генерации видео: от текста к реалистичным роликам", "desc": "Представлена модель Step-Video-T2V - предобученная нейросеть для генерации вид
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#interpretability", "#reasoning"], "emoji": "🧠", "ru": {"title": "ZeroBench: невозможный визуальный тест для мультимодальных моделей", "desc": "В статье представлен новый визуальный бенчмарк ZeroBench, который полностью невозможно решить для современных мультимо
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#benchmark", "#diffusion", "#training"], "emoji": "🌊", "ru": {"title": "Диффузионные модели бросают вызов авторегрессии в обработке языка", "desc": "Статья представляет LLaDA - новую диффузионную модель для обработки естественного языка, альтернатив
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#alignment", "#training", "#interpretability", "#open_source", "#rlhf", "#dataset"], "emoji": "🤖", "ru": {"title": "Новый подход к согласованию мультимодальных ИИ-моделей с человеческими предпочтениями", "desc": "Исследователи представили MM-RLHF - набор данных из 120 
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#agents", "#rl", "#training", "#inference"], "emoji": "🧠", "ru": {"title": "Усиление LLM для решения сложных задач: многомодельный подход", "desc": "Статья описывает подход к улучшению производительности языковых моделей (LLM) в решении сложны
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#data", "#optimization"], "emoji": "🔢", "ru": {"title": "FoNE: революция в обработке чисел для языковых моделей", "desc": "Исследователи предложили новый метод кодирования чисел для больших языковых моделей, названный Fourier Number Embedding (FoNE). Fo
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#training", "#synthetic", "#architecture"], "emoji": "🔍", "ru": {"title": "Локализация текста в диффузионных моделях для повышения эффективности генерации", "desc": "Статья представляет новый подход к улучшению генерации текста в диффузионных моделях для создани
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#games", "#agents", "#reasoning"], "emoji": "🧠", "ru": {"title": "Пространственно-временная память повышает эффективность воплощенных агентов", "desc": "Статья представляет новый фреймворк под названием Spatio-Temporal Memory Agent (STMA) для улучшения планирования и выполнения зада
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#training", "#cv", "#data", "#diffusion", "#optimization"], "emoji": "🚀", "ru": {"title": "Ускорение управляемой генерации изображений без потери качества", "desc": "Статья представляет новый алгоритм MRS (MR Sampler) для ускорения процесса семплирования в Mean Reverting (MR) Diffus
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#ethics", "#multimodal", "#training", "#security", "#rlhf"], "emoji": "🕵️", "ru": {"title": "LLM против LLM: новый фронт в безопасности ИИ", "desc": "Статья представляет новый подход к тестированию безопасности больших языковых моделей (LLM), использующий сами LLM в качестве 'красно
[17.02.2025 09:12] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#reasoning", "#science", "#agi", "#games", "#optimization", "#dataset", "#agents"], "emoji": "🚗", "ru": {"title": "LLM на службе кооперативного автономного вождения", "desc": "Статья представляет новый подход к кооперативному автономному вождению с исп
[17.02.2025 09:12] Trying to get texts in Chinese.
[17.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.
[17.02.2025 09:12] Mistral response. {"id": "7d474964ad7a4ada9d51d71f259e5527", "object": "chat.completion", "created": 1739783545, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u5404\u79cd\u9886\u57df\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u6210\u4e3a\u9996\u9009\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4f9d\u8d56\u591a\u6b21\u987a\u5e8f\u524d\u5411\u4f20\u9012\uff0c\u663e\u8457\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\u3002\u4ee5\u524d\u7684\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u6216\u91cd\u7528\u4e2d\u95f4\u7ed3\u679c\uff0c\u672a\u80fd\u5229\u7528\u56fe\u50cf\u5185\u90e8\u7a7a\u95f4\u533a\u57df\u7684\u53d8\u5316\u3002\u901a\u8fc7\u5229\u7528\u6269\u6563\u53d8\u538b\u5668\uff08DiTs\uff09\u5904\u7406\u53ef\u53d8\u6570\u91cf\u7684\u6807\u8bb0\u7684\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86RAS\uff0c\u4e00\u79cd\u65b0\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u7b56\u7565\uff0c\u6839\u636eDiT\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u52a8\u6001\u5206\u914d\u56fe\u50cf\u5185\u4e0d\u540c\u533a\u57df\u7684\u91c7\u6837\u6bd4\u7387\u3002\u6211\u4eec\u7684\u5173\u952e\u89c2\u5bdf\u662f\uff0c\u5728\u6bcf\u4e2a\u91c7\u6837\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u96c6\u4e2d\u5728\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u5173\u6ce8\u533a\u57df\u5728\u8fde\u7eed\u6b65\u9aa4\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fde\u7eed\u6027\u3002\u5229\u7528\u8fd9\u4e00\u6d1e\u5bdf\uff0cRAS\u4ec5\u66f4\u65b0\u5f53\u524d\u5173\u6ce8\u7684\u533a\u57df\uff0c\u800c\u5176\u4ed6\u533a\u57df\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7f13\u5b58\u566a\u58f0\u66f4\u65b0\u3002\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u6839\u636e\u524d\u4e00\u6b65\u7684\u8f93\u51fa\u786e\u5b9a\uff0c\u5229\u7528\u4e86\u6211\u4eec\u89c2\u5bdf\u5230\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5728Stable Diffusion 3\u548cLumina-Next-T2I\u4e0a\u8bc4\u4f30RAS\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad82.36\u500d\u548c2.51\u500d\u7684\u52a0\u901f\uff0c\u751f\u6210\u8d28\u91cf\u4ec5\u8f7b\u5fae\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cRAS\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e0b\u63d0\u4f9b\u4e86\u76f8\u4f3c\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e861.6\u500d\u7684\u52a0\u901f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u66f4\u9ad8\u6548\u7684\u6269\u6563\u53d8\u538b\u5668\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u589e\u5f3a\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 362, "total_tokens": 808, "completion_tokens": 446}}
[17.02.2025 09:12] Response: 扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。
[17.02.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。
[17.02.2025 09:13] Mistral response. {"id": "057f2ebfe09c43808c5e972a2ca3fc1f", "object": "chat.completion", "created": 1739783553, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u5404\u79cd\u9886\u57df\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u6210\u4e3a\u9996\u9009\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4f9d\u8d56\u591a\u6b21\u987a\u5e8f\u524d\u5411\u4f20\u9012\uff0c\u663e\u8457\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\u3002\u4ee5\u524d\u7684\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u6216\u91cd\u7528\u4e2d\u95f4\u7ed3\u679c\uff0c\u672a\u80fd\u5229\u7528\u56fe\u50cf\u5185\u90e8\u7a7a\u95f4\u533a\u57df\u7684\u53d8\u5316\u3002\u901a\u8fc7\u5229\u7528\u6269\u6563\u53d8\u538b\u5668\uff08DiTs\uff09\u5904\u7406\u53ef\u53d8\u6570\u91cf\u7684\u6807\u8bb0\u7684\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86RAS\uff0c\u4e00\u79cd\u65b0\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u7b56\u7565\uff0c\u6839\u636eDiT\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u52a8\u6001\u5206\u914d\u56fe\u50cf\u5185\u4e0d\u540c\u533a\u57df\u7684\u91c7\u6837\u6bd4\u7387\u3002\u6211\u4eec\u7684\u5173\u952e\u89c2\u5bdf\u662f\uff0c\u5728\u6bcf\u4e2a\u91c7\u6837\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u96c6\u4e2d\u5728\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u533a\u57df\uff0c\u8fd9\u4e9b\u5173\u6ce8\u533a\u57df\u5728\u8fde\u7eed\u6b65\u9aa4\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8fde\u7eed\u6027\u3002\u5229\u7528\u8fd9\u4e00\u6d1e\u5bdf\uff0cRAS\u4ec5\u66f4\u65b0\u5f53\u524d\u5173\u6ce8\u7684\u533a\u57df\uff0c\u800c\u5176\u4ed6\u533a\u57df\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7f13\u5b58\u566a\u58f0\u66f4\u65b0\u3002\u6a21\u578b\u7684\u5173\u6ce8\u70b9\u6839\u636e\u524d\u4e00\u6b65\u7684\u8f93\u51fa\u786e\u5b9a\uff0c\u5229\u7528\u4e86\u6211\u4eec\u89c2\u5bdf\u5230\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5728Stable Diffusion 3\u548cLumina-Next-T2I\u4e0a\u8bc4\u4f30RAS\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad82.36\u500d\u548c2.51\u500d\u7684\u52a0\u901f\uff0c\u751f\u6210\u8d28\u91cf\u4ec5\u8f7b\u5fae\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cRAS\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e0b\u63d0\u4f9b\u4e86\u76f8\u4f3c\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e861.6\u500d\u7684\u52a0\u901f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u66f4\u9ad8\u6548\u7684\u6269\u6563\u53d8\u538b\u5668\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u589e\u5f3a\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002\n\nku\u00f2 s\u00e0n m\u00f3 x\u00edng (DMs) z\u00e0i g\u00e8 zh\u01d2ng l\u01d0ng y\u00f9 de sh\u0113ng ch\u00e9ng r\u00e8n w\u00f9 zh\u014dng ch\u00e9ng w\u00e9i sh\u01d2u xu\u01cen. r\u00e1n \u00e9r, t\u0101 men y\u012b l\u00e0i du\u014d c\u00ec sh\u00f9n x\u00f9 qi\u00e1n xi\u0101ng chu\u00e1n d\u00ec, xi\u01cen zh\u00f9 xi\u00e0n zh\u00ec le sh\u00ed sh\u00ed x\u00ecng n\u00e9ng. y\u01d0 qi\u00e1n de ji\u0101 s\u00f9 f\u0101ng f\u01ce zh\u01d4 y\u00e0o j\u012b zh\u014dng z\u00e0i ji\u01cen sh\u01ceo c\u01cei y\u00e0ng b\u00f9 zh\u00f2u hu\u00f2 ch\u00f3ng y\u00f2ng zh\u014dng ji\u0101n ji\u00e9 gu\u01d2, w\u00e8i n\u00e9ng l\u00ec y\u00f2ng t\u00fa xi\u00e0ng n\u00e8i b\u00f9 k\u014dng ji\u0101n q\u016b y\u00f9 de bi\u00e0n hu\u00e0. t\u014dng gu\u00f2 l\u00ec y\u00f2ng ku\u00f2 s\u00e0n bi\u00e0n sh\u016b zh\u01d4 (DiTs) ch\u01d4 l\u01d0 k\u011b bi\u00e0n sh\u00f9 li\u00e0ng de bi\u0101o j\u00ec de l\u00edng hu\u00f3 x\u00ecng, w\u01d2 men y\u01d0n r\u00f9 le RAS, y\u012b zh\u01d2ng x\u012bn de w\u00fa x\u016b x\u00f9n li\u00e0n de c\u01cei y\u00e0ng c\u00e8 l\u00fc\u00e8, g\u0113n j\u00f9 DiT m\u00f3 x\u00edng de gu\u0101n zh\u00f9 di\u01cen d\u00f2ng t\u00e0i f\u0113n p\u00e8i t\u00fa xi\u00e0ng n\u00e8i b\u00f9 t\u014dng q\u016b y\u00f9 de c\u01cei y\u00e0ng b\u01d0 l\u01dc. w\u01d2 men de gu\u01cen ji\u00e0n gu\u0101n ch\u00e1 sh\u00ec, z\u00e0i m\u011bi g\u00e8 c\u01cei y\u00e0ng b\u00f9 zh\u00f2u zh\u014dng, m\u00f3 x\u00edng j\u00ed zh\u014dng z\u00e0i y\u01d4 y\u00ec sh\u00e0ng y\u01d2u y\u00ec y\u00ec de q\u016b y\u00f9, zh\u00e8 xi\u0113 gu\u0101n zh\u00f9 q\u016b y\u00f9 z\u00e0i li\u00e1n x\u00f9 b\u00f9 zh\u00f2u zh\u014dng bi\u01ceo xi\u00e0n ch\u016b qi\u00e1ng d\u00e0 de li\u00e1n x\u00f9 x\u00ecng. l\u00ec y\u00f2ng zh\u00e8 y\u012b d\u00f2ng ch\u00e1, RAS j\u01d0n g\u0113ng x\u012bn sh\u01d0 d\u0101ng qi\u00e1n gu\u0101n zh\u00f9 de q\u016b y\u00f9, \u00e9r q\u00ed t\u0101 q\u016b y\u00f9 sh\u01d0 y\u00f2ng sh\u00e0ng y\u012b b\u00f9 de hu\u01cen c\u00f9n z\u00e0o sh\u0113ng g\u0113ng x\u012bn. m\u00f3 x\u00edng de gu\u0101n zh\u00f9 di\u01cen g\u0113n j\u00f9 qi\u00e1n y\u012b b\u00f9 de sh\u016b ch\u016b qu\u00e8 d\u00ecng, l\u00ec y\u00f2ng le w\u01d2 men gu\u0101n ch\u00e1 d\u00e0o de sh\u00ed ji\u0101n y\u012b zh\u00ec x\u00ecng. w\u01d2 men z\u00e0i Stable Diffusion 3 h\u00e9 Lumina-Next-T2I sh\u00e0ng p\u00edng gu\u01ce RAS, f\u0113n bi\u00e9 sh\u00ed xi\u00e0n le zu\u00ec g\u0101o 2.36 b\u00e8i h\u00e9 2.51 b\u00e8i de ji\u0101 s\u00f9, sh\u0113ng ch\u00e9ng zh\u00ec li\u00e0ng j\u01d0n q\u012bng w\u0113i xi\u00e0 ji\u00e0ng. c\u01d0 w\u00e0i, y\u00f2ng h\u00f9 y\u00e1n ji\u016b bi\u01ceo m\u00edng, RAS z\u00e0i r\u00e9n l\u00e8i p\u00edng ji\u00e0 xi\u00e0 t\u00ed g\u014dng le xi\u0101ng s\u00ec de zh\u00ec li\u00e0ng, t\u00f3ng sh\u00ed sh\u00ed xi\u00e0n le 1.6 b\u00e8i de ji\u0101 s\u00f9. w\u01d2 men de f\u0101ng f\u01ce z\u00e0i g\u00e8ng g\u0101o xi\u00e0o de ku\u00f2 s\u00e0n bi\u00e0n sh\u016b zh\u01d4 f\u0101ng mi\u00e0n zh\u01d4 d\u00e9 d\u00e0o le zh\u00f2ng y\u00e0o j\u00ecn zh\u01cen, z\u0113ng qi\u00e1ng le t\u0101 men z\u00e0i sh\u00ed sh\u00ed y\u00ecng y\u00f2ng zh\u014dng de qi\u00e1n l\u00ec."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 461, "total_tokens": 1886, "completion_tokens": 1425}}
[17.02.2025 09:13] Response: 扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。

kuò sàn mó xíng (DMs) zài gè zhǒng lǐng yù de shēng chéng rèn wù zhōng chéng wéi shǒu xuǎn. rán ér, tā men yī lài duō cì shùn xù qián xiāng chuán dì, xiǎn zhù xiàn zhì le shí shí xìng néng. yǐ qián de jiā sù fāng fǎ zhǔ yào jī zhōng zài jiǎn shǎo cǎi yàng bù zhòu huò chóng yòng zhōng jiān jié guǒ, wèi néng lì yòng tú xiàng nèi bù kōng jiān qū yù de biàn huà. tōng guò lì yòng kuò sàn biàn shū zhǔ (DiTs) chǔ lǐ kě biàn shù liàng de biāo jì de líng huó xìng, wǒ men yǐn rù le RAS, yī zhǒng xīn de wú xū xùn liàn de cǎi yàng cè lüè, gēn jù DiT mó xíng de guān zhù diǎn dòng tài fēn pèi tú xiàng nèi bù tōng qū yù de cǎi yàng bǐ lǜ. wǒ men de guǎn jiàn guān chá shì, zài měi gè cǎi yàng bù zhòu zhōng, mó xíng jí zhōng zài yǔ yì shàng yǒu yì yì de qū yù, zhè xiē guān zhù qū yù zài lián xù bù zhòu zhōng biǎo xiàn chū qiáng dà de lián xù xìng. lì yòng zhè yī dòng chá, RAS jǐn gēng xīn shǐ dāng qián guān zhù de qū yù, ér qí tā qū yù shǐ yòng shàng yī bù de huǎn cùn zào shēng gēng xīn. mó xíng de guān zhù diǎn gēn jù qián yī bù de shū chū què dìng, lì yòng le wǒ men guān chá dào de shí jiān yī zhì xìng. wǒ men zài Stable Diffusion 3 hé Lumina-Next-T2I shàng píng guǎ RAS, fēn bié shí xiàn le zuì gāo 2.36 bèi hé 2.51 bèi de jiā sù, shēng chéng zhì liàng jǐn qīng wēi xià jiàng. cǐ wài, yòng hù yán jiū biǎo míng, RAS zài rén lèi píng jià xià tí gōng le xiāng sì de zhì liàng, tóng shí shí xiàn le 1.6 bèi de jiā sù. wǒ men de fāng fǎ zài gèng gāo xiào de kuò sàn biàn shū zhǔ fāng miàn zhǔ dé dào le zhòng yào jìn zhǎn, zēng qiáng le tā men zài shí shí yìng yòng zhōng de qián lì.
[17.02.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。
[17.02.2025 09:13] Mistral response. {"id": "a65a28ecfc50480a9d4ef7a5a1411e57", "object": "chat.completion", "created": 1739783582, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u6269\u6563\u6a21\u578b\", \"pinyin\": \"ku\u00f2 s\u00e0n m\u00f3 x\u00edng\", \"trans\": \"diffusion model\"},\n    {\"word\": \"\u9996\u9009\", \"pinyin\": \"sh\u01d2u xu\u01cen\", \"trans\": \"preferred choice\"},\n    {\"word\": \"\u4f9d\u8d56\", \"pinyin\": \"y\u012b l\u00e0i\", \"trans\": \"depend on\"},\n    {\"word\": \"\u987a\u5e8f\", \"pinyin\": \"sh\u00f9n x\u00f9\", \"trans\": \"sequential\"},\n    {\"word\": \"\u524d\u5411\u4f20\u9012\", \"pinyin\": \"qi\u00e1n xi\u00e0ng chu\u00e1n d\u00ec\", \"trans\": \"forward pass\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u9650\u5236\", \"pinyin\": \"xi\u00e0n zh\u00ec\", \"trans\": \"limit\"},\n    {\"word\": \"\u5b9e\u65f6\u6027\u80fd\", \"pinyin\": \"sh\u00ed sh\u00ed x\u00ecng n\u00e9ng\", \"trans\": \"real-time performance\"},\n    {\"word\": \"\u52a0\u901f\", \"pinyin\": \"ji\u0101 s\u00f9\", \"trans\": \"accelerate\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ng f\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u96c6\u4e2d\", \"pinyin\": \"j\u00ed zh\u014dng\", \"trans\": \"focus on\"},\n    {\"word\": \"\u51cf\u5c11\", \"pinyin\": \"ji\u01cen sh\u01ceo\", \"trans\": \"reduce\"},\n    {\"word\": \"\u91c7\u6837\u6b65\u9aa4\", \"pinyin\": \"c\u01cei y\u00e0ng b\u00f9 zh\u00f2u\", \"trans\": \"sampling steps\"},\n    {\"word\": \"\u91cd\u7528\", \"pinyin\": \"ch\u00f3ng y\u00f2ng\", \"trans\": \"reuse\"},\n    {\"word\": \"\u4e2d\u95f4\u7ed3\u679c\", \"pinyin\": \"zh\u014dng ji\u0101n ji\u00e9 gu\u01d2\", \"trans\": \"intermediate results\"},\n    {\"word\": \"\u5229\u7528\", \"pinyin\": \"l\u00ec y\u00f2ng\", \"trans\": \"utilize\"},\n    {\"word\": \"\u56fe\u50cf\", \"pinyin\": \"t\u00fa xi\u00e0ng\", \"trans\": \"image\"},\n    {\"word\": \"\u5185\u90e8\u7a7a\u95f4\u533a\u57df\", \"pinyin\": \"n\u00e8i b\u00f9 k\u014dng ji\u0101n q\u016b y\u00f9\", \"trans\": \"internal spatial regions\"},\n    {\"word\": \"\u53d8\u5316\", \"pinyin\": \"bi\u00e0n hu\u00e0\", \"trans\": \"change\"},\n    {\"word\": \"\u6269\u6563\u53d8\u538b\u5668\", \"pinyin\": \"ku\u00f2 s\u00e0n bi\u00e0n y\u0101 q\u00ec\", \"trans\": \"diffusion transformer\"},\n    {\"word\": \"\u7075\u6d3b\u6027\", \"pinyin\": \"l\u00edng hu\u00f3 x\u00ecng\", \"trans\": \"flexibility\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"RAS\", \"pinyin\": \"RAS\", \"trans\": \"RAS\"},\n    {\"word\": \"\u91c7\u6837\u7b56\u7565\", \"pinyin\": \"c\u01cei y\u00e0ng c\u00e8 l\u00fc\u00e8\", \"trans\": \"sampling strategy\"},\n    {\"word\": \"\u52a8\u6001\u5206\u914d\", \"pinyin\": \"d\u00f2ng t\u00e0i f\u0113n p\u00e8i\", \"trans\": \"dynamic allocation\"},\n    {\"word\": \"\u5173\u6ce8\u70b9\", \"pinyin\": \"gu\u0101n zh\u00f9 di\u01cen\", \"trans\": \"focus points\"},\n    {\"word\": \"\u5173\u952e\u89c2\u5bdf\", \"pinyin\": \"gu\u01cen ji\u00e0n gu\u0101n ch\u00e1\", \"trans\": \"key observation\"},\n    {\"word\": \"\u8bed\u4e49\", \"pinyin\": \"y\u01d4 y\u00ec\", \"trans\": \"semantic\"},\n    {\"word\": \"\u6709\u610f\u4e49\", \"pinyin\": \"y\u01d2u y\u00ec y\u00ec\", \"trans\": \"meaningful\"},\n    {\"word\": \"\u8fde\u7eed\u6b65\u9aa4\", \"pinyin\": \"li\u00e1n x\u00f9 b\u00f9 zh\u00f2u\", \"trans\": \"continuous steps\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u8fde\u7eed\u6027\", \"pinyin\": \"li\u00e1n x\u00f9 x\u00ecng\", \"trans\": \"continuity\"},\n    {\"word\": \"\u6d1e\u5bdf\", \"pinyin\": \"d\u00f2ng ch\u00e1\", \"trans\": \"insight\"},\n    {\"word\": \"\u66f4\u65b0\", \"pinyin\": \"g\u0113ng x\u012bn\", \"trans\": \"update\"},\n    {\"word\": \"\u7f13\u5b58\u566a\u58f0\", \"pinyin\": \"hu\u01cen c\u00fan z\u00e0o sh\u0113ng\", \"trans\": \"cached noise\"},\n    {\"word\": \"\u786e\u5b9a\", \"pinyin\": \"qu\u00e8 d\u00ecng\", \"trans\": \"determine\"},\n    {\"word\": \"\u65f6\u95f4\u4e00\u81f4\u6027\", \"pinyin\": \"sh\u00ed ji\u0101n y\u012b zh\u00ec x\u00ecng\", \"trans\": \"temporal consistency\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"Stable Diffusion 3\", \"pinyin\": \"Stable Diffusion 3\", \"trans\": \"Stable Diffusion 3\"},\n    {\"word\": \"Lumina-Next-T2I\", \"pinyin\": \"Lumina-Next-T2I\", \"trans\": \"Lumina-Next-T2I\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u52a0\u901f\", \"pinyin\": \"ji\u0101 s\u00f9\", \"trans\": \"acceleration\"},\n    {\"word\": \"\u751f\u6210\u8d28\u91cf\", \"pinyin\": \"sh\u0113ng ch\u00e9ng zh\u00ec li\u00e0ng\", \"trans\": \"generation quality\"},\n    {\"word\": \"\u8f7b\u5fae\u4e0b\u964d\", \"pinyin\": \"q\u012bng w\u0113i xi\u00e0 ji\u00e0ng\", \"trans\": \"slight decrease\"},\n    {\"word\": \"\u7528\u6237\u7814\u7a76\", \"pinyin\": \"y\u00f2ng h\u00f9 y\u00e1n ji\u016b\", \"trans\": \"user study\"},\n    {\"word\": \"\u4eba\u7c7b\u8bc4\u4f30\", \"pinyin\": \"r\u00e9n l\u00e8i p\u00edng g\u016b\", \"trans\": \"human evaluation\"},\n    {\"word\": \"\u76f8\u4f3c\", \"pinyin\": \"xi\u0101ng s\u00ec\", \"trans\": \"similar\"},\n    {\"word\": \"\u6f5c\u529b\", \"pinyin\": \"qi\u00e1n l\u00ec\", \"trans\": \"potential\"},\n    {\"word\": \"\u91cd\u8981\u8fdb\u5c55\", \"pinyin\": \"zh\u00f2ng y\u00e0o j\u00ecn zh\u01cen\", \"trans\": \"significant progress\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 493, "total_tokens": 2101, "completion_tokens": 1608}}
[17.02.2025 09:13] Response: [
    {"word": "扩散模型", "pinyin": "kuò sàn mó xíng", "trans": "diffusion model"},
    {"word": "首选", "pinyin": "shǒu xuǎn", "trans": "preferred choice"},
    {"word": "依赖", "pinyin": "yī lài", "trans": "depend on"},
    {"word": "顺序", "pinyin": "shùn xù", "trans": "sequential"},
    {"word": "前向传递", "pinyin": "qián xiàng chuán dì", "trans": "forward pass"},
    {"word": "显著", "pinyin": "xiǎn zhù", "trans": "significant"},
    {"word": "限制", "pinyin": "xiàn zhì", "trans": "limit"},
    {"word": "实时性能", "pinyin": "shí shí xìng néng", "trans": "real-time performance"},
    {"word": "加速", "pinyin": "jiā sù", "trans": "accelerate"},
    {"word": "方法", "pinyin": "fāng fǎ", "trans": "method"},
    {"word": "集中", "pinyin": "jí zhōng", "trans": "focus on"},
    {"word": "减少", "pinyin": "jiǎn shǎo", "trans": "reduce"},
    {"word": "采样步骤", "pinyin": "cǎi yàng bù zhòu", "trans": "sampling steps"},
    {"word": "重用", "pinyin": "chóng yòng", "trans": "reuse"},
    {"word": "中间结果", "pinyin": "zhōng jiān jié guǒ", "trans": "intermediate results"},
    {"word": "利用", "pinyin": "lì yòng", "trans": "utilize"},
    {"word": "图像", "pinyin": "tú xiàng", "trans": "image"},
    {"word": "内部空间区域", "pinyin": "nèi bù kōng jiān qū yù", "trans": "internal spatial regions"},
    {"word": "变化", "pinyin": "biàn huà", "trans": "change"},
    {"word": "扩散变压器", "pinyin": "kuò sàn biàn yā qì", "trans": "diffusion transformer"},
    {"word": "灵活性", "pinyin": "líng huó xìng", "trans": "flexibility"},
    {"word": "引入", "pinyin": "yǐn rù", "trans": "introduce"},
    {"word": "RAS", "pinyin": "RAS", "trans": "RAS"},
    {"word": "采样策略", "pinyin": "cǎi yàng cè lüè", "trans": "sampling strategy"},
    {"word": "动态分配", "pinyin": "dòng tài fēn pèi", "trans": "dynamic allocation"},
    {"word": "关注点", "pinyin": "guān zhù diǎn", "trans": "focus points"},
    {"word": "关键观察", "pinyin": "guǎn jiàn guān chá", "trans": "key observation"},
    {"word": "语义", "pinyin": "yǔ yì", "trans": "semantic"},
    {"word": "有意义", "pinyin": "yǒu yì yì", "trans": "meaningful"},
    {"word": "连续步骤", "pinyin": "lián xù bù zhòu", "trans": "continuous steps"},
    {"word": "表现", "pinyin": "biǎo xiàn", "trans": "performance"},
    {"word": "连续性", "pinyin": "lián xù xìng", "trans": "continuity"},
    {"word": "洞察", "pinyin": "dòng chá", "trans": "insight"},
    {"word": "更新", "pinyin": "gēng xīn", "trans": "update"},
    {"word": "缓存噪声", "pinyin": "huǎn cún zào shēng", "trans": "cached noise"},
    {"word": "确定", "pinyin": "què dìng", "trans": "determine"},
    {"word": "时间一致性", "pinyin": "shí jiān yī zhì xìng", "trans": "temporal consistency"},
    {"word": "评估", "pinyin": "píng gū", "trans": "evaluate"},
    {"word": "Stable Diffusion 3", "pinyin": "Stable Diffusion 3", "trans": "Stable Diffusion 3"},
    {"word": "Lumina-Next-T2I", "pinyin": "Lumina-Next-T2I", "trans": "Lumina-Next-T2I"},
    {"word": "实现", "pinyin": "shí xiàn", "trans": "achieve"},
    {"word": "加速", "pinyin": "jiā sù", "trans": "acceleration"},
    {"word": "生成质量", "pinyin": "shēng chéng zhì liàng", "trans": "generation quality"},
    {"word": "轻微下降", "pinyin": "qīng wēi xià jiàng", "trans": "slight decrease"},
    {"word": "用户研究", "pinyin": "yòng hù yán jiū", "trans": "user study"},
    {"word": "人类评估", "pinyin": "rén lèi píng gū", "trans": "human evaluation"},
    {"word": "相似", "pinyin": "xiāng sì", "trans": "similar"},
    {"word": "潜力", "pinyin": "qián lì", "trans": "potential"},
    {"word": "重要进展", "pinyin": "zhòng yào jìn zhǎn", "trans": "significant progress"}
]
[17.02.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

扩散模型（DMs）在各种领域的生成任务中成为首选。然而，它们依赖多次顺序前向传递，显著限制了实时性能。以前的加速方法主要集中在减少采样步骤或重用中间结果，未能利用图像内部空间区域的变化。通过利用扩散变压器（DiTs）处理可变数量的标记的灵活性，我们引入了RAS，一种新的无需训练的采样策略，根据DiT模型的关注点动态分配图像内不同区域的采样比率。我们的关键观察是，在每个采样步骤中，模型集中在语义上有意义的区域，这些关注区域在连续步骤中表现出强大的连续性。利用这一洞察，RAS仅更新当前关注的区域，而其他区域使用上一步的缓存噪声更新。模型的关注点根据前一步的输出确定，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估RAS，分别实现了最高2.36倍和2.51倍的加速，生成质量仅轻微下降。此外，用户研究表明，RAS在人类评估下提供了相似的质量，同时实现了1.6倍的加速。我们的方法在更高效的扩散变压器方面取得了重要进展，增强了它们在实时应用中的潜力。
[17.02.2025 09:13] Mistral response. {"id": "7747d507e314491aaf87c1c5ece9cd98", "object": "chat.completion", "created": 1739783612, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 459, "total_tokens": 771, "completion_tokens": 312}}
[17.02.2025 09:13] Response: Diffusion models (DMs) have become the preferred choice for generative tasks in various fields. However, they rely on multiple sequential forward passes, significantly limiting real-time performance. Previous acceleration methods have primarily focused on reducing sampling steps or reusing intermediate results, failing to leverage variations in spatial regions within images. By exploiting the flexibility of diffusion transformers (DiTs) in handling a variable number of tokens, we introduce RAS, a new training-free sampling strategy that dynamically allocates sampling ratios to different regions within an image based on the attention focus of the DiT model. Our key observation is that, at each sampling step, the model concentrates on semantically meaningful regions, and these attention regions exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the currently attended regions, while other regions are updated using cached noise from the previous step. The model's attention focus is determined based on the output from the previous step, utilizing the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving up to 2.36x and 2.51x speedup, respectively, with only a slight decrease in generation quality. Additionally, user studies indicate that RAS provides similar quality under human evaluation while achieving a 1.6x speedup. Our method represents a significant advancement in more efficient diffusion transformers, enhancing their potential for real-time applications.
[17.02.2025 09:13] Renaming data file.
[17.02.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-02-17.json
[17.02.2025 09:13] Saving new data file.
[17.02.2025 09:13] Generating page.
[17.02.2025 09:13] Renaming previous page.
[17.02.2025 09:13] Renaming previous data. index.html to ./d/2025-02-17.html
[17.02.2025 09:13] [Experimental] Generating Chinese page for reading.
[17.02.2025 09:13] Chinese vocab [{'word': '扩散模型', 'pinyin': 'kuò sàn mó xíng', 'trans': 'diffusion model'}, {'word': '首选', 'pinyin': 'shǒu xuǎn', 'trans': 'preferred choice'}, {'word': '依赖', 'pinyin': 'yī lài', 'trans': 'depend on'}, {'word': '顺序', 'pinyin': 'shùn xù', 'trans': 'sequential'}, {'word': '前向传递', 'pinyin': 'qián xiàng chuán dì', 'trans': 'forward pass'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '限制', 'pinyin': 'xiàn zhì', 'trans': 'limit'}, {'word': '实时性能', 'pinyin': 'shí shí xìng néng', 'trans': 'real-time performance'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '集中', 'pinyin': 'jí zhōng', 'trans': 'focus on'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '采样步骤', 'pinyin': 'cǎi yàng bù zhòu', 'trans': 'sampling steps'}, {'word': '重用', 'pinyin': 'chóng yòng', 'trans': 'reuse'}, {'word': '中间结果', 'pinyin': 'zhōng jiān jié guǒ', 'trans': 'intermediate results'}, {'word': '利用', 'pinyin': 'lì yòng', 'trans': 'utilize'}, {'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '内部空间区域', 'pinyin': 'nèi bù kōng jiān qū yù', 'trans': 'internal spatial regions'}, {'word': '变化', 'pinyin': 'biàn huà', 'trans': 'change'}, {'word': '扩散变压器', 'pinyin': 'kuò sàn biàn yā qì', 'trans': 'diffusion transformer'}, {'word': '灵活性', 'pinyin': 'líng huó xìng', 'trans': 'flexibility'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': 'RAS', 'pinyin': 'RAS', 'trans': 'RAS'}, {'word': '采样策略', 'pinyin': 'cǎi yàng cè lüè', 'trans': 'sampling strategy'}, {'word': '动态分配', 'pinyin': 'dòng tài fēn pèi', 'trans': 'dynamic allocation'}, {'word': '关注点', 'pinyin': 'guān zhù diǎn', 'trans': 'focus points'}, {'word': '关键观察', 'pinyin': 'guǎn jiàn guān chá', 'trans': 'key observation'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '有意义', 'pinyin': 'yǒu yì yì', 'trans': 'meaningful'}, {'word': '连续步骤', 'pinyin': 'lián xù bù zhòu', 'trans': 'continuous steps'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '连续性', 'pinyin': 'lián xù xìng', 'trans': 'continuity'}, {'word': '洞察', 'pinyin': 'dòng chá', 'trans': 'insight'}, {'word': '更新', 'pinyin': 'gēng xīn', 'trans': 'update'}, {'word': '缓存噪声', 'pinyin': 'huǎn cún zào shēng', 'trans': 'cached noise'}, {'word': '确定', 'pinyin': 'què dìng', 'trans': 'determine'}, {'word': '时间一致性', 'pinyin': 'shí jiān yī zhì xìng', 'trans': 'temporal consistency'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': 'Stable Diffusion 3', 'pinyin': 'Stable Diffusion 3', 'trans': 'Stable Diffusion 3'}, {'word': 'Lumina-Next-T2I', 'pinyin': 'Lumina-Next-T2I', 'trans': 'Lumina-Next-T2I'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'acceleration'}, {'word': '生成质量', 'pinyin': 'shēng chéng zhì liàng', 'trans': 'generation quality'}, {'word': '轻微下降', 'pinyin': 'qīng wēi xià jiàng', 'trans': 'slight decrease'}, {'word': '用户研究', 'pinyin': 'yòng hù yán jiū', 'trans': 'user study'}, {'word': '人类评估', 'pinyin': 'rén lèi píng gū', 'trans': 'human evaluation'}, {'word': '相似', 'pinyin': 'xiāng sì', 'trans': 'similar'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '重要进展', 'pinyin': 'zhòng yào jìn zhǎn', 'trans': 'significant progress'}]
[17.02.2025 09:13] Renaming previous Chinese page.
[17.02.2025 09:13] Renaming previous data. zh.html to ./d/2025-02-16_zh_reading_task.html
[17.02.2025 09:13] Writing Chinese reading task.
[17.02.2025 09:13] Writing result.
[17.02.2025 09:13] Renaming log file.
[17.02.2025 09:13] Renaming previous data. log.txt to ./logs/2025-02-17_last_log.txt
