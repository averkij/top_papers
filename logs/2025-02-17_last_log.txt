[17.02.2025 13:19] Read previous papers.
[17.02.2025 13:19] Generating top page (month).
[17.02.2025 13:19] Writing top page (month).
[17.02.2025 14:10] Read previous papers.
[17.02.2025 14:10] Get feed.
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10389
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10248
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09992
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09696
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10391
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09935
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09955
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10235
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07586
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09741
[17.02.2025 14:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.09411
[17.02.2025 14:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.10140
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09638
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10177
[17.02.2025 14:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.07780
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.07856
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09980
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08769
[17.02.2025 14:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10173
[17.02.2025 14:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.02.2025 14:10] No deleted papers detected.
[17.02.2025 14:10] Downloading and parsing papers (pdf, html). Total: 19.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.10389.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.10389.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.10389.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.10248.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.10248.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.10248.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09992.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.09992.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.09992.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09696.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.09696.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.09696.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.10391.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.10391.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.10391.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09935.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.09935.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.09935.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09955.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.09955.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.09955.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.10235.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.10235.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.10235.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.07586.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.07586.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.07586.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09741.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.09741.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.09741.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09411.
[17.02.2025 14:10] Downloading paper 2502.09411 from http://arxiv.org/pdf/2502.09411v1...
[17.02.2025 14:10] Extracting affiliations from text.
[17.02.2025 14:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation Rotem Shalev-Arkushin 1 Rinon Gal 1 2 Amit H. Bermano 1 Ohad Fried 3 5 2 0 2 3 1 ] . [ 1 1 1 4 9 0 . 2 0 5 2 : r Figure 1: Using references broadens the generation capabilities of image generation models. Given text prompt, ImageRAG dynamically retrieves relevant images and provides them to base text-to-image model (T2I). ImageRAG works with different models, such as SDXL (A) or OmniGen (B, C), and different controls, e.g. text (A, B) or personalization (C). "
[17.02.2025 14:10] Response: ```python
[]
```
[17.02.2025 14:10] Extracting affiliations from text.
[17.02.2025 14:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation Rotem Shalev-Arkushin 1 Rinon Gal 1 2 Amit H. Bermano 1 Ohad Fried 3 5 2 0 2 3 1 ] . [ 1 1 1 4 9 0 . 2 0 5 2 : r Figure 1: Using references broadens the generation capabilities of image generation models. Given text prompt, ImageRAG dynamically retrieves relevant images and provides them to base text-to-image model (T2I). ImageRAG works with different models, such as SDXL (A) or OmniGen (B, C), and different controls, e.g. text (A, B) or personalization (C).1. Introduction Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of RetrievalAugmented Generation (RAG) with image generation models. We propose ImageRAG, method that dynamically retrieves relevant images based on given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models. Our project page is available at: https:// rotem-shalev.github.io/ImageRAG 1Tel Aviv University 2NVIDIA 3Reichman University. Correspondence roo@gmail.com>. to: Rotem Shalev-Arkushin <rotem1 Diffusion models (Ho et al., 2020) have recently revolutionized image generation, offering high-quality, diverse, and realistic visual content (Dhariwal & Nichol, 2021; Rombach et al., 2022). They enable text-to-image generation as well as wide range of tasks, from layout-based synthesis to image editing and style transfer (Avrahami et al., 2022; Hertz et al., 2022; Mokady et al., 2023; Avrahami et al., 2023b; Zhang et al., 2023; Brooks et al., 2023; Nitzan et al., 2024). Of course, these large models require great amounts of training data, substantial training durations, and extensive computational resources. As result, contemporary text-to-image (T2I) models, that are limited to the data they were trained on, struggle with generating user-specific concepts or updated content. Additionally, they have difficulty with generating rare concepts, stylized content, or fine-grained categories (e.g., specific dog breed), even if they were trained on images containing them (Samuel et al., 2024b; Haviv et al., 2024). In these cases, diffusion models tend to hallucinate, and potentially generate content unrelated to the textual prompt (see Figure 2). To tackle these problems, several approaches have been proposed for personalized (Gal et al., 2023a; Ruiz et al., 2023; Voynov et al., 2023; Arar et al., 2024), stylized (Hu et al., 2021), or rare concept generation (Li et al., 2024; Samuel et al., 2024b). ImageRAG+Reference) S ( a ) i ( h Figure 2: Hallucinations. When models do not know the meaning of prompt, they may hallucinate and generate unrelated images (left). By applying our method to retrieve and utilize relevant references (mid), the base models can generate appropriate images (right). Most approaches, however, require training or specialized optimization techniques for each new concept. We note that similar problems exist with text generation using Large Language Models (LLMs). LLMs struggle to generate text based on real-world facts, proprietary or updated data, and tend to hallucinate when lacking sufficient knowledge (Brown, 2020; Ji et al., 2023). To solve these problems, the Natural Language Processing community has adopted Retrieval Augmented Generation (RAG) (Lewis et al., 2020). RAG is method for dynamically retrieving the most relevant information to given task from external sources and supplying it to an LLM as context input, enabling it to generate contextually accurate and task-specific responses. Investigating this idea for images, we notice previous works employing image retrieval for better image generation (Chen et al., 2022; Sheynin et al., 2022; Blattmann et al., 2022; Hu et al., 2024), are based on models trained specifically for the task, hindering wide applicability. In contrast, we propose ImageRAG, method that dynamically retrieves and provides images as references to pretrained T2I models, to enhance their generation capabilities. Our method does not require additional training over the retrieved content or specifically for RAG. Instead, we use T2I models in the same vein as the common use of LLMs, and use reference images during sampling for guided generation. Compared to the language case, in the visual domain, the context is more limited. Hence, we cannot simply provide references for all the concepts in prompt. We note that to apply RAG for image generation, we need to answer the questions of which images to use as context, how to retrieve them, and how to use the retrieved images to successfully generate required concept. In this work, we address these questions by offering novel method that dynamically chooses the most relevant and useful examples 2 given prompt, and uses them as references, to guide the model toward generating the required results. Leveraging T2I models ability to produce many concepts, we only pass concepts the models struggle to generate, focusing on the gaps. To understand what the challenging concepts are, we suggest novel method that dynamically chooses images to retrieve for given prompt using step-by-step method and Vision-Language Model (VLM). Specifically, we ask the VLM to identify missing image components and suggest concepts that could be retrieved, and use them as references to guide the generation. Our approach is not related to specific T2I model. To demonstrate it, we apply ImageRAG to two model types: T2I models designed to allow in-context learning (ICL) (Brown, 2020); and T2I models augmented with an IP-adapter imageencoder (Ye et al., 2023) that allows image prompting. In ICL, the generative model is provided with set of taskspecific examples in the prompt (as context input) and is expected to perform similar task on new input. ICL does not require additional training or large amounts of data, and offers way to adapt the model to new tasks or domains by supplying it with unseen information at"
[17.02.2025 14:10] Mistral response. {"id": "bebf5bf8df7446f2b1017c0be756207a", "object": "chat.completion", "created": 1739801430, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Tel Aviv University', 'NVIDIA', 'Reichman University']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1675, "total_tokens": 1699, "completion_tokens": 24}}
[17.02.2025 14:10] Response: ```python
['Tel Aviv University', 'NVIDIA', 'Reichman University']
```
[17.02.2025 14:10] Deleting PDF ./assets/pdf/2502.09411.pdf.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.10140.
[17.02.2025 14:10] Downloading paper 2502.10140 from http://arxiv.org/pdf/2502.10140v1...
[17.02.2025 14:10] Extracting affiliations from text.
[17.02.2025 14:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"small Models, BIG Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages Daniil Gurgurov1,3 Ivan Vykopal2,4 Josef van Genabith3 Simon Ostermann3 1University of Saarland 2Brno University of Technology 3German Research Center for Artificial Intelligence (DFKI) 4Kempelen Institute of Intelligent Technologies (KInIT) 5 2 0 2 4 1 ] . [ 1 0 4 1 0 1 . 2 0 5 2 : r {daniil.gurgurov, josef.van_genabith, simon.ostermann}@dfki.de, ivan.vykopal@kinit.sk "
[17.02.2025 14:10] Response: ```python
[
    "University of Saarland",
    "Brno University of Technology",
    "German Research Center for Artificial Intelligence (DFKI)",
    "Kempelen Institute of Intelligent Technologies (KInIT)"
]
```
[17.02.2025 14:10] Deleting PDF ./assets/pdf/2502.10140.pdf.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09638.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.09638.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.09638.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.10177.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.10177.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.10177.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.07780.
[17.02.2025 14:10] Downloading paper 2502.07780 from http://arxiv.org/pdf/2502.07780v1...
[17.02.2025 14:10] Extracting affiliations from text.
[17.02.2025 14:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 0 8 7 7 0 . 2 0 5 2 : r DarwinLM: Evolutionary Structured Pruning of Large Language Models Shengkun Tang 1 Oliver Sieberling 2 Eldar Kurtic 3 4 Zhiqiang Shen 1 Dan Alistarh "
[17.02.2025 14:10] Response: ```python
[]
```
[17.02.2025 14:10] Extracting affiliations from text.
[17.02.2025 14:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 0 8 7 7 0 . 2 0 5 2 : r DarwinLM: Evolutionary Structured Pruning of Large Language Models Shengkun Tang 1 Oliver Sieberling 2 Eldar Kurtic 3 4 Zhiqiang Shen 1 Dan AlistarhLarge Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for nonuniform model compression. However, pruning method should not only identify capable substructure, but also account for post-compression training. To this end, we propose DarwinLM, method for training-aware structured pruning. DarwinLM builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of posttraining, we incorporate lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, DarwinLM surpasses ShearedLlama while requiring 5 less training data during post-compression training. 1. Introduction The high accuracy of Transformer-based models on wide range of tasks comes with massive computational requirements, which hinders deployability. long line of research has been conducted to reduce the computational cost of large language models via methods such as quantization (Frantar et al., 2022; Dettmers et al., 2023), pruning (Xia 1Department of Machine Learning, MBZUAI, Abu Dhabi, UAE 2ETH Zurich, Zurich, Switzerland 3ISTA, Vienna, Austria 4Red Hat AI, Boston, USA. Correspondence to: Dan Alistarh <dan.alistarh@ist.ac.at>. 1 et al., 2024; Frantar & Alistarh, 2023) and distillation (Hsieh et al., 2023). In this work, we explore structured pruning on large language models (LLMs) by removing whole rows or columns in weight matrix, resulting in regular but smaller matrices. Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation. While conventional pruning methods generally prune each layer or block uniformly, non-uniform compression methods, e.g. (Yin et al., 2023; Sieberling et al., 2024), showed that different LLM layers can have significantly different sensitivities to pruning, which can be leveraged to obtain higher compression while preserving accuracy. To address this, smaller-scale pruning methods such as ZipLM (Kurtic et al., 2024) propose to utilize dynamic-programmingbased search (Frantar & Alistarh, 2022) to obtain sparse model with runtime guarantees. However, there are several challenges for methods such as ZipLM on large-scale models such as Llama-2-7B (Touvron et al., 2023): for instance, ZipLM only considers the local layer-wise error during the search, which is not consistent with performance on in-context learning (ICL) or downstream tasks. Overview. In this paper, we propose new structured pruning method based on evolutionary search called DarwinLM, which addresses these issues. Specifically, the search starts from parent model, generated by pruning the original model using second-order information. In each search step, it then generates offspring candidate models by copying the parent and shifting sparsity from one layer to another, by what we call level switch mutation. Moreover, to obtain the optimal sparse model given additional training, we use small-scale dataset to finetune each generated offspring, and select the best offspring after finetuning. Empirically, our method scales to LLMs, and achieves state-of-the-art performance on both one-shot and post-training settings compared with previous pruning methods. For instance, we obtain smaller and better sparse models on both Llama-2, Llama-3.1 and Qwen-2.5 compared with the state-of-theart ZipLM pruning method; furthermore, on Llama-2-7B, our sparse model with 2.7B parameters outperforms the ShearedLlama-2.7B structured pruned model, while using 5 less data for post-compression training (50B Tokens vs. 10B Tokens). Moreover, DarwinLM with 4.6B paramDarwinLM: Evolutionary Structured Pruning of Large Language Models eters pruned from Llama-3.1 surpasses an OLMO model with 7B parameters pre-trained on 250 more data (10B Tokens vs 250T tokens). We also compare our method with line of more coarse-grained structured pruning methods including ShortGPT (Men et al., 2024), Shortened-Llama (Kim et al., 2024), and EvoPress (Sieberling et al., 2024) in one-shot setting, showing that DarwinLM provides better performance across compression rates. process that is split into regularized fine-tuning, pruning, and further fine-tuning. In addition, it implements dynamic batching, which adjusts the composition of sampled data in each training batch dynamically, based on varying loss proxies across different evaluation domains. By contrast, DarwinLM provides more accurate structured pruning, combining evolutionary search and second-order information, requiring only fraction of the data to recover accuracy. We summarize our contributions as follows: We propose new structured pruning method that unifies second-order pruning and evolutionary search, with the goal of producing the optimal non-uniformly thinned model given fixed size or runtime constraint. Importantly, the resulting models can be run efficiently on any hardware, and can meet hardware-specific performance constraints. At the technical level, we introduce pruning technique that leverages second-order information with training-aware offspring selection technique, to identify the non-uniform sparse model that meets the performance requirement and also has the highest potential for accuracy gains during post-compression training. Empirical results show that this approach enhances performance both in terms of loss, but also in terms of performance on downstream tasks. We validate the effectiveness of DarwinLM through extensive experiments. For instance, our method compresses Ll"
[17.02.2025 14:10] Mistral response. {"id": "a1a24e6efd6b43d084a724e886bb7a18", "object": "chat.completion", "created": 1739801445, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Department of Machine Learning, MBZUAI, Abu Dhabi, UAE', 'ETH Zurich, Zurich, Switzerland', 'ISTA, Vienna, Austria', 'Red Hat AI, Boston, USA']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1658, "total_tokens": 1711, "completion_tokens": 53}}
[17.02.2025 14:10] Response: ```python
['Department of Machine Learning, MBZUAI, Abu Dhabi, UAE', 'ETH Zurich, Zurich, Switzerland', 'ISTA, Vienna, Austria', 'Red Hat AI, Boston, USA']
```
[17.02.2025 14:10] Deleting PDF ./assets/pdf/2502.07780.pdf.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.07856.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.07856.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.07856.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.09980.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.09980.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.09980.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.08769.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.08769.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.08769.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Downloading and parsing paper https://huggingface.co/papers/2502.10173.
[17.02.2025 14:10] Extra JSON file exists (./assets/json/2502.10173.json), skip PDF parsing.
[17.02.2025 14:10] Paper image links file exists (./assets/img_data/2502.10173.json), skip HTML parsing.
[17.02.2025 14:10] Success.
[17.02.2025 14:10] Enriching papers with extra data.
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 0. Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps o...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 1. We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal comp...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 2. Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward da...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 3. Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model pro...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 4. Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), w...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 5. Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content wi...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 6. Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) ...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 7. Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these crit...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 8. This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start f...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 9. Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks....
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 10. Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically ...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 11. Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better f...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 12. Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or oth...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 13. A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning a...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 14. Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 15. In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of ...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 16. Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V)...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 17. Masked Image Modeling (MIM) offers a promising approach to self-supervised representation learning, however existing MIM models still lag behind the state-of-the-art. In this paper, we systematically analyze target representations, loss functions, and architectures, to introduce CAPI - a novel pure-...
[17.02.2025 14:10] ********************************************************************************
[17.02.2025 14:10] Abstract 18. Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerat...
[17.02.2025 14:10] Read previous papers.
[17.02.2025 14:10] Generating reviews via LLM API.
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#diffusion", "#architecture", "#dataset"], "emoji": "🚀", "ru": {"title": "Ускорение диффузионных трансформеров без потери качества", "desc": "Статья представляет новый метод ускорения диффузионных моделей под названием RAS. Этот метод осно
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#training", "#open_source", "#diffusion", "#video", "#architecture"], "emoji": "🎬", "ru": {"title": "Революция в генерации видео: от текста к реалистичным роликам", "desc": "Представлена модель Step-Video-T2V - предобученная нейросеть для генерации вид
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#benchmark", "#diffusion", "#training"], "emoji": "🌊", "ru": {"title": "Диффузионные модели бросают вызов авторегрессии в обработке языка", "desc": "Статья представляет LLaDA - новую диффузионную модель для обработки естественного языка, альтернатив
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#interpretability", "#reasoning"], "emoji": "🧠", "ru": {"title": "ZeroBench: невозможный визуальный тест для мультимодальных моделей", "desc": "В статье представлен новый визуальный бенчмарк ZeroBench, который полностью невозможно решить для современных мультимо
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#benchmark", "#alignment", "#training", "#interpretability", "#open_source", "#rlhf", "#dataset"], "emoji": "🤖", "ru": {"title": "Новый подход к согласованию мультимодальных ИИ-моделей с человеческими предпочтениями", "desc": "Исследователи представили MM-RLHF - набор данных из 120 
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#training", "#synthetic", "#architecture"], "emoji": "🔍", "ru": {"title": "Локализация текста в диффузионных моделях для повышения эффективности генерации", "desc": "Статья представляет новый подход к улучшению генерации текста в диффузионных моделях для создани
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#agents", "#rl", "#training", "#inference"], "emoji": "🧠", "ru": {"title": "Усиление LLM для решения сложных задач: многомодельный подход", "desc": "Статья описывает подход к улучшению производительности языковых моделей (LLM) в решении сложны
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#data", "#synthetic", "#dataset", "#inference", "#optimization", "#training"], "emoji": "📊", "ru": {"title": "AdaPTS: Адаптация одномерных моделей для многомерного прогнозирования временных рядов", "desc": "Исследование представляет AdaPTS - фреймворк, использующий адаптеры для прим
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#interpretability"], "emoji": "🗣️", "ru": {"title": "Неологизмы как мост между человеком и ИИ", "desc": "В статье утверждается, что для понимания искусственного интеллекта недостаточно существующего человеческого словаря. Авторы предлагают разрабатывать 
[17.02.2025 14:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#data", "#optimization"], "emoji": "🔢", "ru": {"title": "FoNE: революция в обработке чисел для языковых моделей", "desc": "Исследователи предложили новый метод кодирования чисел для больших языковых моделей, названный Fourier Number Embedding (FoNE). Fo
[17.02.2025 14:10] Querying the API.
[17.02.2025 14:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: https://rotem-shalev.github.io/ImageRAG
[17.02.2025 14:10] Response: {
  "desc": "Статья представляет метод ImageRAG, который использует технологию извлечения и дополнения генерации (RAG) для улучшения работы диффузионных моделей при создании редких или невиданных концепций. ImageRAG динамически извлекает релевантные изображения на основе текстового запроса и использует их в качестве контекста для управления процессом генерации. В отличие от предыдущих подходов, ImageRAG не требует специального обучения для работы с извлеченными изображениями, а использует возможности существующих моделей обусловливания изображений. Метод показывает значительное улучшение в генерации редких и детализированных концепций с использованием различных базовых моделей.",
  "emoji": "🔍",
  "title": "ImageRAG: Улучшение генерации изображений с помощью извлечения и дополнения"
}
[17.02.2025 14:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: https://rotem-shalev.github.io/ImageRAG"

[17.02.2025 14:10] Response: ```python
["RAG", "CV", "MULTIMODAL"]
```
[17.02.2025 14:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: https://rotem-shalev.github.io/ImageRAG"

[17.02.2025 14:10] Response: ```python
["DIFFUSION", "TRANSFER_LEARNING"]
```
[17.02.2025 14:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces ImageRAG, a novel method that enhances image generation by integrating Retrieval-Augmented Generation (RAG) techniques. Unlike previous methods that required specific training for retrieval-based generation, ImageRAG utilizes existing image conditioning models to dynamically retrieve relevant images based on text prompts. This approach allows for improved synthesis of rare and fine-grained visual concepts by providing contextual guidance during the generation process. The adaptability of ImageRAG across various model types demonstrates its effectiveness in producing high-quality and diverse visual content.","title":"Enhancing Image Generation with Dynamic Retrieval"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces ImageRAG, a novel method that enhances image generation by integrating Retrieval-Augmented Generation (RAG) techniques. Unlike previous methods that required specific training for retrieval-based generation, ImageRAG utilizes existing image conditioning models to dynamically retrieve relevant images based on text prompts. This approach allows for improved synthesis of rare and fine-grained visual concepts by providing contextual guidance during the generation process. The adaptability of ImageRAG across various model types demonstrates its effectiveness in producing high-quality and diverse visual content.', title='Enhancing Image Generation with Dynamic Retrieval'))
[17.02.2025 14:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"扩散模型可以生成高质量和多样化的视觉内容，但在生成稀有或未见过的概念时存在困难。为了解决这个问题，我们探索了结合检索增强生成（RAG）与图像生成模型的方法。我们提出了ImageRAG，这是一种根据给定文本提示动态检索相关图像的方法，并将这些图像作为上下文来指导生成过程。与之前需要专门训练的检索生成模型不同，ImageRAG利用现有图像条件模型的能力，无需特定的RAG训练，适应性强，能够在不同模型类型中应用。","title":"ImageRAG：提升稀有概念生成的创新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='扩散模型可以生成高质量和多样化的视觉内容，但在生成稀有或未见过的概念时存在困难。为了解决这个问题，我们探索了结合检索增强生成（RAG）与图像生成模型的方法。我们提出了ImageRAG，这是一种根据给定文本提示动态检索相关图像的方法，并将这些图像作为上下文来指导生成过程。与之前需要专门训练的检索生成模型不同，ImageRAG利用现有图像条件模型的能力，无需特定的RAG训练，适应性强，能够在不同模型类型中应用。', title='ImageRAG：提升稀有概念生成的创新方法'))
[17.02.2025 14:10] Querying the API.
[17.02.2025 14:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.
[17.02.2025 14:11] Response: {
  "desc": "Исследование посвящено адаптации многоязычных моделей (mLMs) для языков с ограниченными ресурсами (LRLs) с использованием эффективных по параметрам методов на основе адаптеров. Авторы сравнивают три архитектуры адаптеров: Sequential Bottleneck, Invertible Bottleneck и Low-Rank Adaptation, оценивая их эффективность на различных задачах обработки естественного языка. Результаты показывают, что адаптация на небольших наборах данных (до 1 ГБ неструктурированного текста или несколько МБ структурированных знаний) улучшает производительность моделей. Исследование также демонстрирует, что для LRLs более эффективны меньшие mLMs, чем крупные языковые модели типа LLaMA-3 или GPT-4.",

  "emoji": "🌍",

  "title": "Эффективная адаптация многоязычных моделей для языков с ограниченными ресурсами"
}
[17.02.2025 14:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage."

[17.02.2025 14:11] Response: ```python
['MULTILINGUAL', 'TRAINING', 'SMALL_MODELS']
```
[17.02.2025 14:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage."

[17.02.2025 14:11] Response: ```python
["LOW_RESOURCE", "TRANSFER_LEARNING"]
```
[17.02.2025 14:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges of processing low-resource languages (LRLs) in natural language processing (NLP) by exploring the use of smaller multilingual models (mLMs) like mBERT and XLM-R. It investigates parameter-efficient adapter-based methods to adapt these mLMs to LRLs, focusing on three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. The study demonstrates that even small adaptation datasets can significantly enhance performance in both intrinsic and extrinsic NLP tasks. Results indicate that Sequential Bottleneck adapters are particularly effective for language modeling, while Invertible Bottleneck adapters perform better on downstream tasks, highlighting the advantages of adapter-based methods over full fine-tuning.","title":"Empowering Low-Resource Languages with Efficient Multilingual Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges of processing low-resource languages (LRLs) in natural language processing (NLP) by exploring the use of smaller multilingual models (mLMs) like mBERT and XLM-R. It investigates parameter-efficient adapter-based methods to adapt these mLMs to LRLs, focusing on three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. The study demonstrates that even small adaptation datasets can significantly enhance performance in both intrinsic and extrinsic NLP tasks. Results indicate that Sequential Bottleneck adapters are particularly effective for language modeling, while Invertible Bottleneck adapters perform better on downstream tasks, highlighting the advantages of adapter-based methods over full fine-tuning.', title='Empowering Low-Resource Languages with Efficient Multilingual Models'))
[17.02.2025 14:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"低资源语言（LRLs）在自然语言处理（NLP）中面临数据不足的重大挑战。当前的先进大型语言模型（LLMs）在处理LRLs时仍然存在困难，而较小的多语言模型（mLMs）如mBERT和XLM-R由于其容量更适合低训练数据量，展现出更大的潜力。本文系统研究了基于参数高效适配器的方法，评估了三种架构：顺序瓶颈、可逆瓶颈和低秩适配。研究表明，使用小型适配数据集可以在语言建模和下游任务中取得显著提升，尤其是顺序瓶颈适配器在语言建模方面表现优异。","title":"小型多语言模型助力低资源语言处理"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='低资源语言（LRLs）在自然语言处理（NLP）中面临数据不足的重大挑战。当前的先进大型语言模型（LLMs）在处理LRLs时仍然存在困难，而较小的多语言模型（mLMs）如mBERT和XLM-R由于其容量更适合低训练数据量，展现出更大的潜力。本文系统研究了基于参数高效适配器的方法，评估了三种架构：顺序瓶颈、可逆瓶颈和低秩适配。研究表明，使用小型适配数据集可以在语言建模和下游任务中取得显著提升，尤其是顺序瓶颈适配器在语言建模方面表现优异。', title='小型多语言模型助力低资源语言处理'))
[17.02.2025 14:11] Using data from previous issue: {"categories": ["#ethics", "#multimodal", "#training", "#security", "#rlhf"], "emoji": "🕵️", "ru": {"title": "LLM против LLM: новый фронт в безопасности ИИ", "desc": "Статья представляет новый подход к тестированию безопасности больших языковых моделей (LLM), использующий сами LLM в качестве 'красно
[17.02.2025 14:11] Using data from previous issue: {"categories": ["#games", "#agents", "#reasoning"], "emoji": "🧠", "ru": {"title": "Пространственно-временная память повышает эффективность воплощенных агентов", "desc": "Статья представляет новый фреймворк под названием Spatio-Temporal Memory Agent (STMA) для улучшения планирования и выполнения зада
[17.02.2025 14:11] Querying the API.
[17.02.2025 14:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for training-aware structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.
[17.02.2025 14:11] Response: {
  "desc": "Это исследование представляет новый метод структурированной обрезки для больших языковых моделей (БЯМ), называемый \sysname. Метод использует эволюционный поиск для генерации и отбора подмоделей, учитывая эффект пост-обучения. \sysname включает легковесный многоступенчатый процесс обучения для оценки потенциала моделей. Эксперименты на Llama-2-7B, Llama-3.1-8B и Qwen-2.5-14B-Instruct показали превосходные результаты по сравнению с существующими методами, требуя при этом в 5 раз меньше данных для пост-обучения.",
  "emoji": "✂️",
  "title": "Эволюционная обрезка БЯМ с учетом пост-обучения"
}
[17.02.2025 14:11] Renaming some terms.
[17.02.2025 14:11] Error. Failed to parse JSON from LLM. {
  "desc": "Это исследование представляет новый метод структурированной обрезки для больших языковых моделей (LLM), называемый \sysname. Метод использует эволюционный поиск для генерации и отбора подмоделей, учитывая эффект пост-обучения. \sysname включает легковесный многоступенчатый процесс обучения для оценки потенциала моделей. Эксперименты на Llama-2-7B, Llama-3.1-8B и Qwen-2.5-14B-Instruct показали превосходные результаты по сравнению с существующими методами, требуя при этом в 5 раз меньше данных для пост-обучения.",
  "emoji": "✂️",
  "title": "Эволюционная обрезка LLM с учетом пост-обучения"
}
[17.02.2025 14:11] Fallback to OpenAI.
[17.02.2025 14:11] Response: ParsedChatCompletionMessage[ArticleFull](content='{"desc":"В статье рассматривается проблема высокой вычислительной стоимости больших языковых моделей (LLM) и предлагается метод структурированной обрезки для их оптимизации. Метод \\u001710\\u00187 использует эволюционный подход, создавая и отбирая наиболее эффективные подмодели. Важной частью метода является обучение после обрезки, что позволяет улучшить производительность моделей. Эксперименты показывают, что \\u001710\\u00187 превосходит существующие методы, требуя меньше данных для обучения.","emoji":"🧬","title":"Эволюционная оптимизация языковых моделей"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=ArticleFull(desc='В статье рассматривается проблема высокой вычислительной стоимости больших языковых моделей (LLM) и предлагается метод структурированной обрезки для их оптимизации. Метод \x1710\x187 использует эволюционный подход, создавая и отбирая наиболее эффективные подмодели. Важной частью метода является обучение после обрезки, что позволяет улучшить производительность моделей. Эксперименты показывают, что \x1710\x187 превосходит существующие методы, требуя меньше данных для обучения.', emoji='🧬', title='Эволюционная оптимизация языковых моделей'))
[17.02.2025 14:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for training-aware structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training."

[17.02.2025 14:11] Response: ```python
['INFERENCE', 'TRAINING', 'SMALL_MODELS']
```
[17.02.2025 14:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for training-aware structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training."

[17.02.2025 14:11] Response: ```python
["OPTIMIZATION"]
```
[17.02.2025 14:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method called \\textit{sysname} for structured pruning of large language models (LLMs) to improve their efficiency in natural language processing tasks. The method uses an evolutionary search process to create multiple model variations, selecting the best-performing ones while incorporating a lightweight training process to enhance post-compression performance. By focusing on the varying sensitivities of different model components, \\textit{sysname} achieves effective model compression without sacrificing accuracy. The results show that \\textit{sysname} outperforms existing methods, requiring significantly less training data while maintaining high performance on various LLMs.","title":"Efficient Pruning for Powerful Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method called \textit{sysname} for structured pruning of large language models (LLMs) to improve their efficiency in natural language processing tasks. The method uses an evolutionary search process to create multiple model variations, selecting the best-performing ones while incorporating a lightweight training process to enhance post-compression performance. By focusing on the varying sensitivities of different model components, \textit{sysname} achieves effective model compression without sacrificing accuracy. The results show that \textit{sysname} outperforms existing methods, requiring significantly less training data while maintaining high performance on various LLMs.', title='Efficient Pruning for Powerful Language Models'))
[17.02.2025 14:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"大型语言模型（LLMs）在自然语言处理任务中取得了显著成功，但其巨大的计算成本限制了其广泛应用，尤其是在实时应用中。结构化剪枝是一种有效的解决方案，通过压缩模型并直接提供端到端的速度提升。不同模型组件对剪枝的敏感性不同，因此需要非均匀的模型压缩方法。我们提出了一种名为\\textit{sysname}的训练感知结构化剪枝方法，通过进化搜索过程生成多个后代模型，并在每一代中选择最适合的模型进行生存。","title":"训练感知的结构化剪枝方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='大型语言模型（LLMs）在自然语言处理任务中取得了显著成功，但其巨大的计算成本限制了其广泛应用，尤其是在实时应用中。结构化剪枝是一种有效的解决方案，通过压缩模型并直接提供端到端的速度提升。不同模型组件对剪枝的敏感性不同，因此需要非均匀的模型压缩方法。我们提出了一种名为\textit{sysname}的训练感知结构化剪枝方法，通过进化搜索过程生成多个后代模型，并在每一代中选择最适合的模型进行生存。', title='训练感知的结构化剪枝方法'))
[17.02.2025 14:11] Using data from previous issue: {"categories": ["#training", "#cv", "#data", "#diffusion", "#optimization"], "emoji": "🚀", "ru": {"title": "Ускорение управляемой генерации изображений без потери качества", "desc": "Статья представляет новый алгоритм MRS (MR Sampler) для ускорения процесса семплирования в Mean Reverting (MR) Diffus
[17.02.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#reasoning", "#science", "#agi", "#games", "#optimization", "#dataset", "#agents"], "emoji": "🚗", "ru": {"title": "LLM на службе кооперативного автономного вождения", "desc": "Статья представляет новый подход к кооперативному автономному вождению с исп
[17.02.2025 14:11] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training", "#architecture", "#dataset", "#open_source"], "emoji": "🧠", "ru": {"title": "CAPI: Прорыв в самообучаемом представлении изображений", "desc": "Статья представляет CAPI - новую систему для самообучаемого представления изображений на основе маскиров
[17.02.2025 14:11] Using data from previous issue: {"categories": ["#architecture", "#agents", "#dataset"], "emoji": "🧬", "ru": {"title": "VibeGen: ИИ-дизайн белков с заданной динамикой", "desc": "VibeGen - это генеративная ИИ-система для проектирования белков с заданными динамическими свойствами. Она использует двойную архитектуру с моделью-дизайне
[17.02.2025 14:11] Loading Chinese text from previous data.
[17.02.2025 14:11] Renaming data file.
[17.02.2025 14:11] Renaming previous data. hf_papers.json to ./d/2025-02-17.json
[17.02.2025 14:11] Saving new data file.
[17.02.2025 14:11] Generating page.
[17.02.2025 14:11] Renaming previous page.
[17.02.2025 14:11] Renaming previous data. index.html to ./d/2025-02-17.html
[17.02.2025 14:11] [Experimental] Generating Chinese page for reading.
[17.02.2025 14:11] Chinese vocab [{'word': '扩散模型', 'pinyin': 'kuò sàn mó xíng', 'trans': 'diffusion model'}, {'word': '首选', 'pinyin': 'shǒu xuǎn', 'trans': 'preferred choice'}, {'word': '依赖', 'pinyin': 'yī lài', 'trans': 'depend on'}, {'word': '顺序', 'pinyin': 'shùn xù', 'trans': 'sequential'}, {'word': '前向传递', 'pinyin': 'qián xiàng chuán dì', 'trans': 'forward pass'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '限制', 'pinyin': 'xiàn zhì', 'trans': 'limit'}, {'word': '实时性能', 'pinyin': 'shí shí xìng néng', 'trans': 'real-time performance'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '集中', 'pinyin': 'jí zhōng', 'trans': 'focus on'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '采样步骤', 'pinyin': 'cǎi yàng bù zhòu', 'trans': 'sampling steps'}, {'word': '重用', 'pinyin': 'chóng yòng', 'trans': 'reuse'}, {'word': '中间结果', 'pinyin': 'zhōng jiān jié guǒ', 'trans': 'intermediate results'}, {'word': '利用', 'pinyin': 'lì yòng', 'trans': 'utilize'}, {'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '内部空间区域', 'pinyin': 'nèi bù kōng jiān qū yù', 'trans': 'internal spatial regions'}, {'word': '变化', 'pinyin': 'biàn huà', 'trans': 'change'}, {'word': '扩散变压器', 'pinyin': 'kuò sàn biàn yā qì', 'trans': 'diffusion transformer'}, {'word': '灵活性', 'pinyin': 'líng huó xìng', 'trans': 'flexibility'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': 'RAS', 'pinyin': 'RAS', 'trans': 'RAS'}, {'word': '采样策略', 'pinyin': 'cǎi yàng cè lüè', 'trans': 'sampling strategy'}, {'word': '动态分配', 'pinyin': 'dòng tài fēn pèi', 'trans': 'dynamic allocation'}, {'word': '关注点', 'pinyin': 'guān zhù diǎn', 'trans': 'focus points'}, {'word': '关键观察', 'pinyin': 'guǎn jiàn guān chá', 'trans': 'key observation'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '有意义', 'pinyin': 'yǒu yì yì', 'trans': 'meaningful'}, {'word': '连续步骤', 'pinyin': 'lián xù bù zhòu', 'trans': 'continuous steps'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '连续性', 'pinyin': 'lián xù xìng', 'trans': 'continuity'}, {'word': '洞察', 'pinyin': 'dòng chá', 'trans': 'insight'}, {'word': '更新', 'pinyin': 'gēng xīn', 'trans': 'update'}, {'word': '缓存噪声', 'pinyin': 'huǎn cún zào shēng', 'trans': 'cached noise'}, {'word': '确定', 'pinyin': 'què dìng', 'trans': 'determine'}, {'word': '时间一致性', 'pinyin': 'shí jiān yī zhì xìng', 'trans': 'temporal consistency'}, {'word': '评估', 'pinyin': 'píng gū', 'trans': 'evaluate'}, {'word': 'Stable Diffusion 3', 'pinyin': 'Stable Diffusion 3', 'trans': 'Stable Diffusion 3'}, {'word': 'Lumina-Next-T2I', 'pinyin': 'Lumina-Next-T2I', 'trans': 'Lumina-Next-T2I'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'acceleration'}, {'word': '生成质量', 'pinyin': 'shēng chéng zhì liàng', 'trans': 'generation quality'}, {'word': '轻微下降', 'pinyin': 'qīng wēi xià jiàng', 'trans': 'slight decrease'}, {'word': '用户研究', 'pinyin': 'yòng hù yán jiū', 'trans': 'user study'}, {'word': '人类评估', 'pinyin': 'rén lèi píng gū', 'trans': 'human evaluation'}, {'word': '相似', 'pinyin': 'xiāng sì', 'trans': 'similar'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '重要进展', 'pinyin': 'zhòng yào jìn zhǎn', 'trans': 'significant progress'}]
[17.02.2025 14:11] Renaming previous Chinese page.
[17.02.2025 14:11] Renaming previous data. zh.html to ./d/2025-02-16_zh_reading_task.html
[17.02.2025 14:11] Writing Chinese reading task.
[17.02.2025 14:11] Writing result.
[17.02.2025 14:11] Renaming log file.
[17.02.2025 14:11] Renaming previous data. log.txt to ./logs/2025-02-17_last_log.txt
