[13.05.2025 08:16] Read previous papers.
[13.05.2025 08:16] Generating top page (month).
[13.05.2025 08:16] Writing top page (month).
[13.05.2025 09:12] Read previous papers.
[13.05.2025 09:12] Get feed.
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07062
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07608
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07747
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07787
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06548
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07447
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07818
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07293
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03733
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07796
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07596
[13.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07819
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07793
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06324
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04918
[13.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07260
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07086
[13.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.05.2025 09:12] No deleted papers detected.
[13.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 17.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07062.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07062.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07062.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07608.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07608.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07608.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07747.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07747.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07747.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07787.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07787.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07787.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.06548.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.06548.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.06548.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07447.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07447.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07447.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07818.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07818.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07818.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07293.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07293.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07293.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.03733.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.03733.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.03733.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07796.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07796.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07796.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07596.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07596.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07596.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07819.
[13.05.2025 09:12] Downloading paper 2505.07819 from http://arxiv.org/pdf/2505.07819v1...
[13.05.2025 09:12] Extracting affiliations from text.
[13.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 9 1 8 7 0 . 5 0 5 2 : r H3DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning Yiyang Lu1, Yufeng Tian4, Zhecheng Yuan1,2,3, Xianbang Wang1, Pu Hua1,2,3, Zhengrong Xue1,2,3, Huazhe Xu1,2,3 1 Tsinghua University IIIS, 2 Shanghai Qi Zhi Institute, 3 Shanghai AI Lab, 4 Harbin Institute of Technology luyy24@mails.tsinghua.edu.cn, huazhe xu@mail.tsinghua.edu.cn Figure 1: H3DP can not only achieve superior performance across 44 tasks on 5 simulation benchmarks, but also handle long-horizon challenging manipulation tasks in cluttered real-world scenarios. Abstract: Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy (H3DP), novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H3DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H3DP yields +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/. Keywords: Imitation Learning, Representation Learning, Diffusion Model Equal Contribution Visuomotor policy learning has emerged as prevailing paradigm in robotic manipulation [1, 2, 3, 4, 5]. Existing approaches have increasi"
[13.05.2025 09:12] Response: ```python
[
    "Tsinghua University IIIS",
    "Shanghai Qi Zhi Institute",
    "Shanghai AI Lab",
    "Harbin Institute of Technology"
]
```
[13.05.2025 09:12] Deleting PDF ./assets/pdf/2505.07819.pdf.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07793.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07793.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07793.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.06324.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.06324.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.06324.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04918.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04918.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04918.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07260.
[13.05.2025 09:12] Downloading paper 2505.07260 from http://arxiv.org/pdf/2505.07260v1...
[13.05.2025 09:12] Extracting affiliations from text.
[13.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 0 6 2 7 0 . 5 0 5 2 : r UMoE: Unifying Attention and FFN with Shared Experts Yuanhang Yang1, Chaozheng Wang2, Jing Li3 1Institute of Science Tokyo, Tokyo, Japan 2The Chinese University of Hong Kong, Hong Kong, China 3Hong Kong Polytechnic University, Hong Kong, China yang.y.ea2c@m.isct.ac.jp czwang23@cse.cuhk.edu.hk jing-amelia.li@polyu.edu.hk "
[13.05.2025 09:12] Response: ```python
[
    "Institute of Science Tokyo, Tokyo, Japan",
    "The Chinese University of Hong Kong, Hong Kong, China",
    "Hong Kong Polytechnic University, Hong Kong, China"
]
```
[13.05.2025 09:12] Deleting PDF ./assets/pdf/2505.07260.pdf.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07086.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07086.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07086.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Enriching papers with extra data.
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 0. We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 1. We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 2. While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 3. Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 4. Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavo...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 5. Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 6. Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face criti...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 7. Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biase...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 8. LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructi...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 9. Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain perfo...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 10. Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 11. Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In t...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 12. A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 13. As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge,...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 14. Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-i...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 15. Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model perfo...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 16. Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing ap...
[13.05.2025 09:12] Read previous papers.
[13.05.2025 09:12] Generating reviews via LLM API.
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#survey", "#architecture", "#training", "#reasoning", "#data"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸", "desc": "Seed1.5-VL - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ñ
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#plp", "#reasoning", "#optimization", "#dataset", "#math", "#rl", "#data", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "MiMo-7B: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹", "desc": "MiMo-7B - ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#data", "#diffusion", "#transfer_learning", "#3d", "#benchmark"], "emoji": "ğŸ§Š", "ru": {"title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ AI-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Step1X-3D - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#math", "#training", "#small_models", "#reasoning", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼: ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´Ñ€ÑƒĞ³ Ñƒ Ğ´Ñ€ÑƒĞ³Ğ°", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ² 'Ğ»Ğ¾Ğ²ÑƒÑˆĞºÑƒ Ğ´
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#small_models", "#rl", "#dataset", "#data"], "emoji": "ğŸ¤–", "ru": {"title": "ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğœ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLaMA 2-7B, LL
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#training", "#cv"], "emoji": "ğŸ”„", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾ Ğ¼Ğ°Ğ»Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. 
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#alignment", "#optimization", "#video", "#multimodal", "#rl", "#diffusion", "#benchmark", "#rlhf"], "emoji": "ğŸ¨", "ru": {"title": "DanceGRPO: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DanceGRPO - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº 
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#dataset", "#benchmark", "#data", "#optimization", "#small_models"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¯Ğœ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ AttentionInfluence Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#agents", "#dataset"], "emoji": "ğŸŒ", "ru": {"title": "WebGen-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸", "desc": "WebGen-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµĞ±-Ñ
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training"], "emoji": "ğŸ“ˆ", "ru": {"title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ (CPT) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ 
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#agents", "#optimization", "#hallucinations", "#rl", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞºĞ°Ñ‚ÑŒ, Ğ° ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒÑÑ ÑĞµĞ±Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼
[13.05.2025 09:12] Querying the API.
[13.05.2025 09:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.
[13.05.2025 09:13] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ - Ğ¢Ñ€Ğ¸ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ (H^3DP). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ: Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. H^3DP ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸.",

  "emoji": "ğŸ¤–",

  "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²"
}
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."

[13.05.2025 09:13] Response: ```python
["AGENTS", "ROBOTICS"]
```
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."

[13.05.2025 09:13] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.","title":"Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.', title='Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration'))
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºä¸‰å±‚æ¬¡æ‰©æ•£ç­–ç•¥ï¼ˆH^{\\textbf{3}DPï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å±‚æ¬¡ç»“æ„ï¼Œå¢å¼ºäº†è§†è§‰ç‰¹å¾ä¸åŠ¨ä½œç”Ÿæˆä¹‹é—´çš„ç»“åˆã€‚H^{3}DPåŒ…å«ä¸‰ä¸ªå±‚æ¬¡ï¼šåŸºäºæ·±åº¦ä¿¡æ¯çš„è¾“å…¥åˆ†å±‚ã€å¤šå°ºåº¦è§†è§‰è¡¨ç¤ºå’Œå±‚æ¬¡æ¡ä»¶æ‰©æ•£è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH^{3}DPåœ¨44ä¸ªä»¿çœŸä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å¹³å‡æé«˜äº†27.5%çš„æ€§èƒ½ï¼Œå¹¶åœ¨å››ä¸ªå¤æ‚çš„åŒæ‰‹çœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚","title":"å¢å¼ºè§†è§‰ä¸åŠ¨ä½œç”Ÿæˆçš„ä¸‰å±‚æ¬¡å­¦ä¹ æ¡†æ¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¿åŠ¨ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºä¸‰å±‚æ¬¡æ‰©æ•£ç­–ç•¥ï¼ˆH^{\textbf{3}DPï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å±‚æ¬¡ç»“æ„ï¼Œå¢å¼ºäº†è§†è§‰ç‰¹å¾ä¸åŠ¨ä½œç”Ÿæˆä¹‹é—´çš„ç»“åˆã€‚H^{3}DPåŒ…å«ä¸‰ä¸ªå±‚æ¬¡ï¼šåŸºäºæ·±åº¦ä¿¡æ¯çš„è¾“å…¥åˆ†å±‚ã€å¤šå°ºåº¦è§†è§‰è¡¨ç¤ºå’Œå±‚æ¬¡æ¡ä»¶æ‰©æ•£è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH^{3}DPåœ¨44ä¸ªä»¿çœŸä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å¹³å‡æé«˜äº†27.5%çš„æ€§èƒ½ï¼Œå¹¶åœ¨å››ä¸ªå¤æ‚çš„åŒæ‰‹çœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚', title='å¢å¼ºè§†è§‰ä¸åŠ¨ä½œç”Ÿæˆçš„ä¸‰å±‚æ¬¡å­¦ä¹ æ¡†æ¶'))
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture", "#long_context"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ§Ğ°Ğ½ĞºĞ¸ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑƒĞ±-ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ…
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "ğŸ”", "ru": {"title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ² LLM: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ¾Ğº
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#graphs", "#architecture", "#training"], "emoji": "ğŸŒ", "ru": {"title": "PASSAT: Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸", "desc": "PASSAT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ
[13.05.2025 09:13] Querying the API.
[13.05.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.
[13.05.2025 09:13] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (Sparse Mixture of Experts, MoE) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Transformer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ MoE Ğ´Ğ»Ñ ÑĞ»Ğ¾ĞµĞ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ², Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ UMoE, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ° ÑÑ‡ĞµÑ‚ MoE Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. UMoE Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸.",
  "emoji": "ğŸ§ ",
  "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ MoE Ğ² Transformer: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ"
}
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components."

[13.05.2025 09:13] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components."

[13.05.2025 09:13] Response: ```python
["OPTIMIZATION"]
```
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.","title":"Unifying MoE for Enhanced Transformer Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.', title='Unifying MoE for Enhanced Transformer Performance'))
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç”¨äºæ‰©å±•Transformeræ¨¡å‹ã€‚è™½ç„¶æ—©æœŸçš„ç ”ç©¶ä¸»è¦å°†MoEåº”ç”¨äºå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å±‚ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢ç´¢å°†MoEæ‰©å±•åˆ°æ³¨æ„åŠ›å±‚ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç°æœ‰çš„åŸºäºæ³¨æ„åŠ›çš„MoEå±‚éœ€è¦ä¸“é—¨çš„å®ç°ï¼Œå¹¶ä¸”ä¸åŸºäºFFNçš„å±‚ç›¸æ¯”ï¼Œæ€§èƒ½ä¸å°½å¦‚äººæ„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ³¨æ„åŠ›æœºåˆ¶é‡æ„ï¼Œç»Ÿä¸€äº†æ³¨æ„åŠ›å±‚å’ŒFFNå±‚ä¸­çš„MoEè®¾è®¡ï¼Œæå‡ºçš„UMoEæ¶æ„é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„MoEå±‚å®ç°äº†æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†FFNå’Œæ³¨æ„åŠ›ç»„ä»¶ä¹‹é—´çš„é«˜æ•ˆå‚æ•°å…±äº«ã€‚","title":"ç»Ÿä¸€æ³¨æ„åŠ›ä¸å‰é¦ˆç½‘ç»œçš„ç¨€ç–ä¸“å®¶æ¶æ„"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç”¨äºæ‰©å±•Transformeræ¨¡å‹ã€‚è™½ç„¶æ—©æœŸçš„ç ”ç©¶ä¸»è¦å°†MoEåº”ç”¨äºå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å±‚ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢ç´¢å°†MoEæ‰©å±•åˆ°æ³¨æ„åŠ›å±‚ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç°æœ‰çš„åŸºäºæ³¨æ„åŠ›çš„MoEå±‚éœ€è¦ä¸“é—¨çš„å®ç°ï¼Œå¹¶ä¸”ä¸åŸºäºFFNçš„å±‚ç›¸æ¯”ï¼Œæ€§èƒ½ä¸å°½å¦‚äººæ„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ³¨æ„åŠ›æœºåˆ¶é‡æ„ï¼Œç»Ÿä¸€äº†æ³¨æ„åŠ›å±‚å’ŒFFNå±‚ä¸­çš„MoEè®¾è®¡ï¼Œæå‡ºçš„UMoEæ¶æ„é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„MoEå±‚å®ç°äº†æ›´ä¼˜çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†FFNå’Œæ³¨æ„åŠ›ç»„ä»¶ä¹‹é—´çš„é«˜æ•ˆå‚æ•°å…±äº«ã€‚', title='ç»Ÿä¸€æ³¨æ„åŠ›ä¸å‰é¦ˆç½‘ç»œçš„ç¨€ç–ä¸“å®¶æ¶æ„'))
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#science", "#data", "#optimization"], "emoji": "ğŸ§¬", "ru": {"title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multi-Obje
[13.05.2025 09:13] Trying to get texts in Chinese.
[13.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)
[13.05.2025 09:13] Mistral response. {"id": "319ea83926f248ccbec47a113e2d443d", "object": "chat.completion", "created": 1747127603, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86 Seed1.5-VL\uff0c\u4e00\u4e2a\u7528\u4e8e\u63a8\u8fdb\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u3002\u5b83\u7531\u4e00\u4e2a 532M \u53c2\u6570\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e00\u4e2a 20B \u6d3b\u8dc3\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09LLM \u7ec4\u6210\u3002\u5c3d\u7ba1\u67b6\u6784\u76f8\u5bf9\u7d27\u51d1\uff0c\u4f46\u5b83\u5728 38 \u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e OpenAI CUA \u548c Claude 3.7\u3002Seed1.5-VL \u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u63a8\u7406\u6311\u6218\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u80fd\u529b\u80fd\u63a8\u52a8\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 337, "total_tokens": 530, "completion_tokens": 193}}
[13.05.2025 09:13] Response: æˆ‘ä»¬ä»‹ç»äº† Seed1.5-VLï¼Œä¸€ä¸ªç”¨äºæ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚å®ƒç”±ä¸€ä¸ª 532M å‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œä¸€ä¸ª 20B æ´»è·ƒå‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰LLM ç»„æˆã€‚å°½ç®¡æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†å®ƒåœ¨ 38 ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ä»£ç†ä»»åŠ¡ä¸­ä¼˜äº OpenAI CUA å’Œ Claude 3.7ã€‚Seed1.5-VL è¿˜å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›èƒ½åŠ›èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚
[13.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æˆ‘ä»¬ä»‹ç»äº† Seed1.5-VLï¼Œä¸€ä¸ªç”¨äºæ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚å®ƒç”±ä¸€ä¸ª 532M å‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œä¸€ä¸ª 20B æ´»è·ƒå‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰LLM ç»„æˆã€‚å°½ç®¡æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†å®ƒåœ¨ 38 ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ä»£ç†ä»»åŠ¡ä¸­ä¼˜äº OpenAI CUA å’Œ Claude 3.7ã€‚Seed1.5-VL è¿˜å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›èƒ½åŠ›èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚
[13.05.2025 09:14] Mistral response. {"id": "78f809fdd46e4d95a3a3ab22c94ffbcc", "object": "chat.completion", "created": 1747127606, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86 Seed1.5-VL\uff0c\u4e00\u4e2a\u7528\u4e8e\u63a8\u8fdb\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u3002\u5b83\u7531\u4e00\u4e2a 532M \u53c2\u6570\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e00\u4e2a 20B \u6d3b\u8dc3\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09LLM \u7ec4\u6210\u3002\u5c3d\u7ba1\u67b6\u6784\u76f8\u5bf9\u7d27\u51d1\uff0c\u4f46\u5b83\u5728 38 \u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e OpenAI CUA \u548c Claude 3.7\u3002Seed1.5-VL \u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u63a8\u7406\u6311\u6218\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u80fd\u529b\u80fd\u63a8\u52a8\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\n\nW\u01d2men ji\u00e8sh\u00e0o le Seed1.5-VL, y\u012bg\u00e8 y\u00f2ngy\u00fa tu\u012bj\u00ecn t\u014dngy\u00f2ng du\u014d m\u00f3sh\u00ec l\u01d0ji\u011b h\u00e9 tu\u012bl\u01d0 de sh\u00ecju\u00e9-y\u01d4y\u00e1n j\u012bch\u01d4 m\u00f3x\u00edng. T\u0101 y\u00f3u y\u012bg\u00e8 532M c\u0101nsh\u00f9 de sh\u00ecju\u00e9 bi\u0101nm\u01ceq\u00ec h\u00e9 y\u012bg\u00e8 20B hu\u00f3yu\u00e8 c\u0101nsh\u00f9 de h\u00f9nh\u00e9 zhu\u0101nji\u0101 (MoE) LLM z\u01d4ch\u00e9ng. J\u01d0ngu\u01cen ji\u00e0g\u00f2u xi\u0101ngdu\u00ec j\u01d0ncu\u01d2, d\u00e0n t\u0101 z\u00e0i 38 g\u00e8 g\u014dngg\u00f2ng j\u012bzh\u01d4n c\u00e8sh\u00ec zh\u014dng bi\u01ceoxi\u00e0n ch\u016bs\u00e8, b\u00ecng z\u00e0i d\u00e0il\u01d0 r\u00e8nw\u00f9 zh\u014dng y\u014duy\u00fa OpenAI CUA h\u00e9 Claude 3.7. Seed1.5-VL h\u00e1i zh\u01censh\u00ec le qi\u00e1ngd\u00e0 de tu\u012bl\u01d0 n\u00e9ngl\u00ec, sh\u00ecy\u00f2ng y\u00fa du\u014d m\u00f3sh\u00ec tu\u012bl\u01d0 ti\u01ceozh\u00e0n. W\u01d2men x\u012bw\u00e0ng zh\u00e8xi\u0113 n\u00e9ngl\u00ec n\u00e9ng tu\u012bd\u00f2ng g\u00e8ng gu\u01cengf\u00e0n de y\u00ecngy\u00f2ng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 207, "total_tokens": 746, "completion_tokens": 539}}
[13.05.2025 09:14] Response: æˆ‘ä»¬ä»‹ç»äº† Seed1.5-VLï¼Œä¸€ä¸ªç”¨äºæ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚å®ƒç”±ä¸€ä¸ª 532M å‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œä¸€ä¸ª 20B æ´»è·ƒå‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰LLM ç»„æˆã€‚å°½ç®¡æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†å®ƒåœ¨ 38 ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ä»£ç†ä»»åŠ¡ä¸­ä¼˜äº OpenAI CUA å’Œ Claude 3.7ã€‚Seed1.5-VL è¿˜å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›èƒ½åŠ›èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

WÇ’men jiÃ¨shÃ o le Seed1.5-VL, yÄ«gÃ¨ yÃ²ngyÃº tuÄ«jÃ¬n tÅngyÃ²ng duÅ mÃ³shÃ¬ lÇjiÄ› hÃ© tuÄ«lÇ de shÃ¬juÃ©-yÇ”yÃ¡n jÄ«chÇ” mÃ³xÃ­ng. TÄ yÃ³u yÄ«gÃ¨ 532M cÄnshÃ¹ de shÃ¬juÃ© biÄnmÇqÃ¬ hÃ© yÄ«gÃ¨ 20B huÃ³yuÃ¨ cÄnshÃ¹ de hÃ¹nhÃ© zhuÄnjiÄ (MoE) LLM zÇ”chÃ©ng. JÇnguÇn jiÃ gÃ²u xiÄngduÃ¬ jÇncuÇ’, dÃ n tÄ zÃ i 38 gÃ¨ gÅnggÃ²ng jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨, bÃ¬ng zÃ i dÃ ilÇ rÃ¨nwÃ¹ zhÅng yÅuyÃº OpenAI CUA hÃ© Claude 3.7. Seed1.5-VL hÃ¡i zhÇnshÃ¬ le qiÃ¡ngdÃ  de tuÄ«lÇ nÃ©nglÃ¬, shÃ¬yÃ²ng yÃº duÅ mÃ³shÃ¬ tuÄ«lÇ tiÇozhÃ n. WÇ’men xÄ«wÃ ng zhÃ¨xiÄ“ nÃ©nglÃ¬ nÃ©ng tuÄ«dÃ²ng gÃ¨ng guÇngfÃ n de yÃ¬ngyÃ²ng.
[13.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æˆ‘ä»¬ä»‹ç»äº† Seed1.5-VLï¼Œä¸€ä¸ªç”¨äºæ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚å®ƒç”±ä¸€ä¸ª 532M å‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œä¸€ä¸ª 20B æ´»è·ƒå‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰LLM ç»„æˆã€‚å°½ç®¡æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†å®ƒåœ¨ 38 ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ä»£ç†ä»»åŠ¡ä¸­ä¼˜äº OpenAI CUA å’Œ Claude 3.7ã€‚Seed1.5-VL è¿˜å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›èƒ½åŠ›èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚
[13.05.2025 09:14] Mistral response. {"id": "fce996f36061495496e072382d4222cf", "object": "chat.completion", "created": 1747127646, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8 sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"Seed1.5-VL\", \"pinyin\": \"Seed1.5-VL\", \"trans\": \"Seed1.5-VL\"},\n    {\"word\": \"\u63a8\u8fdb\", \"pinyin\": \"tu\u012b j\u00ecn\", \"trans\": \"promote\"},\n    {\"word\": \"\u901a\u7528\", \"pinyin\": \"t\u014dng y\u00f2ng\", \"trans\": \"general\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 shu\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u7406\u89e3\", \"pinyin\": \"l\u01d0 ji\u011b\", \"trans\": \"understanding\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012b l\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u89c6\u89c9\", \"pinyin\": \"sh\u00ec ju\u00e9\", \"trans\": \"visual\"},\n    {\"word\": \"\u8bed\u8a00\", \"pinyin\": \"y\u01d4 y\u00e1n\", \"trans\": \"language\"},\n    {\"word\": \"\u57fa\u7840\u6a21\u578b\", \"pinyin\": \"j\u012b ch\u01d4 m\u00f3 x\u00edng\", \"trans\": \"foundation model\"},\n    {\"word\": \"\u7531\", \"pinyin\": \"y\u00f3u\", \"trans\": \"consist of\"},\n    {\"word\": \"\u53c2\u6570\", \"pinyin\": \"c\u0101n sh\u01d4\", \"trans\": \"parameters\"},\n    {\"word\": \"\u89c6\u89c9\u7f16\u7801\u5668\", \"pinyin\": \"sh\u00ec ju\u00e9 bi\u0101n m\u01ce q\u00ec\", \"trans\": \"visual encoder\"},\n    {\"word\": \"\u6df7\u5408\u4e13\u5bb6\", \"pinyin\": \"h\u00f9n h\u00e9 zhu\u0101n ji\u0101\", \"trans\": \"mixture of experts\"},\n    {\"word\": \"LLM\", \"pinyin\": \"LLM\", \"trans\": \"LLM\"},\n    {\"word\": \"\u7ec4\u6210\", \"pinyin\": \"z\u01d4 ch\u00e9ng\", \"trans\": \"composed of\"},\n    {\"word\": \"\u5c3d\u7ba1\", \"pinyin\": \"j\u01d0n gu\u01cen\", \"trans\": \"although\"},\n    {\"word\": \"\u67b6\u6784\", \"pinyin\": \"ji\u00e0 g\u00f2u\", \"trans\": \"architecture\"},\n    {\"word\": \"\u76f8\u5bf9\", \"pinyin\": \"xi\u0101ng du\u00ec\", \"trans\": \"relatively\"},\n    {\"word\": \"\u7d27\u51d1\", \"pinyin\": \"j\u01d0n c\u00f2u\", \"trans\": \"compact\"},\n    {\"word\": \"\u4f46\", \"pinyin\": \"d\u00e0n\", \"trans\": \"but\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016b s\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u516c\u5171\", \"pinyin\": \"g\u014dng g\u00f2ng\", \"trans\": \"public\"},\n    {\"word\": \"\u57fa\u51c6\u6d4b\u8bd5\", \"pinyin\": \"j\u012b zh\u01d4n c\u00e8 sh\u00ec\", \"trans\": \"benchmark tests\"},\n    {\"word\": \"\u4ee3\u7406\u4efb\u52a1\", \"pinyin\": \"d\u00e0i l\u01d0 r\u00e8n w\u00f9\", \"trans\": \"proxy tasks\"},\n    {\"word\": \"\u4f18\u4e8e\", \"pinyin\": \"y\u014du y\u00fa\", \"trans\": \"superior to\"},\n    {\"word\": \"OpenAI\", \"pinyin\": \"OpenAI\", \"trans\": \"OpenAI\"},\n    {\"word\": \"CUA\", \"pinyin\": \"CUA\", \"trans\": \"CUA\"},\n    {\"word\": \"Claude\", \"pinyin\": \"Claude\", \"trans\": \"Claude\"},\n    {\"word\": \"\u5c55\u793a\", \"pinyin\": \"zh\u01cen sh\u00ec\", \"trans\": \"demonstrate\"},\n    {\"word\": \"\u5f3a\u5927\", \"pinyin\": \"qi\u00e1ng d\u00e0\", \"trans\": \"powerful\"},\n    {\"word\": \"\u9002\u7528\u4e8e\", \"pinyin\": \"sh\u00ec y\u00f2ng y\u00fa\", \"trans\": \"applicable to\"},\n    {\"word\": \"\u6311\u6218\", \"pinyin\": \"ti\u01ceo zh\u00e0n\", \"trans\": \"challenge\"},\n    {\"word\": \"\u5e0c\u671b\", \"pinyin\": \"x\u012b w\u00e0ng\", \"trans\": \"hope\"},\n    {\"word\": \"\u63a8\u52a8\", \"pinyin\": \"tu\u012b d\u00f2ng\", \"trans\": \"drive\"},\n    {\"word\": \"\u5e7f\u6cdb\", \"pinyin\": \"gu\u01ceng f\u00e0n\", \"trans\": \"widespread\"},\n    {\"word\": \"\u5e94\u7528\", \"pinyin\": \"y\u00ecng y\u00f2ng\", \"trans\": \"application\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 239, "total_tokens": 1370, "completion_tokens": 1131}}
[13.05.2025 09:14] Response: [
    {"word": "ä»‹ç»", "pinyin": "jiÃ¨ shÃ o", "trans": "introduce"},
    {"word": "Seed1.5-VL", "pinyin": "Seed1.5-VL", "trans": "Seed1.5-VL"},
    {"word": "æ¨è¿›", "pinyin": "tuÄ« jÃ¬n", "trans": "promote"},
    {"word": "é€šç”¨", "pinyin": "tÅng yÃ²ng", "trans": "general"},
    {"word": "å¤šæ¨¡æ€", "pinyin": "duÅ mÃ³ shuÃ i", "trans": "multimodal"},
    {"word": "ç†è§£", "pinyin": "lÇ jiÄ›", "trans": "understanding"},
    {"word": "æ¨ç†", "pinyin": "tuÄ« lÇ", "trans": "reasoning"},
    {"word": "è§†è§‰", "pinyin": "shÃ¬ juÃ©", "trans": "visual"},
    {"word": "è¯­è¨€", "pinyin": "yÇ” yÃ¡n", "trans": "language"},
    {"word": "åŸºç¡€æ¨¡å‹", "pinyin": "jÄ« chÇ” mÃ³ xÃ­ng", "trans": "foundation model"},
    {"word": "ç”±", "pinyin": "yÃ³u", "trans": "consist of"},
    {"word": "å‚æ•°", "pinyin": "cÄn shÇ”", "trans": "parameters"},
    {"word": "è§†è§‰ç¼–ç å™¨", "pinyin": "shÃ¬ juÃ© biÄn mÇ qÃ¬", "trans": "visual encoder"},
    {"word": "æ··åˆä¸“å®¶", "pinyin": "hÃ¹n hÃ© zhuÄn jiÄ", "trans": "mixture of experts"},
    {"word": "LLM", "pinyin": "LLM", "trans": "LLM"},
    {"word": "ç»„æˆ", "pinyin": "zÇ” chÃ©ng", "trans": "composed of"},
    {"word": "å°½ç®¡", "pinyin": "jÇn guÇn", "trans": "although"},
    {"word": "æ¶æ„", "pinyin": "jiÃ  gÃ²u", "trans": "architecture"},
    {"word": "ç›¸å¯¹", "pinyin": "xiÄng duÃ¬", "trans": "relatively"},
    {"word": "ç´§å‡‘", "pinyin": "jÇn cÃ²u", "trans": "compact"},
    {"word": "ä½†", "pinyin": "dÃ n", "trans": "but"},
    {"word": "è¡¨ç°", "pinyin": "biÇo xiÃ n", "trans": "performance"},
    {"word": "å‡ºè‰²", "pinyin": "chÅ« sÃ¨", "trans": "outstanding"},
    {"word": "å…¬å…±", "pinyin": "gÅng gÃ²ng", "trans": "public"},
    {"word": "åŸºå‡†æµ‹è¯•", "pinyin": "jÄ« zhÇ”n cÃ¨ shÃ¬", "trans": "benchmark tests"},
    {"word": "ä»£ç†ä»»åŠ¡", "pinyin": "dÃ i lÇ rÃ¨n wÃ¹", "trans": "proxy tasks"},
    {"word": "ä¼˜äº", "pinyin": "yÅu yÃº", "trans": "superior to"},
    {"word": "OpenAI", "pinyin": "OpenAI", "trans": "OpenAI"},
    {"word": "CUA", "pinyin": "CUA", "trans": "CUA"},
    {"word": "Claude", "pinyin": "Claude", "trans": "Claude"},
    {"word": "å±•ç¤º", "pinyin": "zhÇn shÃ¬", "trans": "demonstrate"},
    {"word": "å¼ºå¤§", "pinyin": "qiÃ¡ng dÃ ", "trans": "powerful"},
    {"word": "é€‚ç”¨äº", "pinyin": "shÃ¬ yÃ²ng yÃº", "trans": "applicable to"},
    {"word": "æŒ‘æˆ˜", "pinyin": "tiÇo zhÃ n", "trans": "challenge"},
    {"word": "å¸Œæœ›", "pinyin": "xÄ« wÃ ng", "trans": "hope"},
    {"word": "æ¨åŠ¨", "pinyin": "tuÄ« dÃ²ng", "trans": "drive"},
    {"word": "å¹¿æ³›", "pinyin": "guÇng fÃ n", "trans": "widespread"},
    {"word": "åº”ç”¨", "pinyin": "yÃ¬ng yÃ²ng", "trans": "application"}
]
[13.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æˆ‘ä»¬ä»‹ç»äº† Seed1.5-VLï¼Œä¸€ä¸ªç”¨äºæ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚å®ƒç”±ä¸€ä¸ª 532M å‚æ•°çš„è§†è§‰ç¼–ç å™¨å’Œä¸€ä¸ª 20B æ´»è·ƒå‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰LLM ç»„æˆã€‚å°½ç®¡æ¶æ„ç›¸å¯¹ç´§å‡‘ï¼Œä½†å®ƒåœ¨ 38 ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ä»£ç†ä»»åŠ¡ä¸­ä¼˜äº OpenAI CUA å’Œ Claude 3.7ã€‚Seed1.5-VL è¿˜å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€æ¨ç†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›èƒ½åŠ›èƒ½æ¨åŠ¨æ›´å¹¿æ³›çš„åº”ç”¨ã€‚
[13.05.2025 09:15] Mistral response. {"id": "748cb84f90fd43dca1354590e39c6701", "object": "chat.completion", "created": 1747127697, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduce Seed1.5-VL, a vision-language foundation model designed to advance general multimodal understanding and reasoning. It consists of a 532M parameter visual encoder and a 20B active parameter Mixture of Experts (MoE) LLM. Despite its relatively compact architecture, it performs exceptionally well on 38 public benchmarks and outperforms OpenAI CUA and Claude 3.7 in agent tasks. Seed1.5-VL also demonstrates strong reasoning capabilities, making it suitable for multimodal reasoning challenges. We hope these capabilities will drive a broader range of applications."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 205, "total_tokens": 340, "completion_tokens": 135}}
[13.05.2025 09:15] Response: We introduce Seed1.5-VL, a vision-language foundation model designed to advance general multimodal understanding and reasoning. It consists of a 532M parameter visual encoder and a 20B active parameter Mixture of Experts (MoE) LLM. Despite its relatively compact architecture, it performs exceptionally well on 38 public benchmarks and outperforms OpenAI CUA and Claude 3.7 in agent tasks. Seed1.5-VL also demonstrates strong reasoning capabilities, making it suitable for multimodal reasoning challenges. We hope these capabilities will drive a broader range of applications.
[13.05.2025 09:15] Renaming data file.
[13.05.2025 09:15] Renaming previous data. hf_papers.json to ./d/2025-05-13.json
[13.05.2025 09:15] Saving new data file.
[13.05.2025 09:15] Generating page.
[13.05.2025 09:15] Renaming previous page.
[13.05.2025 09:15] Renaming previous data. index.html to ./d/2025-05-13.html
[13.05.2025 09:15] [Experimental] Generating Chinese page for reading.
[13.05.2025 09:15] Chinese vocab [{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'}, {'word': 'Seed1.5-VL', 'pinyin': 'Seed1.5-VL', 'trans': 'Seed1.5-VL'}, {'word': 'æ¨è¿›', 'pinyin': 'tuÄ« jÃ¬n', 'trans': 'promote'}, {'word': 'é€šç”¨', 'pinyin': 'tÅng yÃ²ng', 'trans': 'general'}, {'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ shuÃ i', 'trans': 'multimodal'}, {'word': 'ç†è§£', 'pinyin': 'lÇ jiÄ›', 'trans': 'understanding'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'}, {'word': 'è¯­è¨€', 'pinyin': 'yÇ” yÃ¡n', 'trans': 'language'}, {'word': 'åŸºç¡€æ¨¡å‹', 'pinyin': 'jÄ« chÇ” mÃ³ xÃ­ng', 'trans': 'foundation model'}, {'word': 'ç”±', 'pinyin': 'yÃ³u', 'trans': 'consist of'}, {'word': 'å‚æ•°', 'pinyin': 'cÄn shÇ”', 'trans': 'parameters'}, {'word': 'è§†è§‰ç¼–ç å™¨', 'pinyin': 'shÃ¬ juÃ© biÄn mÇ qÃ¬', 'trans': 'visual encoder'}, {'word': 'æ··åˆä¸“å®¶', 'pinyin': 'hÃ¹n hÃ© zhuÄn jiÄ', 'trans': 'mixture of experts'}, {'word': 'LLM', 'pinyin': 'LLM', 'trans': 'LLM'}, {'word': 'ç»„æˆ', 'pinyin': 'zÇ” chÃ©ng', 'trans': 'composed of'}, {'word': 'å°½ç®¡', 'pinyin': 'jÇn guÇn', 'trans': 'although'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'}, {'word': 'ç›¸å¯¹', 'pinyin': 'xiÄng duÃ¬', 'trans': 'relatively'}, {'word': 'ç´§å‡‘', 'pinyin': 'jÇn cÃ²u', 'trans': 'compact'}, {'word': 'ä½†', 'pinyin': 'dÃ n', 'trans': 'but'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}, {'word': 'å…¬å…±', 'pinyin': 'gÅng gÃ²ng', 'trans': 'public'}, {'word': 'åŸºå‡†æµ‹è¯•', 'pinyin': 'jÄ« zhÇ”n cÃ¨ shÃ¬', 'trans': 'benchmark tests'}, {'word': 'ä»£ç†ä»»åŠ¡', 'pinyin': 'dÃ i lÇ rÃ¨n wÃ¹', 'trans': 'proxy tasks'}, {'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}, {'word': 'OpenAI', 'pinyin': 'OpenAI', 'trans': 'OpenAI'}, {'word': 'CUA', 'pinyin': 'CUA', 'trans': 'CUA'}, {'word': 'Claude', 'pinyin': 'Claude', 'trans': 'Claude'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'}, {'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ng dÃ ', 'trans': 'powerful'}, {'word': 'é€‚ç”¨äº', 'pinyin': 'shÃ¬ yÃ²ng yÃº', 'trans': 'applicable to'}, {'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'}, {'word': 'å¸Œæœ›', 'pinyin': 'xÄ« wÃ ng', 'trans': 'hope'}, {'word': 'æ¨åŠ¨', 'pinyin': 'tuÄ« dÃ²ng', 'trans': 'drive'}, {'word': 'å¹¿æ³›', 'pinyin': 'guÇng fÃ n', 'trans': 'widespread'}, {'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'application'}]
[13.05.2025 09:15] Renaming previous Chinese page.
[13.05.2025 09:15] Renaming previous data. zh.html to ./d/2025-05-12_zh_reading_task.html
[13.05.2025 09:15] Writing Chinese reading task.
[13.05.2025 09:15] Writing result.
[13.05.2025 09:15] Renaming log file.
[13.05.2025 09:15] Renaming previous data. log.txt to ./logs/2025-05-13_last_log.txt
