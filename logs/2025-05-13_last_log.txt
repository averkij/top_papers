[13.05.2025 08:16] Read previous papers.
[13.05.2025 08:16] Generating top page (month).
[13.05.2025 08:16] Writing top page (month).
[13.05.2025 09:12] Read previous papers.
[13.05.2025 09:12] Get feed.
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07062
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07608
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07747
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07787
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06548
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07447
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07818
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07293
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03733
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07796
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07596
[13.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07819
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07793
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06324
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04918
[13.05.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07260
[13.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07086
[13.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.05.2025 09:12] No deleted papers detected.
[13.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 17.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07062.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07062.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07062.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07608.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07608.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07608.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07747.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07747.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07747.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07787.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07787.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07787.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.06548.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.06548.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.06548.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07447.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07447.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07447.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07818.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07818.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07818.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07293.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07293.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07293.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.03733.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.03733.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.03733.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07796.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07796.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07796.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07596.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07596.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07596.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07819.
[13.05.2025 09:12] Downloading paper 2505.07819 from http://arxiv.org/pdf/2505.07819v1...
[13.05.2025 09:12] Extracting affiliations from text.
[13.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 9 1 8 7 0 . 5 0 5 2 : r H3DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning Yiyang Lu1, Yufeng Tian4, Zhecheng Yuan1,2,3, Xianbang Wang1, Pu Hua1,2,3, Zhengrong Xue1,2,3, Huazhe Xu1,2,3 1 Tsinghua University IIIS, 2 Shanghai Qi Zhi Institute, 3 Shanghai AI Lab, 4 Harbin Institute of Technology luyy24@mails.tsinghua.edu.cn, huazhe xu@mail.tsinghua.edu.cn Figure 1: H3DP can not only achieve superior performance across 44 tasks on 5 simulation benchmarks, but also handle long-horizon challenging manipulation tasks in cluttered real-world scenarios. Abstract: Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy (H3DP), novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H3DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H3DP yields +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/. Keywords: Imitation Learning, Representation Learning, Diffusion Model Equal Contribution Visuomotor policy learning has emerged as prevailing paradigm in robotic manipulation [1, 2, 3, 4, 5]. Existing approaches have increasi"
[13.05.2025 09:12] Response: ```python
[
    "Tsinghua University IIIS",
    "Shanghai Qi Zhi Institute",
    "Shanghai AI Lab",
    "Harbin Institute of Technology"
]
```
[13.05.2025 09:12] Deleting PDF ./assets/pdf/2505.07819.pdf.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07793.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07793.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07793.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.06324.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.06324.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.06324.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04918.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04918.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04918.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07260.
[13.05.2025 09:12] Downloading paper 2505.07260 from http://arxiv.org/pdf/2505.07260v1...
[13.05.2025 09:12] Extracting affiliations from text.
[13.05.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 0 6 2 7 0 . 5 0 5 2 : r UMoE: Unifying Attention and FFN with Shared Experts Yuanhang Yang1, Chaozheng Wang2, Jing Li3 1Institute of Science Tokyo, Tokyo, Japan 2The Chinese University of Hong Kong, Hong Kong, China 3Hong Kong Polytechnic University, Hong Kong, China yang.y.ea2c@m.isct.ac.jp czwang23@cse.cuhk.edu.hk jing-amelia.li@polyu.edu.hk "
[13.05.2025 09:12] Response: ```python
[
    "Institute of Science Tokyo, Tokyo, Japan",
    "The Chinese University of Hong Kong, Hong Kong, China",
    "Hong Kong Polytechnic University, Hong Kong, China"
]
```
[13.05.2025 09:12] Deleting PDF ./assets/pdf/2505.07260.pdf.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.07086.
[13.05.2025 09:12] Extra JSON file exists (./assets/json/2505.07086.json), skip PDF parsing.
[13.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.07086.json), skip HTML parsing.
[13.05.2025 09:12] Success.
[13.05.2025 09:12] Enriching papers with extra data.
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 0. We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 1. We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 2. While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 3. Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 4. Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavo...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 5. Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 6. Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face criti...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 7. Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biase...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 8. LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructi...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 9. Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain perfo...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 10. Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 11. Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In t...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 12. A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are ...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 13. As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge,...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 14. Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-i...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 15. Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model perfo...
[13.05.2025 09:12] ********************************************************************************
[13.05.2025 09:12] Abstract 16. Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing ap...
[13.05.2025 09:12] Read previous papers.
[13.05.2025 09:12] Generating reviews via LLM API.
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#survey", "#architecture", "#training", "#reasoning", "#data"], "emoji": "🧠", "ru": {"title": "Компактная мультимодальная модель с выдающимися способностями", "desc": "Seed1.5-VL - это мультимодальная модель, сочетающая зрение и язык для общего понимания и рас
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#plp", "#reasoning", "#optimization", "#dataset", "#math", "#rl", "#data", "#training"], "emoji": "🧠", "ru": {"title": "MiMo-7B: Мощная языковая модель для сложных рассуждений", "desc": "MiMo-7B - это большая языковая модель, оптимизированная для задач рассуждения. В процессе предва
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#data", "#diffusion", "#transfer_learning", "#3d", "#benchmark"], "emoji": "🧊", "ru": {"title": "Открытая платформа для AI-генерации 3D-объектов нового поколения", "desc": "Статья представляет Step1X-3D - открытую систему для генерации 3D
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#math", "#training", "#small_models", "#reasoning", "#dataset"], "emoji": "🧠", "ru": {"title": "Коллективный разум: как модели машинного обучения учатся друг у друга", "desc": "Исследование показывает, что крупные модели рассуждений (LRM) могут попадать в 'ловушку д
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#small_models", "#rl", "#dataset", "#data"], "emoji": "🤖", "ru": {"title": "Малые модели и RL улучшают генерацию инструкций для обучения больших ЯМ", "desc": "Статья исследует эффективность малых открытых языковых моделей (LLaMA 2-7B, LL
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#training", "#cv"], "emoji": "🔄", "ru": {"title": "Единый фреймворк для непрерывных генеративных моделей: от многошаговых до малошаговых", "desc": "Статья представляет унифицированный подход к обучению и сэмплированию непрерывных генеративных моделей. 
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#alignment", "#optimization", "#video", "#multimodal", "#rl", "#diffusion", "#benchmark", "#rlhf"], "emoji": "🎨", "ru": {"title": "DanceGRPO: Революция в обучении с подкреплением для генерации визуального контента", "desc": "Статья представляет DanceGRPO - унифицированный фреймворк 
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#dataset", "#benchmark", "#data", "#optimization", "#small_models"], "emoji": "🧠", "ru": {"title": "Улучшение способности ЯМ к рассуждениям через умный отбор данных", "desc": "Статья представляет метод AttentionInfluence для отбора данных, улучшающих спосо
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#agents", "#dataset"], "emoji": "🌐", "ru": {"title": "WebGen-Bench: Новый рубеж в оценке LLM-агентов для веб-разработки", "desc": "WebGen-Bench - это новый бенчмарк для оценки способности LLM-агентов создавать многофайловые веб-с
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training"], "emoji": "📈", "ru": {"title": "Раскрытие секретов непрерывного предобучения языковых моделей", "desc": "Статья исследует динамику обучения при непрерывном предобучении (CPT) больших языковых моделей. Авторы наблюдают, как меняется 
[13.05.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#agents", "#optimization", "#hallucinations", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "Умный поиск: когда искать, а когда довериться себе", "desc": "Статья представляет новый подход к улучшению работы больших языковых моделей (LLM) с использованием м
[13.05.2025 09:12] Querying the API.
[13.05.2025 09:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.
[13.05.2025 09:13] Response: {
  "desc": "Статья представляет новый подход к обучению визуомоторной политики для робототехнической манипуляции - Триединую Иерархическую Диффузионную Политику (H^3DP). Этот метод использует трехуровневую иерархию: послойную организацию входных данных с учетом глубины, многомасштабное представление визуальных признаков и иерархически обусловленный процесс диффузии для генерации действий. H^3DP усиливает интеграцию между визуальным восприятием и предсказанием действий. Эксперименты показывают значительное улучшение производительности как в симуляции, так и в реальных задачах манипуляции.",

  "emoji": "🤖",

  "title": "Иерархическое обучение для улучшения визуомоторного контроля роботов"
}
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."

[13.05.2025 09:13] Response: ```python
["AGENTS", "ROBOTICS"]
```
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/."

[13.05.2025 09:13] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.","title":"Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.', title='Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration'))
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的视觉运动策略学习框架，称为三层次扩散策略（H^{\\textbf{3}DP）。该框架通过引入层次结构，增强了视觉特征与动作生成之间的结合。H^{3}DP包含三个层次：基于深度信息的输入分层、多尺度视觉表示和层次条件扩散过程。实验结果表明，H^{3}DP在44个仿真任务中相较于基线方法平均提高了27.5%的性能，并在四个复杂的双手真实世界操作任务中表现优异。","title":"增强视觉与动作生成的三层次学习框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的视觉运动策略学习框架，称为三层次扩散策略（H^{\textbf{3}DP）。该框架通过引入层次结构，增强了视觉特征与动作生成之间的结合。H^{3}DP包含三个层次：基于深度信息的输入分层、多尺度视觉表示和层次条件扩散过程。实验结果表明，H^{3}DP在44个仿真任务中相较于基线方法平均提高了27.5%的性能，并在四个复杂的双手真实世界操作任务中表现优异。', title='增强视觉与动作生成的三层次学习框架'))
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture", "#long_context"], "emoji": "🧠", "ru": {"title": "Чанки побеждают рекуррентность в обработке длинных контекстов", "desc": "Исследование фокусируется на рекуррентных суб-квадратичных моделях для обработки длинных контекстов в больших языковых
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "🔍", "ru": {"title": "Повышение надежности атрибуции в LLM: от текстового включения до механизма внимания", "desc": "Эта статья посвящена проблеме атрибуции в больших языковых моделях (LLM) при работе с док
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#graphs", "#architecture", "#training"], "emoji": "🌎", "ru": {"title": "PASSAT: Физически обоснованный прогноз погоды с учетом топологии Земли", "desc": "PASSAT - это новая модель глубокого обучения для прогнозирования погоды, учитывающая физику
[13.05.2025 09:13] Querying the API.
[13.05.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.
[13.05.2025 09:13] Response: {
  "desc": "Статья представляет новый подход к архитектуре Разреженной смеси экспертов (Sparse Mixture of Experts, MoE) для масштабирования моделей Transformer. Авторы предлагают унифицированный дизайн MoE для слоев внимания и полносвязных слоев, переформулируя механизм внимания. Новая архитектура, названная UMoE, демонстрирует улучшенную производительность за счет MoE в слоях внимания. UMoE также позволяет эффективно разделять параметры между компонентами внимания и полносвязными слоями.",
  "emoji": "🧠",
  "title": "Унификация MoE в Transformer: повышение эффективности через переосмысление внимания"
}
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components."

[13.05.2025 09:13] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[13.05.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components."

[13.05.2025 09:13] Response: ```python
["OPTIMIZATION"]
```
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.","title":"Unifying MoE for Enhanced Transformer Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.', title='Unifying MoE for Enhanced Transformer Performance'))
[13.05.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"稀疏专家混合（MoE）架构是一种有前景的方法，用于扩展Transformer模型。虽然早期的研究主要将MoE应用于前馈网络（FFN）层，但最近的研究开始探索将MoE扩展到注意力层，以提高模型性能。现有的基于注意力的MoE层需要专门的实现，并且与基于FFN的层相比，性能不尽如人意。本文提出了一种新颖的注意力机制重构，统一了注意力层和FFN层中的MoE设计，提出的UMoE架构通过基于注意力的MoE层实现了更优的性能，同时实现了FFN和注意力组件之间的高效参数共享。","title":"统一注意力与前馈网络的稀疏专家架构"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='稀疏专家混合（MoE）架构是一种有前景的方法，用于扩展Transformer模型。虽然早期的研究主要将MoE应用于前馈网络（FFN）层，但最近的研究开始探索将MoE扩展到注意力层，以提高模型性能。现有的基于注意力的MoE层需要专门的实现，并且与基于FFN的层相比，性能不尽如人意。本文提出了一种新颖的注意力机制重构，统一了注意力层和FFN层中的MoE设计，提出的UMoE架构通过基于注意力的MoE层实现了更优的性能，同时实现了FFN和注意力组件之间的高效参数共享。', title='统一注意力与前馈网络的稀疏专家架构'))
[13.05.2025 09:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#science", "#data", "#optimization"], "emoji": "🧬", "ru": {"title": "Многоцелевая оптимизация биологических последовательностей с помощью дискретного сопоставления потоков", "desc": "Статья представляет новый метод машинного обучения под названием Multi-Obje
[13.05.2025 09:13] Trying to get texts in Chinese.
[13.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)
[13.05.2025 09:13] Mistral response. {"id": "319ea83926f248ccbec47a113e2d443d", "object": "chat.completion", "created": 1747127603, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86 Seed1.5-VL\uff0c\u4e00\u4e2a\u7528\u4e8e\u63a8\u8fdb\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u3002\u5b83\u7531\u4e00\u4e2a 532M \u53c2\u6570\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e00\u4e2a 20B \u6d3b\u8dc3\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09LLM \u7ec4\u6210\u3002\u5c3d\u7ba1\u67b6\u6784\u76f8\u5bf9\u7d27\u51d1\uff0c\u4f46\u5b83\u5728 38 \u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e OpenAI CUA \u548c Claude 3.7\u3002Seed1.5-VL \u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u63a8\u7406\u6311\u6218\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u80fd\u529b\u80fd\u63a8\u52a8\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 337, "total_tokens": 530, "completion_tokens": 193}}
[13.05.2025 09:13] Response: 我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。
[13.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。
[13.05.2025 09:14] Mistral response. {"id": "78f809fdd46e4d95a3a3ab22c94ffbcc", "object": "chat.completion", "created": 1747127606, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86 Seed1.5-VL\uff0c\u4e00\u4e2a\u7528\u4e8e\u63a8\u8fdb\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u3002\u5b83\u7531\u4e00\u4e2a 532M \u53c2\u6570\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e00\u4e2a 20B \u6d3b\u8dc3\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09LLM \u7ec4\u6210\u3002\u5c3d\u7ba1\u67b6\u6784\u76f8\u5bf9\u7d27\u51d1\uff0c\u4f46\u5b83\u5728 38 \u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e OpenAI CUA \u548c Claude 3.7\u3002Seed1.5-VL \u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u63a8\u7406\u6311\u6218\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e9b\u80fd\u529b\u80fd\u63a8\u52a8\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\n\nW\u01d2men ji\u00e8sh\u00e0o le Seed1.5-VL, y\u012bg\u00e8 y\u00f2ngy\u00fa tu\u012bj\u00ecn t\u014dngy\u00f2ng du\u014d m\u00f3sh\u00ec l\u01d0ji\u011b h\u00e9 tu\u012bl\u01d0 de sh\u00ecju\u00e9-y\u01d4y\u00e1n j\u012bch\u01d4 m\u00f3x\u00edng. T\u0101 y\u00f3u y\u012bg\u00e8 532M c\u0101nsh\u00f9 de sh\u00ecju\u00e9 bi\u0101nm\u01ceq\u00ec h\u00e9 y\u012bg\u00e8 20B hu\u00f3yu\u00e8 c\u0101nsh\u00f9 de h\u00f9nh\u00e9 zhu\u0101nji\u0101 (MoE) LLM z\u01d4ch\u00e9ng. J\u01d0ngu\u01cen ji\u00e0g\u00f2u xi\u0101ngdu\u00ec j\u01d0ncu\u01d2, d\u00e0n t\u0101 z\u00e0i 38 g\u00e8 g\u014dngg\u00f2ng j\u012bzh\u01d4n c\u00e8sh\u00ec zh\u014dng bi\u01ceoxi\u00e0n ch\u016bs\u00e8, b\u00ecng z\u00e0i d\u00e0il\u01d0 r\u00e8nw\u00f9 zh\u014dng y\u014duy\u00fa OpenAI CUA h\u00e9 Claude 3.7. Seed1.5-VL h\u00e1i zh\u01censh\u00ec le qi\u00e1ngd\u00e0 de tu\u012bl\u01d0 n\u00e9ngl\u00ec, sh\u00ecy\u00f2ng y\u00fa du\u014d m\u00f3sh\u00ec tu\u012bl\u01d0 ti\u01ceozh\u00e0n. W\u01d2men x\u012bw\u00e0ng zh\u00e8xi\u0113 n\u00e9ngl\u00ec n\u00e9ng tu\u012bd\u00f2ng g\u00e8ng gu\u01cengf\u00e0n de y\u00ecngy\u00f2ng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 207, "total_tokens": 746, "completion_tokens": 539}}
[13.05.2025 09:14] Response: 我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。

Wǒmen jièshào le Seed1.5-VL, yīgè yòngyú tuījìn tōngyòng duō móshì lǐjiě hé tuīlǐ de shìjué-yǔyán jīchǔ móxíng. Tā yóu yīgè 532M cānshù de shìjué biānmǎqì hé yīgè 20B huóyuè cānshù de hùnhé zhuānjiā (MoE) LLM zǔchéng. Jǐnguǎn jiàgòu xiāngduì jǐncuǒ, dàn tā zài 38 gè gōnggòng jīzhǔn cèshì zhōng biǎoxiàn chūsè, bìng zài dàilǐ rènwù zhōng yōuyú OpenAI CUA hé Claude 3.7. Seed1.5-VL hái zhǎnshì le qiángdà de tuīlǐ nénglì, shìyòng yú duō móshì tuīlǐ tiǎozhàn. Wǒmen xīwàng zhèxiē nénglì néng tuīdòng gèng guǎngfàn de yìngyòng.
[13.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。
[13.05.2025 09:14] Mistral response. {"id": "fce996f36061495496e072382d4222cf", "object": "chat.completion", "created": 1747127646, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8 sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"Seed1.5-VL\", \"pinyin\": \"Seed1.5-VL\", \"trans\": \"Seed1.5-VL\"},\n    {\"word\": \"\u63a8\u8fdb\", \"pinyin\": \"tu\u012b j\u00ecn\", \"trans\": \"promote\"},\n    {\"word\": \"\u901a\u7528\", \"pinyin\": \"t\u014dng y\u00f2ng\", \"trans\": \"general\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 shu\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u7406\u89e3\", \"pinyin\": \"l\u01d0 ji\u011b\", \"trans\": \"understanding\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012b l\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u89c6\u89c9\", \"pinyin\": \"sh\u00ec ju\u00e9\", \"trans\": \"visual\"},\n    {\"word\": \"\u8bed\u8a00\", \"pinyin\": \"y\u01d4 y\u00e1n\", \"trans\": \"language\"},\n    {\"word\": \"\u57fa\u7840\u6a21\u578b\", \"pinyin\": \"j\u012b ch\u01d4 m\u00f3 x\u00edng\", \"trans\": \"foundation model\"},\n    {\"word\": \"\u7531\", \"pinyin\": \"y\u00f3u\", \"trans\": \"consist of\"},\n    {\"word\": \"\u53c2\u6570\", \"pinyin\": \"c\u0101n sh\u01d4\", \"trans\": \"parameters\"},\n    {\"word\": \"\u89c6\u89c9\u7f16\u7801\u5668\", \"pinyin\": \"sh\u00ec ju\u00e9 bi\u0101n m\u01ce q\u00ec\", \"trans\": \"visual encoder\"},\n    {\"word\": \"\u6df7\u5408\u4e13\u5bb6\", \"pinyin\": \"h\u00f9n h\u00e9 zhu\u0101n ji\u0101\", \"trans\": \"mixture of experts\"},\n    {\"word\": \"LLM\", \"pinyin\": \"LLM\", \"trans\": \"LLM\"},\n    {\"word\": \"\u7ec4\u6210\", \"pinyin\": \"z\u01d4 ch\u00e9ng\", \"trans\": \"composed of\"},\n    {\"word\": \"\u5c3d\u7ba1\", \"pinyin\": \"j\u01d0n gu\u01cen\", \"trans\": \"although\"},\n    {\"word\": \"\u67b6\u6784\", \"pinyin\": \"ji\u00e0 g\u00f2u\", \"trans\": \"architecture\"},\n    {\"word\": \"\u76f8\u5bf9\", \"pinyin\": \"xi\u0101ng du\u00ec\", \"trans\": \"relatively\"},\n    {\"word\": \"\u7d27\u51d1\", \"pinyin\": \"j\u01d0n c\u00f2u\", \"trans\": \"compact\"},\n    {\"word\": \"\u4f46\", \"pinyin\": \"d\u00e0n\", \"trans\": \"but\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016b s\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u516c\u5171\", \"pinyin\": \"g\u014dng g\u00f2ng\", \"trans\": \"public\"},\n    {\"word\": \"\u57fa\u51c6\u6d4b\u8bd5\", \"pinyin\": \"j\u012b zh\u01d4n c\u00e8 sh\u00ec\", \"trans\": \"benchmark tests\"},\n    {\"word\": \"\u4ee3\u7406\u4efb\u52a1\", \"pinyin\": \"d\u00e0i l\u01d0 r\u00e8n w\u00f9\", \"trans\": \"proxy tasks\"},\n    {\"word\": \"\u4f18\u4e8e\", \"pinyin\": \"y\u014du y\u00fa\", \"trans\": \"superior to\"},\n    {\"word\": \"OpenAI\", \"pinyin\": \"OpenAI\", \"trans\": \"OpenAI\"},\n    {\"word\": \"CUA\", \"pinyin\": \"CUA\", \"trans\": \"CUA\"},\n    {\"word\": \"Claude\", \"pinyin\": \"Claude\", \"trans\": \"Claude\"},\n    {\"word\": \"\u5c55\u793a\", \"pinyin\": \"zh\u01cen sh\u00ec\", \"trans\": \"demonstrate\"},\n    {\"word\": \"\u5f3a\u5927\", \"pinyin\": \"qi\u00e1ng d\u00e0\", \"trans\": \"powerful\"},\n    {\"word\": \"\u9002\u7528\u4e8e\", \"pinyin\": \"sh\u00ec y\u00f2ng y\u00fa\", \"trans\": \"applicable to\"},\n    {\"word\": \"\u6311\u6218\", \"pinyin\": \"ti\u01ceo zh\u00e0n\", \"trans\": \"challenge\"},\n    {\"word\": \"\u5e0c\u671b\", \"pinyin\": \"x\u012b w\u00e0ng\", \"trans\": \"hope\"},\n    {\"word\": \"\u63a8\u52a8\", \"pinyin\": \"tu\u012b d\u00f2ng\", \"trans\": \"drive\"},\n    {\"word\": \"\u5e7f\u6cdb\", \"pinyin\": \"gu\u01ceng f\u00e0n\", \"trans\": \"widespread\"},\n    {\"word\": \"\u5e94\u7528\", \"pinyin\": \"y\u00ecng y\u00f2ng\", \"trans\": \"application\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 239, "total_tokens": 1370, "completion_tokens": 1131}}
[13.05.2025 09:14] Response: [
    {"word": "介绍", "pinyin": "jiè shào", "trans": "introduce"},
    {"word": "Seed1.5-VL", "pinyin": "Seed1.5-VL", "trans": "Seed1.5-VL"},
    {"word": "推进", "pinyin": "tuī jìn", "trans": "promote"},
    {"word": "通用", "pinyin": "tōng yòng", "trans": "general"},
    {"word": "多模态", "pinyin": "duō mó shuài", "trans": "multimodal"},
    {"word": "理解", "pinyin": "lǐ jiě", "trans": "understanding"},
    {"word": "推理", "pinyin": "tuī lǐ", "trans": "reasoning"},
    {"word": "视觉", "pinyin": "shì jué", "trans": "visual"},
    {"word": "语言", "pinyin": "yǔ yán", "trans": "language"},
    {"word": "基础模型", "pinyin": "jī chǔ mó xíng", "trans": "foundation model"},
    {"word": "由", "pinyin": "yóu", "trans": "consist of"},
    {"word": "参数", "pinyin": "cān shǔ", "trans": "parameters"},
    {"word": "视觉编码器", "pinyin": "shì jué biān mǎ qì", "trans": "visual encoder"},
    {"word": "混合专家", "pinyin": "hùn hé zhuān jiā", "trans": "mixture of experts"},
    {"word": "LLM", "pinyin": "LLM", "trans": "LLM"},
    {"word": "组成", "pinyin": "zǔ chéng", "trans": "composed of"},
    {"word": "尽管", "pinyin": "jǐn guǎn", "trans": "although"},
    {"word": "架构", "pinyin": "jià gòu", "trans": "architecture"},
    {"word": "相对", "pinyin": "xiāng duì", "trans": "relatively"},
    {"word": "紧凑", "pinyin": "jǐn còu", "trans": "compact"},
    {"word": "但", "pinyin": "dàn", "trans": "but"},
    {"word": "表现", "pinyin": "biǎo xiàn", "trans": "performance"},
    {"word": "出色", "pinyin": "chū sè", "trans": "outstanding"},
    {"word": "公共", "pinyin": "gōng gòng", "trans": "public"},
    {"word": "基准测试", "pinyin": "jī zhǔn cè shì", "trans": "benchmark tests"},
    {"word": "代理任务", "pinyin": "dài lǐ rèn wù", "trans": "proxy tasks"},
    {"word": "优于", "pinyin": "yōu yú", "trans": "superior to"},
    {"word": "OpenAI", "pinyin": "OpenAI", "trans": "OpenAI"},
    {"word": "CUA", "pinyin": "CUA", "trans": "CUA"},
    {"word": "Claude", "pinyin": "Claude", "trans": "Claude"},
    {"word": "展示", "pinyin": "zhǎn shì", "trans": "demonstrate"},
    {"word": "强大", "pinyin": "qiáng dà", "trans": "powerful"},
    {"word": "适用于", "pinyin": "shì yòng yú", "trans": "applicable to"},
    {"word": "挑战", "pinyin": "tiǎo zhàn", "trans": "challenge"},
    {"word": "希望", "pinyin": "xī wàng", "trans": "hope"},
    {"word": "推动", "pinyin": "tuī dòng", "trans": "drive"},
    {"word": "广泛", "pinyin": "guǎng fàn", "trans": "widespread"},
    {"word": "应用", "pinyin": "yìng yòng", "trans": "application"}
]
[13.05.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

我们介绍了 Seed1.5-VL，一个用于推进通用多模态理解和推理的视觉-语言基础模型。它由一个 532M 参数的视觉编码器和一个 20B 活跃参数的混合专家（MoE）LLM 组成。尽管架构相对紧凑，但它在 38 个公共基准测试中表现出色，并在代理任务中优于 OpenAI CUA 和 Claude 3.7。Seed1.5-VL 还展示了强大的推理能力，适用于多模态推理挑战。我们希望这些能力能推动更广泛的应用。
[13.05.2025 09:15] Mistral response. {"id": "748cb84f90fd43dca1354590e39c6701", "object": "chat.completion", "created": 1747127697, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduce Seed1.5-VL, a vision-language foundation model designed to advance general multimodal understanding and reasoning. It consists of a 532M parameter visual encoder and a 20B active parameter Mixture of Experts (MoE) LLM. Despite its relatively compact architecture, it performs exceptionally well on 38 public benchmarks and outperforms OpenAI CUA and Claude 3.7 in agent tasks. Seed1.5-VL also demonstrates strong reasoning capabilities, making it suitable for multimodal reasoning challenges. We hope these capabilities will drive a broader range of applications."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 205, "total_tokens": 340, "completion_tokens": 135}}
[13.05.2025 09:15] Response: We introduce Seed1.5-VL, a vision-language foundation model designed to advance general multimodal understanding and reasoning. It consists of a 532M parameter visual encoder and a 20B active parameter Mixture of Experts (MoE) LLM. Despite its relatively compact architecture, it performs exceptionally well on 38 public benchmarks and outperforms OpenAI CUA and Claude 3.7 in agent tasks. Seed1.5-VL also demonstrates strong reasoning capabilities, making it suitable for multimodal reasoning challenges. We hope these capabilities will drive a broader range of applications.
[13.05.2025 09:15] Renaming data file.
[13.05.2025 09:15] Renaming previous data. hf_papers.json to ./d/2025-05-13.json
[13.05.2025 09:15] Saving new data file.
[13.05.2025 09:15] Generating page.
[13.05.2025 09:15] Renaming previous page.
[13.05.2025 09:15] Renaming previous data. index.html to ./d/2025-05-13.html
[13.05.2025 09:15] [Experimental] Generating Chinese page for reading.
[13.05.2025 09:15] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': 'Seed1.5-VL', 'pinyin': 'Seed1.5-VL', 'trans': 'Seed1.5-VL'}, {'word': '推进', 'pinyin': 'tuī jìn', 'trans': 'promote'}, {'word': '通用', 'pinyin': 'tōng yòng', 'trans': 'general'}, {'word': '多模态', 'pinyin': 'duō mó shuài', 'trans': 'multimodal'}, {'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understanding'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'}, {'word': '基础模型', 'pinyin': 'jī chǔ mó xíng', 'trans': 'foundation model'}, {'word': '由', 'pinyin': 'yóu', 'trans': 'consist of'}, {'word': '参数', 'pinyin': 'cān shǔ', 'trans': 'parameters'}, {'word': '视觉编码器', 'pinyin': 'shì jué biān mǎ qì', 'trans': 'visual encoder'}, {'word': '混合专家', 'pinyin': 'hùn hé zhuān jiā', 'trans': 'mixture of experts'}, {'word': 'LLM', 'pinyin': 'LLM', 'trans': 'LLM'}, {'word': '组成', 'pinyin': 'zǔ chéng', 'trans': 'composed of'}, {'word': '尽管', 'pinyin': 'jǐn guǎn', 'trans': 'although'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '相对', 'pinyin': 'xiāng duì', 'trans': 'relatively'}, {'word': '紧凑', 'pinyin': 'jǐn còu', 'trans': 'compact'}, {'word': '但', 'pinyin': 'dàn', 'trans': 'but'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '公共', 'pinyin': 'gōng gòng', 'trans': 'public'}, {'word': '基准测试', 'pinyin': 'jī zhǔn cè shì', 'trans': 'benchmark tests'}, {'word': '代理任务', 'pinyin': 'dài lǐ rèn wù', 'trans': 'proxy tasks'}, {'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}, {'word': 'OpenAI', 'pinyin': 'OpenAI', 'trans': 'OpenAI'}, {'word': 'CUA', 'pinyin': 'CUA', 'trans': 'CUA'}, {'word': 'Claude', 'pinyin': 'Claude', 'trans': 'Claude'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '强大', 'pinyin': 'qiáng dà', 'trans': 'powerful'}, {'word': '适用于', 'pinyin': 'shì yòng yú', 'trans': 'applicable to'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '希望', 'pinyin': 'xī wàng', 'trans': 'hope'}, {'word': '推动', 'pinyin': 'tuī dòng', 'trans': 'drive'}, {'word': '广泛', 'pinyin': 'guǎng fàn', 'trans': 'widespread'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'}]
[13.05.2025 09:15] Renaming previous Chinese page.
[13.05.2025 09:15] Renaming previous data. zh.html to ./d/2025-05-12_zh_reading_task.html
[13.05.2025 09:15] Writing Chinese reading task.
[13.05.2025 09:15] Writing result.
[13.05.2025 09:15] Renaming log file.
[13.05.2025 09:15] Renaming previous data. log.txt to ./logs/2025-05-13_last_log.txt
