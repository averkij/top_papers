[13.05.2025 12:22] Read previous papers.
[13.05.2025 12:22] Generating top page (month).
[13.05.2025 12:22] Writing top page (month).
[13.05.2025 13:27] Read previous papers.
[13.05.2025 13:27] Get feed.
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07062
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07608
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07747
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07787
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07447
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06548
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07818
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07293
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03733
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07263
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07796
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07596
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07819
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07812
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07793
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07260
[13.05.2025 13:27] Extract page data from URL. URL: https://huggingface.co/papers/2505.06176
[13.05.2025 13:27] Extract page data from URL. URL: https://huggingface.co/papers/2505.00612
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06324
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04918
[13.05.2025 13:27] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07086
[13.05.2025 13:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.05.2025 13:27] No deleted papers detected.
[13.05.2025 13:27] Downloading and parsing papers (pdf, html). Total: 21.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07062.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07062.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07062.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07608.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07608.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07608.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07747.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07747.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07747.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07787.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07787.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07787.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07447.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07447.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07447.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.06548.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.06548.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.06548.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07818.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07818.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07818.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07293.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07293.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07293.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.03733.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.03733.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.03733.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07263.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07263.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07263.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07796.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07796.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07796.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07596.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07596.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07596.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07819.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07819.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07819.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07812.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07812.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07812.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07793.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07793.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07793.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07260.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07260.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07260.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.06176.
[13.05.2025 13:27] Downloading paper 2505.06176 from http://arxiv.org/pdf/2505.06176v1...
[13.05.2025 13:27] Extracting affiliations from text.
[13.05.2025 13:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MonetGPT: Solving Puzzles Enhances MLLMs Image Retouching Skills NILADRI SHEKHAR DUTT, University College London, UK DUYGU CEYLAN, Adobe Research, UK NILOY J. MITRA, University College London, Adobe Research, UK 5 2 0 2 9 ] . [ 1 6 7 1 6 0 . 5 0 5 2 : r Fig. 1. We present MonetGPT, an image operation-aware multimodal large language model (MLLM), that provides automatic suggestions for image retouching. Given photograph (left), MonetGPT analyzes it to identify set of issues and possible adjustments to fix them. The solution steps are then translated to set of procedural operations, along with respective parameter settings, drawing from given library of operations, which occurs in three stages. (Visual puzzles, on which we train the MLLM, are not shown here.) Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially-designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given set of expert-edited photos, we synthesize reasoning dataset by procedurally manipulating the expert edits and then grounding pretrained LLM on the visua"
[13.05.2025 13:27] Response: ```python
[
    "University College London, UK",
    "Adobe Research, UK",
    "University College London, Adobe Research, UK"
]
```
[13.05.2025 13:27] Deleting PDF ./assets/pdf/2505.06176.pdf.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.00612.
[13.05.2025 13:27] Downloading paper 2505.00612 from http://arxiv.org/pdf/2505.00612v1...
[13.05.2025 13:27] Extracting affiliations from text.
[13.05.2025 13:27] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation D. Sculley 1 Will Cukierski 2 Phil Culliton 2 Sohier Dane 2 Maggie Demkin 2 Ryan Holbrook 2 Addison Howard 2 Paul Mooney 2 Walter Reade 2 Megan Risdal 2 Nate Keating 2 5 2 0 2 1 ] . [ 1 2 1 6 0 0 . 5 0 5 2 : r Abstract In this position paper, we observe that empirical evaluation in Generative AI is at crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of leakage and contamination are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value. 1. Introduction As Generative AI (GenAI) models such as Large Language Models (LLMs) become ever more important to the field and to the world at large, it has become clear that performing empirical evaluation of these models and methods is extremely difficult to do in rigorous and comprehensive way. This difficulty is of course not due to lack of effort or expertise by researchers. Indeed, enormous effort and resources have been poured into creating myriad benchmarks and test cases (Chiang et al., 2024b; Fourrier et al., 202"
[13.05.2025 13:27] Response: ```python
[]
```
[13.05.2025 13:27] Extracting affiliations from text.
[13.05.2025 13:27] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation D. Sculley 1 Will Cukierski 2 Phil Culliton 2 Sohier Dane 2 Maggie Demkin 2 Ryan Holbrook 2 Addison Howard 2 Paul Mooney 2 Walter Reade 2 Megan Risdal 2 Nate Keating 2 5 2 0 2 1 ] . [ 1 2 1 6 0 0 . 5 0 5 2 : r Abstract In this position paper, we observe that empirical evaluation in Generative AI is at crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of leakage and contamination are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value. 1. Introduction As Generative AI (GenAI) models such as Large Language Models (LLMs) become ever more important to the field and to the world at large, it has become clear that performing empirical evaluation of these models and methods is extremely difficult to do in rigorous and comprehensive way. This difficulty is of course not due to lack of effort or expertise by researchers. Indeed, enormous effort and resources have been poured into creating myriad benchmarks and test cases (Chiang et al., 2024b; Fourrier et al., 2024; 1Work done at Kaggle 2Kaggle, Inc. Correspondence to: D. Sculley <d@sculley.ai>, Nate Keating <natekeating@kaggle.com>, Megan Risdal <meg@kaggle.com>. Hendrycks et al., 2021; Cobbe et al., 2021; Zellers et al., 2019; Chen et al., 2021b). However, even accounting for these many important efforts and achievements, our position is that the current state of evaluation is insufficient to meet the needs of this moment in GenAI for the field and for the world. In our view, the root cause of this insufficiency is that the evaluation needs of GenAI models fundamentally break the paradigm of traditional benchmarking that served the field of machine learning (ML) so well during decades of remarkable progress. This breakage goes beyond the familiar difficulty of defining what, exactly, is in the training data for an LLM. In our view, we need broader conception of generalization for GenAI that moves beyond the idea of generalizing to new independently drawn examples from stationary distribution, and instead refers to performing well on tasks that are entirely novel from models perspective. This higher bar is rooted in commonsense standards for human intelligence (Chollet, 2019; Dennett, 1991), but has far reaching consequences, most notably that it implies that the problems of data leakage and contamination in evaluation are the most pressing concerns. Together, these factors imply that rigorous and robust evaluation GenAI models requires steady source of novel tasks structured to avoids leakage, contamination, and other forms of inadvertent cheating. Fortunately, AI Competitionssuch as those hosted on Kaggle and similar platformsoffer ready-made solution, providing continual source of new tasks for evaluation and significant structures to avoid leakage and related issues. 1.1. Summarizing Our Position Our position can be summarized by the following points: Traditional paradigms for ML evaluation are illequipped to meet the demands of GenAI Evaluation. Leakage should be viewed by the field as the most important pitfall to avoid in evaluations. GenAI evaluations should be considered leaked the moment it has been shared online or sent over the wire. 1 AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation If we have to choose between reproducibility and robustness in GenAI evaluations, we should choose to prioritize robustness. We should replace the notion of reproducible static benchmarks with repeatable processes and procedures. The field should use established AI Competitions platforms as renewable stream of novel evaluation tasks. The standards and practices developed that help AI Competitions guard against cheating should be viewed by the field as the gold standard for empirical rigor in evaluation. Meta-analyses should be valued as highly in the field of AI as they are in fields such as medicine. 1.2. Structure of this paper In the remainder of this paper, we will first review the most typical structure and assumptions in traditional ML evaluation and discuss why it is insufficient for GenAI evaluations. We will examine the nature of generalization for GenAI, how this leads to specific concerns around leakage, and additionally show how goals of reproducibility and robustness in evaluation may be fundamentally at odds. We will then show how difficult the problem of leakage is even for traditional ML evaluations with some brief case studies, and look at current GenAI benchmarks that are aiming to overcome leakage and contamination. We finish with an examination of the ways that AI Competitions address these issues, discuss our recommendations, and examine alternate viewpoints. Our goal is to provide convincing support of the view that AI Competitions do indeed provide gold standard for empirical rigor in evaluating GenAI models, and that the field should place accordingly high value and attention on their results. 2. Background: Revisiting Benchmarking Traditional ML benchmarking has been founded on the idea of test-train split, in which an evaluation is structured by training model from scratch on given portion of training data and then evaluating that trained model on holdout set of test data (Mitchell, 1997). This conceptual structure is so fundamental to modern ML practice that it may sometimes be taken for granted and not carefully examined. So let us take moment to reflect this basic structure and its implications. In classical supervised ML, the most common traditional setup is to evaluate m"
[13.05.2025 13:27] Mistral response. {"id": "4ac39c6b8f254e9b887703ddaf27bf46", "object": "chat.completion", "created": 1747142847, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Kaggle, Inc.\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1517, "total_tokens": 1531, "completion_tokens": 14}}
[13.05.2025 13:27] Response: ```python
["Kaggle, Inc."]
```
[13.05.2025 13:27] Deleting PDF ./assets/pdf/2505.00612.pdf.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.06324.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.06324.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.06324.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.04918.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.04918.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.04918.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Downloading and parsing paper https://huggingface.co/papers/2505.07086.
[13.05.2025 13:27] Extra JSON file exists (./assets/json/2505.07086.json), skip PDF parsing.
[13.05.2025 13:27] Paper image links file exists (./assets/img_data/2505.07086.json), skip HTML parsing.
[13.05.2025 13:27] Success.
[13.05.2025 13:27] Enriching papers with extra data.
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 0. We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, ...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 1. We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. ...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 2. While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 3. Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 4. Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing ...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 5. Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavo...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 6. Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face criti...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 7. Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biase...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 8. LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructi...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 9. We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scena...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 10. Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain perfo...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 11. Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 12. Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In t...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 13. Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce si...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 14. A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are ...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 15. Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model perfo...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 16. Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedu...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 17. In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that th...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 18. As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge,...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 19. Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-i...
[13.05.2025 13:27] ********************************************************************************
[13.05.2025 13:27] Abstract 20. Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing ap...
[13.05.2025 13:27] Read previous papers.
[13.05.2025 13:27] Generating reviews via LLM API.
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#survey", "#architecture", "#training", "#reasoning", "#data"], "emoji": "üß†", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤—ã–¥–∞—é—â–∏–º–∏—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏", "desc": "Seed1.5-VL - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–æ—á–µ—Ç–∞—é—â–∞—è –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#plp", "#reasoning", "#optimization", "#dataset", "#math", "#rl", "#data", "#training"], "emoji": "üß†", "ru": {"title": "MiMo-7B: –ú–æ—â–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "MiMo-7B - —ç—Ç–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#data", "#diffusion", "#transfer_learning", "#3d", "#benchmark"], "emoji": "üßä", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Step1X-3D - –æ—Ç–∫—Ä—ã—Ç—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#optimization", "#math", "#training", "#small_models", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º: –∫–∞–∫ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –º–æ–≥—É—Ç –ø–æ–ø–∞–¥–∞—Ç—å –≤ '–ª–æ–≤—É—à–∫—É –¥
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#training", "#cv"], "emoji": "üîÑ", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–æ –º–∞–ª–æ—à–∞–≥–æ–≤—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. 
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#small_models", "#rl", "#dataset", "#data"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –∏ RL —É–ª—É—á—à–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ø–ú", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞–ª—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLaMA 2-7B, LL
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#alignment", "#optimization", "#video", "#multimodal", "#rl", "#diffusion", "#benchmark", "#rlhf"], "emoji": "üé®", "ru": {"title": "DanceGRPO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DanceGRPO - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ 
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#reasoning", "#training", "#dataset", "#benchmark", "#data", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ø–ú –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AttentionInfluence –¥–ª—è –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–∞—é—â–∏—Ö —Å–ø–æ—Å–æ
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#agents", "#dataset"], "emoji": "üåê", "ru": {"title": "WebGen-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", "desc": "WebGen-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–µ –≤–µ–±-—Å
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#benchmark", "#alignment", "#open_source", "#training", "#architecture", "#dataset"], "emoji": "üåü", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Skywork-VL Rewar
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training"], "emoji": "üìà", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ (CPT) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –Ω–∞–±–ª—é–¥–∞—é—Ç, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è 
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#agents", "#optimization", "#hallucinations", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–£–º–Ω—ã–π –ø–æ–∏—Å–∫: –∫–æ–≥–¥–∞ –∏—Å–∫–∞—Ç—å, –∞ –∫–æ–≥–¥–∞ –¥–æ–≤–µ—Ä–∏—Ç—å—Å—è —Å–µ–±–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#diffusion", "#agents", "#robotics", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–æ–º–æ—Ç–æ—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≤–∏–∑—É–æ–º–æ—Ç–æ—Ä–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ - –¢—Ä–∏–µ–¥–∏–Ω—É—é –ò–µ—Ä–∞
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#optimization", "#data", "#diffusion", "#training", "#architecture"], "emoji": "üîÑ", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é (VAR) –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture", "#long_context"], "emoji": "üß†", "ru": {"title": "–ß–∞–Ω–∫–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å—É–±-–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è MoE –≤ Transformer: –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (Sparse Mixture of Experts, MoE) –¥–ª
[13.05.2025 13:27] Querying the API.
[13.05.2025 13:27] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.
[13.05.2025 13:27] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—Ç—É—à–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (MLLM). –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—ã—Ä—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –∏ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏—Ö —Å –ø–æ–º–æ—â—å—é –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. MLLM —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ –∑–∞—Ç–µ–º –Ω–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø–æ–Ω—è—Ç–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",

  "emoji": "üñºÔ∏è",

  "title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ—Ç—É—à—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: MLLM –Ω–∞ —Å—Ç—Ä–∞–∂–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"
}
[13.05.2025 13:27] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io."

[13.05.2025 13:27] Response: ```python
['DATASET', 'DATA', 'MULTIMODAL', 'CV', 'TRAINING']
```
[13.05.2025 13:27] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io."

[13.05.2025 13:27] Response: ```python
['INTERPRETABILITY', 'OPTIMIZATION', 'SYNTHETIC']
```
[13.05.2025 13:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the use of a multimodal large language model (MLLM) for retouching raw photographs by critiquing and suggesting edits based on procedural image operations. The authors train the MLLM to understand image processing through visual puzzles, enabling it to plan and propose edit sequences that are user-friendly and maintain the integrity of the original image. By synthesizing a reasoning dataset from expert-edited photos, the model learns to generate understandable retouching operations that preserve object details and resolution. The results demonstrate that this approach offers better explainability and identity preservation compared to traditional generative editing methods.","title":"Empowering Photo Retouching with Intelligent Editing Guidance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the use of a multimodal large language model (MLLM) for retouching raw photographs by critiquing and suggesting edits based on procedural image operations. The authors train the MLLM to understand image processing through visual puzzles, enabling it to plan and propose edit sequences that are user-friendly and maintain the integrity of the original image. By synthesizing a reasoning dataset from expert-edited photos, the model learns to generate understandable retouching operations that preserve object details and resolution. The results demonstrate that this approach offers better explainability and identity preservation compared to traditional generative editing methods.', title='Empowering Photo Retouching with Intelligent Editing Guidance'))
[13.05.2025 13:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÊù•ÊîπËøõÂéüÂßãÁÖßÁâáÁöÑ‰øÆÈ•∞ËøáÁ®ã„ÄÇÊàë‰ª¨ËÆ≠ÁªÉMLLMÁêÜËß£ÂõæÂÉèÂ§ÑÁêÜÊìç‰ΩúÔºåÂπ∂ÈÄöËøáËß£ÂÜ≥ËßÜËßâÈöæÈ¢òÊù•Â¢ûÂº∫ÂÖ∂Êìç‰ΩúÊÑèËØÜ„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üËßÑÂàíÂíåÂª∫ËÆÆÁºñËæëÂ∫èÂàóÔºåÁ°Æ‰øù‰øÆÈ•∞Êìç‰ΩúÂØπÁî®Êà∑ÂèØÁêÜËß£ÔºåÂπ∂‰øùÁïôÂØπË±°ÁªÜËäÇÂíåÂàÜËæ®Áéá„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÁîüÊàêÂíåÁ®ãÂ∫èÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®ÂèØËß£ÈáäÊÄßÂíåË∫´‰ªΩ‰øùÁïôÊñπÈù¢ÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇ","title":"Âà©Áî®Â§öÊ®°ÊÄÅÊ®°ÂûãÊèêÂçáÁÖßÁâá‰øÆÈ•∞Ë¥®Èáè"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÊù•ÊîπËøõÂéüÂßãÁÖßÁâáÁöÑ‰øÆÈ•∞ËøáÁ®ã„ÄÇÊàë‰ª¨ËÆ≠ÁªÉMLLMÁêÜËß£ÂõæÂÉèÂ§ÑÁêÜÊìç‰ΩúÔºåÂπ∂ÈÄöËøáËß£ÂÜ≥ËßÜËßâÈöæÈ¢òÊù•Â¢ûÂº∫ÂÖ∂Êìç‰ΩúÊÑèËØÜ„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üËßÑÂàíÂíåÂª∫ËÆÆÁºñËæëÂ∫èÂàóÔºåÁ°Æ‰øù‰øÆÈ•∞Êìç‰ΩúÂØπÁî®Êà∑ÂèØÁêÜËß£ÔºåÂπ∂‰øùÁïôÂØπË±°ÁªÜËäÇÂíåÂàÜËæ®Áéá„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÁîüÊàêÂíåÁ®ãÂ∫èÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®ÂèØËß£ÈáäÊÄßÂíåË∫´‰ªΩ‰øùÁïôÊñπÈù¢ÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇ', title='Âà©Áî®Â§öÊ®°ÊÄÅÊ®°ÂûãÊèêÂçáÁÖßÁâá‰øÆÈ•∞Ë¥®Èáè'))
[13.05.2025 13:27] Querying the API.
[13.05.2025 13:27] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\em leakage} and {\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.
[13.05.2025 13:27] Response: {
  "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫—Ä–∏–∑–∏—Å –≤ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò, —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –∏ –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –∏—Å—Ç–∏–Ω—ã –∏ —Å–∏–ª—å–Ω—ã–º–∏ –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞–º —É—Ç–µ—á–∫–∏ –∏ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –Ω–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò. –°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –ò–ò –∫–∞–∫ –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏ –≤ –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò.",
  "emoji": "üèÜ",
  "title": "–°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –ò–ò - –∫–ª—é—á –∫ –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[13.05.2025 13:27] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\em leakage} and {\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value."

[13.05.2025 13:27] Response: ```python
["BENCHMARK"]
```
[13.05.2025 13:27] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\em leakage} and {\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value."

[13.05.2025 13:27] Response: ```python
['LEAKAGE', 'EVALUATION']
```
[13.05.2025 13:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the challenges of evaluating Generative AI (GenAI) models, highlighting that traditional machine learning evaluation methods are inadequate. It points out that GenAI models have vast input and output possibilities, lack clear ground truth, and are influenced by previous outputs, complicating their assessment. The authors emphasize that issues like leakage and contamination are critical hurdles in GenAI evaluations. They propose that AI Competitions offer effective strategies to mitigate these problems and should be recognized as a benchmark for rigorous evaluation in the field.","title":"Rethinking Evaluation: AI Competitions as the Gold Standard for GenAI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the challenges of evaluating Generative AI (GenAI) models, highlighting that traditional machine learning evaluation methods are inadequate. It points out that GenAI models have vast input and output possibilities, lack clear ground truth, and are influenced by previous outputs, complicating their assessment. The authors emphasize that issues like leakage and contamination are critical hurdles in GenAI evaluations. They propose that AI Competitions offer effective strategies to mitigate these problems and should be recognized as a benchmark for rigorous evaluation in the field.', title='Rethinking Evaluation: AI Competitions as the Gold Standard for GenAI'))
[13.05.2025 13:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨ËßÇÂØüÂà∞ÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÆûËØÅËØÑ‰º∞Ê≠£Èù¢‰∏¥Âç±Êú∫ÔºåÂõ†‰∏∫‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ËØÑ‰º∞ÂíåÂü∫ÂáÜÁ≠ñÁï•Êó†Ê≥ïÊª°Ë∂≥Áé∞‰ª£ÁîüÊàêÊÄßAIÊ®°ÂûãÂíåÁ≥ªÁªüÁöÑËØÑ‰º∞ÈúÄÊ±Ç„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÂÖ∑ÊúâÂá†‰πéÊó†ÈôêÁöÑËæìÂÖ•ÂíåËæìÂá∫Á©∫Èó¥ÔºåÁº∫‰πèÊòéÁ°ÆÁöÑÁúüÂÆûÁõÆÊ†áÔºåÂπ∂‰∏îÂú®È¢ÑÊµãÊó∂Âº∫ÁÉà‰æùËµñ‰∫é‰πãÂâçÊ®°ÂûãËæìÂá∫ÁöÑ‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÊåáÂá∫ÔºåÊ≥ÑÊºèÂíåÊ±°ÊüìÈóÆÈ¢òÊòØÁîüÊàêÊÄßAIËØÑ‰º∞‰∏≠ÊúÄÈáçË¶Å‰∏îÊúÄÈöæËß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ËÆ§‰∏∫Ôºå‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÈ¢ÜÂüüÂ∑≤ÁªèÂèëÂ±ïÂá∫ÊúâÊïàÁöÑÊé™ÊñΩÊù•Â∫îÂØπÊ≥ÑÊºèÈóÆÈ¢òÔºåÂõ†Ê≠§Â∫îÂ∞ÜÂÖ∂ËßÜ‰∏∫ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÈªÑÈáëÊ†áÂáÜ„ÄÇ","title":"ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜÔºö‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÁöÑ‰ª∑ÂÄº"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨ËßÇÂØüÂà∞ÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÆûËØÅËØÑ‰º∞Ê≠£Èù¢‰∏¥Âç±Êú∫ÔºåÂõ†‰∏∫‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ËØÑ‰º∞ÂíåÂü∫ÂáÜÁ≠ñÁï•Êó†Ê≥ïÊª°Ë∂≥Áé∞‰ª£ÁîüÊàêÊÄßAIÊ®°ÂûãÂíåÁ≥ªÁªüÁöÑËØÑ‰º∞ÈúÄÊ±Ç„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÂÖ∑ÊúâÂá†‰πéÊó†ÈôêÁöÑËæìÂÖ•ÂíåËæìÂá∫Á©∫Èó¥ÔºåÁº∫‰πèÊòéÁ°ÆÁöÑÁúüÂÆûÁõÆÊ†áÔºåÂπ∂‰∏îÂú®È¢ÑÊµãÊó∂Âº∫ÁÉà‰æùËµñ‰∫é‰πãÂâçÊ®°ÂûãËæìÂá∫ÁöÑ‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÊåáÂá∫ÔºåÊ≥ÑÊºèÂíåÊ±°ÊüìÈóÆÈ¢òÊòØÁîüÊàêÊÄßAIËØÑ‰º∞‰∏≠ÊúÄÈáçË¶Å‰∏îÊúÄÈöæËß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ËÆ§‰∏∫Ôºå‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÈ¢ÜÂüüÂ∑≤ÁªèÂèëÂ±ïÂá∫ÊúâÊïàÁöÑÊé™ÊñΩÊù•Â∫îÂØπÊ≥ÑÊºèÈóÆÈ¢òÔºåÂõ†Ê≠§Â∫îÂ∞ÜÂÖ∂ËßÜ‰∏∫ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÈªÑÈáëÊ†áÂáÜ„ÄÇ', title='ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜÔºö‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÁöÑ‰ª∑ÂÄº'))
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#training", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ LLM: –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è –¥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–æ–∫
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#graphs", "#architecture", "#training"], "emoji": "üåé", "ru": {"title": "PASSAT: –§–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã —Å —É—á–µ—Ç–æ–º —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –ó–µ–º–ª–∏", "desc": "PASSAT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≥–æ–¥—ã, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è —Ñ–∏–∑–∏–∫—É
[13.05.2025 13:27] Using data from previous issue: {"categories": ["#training", "#dataset", "#science", "#data", "#optimization"], "emoji": "üß¨", "ru": {"title": "–ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multi-Obje
[13.05.2025 13:27] Loading Chinese text from previous data.
[13.05.2025 13:27] Renaming data file.
[13.05.2025 13:27] Renaming previous data. hf_papers.json to ./d/2025-05-13.json
[13.05.2025 13:27] Saving new data file.
[13.05.2025 13:27] Generating page.
[13.05.2025 13:27] Renaming previous page.
[13.05.2025 13:27] Renaming previous data. index.html to ./d/2025-05-13.html
[13.05.2025 13:27] [Experimental] Generating Chinese page for reading.
[13.05.2025 13:27] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'Seed1.5-VL', 'pinyin': 'Seed1.5-VL', 'trans': 'Seed1.5-VL'}, {'word': 'Êé®Ëøõ', 'pinyin': 'tuƒ´ j√¨n', 'trans': 'promote'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çng y√≤ng', 'trans': 'general'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ shu√†i', 'trans': 'multimodal'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understanding'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«î y√°n', 'trans': 'language'}, {'word': 'Âü∫Á°ÄÊ®°Âûã', 'pinyin': 'jƒ´ ch«î m√≥ x√≠ng', 'trans': 'foundation model'}, {'word': 'Áî±', 'pinyin': 'y√≥u', 'trans': 'consist of'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅn sh«î', 'trans': 'parameters'}, {'word': 'ËßÜËßâÁºñÁ†ÅÂô®', 'pinyin': 'sh√¨ ju√© biƒÅn m«é q√¨', 'trans': 'visual encoder'}, {'word': 'Ê∑∑Âêà‰∏ìÂÆ∂', 'pinyin': 'h√πn h√© zhuƒÅn jiƒÅ', 'trans': 'mixture of experts'}, {'word': 'LLM', 'pinyin': 'LLM', 'trans': 'LLM'}, {'word': 'ÁªÑÊàê', 'pinyin': 'z«î ch√©ng', 'trans': 'composed of'}, {'word': 'Â∞ΩÁÆ°', 'pinyin': 'j«ên gu«én', 'trans': 'although'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'}, {'word': 'Áõ∏ÂØπ', 'pinyin': 'xiƒÅng du√¨', 'trans': 'relatively'}, {'word': 'Á¥ßÂáë', 'pinyin': 'j«ên c√≤u', 'trans': 'compact'}, {'word': '‰ΩÜ', 'pinyin': 'd√†n', 'trans': 'but'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}, {'word': 'ÂÖ¨ÂÖ±', 'pinyin': 'g≈çng g√≤ng', 'trans': 'public'}, {'word': 'Âü∫ÂáÜÊµãËØï', 'pinyin': 'jƒ´ zh«în c√® sh√¨', 'trans': 'benchmark tests'}, {'word': '‰ª£ÁêÜ‰ªªÂä°', 'pinyin': 'd√†i l«ê r√®n w√π', 'trans': 'proxy tasks'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'}, {'word': 'OpenAI', 'pinyin': 'OpenAI', 'trans': 'OpenAI'}, {'word': 'CUA', 'pinyin': 'CUA', 'trans': 'CUA'}, {'word': 'Claude', 'pinyin': 'Claude', 'trans': 'Claude'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«én sh√¨', 'trans': 'demonstrate'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ng d√†', 'trans': 'powerful'}, {'word': 'ÈÄÇÁî®‰∫é', 'pinyin': 'sh√¨ y√≤ng y√∫', 'trans': 'applicable to'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}, {'word': 'Â∏åÊúõ', 'pinyin': 'xƒ´ w√†ng', 'trans': 'hope'}, {'word': 'Êé®Âä®', 'pinyin': 'tuƒ´ d√≤ng', 'trans': 'drive'}, {'word': 'ÂπøÊ≥õ', 'pinyin': 'gu«éng f√†n', 'trans': 'widespread'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'application'}]
[13.05.2025 13:27] Renaming previous Chinese page.
[13.05.2025 13:27] Renaming previous data. zh.html to ./d/2025-05-12_zh_reading_task.html
[13.05.2025 13:27] Writing Chinese reading task.
[13.05.2025 13:27] Writing result.
[13.05.2025 13:27] Renaming log file.
[13.05.2025 13:27] Renaming previous data. log.txt to ./logs/2025-05-13_last_log.txt
