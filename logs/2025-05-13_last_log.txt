[13.05.2025 05:13] Read previous papers.
[13.05.2025 05:13] Generating top page (month).
[13.05.2025 05:13] Writing top page (month).
[13.05.2025 06:16] Read previous papers.
[13.05.2025 06:16] Get feed.
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07062
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07608
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07787
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07747
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06548
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07447
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07818
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03733
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07796
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07293
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07596
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06324
[13.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07086
[13.05.2025 06:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.05.2025 06:16] No deleted papers detected.
[13.05.2025 06:16] Downloading and parsing papers (pdf, html). Total: 13.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07062.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07062.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07062.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07608.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07608.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07608.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07787.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07787.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07787.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07747.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07747.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07747.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.06548.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.06548.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.06548.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07447.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07447.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07447.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07818.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07818.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07818.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.03733.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.03733.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.03733.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07796.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07796.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07796.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07293.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07293.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07293.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07596.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07596.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07596.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.06324.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.06324.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.06324.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.07086.
[13.05.2025 06:16] Extra JSON file exists (./assets/json/2505.07086.json), skip PDF parsing.
[13.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.07086.json), skip HTML parsing.
[13.05.2025 06:16] Success.
[13.05.2025 06:16] Enriching papers with extra data.
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 0. We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, ...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 1. We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. ...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 2. Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 3. While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 4. Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavo...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 5. Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing ...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 6. Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face criti...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 7. LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructi...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 8. Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain perfo...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 9. Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biase...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 10. Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 11. As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge,...
[13.05.2025 06:16] ********************************************************************************
[13.05.2025 06:16] Abstract 12. Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing ap...
[13.05.2025 06:16] Read previous papers.
[13.05.2025 06:16] Generating reviews via LLM API.
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#survey", "#architecture", "#training", "#reasoning", "#data"], "emoji": "üß†", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤—ã–¥–∞—é—â–∏–º–∏—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏", "desc": "Seed1.5-VL - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–æ—á–µ—Ç–∞—é—â–∞—è –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#plp", "#reasoning", "#optimization", "#dataset", "#math", "#rl", "#data", "#training"], "emoji": "üß†", "ru": {"title": "MiMo-7B: –ú–æ—â–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "MiMo-7B - —ç—Ç–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#optimization", "#math", "#training", "#small_models", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º: –∫–∞–∫ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –º–æ–≥—É—Ç –ø–æ–ø–∞–¥–∞—Ç—å –≤ '–ª–æ–≤—É—à–∫—É –¥
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#data", "#diffusion", "#transfer_learning", "#3d", "#benchmark"], "emoji": "üßä", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Step1X-3D - –æ—Ç–∫—Ä—ã—Ç—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#small_models", "#rl", "#dataset", "#data"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –∏ RL —É–ª—É—á—à–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ø–ú", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞–ª—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLaMA 2-7B, LL
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#training", "#cv"], "emoji": "üîÑ", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–æ –º–∞–ª–æ—à–∞–≥–æ–≤—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. 
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#alignment", "#optimization", "#video", "#multimodal", "#rl", "#diffusion", "#benchmark", "#rlhf"], "emoji": "üé®", "ru": {"title": "DanceGRPO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DanceGRPO - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ 
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#agents", "#dataset"], "emoji": "üåê", "ru": {"title": "WebGen-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", "desc": "WebGen-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–µ –≤–µ–±-—Å
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training"], "emoji": "üìà", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ (CPT) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –Ω–∞–±–ª—é–¥–∞—é—Ç, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è 
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#reasoning", "#training", "#dataset", "#benchmark", "#data", "#optimization", "#small_models"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ø–ú –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AttentionInfluence –¥–ª—è –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–∞—é—â–∏—Ö —Å–ø–æ—Å–æ
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#agents", "#optimization", "#hallucinations", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–£–º–Ω—ã–π –ø–æ–∏—Å–∫: –∫–æ–≥–¥–∞ –∏—Å–∫–∞—Ç—å, –∞ –∫–æ–≥–¥–∞ –¥–æ–≤–µ—Ä–∏—Ç—å—Å—è —Å–µ–±–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#training", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ LLM: –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è –¥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–æ–∫
[13.05.2025 06:16] Using data from previous issue: {"categories": ["#training", "#dataset", "#science", "#data", "#optimization"], "emoji": "üß¨", "ru": {"title": "–ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multi-Obje
[13.05.2025 06:16] Loading Chinese text from previous data.
[13.05.2025 06:16] Renaming data file.
[13.05.2025 06:16] Renaming previous data. hf_papers.json to ./d/2025-05-13.json
[13.05.2025 06:16] Saving new data file.
[13.05.2025 06:16] Generating page.
[13.05.2025 06:16] Renaming previous page.
[13.05.2025 06:16] Renaming previous data. index.html to ./d/2025-05-13.html
[13.05.2025 06:16] [Experimental] Generating Chinese page for reading.
[13.05.2025 06:16] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'Á≥ªÂàó', 'pinyin': 'x√¨ li√®', 'trans': 'series'}, {'word': '‰∏ì‰∏∫', 'pinyin': 'zhuƒÅn w√®i', 'trans': 'specially for'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çu hu√†', 'trans': 'optimize'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅn sh√π', 'trans': 'parameters'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅo xi√†o', 'trans': 'efficient'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«én sh√¨', 'trans': 'demonstrate'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'}, {'word': 'Áõ∏ÂΩì', 'pinyin': 'xiƒÅng dƒÅng', 'trans': 'equivalent'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨ su√†n', 'trans': 'compute'}, {'word': 'ËµÑÊ∫ê', 'pinyin': 'zƒ´ yu√°n', 'trans': 'resources'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Ëá™ÂÆö‰πâ', 'pinyin': 'z√¨ d√¨ng y√¨', 'trans': 'customize'}, {'word': 'ÂàÜËØçÂô®', 'pinyin': 'fƒìn c√≠ q√¨', 'trans': 'tokenizer'}, {'word': 'Âπ≥Ë°°', 'pinyin': 'p√≠ng h√©ng', 'trans': 'balance'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«ê l√¨ng', 'trans': 'instruction'}, {'word': 'Á±ªÂûã', 'pinyin': 'l√®i x√≠ng', 'trans': 'type'}, {'word': 'Â≠¶‰π†', 'pinyin': 'xu√© x√≠', 'trans': 'learn'}, {'word': 'Âä†ÊùÉ', 'pinyin': 'jiƒÅ qu√°n', 'trans': 'weighted'}, {'word': '‰∫§ÂèâÁÜµ', 'pinyin': 'jiƒÅo chƒÅ shƒÅng', 'trans': 'cross-entropy'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'}, {'word': 'Ëá™ÈÄÇÂ∫î', 'pinyin': 'z√¨ sh√¨ y√¨ng', 'trans': 'adaptive'}, {'word': 'Â≠¶‰π†Áéá', 'pinyin': 'xu√© x√≠ l«ú', 'trans': 'learning rate'}, {'word': 'Á≠ñÂàí', 'pinyin': 'c√® hu√†', 'trans': 'plan'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅo j√¨', 'trans': 'token'}, {'word': 'ÊñáÊ°£', 'pinyin': 'w√©n d√†ng', 'trans': 'document'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'train'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÊµãËØï', 'pinyin': 'c√® sh√¨', 'trans': 'test'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'perform'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'Á´û‰∫âÂäõ', 'pinyin': 'j√¨ng zhƒìng l√¨', 'trans': 'competitiveness'}, {'word': 'ÈÖçÁΩÆ', 'pinyin': 'p√®i zh√¨', 'trans': 'configuration'}, {'word': 'Á¥ßÂáë', 'pinyin': 'j«ên c√≤u', 'trans': 'compact'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ng d√†', 'trans': 'powerful'}]
[13.05.2025 06:16] Renaming previous Chinese page.
[13.05.2025 06:16] Renaming previous data. zh.html to ./d/2025-05-12_zh_reading_task.html
[13.05.2025 06:16] Writing Chinese reading task.
[13.05.2025 06:16] Writing result.
[13.05.2025 06:16] Renaming log file.
[13.05.2025 06:16] Renaming previous data. log.txt to ./logs/2025-05-13_last_log.txt
