[13.05.2025 07:12] Read previous papers.
[13.05.2025 07:12] Generating top page (month).
[13.05.2025 07:12] Writing top page (month).
[13.05.2025 08:16] Read previous papers.
[13.05.2025 08:16] Get feed.
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07062
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07608
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07747
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07787
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06548
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07447
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07818
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07293
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03733
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07796
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07596
[13.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.07793
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06324
[13.05.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07086
[13.05.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2505.04918
[13.05.2025 08:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.05.2025 08:16] No deleted papers detected.
[13.05.2025 08:16] Downloading and parsing papers (pdf, html). Total: 15.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07062.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07062.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07062.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07608.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07608.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07608.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07747.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07747.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07747.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07787.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07787.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07787.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.06548.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.06548.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.06548.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07447.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07447.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07447.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07818.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07818.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07818.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07293.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07293.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07293.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.03733.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.03733.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.03733.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07796.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07796.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07796.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07596.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07596.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07596.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07793.
[13.05.2025 08:16] Downloading paper 2505.07793 from http://arxiv.org/pdf/2505.07793v1...
[13.05.2025 08:16] Extracting affiliations from text.
[13.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 3 9 7 7 0 . 5 0 5 2 : r Preprint. Under review. Overflow Prevention Enhances Long-Context Recurrent LLMs Assaf Ben-Kish1, Itamar Zimerman1,2, M. Jehanzeb Mirza3, James Glass3, Leonid Karlinsky4, Raja Giryes1 1Tel Aviv University, 2IBM Research, 3MIT CSAIL, 4Xero https://github.com/assafbk/OPRM "
[13.05.2025 08:16] Response: ```python
["Tel Aviv University", "IBM Research", "MIT CSAIL", "Xero"]
```
[13.05.2025 08:16] Deleting PDF ./assets/pdf/2505.07793.pdf.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.06324.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.06324.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.06324.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.07086.
[13.05.2025 08:16] Extra JSON file exists (./assets/json/2505.07086.json), skip PDF parsing.
[13.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.07086.json), skip HTML parsing.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.04918.
[13.05.2025 08:16] Downloading paper 2505.04918 from http://arxiv.org/pdf/2505.04918v1...
[13.05.2025 08:16] Extracting affiliations from text.
[13.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction Jiaqi Zheng1, Qing Ling1, and Yerong Feng2 1Sun Yat-Sen University, 2Shenzhen Institute of Meteorological Innovation 2025-5-9 Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earths surface. In light of these disadvantages, we develop PASSAT, novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earths surface into consideration, other than simply treating it as plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625. 1. Introduction Weather prediction is of paramount importance to social security and economic development, and has attracted extensive research efforts since the ancient time. Among the modern weather prediction methods, numerical weather prediction (NWP) is built upon differential equations that govern the weather evolution [Ran+07; BTB15]. These differential equations attribute the weather evolution to the advection process and the Earth-atmo"
[13.05.2025 08:16] Response: ```python
["Sun Yat-Sen University", "Shenzhen Institute of Meteorological Innovation"]
```
[13.05.2025 08:16] Deleting PDF ./assets/pdf/2505.04918.pdf.
[13.05.2025 08:16] Success.
[13.05.2025 08:16] Enriching papers with extra data.
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 0. We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, ...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 1. We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. ...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 2. While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 3. Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 4. Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavo...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 5. Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing ...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 6. Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face criti...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 7. Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biase...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 8. LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructi...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 9. Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain perfo...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 10. Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 11. A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are ...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 12. As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge,...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 13. Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing ap...
[13.05.2025 08:16] ********************************************************************************
[13.05.2025 08:16] Abstract 14. Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-i...
[13.05.2025 08:16] Read previous papers.
[13.05.2025 08:16] Generating reviews via LLM API.
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#survey", "#architecture", "#training", "#reasoning", "#data"], "emoji": "🧠", "ru": {"title": "Компактная мультимодальная модель с выдающимися способностями", "desc": "Seed1.5-VL - это мультимодальная модель, сочетающая зрение и язык для общего понимания и рас
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#plp", "#reasoning", "#optimization", "#dataset", "#math", "#rl", "#data", "#training"], "emoji": "🧠", "ru": {"title": "MiMo-7B: Мощная языковая модель для сложных рассуждений", "desc": "MiMo-7B - это большая языковая модель, оптимизированная для задач рассуждения. В процессе предва
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#data", "#diffusion", "#transfer_learning", "#3d", "#benchmark"], "emoji": "🧊", "ru": {"title": "Открытая платформа для AI-генерации 3D-объектов нового поколения", "desc": "Статья представляет Step1X-3D - открытую систему для генерации 3D
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#math", "#training", "#small_models", "#reasoning", "#dataset"], "emoji": "🧠", "ru": {"title": "Коллективный разум: как модели машинного обучения учатся друг у друга", "desc": "Исследование показывает, что крупные модели рассуждений (LRM) могут попадать в 'ловушку д
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#small_models", "#rl", "#dataset", "#data"], "emoji": "🤖", "ru": {"title": "Малые модели и RL улучшают генерацию инструкций для обучения больших ЯМ", "desc": "Статья исследует эффективность малых открытых языковых моделей (LLaMA 2-7B, LL
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#training", "#cv"], "emoji": "🔄", "ru": {"title": "Единый фреймворк для непрерывных генеративных моделей: от многошаговых до малошаговых", "desc": "Статья представляет унифицированный подход к обучению и сэмплированию непрерывных генеративных моделей. 
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#alignment", "#optimization", "#video", "#multimodal", "#rl", "#diffusion", "#benchmark", "#rlhf"], "emoji": "🎨", "ru": {"title": "DanceGRPO: Революция в обучении с подкреплением для генерации визуального контента", "desc": "Статья представляет DanceGRPO - унифицированный фреймворк 
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#training", "#dataset", "#benchmark", "#data", "#optimization", "#small_models"], "emoji": "🧠", "ru": {"title": "Улучшение способности ЯМ к рассуждениям через умный отбор данных", "desc": "Статья представляет метод AttentionInfluence для отбора данных, улучшающих спосо
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#agents", "#dataset"], "emoji": "🌐", "ru": {"title": "WebGen-Bench: Новый рубеж в оценке LLM-агентов для веб-разработки", "desc": "WebGen-Bench - это новый бенчмарк для оценки способности LLM-агентов создавать многофайловые веб-с
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training"], "emoji": "📈", "ru": {"title": "Раскрытие секретов непрерывного предобучения языковых моделей", "desc": "Статья исследует динамику обучения при непрерывном предобучении (CPT) больших языковых моделей. Авторы наблюдают, как меняется 
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#agents", "#optimization", "#hallucinations", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "Умный поиск: когда искать, а когда довериться себе", "desc": "Статья представляет новый подход к улучшению работы больших языковых моделей (LLM) с использованием м
[13.05.2025 08:16] Querying the API.
[13.05.2025 08:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.
[13.05.2025 08:16] Response: {
  "desc": "Исследование фокусируется на рекуррентных суб-квадратичных моделях для обработки длинных контекстов в больших языковых моделях. Эксперименты показывают, что даже при обучении на длинных контекстах, модели недостаточно эффективно их используют. Предложенный метод обработки по чанкам, выбирающий наиболее релевантные части входных данных, значительно улучшает производительность ряда моделей на бенчмарке LongBench. Результаты ставят под сомнение способность рекуррентных моделей эффективно использовать зависимости в длинных контекстах.",
  "emoji": "🧠",
  "title": "Чанки побеждают рекуррентность в обработке длинных контекстов"
}
[13.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations."

[13.05.2025 08:16] Response: ```python
['BENCHMARK', 'TRAINING', 'ARCHITECTURE']
```
[13.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations."

[13.05.2025 08:16] Response: ```python
["LONG_CONTEXT"]
```
[13.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the efficiency of large language models (LLMs) that use recurrent sub-quadratic architectures for processing long contexts. The authors find that these models often do not fully utilize their long-context capabilities due to limitations in their fixed-size recurrent memory. They propose a chunk-based inference method that selectively processes the most relevant parts of the input, significantly enhancing performance on long-context tasks. Their results show substantial improvements in various models and challenge the assumption that recurrent models effectively leverage long-range dependencies.","title":"Unlocking Long-Context Potential with Chunk-Based Inference"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the efficiency of large language models (LLMs) that use recurrent sub-quadratic architectures for processing long contexts. The authors find that these models often do not fully utilize their long-context capabilities due to limitations in their fixed-size recurrent memory. They propose a chunk-based inference method that selectively processes the most relevant parts of the input, significantly enhancing performance on long-context tasks. Their results show substantial improvements in various models and challenge the assumption that recurrent models effectively leverage long-range dependencies.', title='Unlocking Long-Context Potential with Chunk-Based Inference'))
[13.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近，长文本模型（LLMs）发展出了一种新的子二次模型，旨在提高长上下文处理的效率。我们研究了主要的长上下文模型，重点关注它们固定大小的递归记忆如何影响性能。实验表明，即使这些模型经过长上下文训练，它们对长上下文的利用仍然不足。我们提出的基于块的推理方法能够识别并处理输入中最相关的部分，从而有效缓解递归记忆的不足，并在多个长上下文任务中取得显著提升。","title":"提升长上下文处理效率的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='最近，长文本模型（LLMs）发展出了一种新的子二次模型，旨在提高长上下文处理的效率。我们研究了主要的长上下文模型，重点关注它们固定大小的递归记忆如何影响性能。实验表明，即使这些模型经过长上下文训练，它们对长上下文的利用仍然不足。我们提出的基于块的推理方法能够识别并处理输入中最相关的部分，从而有效缓解递归记忆的不足，并在多个长上下文任务中取得显著提升。', title='提升长上下文处理效率的新方法'))
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#training", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "🔍", "ru": {"title": "Повышение надежности атрибуции в LLM: от текстового включения до механизма внимания", "desc": "Эта статья посвящена проблеме атрибуции в больших языковых моделях (LLM) при работе с док
[13.05.2025 08:16] Using data from previous issue: {"categories": ["#training", "#dataset", "#science", "#data", "#optimization"], "emoji": "🧬", "ru": {"title": "Многоцелевая оптимизация биологических последовательностей с помощью дискретного сопоставления потоков", "desc": "Статья представляет новый метод машинного обучения под названием Multi-Obje
[13.05.2025 08:16] Querying the API.
[13.05.2025 08:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625.
[13.05.2025 08:16] Response: {
  "desc": "PASSAT - это новая модель глубокого обучения для прогнозирования погоды, учитывающая физику атмосферных процессов и топологию поверхности Земли. Модель решает уравнения адвекции и Навье-Стокса на сферическом многообразии, используя сферическую графовую нейронную сеть для моделирования взаимодействия Земли и атмосферы. PASSAT генерирует начальные поля скоростей, критически важные для решения уравнения адвекции. В экспериментах на данных ERA5 с разрешением 5.625° PASSAT превзошла как современные модели глубокого обучения, так и операционную модель численного прогноза погоды IFS T42.",
  "emoji": "🌎",
  "title": "PASSAT: Физически обоснованный прогноз погоды с учетом топологии Земли"
}
[13.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625."

[13.05.2025 08:16] Response: ```python
['DATASET', 'DATA', 'ARCHITECTURE', 'TRAINING']
```
[13.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625."

[13.05.2025 08:16] Response: ```python
["GRAPHS", "OPTIMIZATION"]
```
[13.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces PASSAT, a deep learning model designed for weather prediction that integrates physical principles and the Earth\'s surface topology. Unlike traditional models, PASSAT incorporates the advection process and the complex interactions between the Earth and atmosphere, using the advection and Navier-Stokes equations. It employs a spherical graph neural network to effectively model these interactions and generate essential initial velocity fields. The results show that PASSAT significantly outperforms existing deep learning models and the operational numerical weather prediction model IFS T42, demonstrating its effectiveness in accurately predicting weather patterns.","title":"PASSAT: Bridging Physics and Topology for Superior Weather Prediction"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces PASSAT, a deep learning model designed for weather prediction that integrates physical principles and the Earth's surface topology. Unlike traditional models, PASSAT incorporates the advection process and the complex interactions between the Earth and atmosphere, using the advection and Navier-Stokes equations. It employs a spherical graph neural network to effectively model these interactions and generate essential initial velocity fields. The results show that PASSAT significantly outperforms existing deep learning models and the operational numerical weather prediction model IFS T42, demonstrating its effectiveness in accurately predicting weather patterns.", title='PASSAT: Bridging Physics and Topology for Superior Weather Prediction'))
[13.05.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新型的天气预测模型PASSAT，该模型结合了物理学和地形信息。PASSAT通过对流过程和地球-大气相互作用来描述天气演变，并考虑了地球表面的拓扑结构。该模型在球面流形上数值求解对流方程和纳维-斯托克斯方程，并利用球面图神经网络捕捉地球-大气的相互作用。实验结果表明，PASSAT在5.625度分辨率的ERA5数据集上优于现有的深度学习天气预测模型和操作性数值天气预测模型IFS T42。","title":"PASSAT：结合物理与拓扑的天气预测新模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新型的天气预测模型PASSAT，该模型结合了物理学和地形信息。PASSAT通过对流过程和地球-大气相互作用来描述天气演变，并考虑了地球表面的拓扑结构。该模型在球面流形上数值求解对流方程和纳维-斯托克斯方程，并利用球面图神经网络捕捉地球-大气的相互作用。实验结果表明，PASSAT在5.625度分辨率的ERA5数据集上优于现有的深度学习天气预测模型和操作性数值天气预测模型IFS T42。', title='PASSAT：结合物理与拓扑的天气预测新模型'))
[13.05.2025 08:16] Loading Chinese text from previous data.
[13.05.2025 08:16] Renaming data file.
[13.05.2025 08:16] Renaming previous data. hf_papers.json to ./d/2025-05-13.json
[13.05.2025 08:16] Saving new data file.
[13.05.2025 08:16] Generating page.
[13.05.2025 08:16] Renaming previous page.
[13.05.2025 08:16] Renaming previous data. index.html to ./d/2025-05-13.html
[13.05.2025 08:16] [Experimental] Generating Chinese page for reading.
[13.05.2025 08:16] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '系列', 'pinyin': 'xì liè', 'trans': 'series'}, {'word': '专为', 'pinyin': 'zhuān wèi', 'trans': 'specially for'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'}, {'word': '参数', 'pinyin': 'cān shù', 'trans': 'parameters'}, {'word': '高效', 'pinyin': 'gāo xiào', 'trans': 'efficient'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '相当', 'pinyin': 'xiāng dāng', 'trans': 'equivalent'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '计算', 'pinyin': 'jì suàn', 'trans': 'compute'}, {'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resources'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '自定义', 'pinyin': 'zì dìng yì', 'trans': 'customize'}, {'word': '分词器', 'pinyin': 'fēn cí qì', 'trans': 'tokenizer'}, {'word': '平衡', 'pinyin': 'píng héng', 'trans': 'balance'}, {'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'instruction'}, {'word': '类型', 'pinyin': 'lèi xíng', 'trans': 'type'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learn'}, {'word': '加权', 'pinyin': 'jiā quán', 'trans': 'weighted'}, {'word': '交叉熵', 'pinyin': 'jiāo chā shāng', 'trans': 'cross-entropy'}, {'word': '损失', 'pinyin': 'sǔn shī', 'trans': 'loss'}, {'word': '自适应', 'pinyin': 'zì shì yìng', 'trans': 'adaptive'}, {'word': '学习率', 'pinyin': 'xué xí lǜ', 'trans': 'learning rate'}, {'word': '策划', 'pinyin': 'cè huà', 'trans': 'plan'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'token'}, {'word': '文档', 'pinyin': 'wén dàng', 'trans': 'document'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '竞争力', 'pinyin': 'jìng zhēng lì', 'trans': 'competitiveness'}, {'word': '配置', 'pinyin': 'pèi zhì', 'trans': 'configuration'}, {'word': '紧凑', 'pinyin': 'jǐn còu', 'trans': 'compact'}, {'word': '强大', 'pinyin': 'qiáng dà', 'trans': 'powerful'}]
[13.05.2025 08:16] Renaming previous Chinese page.
[13.05.2025 08:16] Renaming previous data. zh.html to ./d/2025-05-12_zh_reading_task.html
[13.05.2025 08:16] Writing Chinese reading task.
[13.05.2025 08:16] Writing result.
[13.05.2025 08:16] Renaming log file.
[13.05.2025 08:16] Renaming previous data. log.txt to ./logs/2025-05-13_last_log.txt
