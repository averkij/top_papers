[13.05.2025 04:15] Read previous papers.
[13.05.2025 04:15] Generating top page (month).
[13.05.2025 04:15] Writing top page (month).
[13.05.2025 05:12] Read previous papers.
[13.05.2025 05:12] Get feed.
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07062
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07608
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07787
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07747
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06548
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03733
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07818
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07796
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07596
[13.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07293
[13.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07447
[13.05.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06324
[13.05.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.07086
[13.05.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.05.2025 05:12] No deleted papers detected.
[13.05.2025 05:12] Downloading and parsing papers (pdf, html). Total: 13.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07062.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.07062.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.07062.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07608.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.07608.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.07608.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07787.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.07787.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.07787.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07747.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.07747.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.07747.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.06548.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.06548.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.06548.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.03733.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.03733.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.03733.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07818.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.07818.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.07818.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07796.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.07796.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.07796.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07596.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.07596.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.07596.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07293.
[13.05.2025 05:12] Downloading paper 2505.07293 from http://arxiv.org/pdf/2505.07293v1...
[13.05.2025 05:12] Extracting affiliations from text.
[13.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 3 9 2 7 0 . 5 0 5 2 : r AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection Kai Hua, Steven Wu, Ge Zhang, Ke Shen Corresponding authors "
[13.05.2025 05:12] Response: []
[13.05.2025 05:12] Extracting affiliations from text.
[13.05.2025 05:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 3 9 2 7 0 . 5 0 5 2 : r AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection Kai Hua, Steven Wu, Ge Zhang, Ke ShenCorresponding authorsRecently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domainspecific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, simple yet effective, training-free method without supervision signal. Our approach enables small pretrained language model to act as strong data selector through simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger modelsoffering promising and scalable path for reasoning-centric data selection. Date: May 13, 2025 Correspondence: Kai Hua at huakai.dev@bytedance.com, Ke Shen at shenke@bytedance.comThe identification of high-quality pretraining data has been key factor enabling Large Language Models (LLMs) creation. Commonly recognized high-quality pretraining materials include academic papers (e.g., arXiv), books (e.g., Project Gutenberg), high-quality code (e.g., GitHub), and instruction datasets [25]. Existing approaches often rely on manually curated high-quality seed data to train classifiers for extracting additional high-quality pretraining data from massive web corpora. However, as the size and diversity of LLMs pretraining data requirements continue to grow, these carefully curated classifiers suffer from the high manual effort requirements and relatively low diversity of identified data. This raises critical research question: How can we continue to identify diverse high-quality pretraining data efficiently and scalably? Current mainstream methods[40] typically use supervised or weakly supervised data to train classifiers to identify high-quality data. For instance, LLaMA2[43] uses reference information of Wikipedia documents, which can be seen as weakly supervised data to train fasttext[21] classifier and then recognize Wikipedia1 Figure 1 (a) Performance evolution on comprehensive benchmark evaluations during pretraining. The first 750 billion tokens correspond to the pretraining phase, represented by solid lines, while the subsequent 250 billion tokens represent the learning rate annealing phase, represented by dashed lines, using the same dataset. After around 100 billion tokens, AttentionInfluence-1.3B consistently outperforms the baseline across wide range of tasks on average, including the annealing phase. (b) Training Loss during pretraining. AttentionInfluence-1.3B consistently achieves lower loss than the baseline. like documents. LLaMA3[13] and FineWeb-Edu[32] use LLM-generated responses to train classifier for educational value, which can be regarded as much sparser form of distillation from larger LLM(up to 70B dense parameters) than knowledge distillation[17]. While other approaches like DCLM aim to fit user preferences through utilizing signals of user behavior, these methods may introduce potential bias and do harm to diversity[25]. There are also efforts to train several domain classifiers and combine them for practical use [46]. However, we assume that these methods fail to capture the essence of what makes data reasoning-intensive, and as result, they can be labor-intensive and require significant data engineering efforts. Moreover, there exists risk that the classification results from small models distilled from larger models responses may not improve the final performance of larger models. Therefore, we propose AttentionInfluence, which leverages the intrinsic mechanism of existing LLMs attention heads for pretraining data selection to achieve weak-to-strong generalization. Existing research suggests that feedforward networks (FFNs) store atomic knowledge[12], while attention mechanisms execute algorithms and store procedural knowledge[31, 47]. These mechanistic interpretability insights inspire us to hypothesize that the data activating more important attention heads are high-quality and about procedural knowledge. To be specific, we select the data with relatively larger loss difference when small pretrained language models process them with and without masking retrieval heads. Compared with mainstream data selection methods [21, 25], our method is training-free and more generalizable. To validate AttentionInfluence, we adopt LLaMA2-alike-1.3B pretrained checkpoint for data selection from SmolLM-Corpus. We then pretrain 7B dense language modelSmolLM-7Bas our baseline on the SmolLM-Corpus, 241B-token curated dataset that already applies strong quality filtering with an education-focused classifier (FineWeb-Edu-Dedup) to retain high-quality data. As shown in (a) of Figure 1, despite this strong baseline, AttentionInfluence still yields consistent improvements, demonstrating its ability to further improve the overall data quality through better data selection beyond existing heuristics or classifiers. AttentionInfluence shows consistent improvement against SmolLM-7B across wide range of tasks, demonstrating the effectiveness of the selected data. We further compare AttentionInfluence with trained classifier (FineWeb-Edu Classifier) and find that it selects data that is more balanced and broadly distributed across content categories, and preferentially favors longer and more comprehensive samples. Despite being entirely supervision-free and training-free, AttentionInfluence also shows strong agreement with classifier-based patterns, validating its reliability and generalizability. In summary, our key contributions are as follows: 1. We propose AttentionInflu"
[13.05.2025 05:12] Mistral response. {"id": "e9d9b0b4ca124688bdda0c3acf40312d", "object": "chat.completion", "created": 1747113135, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"bytedance.com\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1541, "total_tokens": 1555, "completion_tokens": 14}}
[13.05.2025 05:12] Response: ```python
["bytedance.com"]
```
[13.05.2025 05:12] Deleting PDF ./assets/pdf/2505.07293.pdf.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07447.
[13.05.2025 05:12] Downloading paper 2505.07447 from http://arxiv.org/pdf/2505.07447v1...
[13.05.2025 05:12] Extracting affiliations from text.
[13.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 7 4 4 7 0 . 5 0 5 2 : r a Peng Sun1,2 Yi Jiang2 Tao Lin1, 1Westlake University 2Zhejiang University sunpeng@westlake.edu.cn, yi_jiang@zju.edu.cn, lintao@westlake.edu.cn "
[13.05.2025 05:12] Response: ```python
["Westlake University", "Zhejiang University"]
```
[13.05.2025 05:12] Deleting PDF ./assets/pdf/2505.07447.pdf.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.06324.
[13.05.2025 05:12] Extra JSON file exists (./assets/json/2505.06324.json), skip PDF parsing.
[13.05.2025 05:12] Paper image links file exists (./assets/img_data/2505.06324.json), skip HTML parsing.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2505.07086.
[13.05.2025 05:12] Downloading paper 2505.07086 from http://arxiv.org/pdf/2505.07086v1...
[13.05.2025 05:12] Extracting affiliations from text.
[13.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design Tong Chen1,2, Yinuo Zhang1,3, Sophia Tang1,4, Pranam Chatterjee1,5,6, 1Department of Biomedical Engineering, Duke University 2Department of Computer Science, Fudan University 3Center of Computational Biology, Duke-NUS Medical School 4Management and Technology Program, University of Pennsylvania 5Department of Computer Science, Duke University 6Department of Biostatistics and Bioinformatics, Duke University Corresponding author: pranam.chatterjee@duke.edu "
[13.05.2025 05:12] Response: ```python
[
    "Department of Biomedical Engineering, Duke University",
    "Department of Computer Science, Fudan University",
    "Center of Computational Biology, Duke-NUS Medical School",
    "Management and Technology Program, University of Pennsylvania",
    "Department of Computer Science, Duke University",
    "Department of Biostatistics and Bioinformatics, Duke University"
]
```
[13.05.2025 05:12] Deleting PDF ./assets/pdf/2505.07086.pdf.
[13.05.2025 05:12] Success.
[13.05.2025 05:12] Enriching papers with extra data.
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 0. We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, ...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 1. We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. ...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 2. Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 3. While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 4. Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavo...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 5. LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructi...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 6. Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face criti...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 7. Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain perfo...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 8. Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 9. Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biase...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 10. Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing ...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 11. As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge,...
[13.05.2025 05:12] ********************************************************************************
[13.05.2025 05:12] Abstract 12. Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing ap...
[13.05.2025 05:12] Read previous papers.
[13.05.2025 05:12] Generating reviews via LLM API.
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#survey", "#architecture", "#training", "#reasoning", "#data"], "emoji": "🧠", "ru": {"title": "Компактная мультимодальная модель с выдающимися способностями", "desc": "Seed1.5-VL - это мультимодальная модель, сочетающая зрение и язык для общего понимания и рас
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#plp", "#reasoning", "#optimization", "#dataset", "#math", "#rl", "#data", "#training"], "emoji": "🧠", "ru": {"title": "MiMo-7B: Мощная языковая модель для сложных рассуждений", "desc": "MiMo-7B - это большая языковая модель, оптимизированная для задач рассуждения. В процессе предва
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#math", "#training", "#small_models", "#reasoning", "#dataset"], "emoji": "🧠", "ru": {"title": "Коллективный разум: как модели машинного обучения учатся друг у друга", "desc": "Исследование показывает, что крупные модели рассуждений (LRM) могут попадать в 'ловушку д
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#architecture", "#data", "#diffusion", "#transfer_learning", "#3d", "#benchmark"], "emoji": "🧊", "ru": {"title": "Открытая платформа для AI-генерации 3D-объектов нового поколения", "desc": "Статья представляет Step1X-3D - открытую систему для генерации 3D
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#small_models", "#rl", "#dataset", "#data"], "emoji": "🤖", "ru": {"title": "Малые модели и RL улучшают генерацию инструкций для обучения больших ЯМ", "desc": "Статья исследует эффективность малых открытых языковых моделей (LLaMA 2-7B, LL
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#games", "#benchmark", "#training", "#agents", "#dataset"], "emoji": "🌐", "ru": {"title": "WebGen-Bench: Новый рубеж в оценке LLM-агентов для веб-разработки", "desc": "WebGen-Bench - это новый бенчмарк для оценки способности LLM-агентов создавать многофайловые веб-с
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#alignment", "#optimization", "#video", "#multimodal", "#rl", "#diffusion", "#benchmark", "#rlhf"], "emoji": "🎨", "ru": {"title": "DanceGRPO: Революция в обучении с подкреплением для генерации визуального контента", "desc": "Статья представляет DanceGRPO - унифицированный фреймворк 
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training"], "emoji": "📈", "ru": {"title": "Раскрытие секретов непрерывного предобучения языковых моделей", "desc": "Статья исследует динамику обучения при непрерывном предобучении (CPT) больших языковых моделей. Авторы наблюдают, как меняется 
[13.05.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#agents", "#optimization", "#hallucinations", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "Умный поиск: когда искать, а когда довериться себе", "desc": "Статья представляет новый подход к улучшению работы больших языковых моделей (LLM) с использованием м
[13.05.2025 05:12] Querying the API.
[13.05.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.
[13.05.2025 05:12] Response: {
  "desc": "Статья представляет метод AttentionInfluence для отбора данных, улучшающих способности языковых моделей к сложным рассуждениям. Этот метод использует небольшую предобученную модель для выбора данных путем маскирования головок внимания без необходимости в дополнительном обучении или разметке. Авторы применили AttentionInfluence к корпусу SmolLM для предобучения 7B-параметровой модели. Результаты показали значительное улучшение на нескольких бенчмарках, требующих знаний и рассуждений.",
  "emoji": "🧠",
  "title": "Улучшение способности ЯМ к рассуждениям через умный отбор данных"
}
[13.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection."

[13.05.2025 05:12] Response: ```python
["DATASET", "DATA", "TRAINING", "BENCHMARK", "SMALL_MODELS"]
```
[13.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection."

[13.05.2025 05:12] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[13.05.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks.","title":"Enhancing Reasoning in LLMs with AttentionInfluence"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks.', title='Enhancing Reasoning in LLMs with AttentionInfluence'))
[13.05.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近，研究者们越来越关注收集推理密集型的预训练数据，以提高大型语言模型（LLMs）的复杂推理能力。以往的方法通常依赖于监督分类器来识别这些数据，这需要人类或LLMs进行标注，常常引入领域特定的偏见。我们提出了一种名为AttentionInfluence的方法，这是一种简单而有效的无监督训练方法。通过简单的注意力头屏蔽操作，我们的方法使得一个小型的预训练语言模型能够作为强大的数据选择器，从而在推理任务中取得显著的性能提升。","title":"无监督推理数据选择的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='最近，研究者们越来越关注收集推理密集型的预训练数据，以提高大型语言模型（LLMs）的复杂推理能力。以往的方法通常依赖于监督分类器来识别这些数据，这需要人类或LLMs进行标注，常常引入领域特定的偏见。我们提出了一种名为AttentionInfluence的方法，这是一种简单而有效的无监督训练方法。通过简单的注意力头屏蔽操作，我们的方法使得一个小型的预训练语言模型能够作为强大的数据选择器，从而在推理任务中取得显著的性能提升。', title='无监督推理数据选择的新方法'))
[13.05.2025 05:12] Querying the API.
[13.05.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.
[13.05.2025 05:12] Response: {
  "desc": "Статья представляет унифицированный подход к обучению и сэмплированию непрерывных генеративных моделей. Авторы объединяют многошаговые методы, такие как диффузия и flow-matching, с малошаговыми подходами вроде consistency models. Их реализация, UCGM-{T,S}, достигает state-of-the-art результатов на ImageNet 256x256, используя диффузионный трансформер. Кроме того, UCGM-S улучшает производительность предобученной модели, снижая FID и количество шагов сэмплирования.",

  "emoji": "🔄",

  "title": "Единый фреймворк для непрерывных генеративных моделей: от многошаговых до малошаговых"
}
[13.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM."

[13.05.2025 05:12] Response: ```python
["CV", "TRAINING"]
```
[13.05.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM."

[13.05.2025 05:12] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[13.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources.","title":"Unifying Generative Models for Superior Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources.', title='Unifying Generative Models for Superior Performance'))
[13.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种统一的连续生成模型框架，旨在整合多步和少步生成方法的训练和采样。通过引入统一的训练和采样器（UCGM-{T,S}），我们实现了最先进的生成性能。实验结果表明，在ImageNet数据集上，UCGM-T能够在20步内将多步模型的FID降低到1.30，而少步模型在仅2步内达到1.42的FID。此外，使用UCGM-S对预训练模型进行改进，FID从250步的1.26降至仅40步的1.06。","title":"统一生成模型，提升生成性能！"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种统一的连续生成模型框架，旨在整合多步和少步生成方法的训练和采样。通过引入统一的训练和采样器（UCGM-{T,S}），我们实现了最先进的生成性能。实验结果表明，在ImageNet数据集上，UCGM-T能够在20步内将多步模型的FID降低到1.30，而少步模型在仅2步内达到1.42的FID。此外，使用UCGM-S对预训练模型进行改进，FID从250步的1.26降至仅40步的1.06。', title='统一生成模型，提升生成性能！'))
[13.05.2025 05:13] Using data from previous issue: {"categories": ["#training", "#interpretability", "#multimodal", "#hallucinations"], "emoji": "🔍", "ru": {"title": "Повышение надежности атрибуции в LLM: от текстового включения до механизма внимания", "desc": "Эта статья посвящена проблеме атрибуции в больших языковых моделях (LLM) при работе с док
[13.05.2025 05:13] Querying the API.
[13.05.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.
[13.05.2025 05:13] Response: {
  "desc": "Статья представляет новый метод машинного обучения под названием Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) для проектирования биологических последовательностей с множественными целевыми функциями. MOG-DFM использует предобученные генеративные модели на основе дискретного сопоставления потоков и направляет их к Парето-эффективным компромиссам между несколькими скалярными целями. Авторы демонстрируют эффективность метода на примерах генерации пептидов с оптимизированными свойствами и проектирования последовательностей ДНК с заданными характеристиками. MOG-DFM показывает себя мощным инструментом для многоцелевого дизайна биомолекулярных последовательностей.",
  "emoji": "🧬",
  "title": "Многоцелевая оптимизация биологических последовательностей с помощью дискретного сопоставления потоков"
}
[13.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design."

[13.05.2025 05:13] Response: ```python
["DATASET", "DATA", "TRAINING"]
```
[13.05.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design."

[13.05.2025 05:13] Response: ```python
["OPTIMIZATION", "SCIENCE"]
```
[13.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM\'s capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering.","title":"Optimizing Biomolecule Design with MOG-DFM"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM's capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering.", title='Optimizing Biomolecule Design with MOG-DFM'))
[13.05.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"在生物分子工程中，设计满足多种功能和生物物理标准的生物序列仍然是一个重要挑战。本文提出了一种名为多目标引导离散流匹配（MOG-DFM）的框架，能够在多个标量目标之间实现帕累托有效的权衡。MOG-DFM通过计算混合排名方向分数和应用自适应超锥过滤器，来引导预训练的离散时间流匹配生成器进行多目标优化。我们展示了MOG-DFM在生成优化的肽结合物和特定增强子类DNA序列方面的有效性，证明了其在多属性引导的生物分子序列设计中的强大能力。","title":"多目标优化，助力生物分子设计"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='在生物分子工程中，设计满足多种功能和生物物理标准的生物序列仍然是一个重要挑战。本文提出了一种名为多目标引导离散流匹配（MOG-DFM）的框架，能够在多个标量目标之间实现帕累托有效的权衡。MOG-DFM通过计算混合排名方向分数和应用自适应超锥过滤器，来引导预训练的离散时间流匹配生成器进行多目标优化。我们展示了MOG-DFM在生成优化的肽结合物和特定增强子类DNA序列方面的有效性，证明了其在多属性引导的生物分子序列设计中的强大能力。', title='多目标优化，助力生物分子设计'))
[13.05.2025 05:13] Loading Chinese text from previous data.
[13.05.2025 05:13] Renaming data file.
[13.05.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-05-13.json
[13.05.2025 05:13] Saving new data file.
[13.05.2025 05:13] Generating page.
[13.05.2025 05:13] Renaming previous page.
[13.05.2025 05:13] Renaming previous data. index.html to ./d/2025-05-13.html
[13.05.2025 05:13] [Experimental] Generating Chinese page for reading.
[13.05.2025 05:13] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '系列', 'pinyin': 'xì liè', 'trans': 'series'}, {'word': '专为', 'pinyin': 'zhuān wèi', 'trans': 'specially for'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'}, {'word': '参数', 'pinyin': 'cān shù', 'trans': 'parameters'}, {'word': '高效', 'pinyin': 'gāo xiào', 'trans': 'efficient'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '相当', 'pinyin': 'xiāng dāng', 'trans': 'equivalent'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '计算', 'pinyin': 'jì suàn', 'trans': 'compute'}, {'word': '资源', 'pinyin': 'zī yuán', 'trans': 'resources'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '自定义', 'pinyin': 'zì dìng yì', 'trans': 'customize'}, {'word': '分词器', 'pinyin': 'fēn cí qì', 'trans': 'tokenizer'}, {'word': '平衡', 'pinyin': 'píng héng', 'trans': 'balance'}, {'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'instruction'}, {'word': '类型', 'pinyin': 'lèi xíng', 'trans': 'type'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learn'}, {'word': '加权', 'pinyin': 'jiā quán', 'trans': 'weighted'}, {'word': '交叉熵', 'pinyin': 'jiāo chā shāng', 'trans': 'cross-entropy'}, {'word': '损失', 'pinyin': 'sǔn shī', 'trans': 'loss'}, {'word': '自适应', 'pinyin': 'zì shì yìng', 'trans': 'adaptive'}, {'word': '学习率', 'pinyin': 'xué xí lǜ', 'trans': 'learning rate'}, {'word': '策划', 'pinyin': 'cè huà', 'trans': 'plan'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'token'}, {'word': '文档', 'pinyin': 'wén dàng', 'trans': 'document'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '竞争力', 'pinyin': 'jìng zhēng lì', 'trans': 'competitiveness'}, {'word': '配置', 'pinyin': 'pèi zhì', 'trans': 'configuration'}, {'word': '紧凑', 'pinyin': 'jǐn còu', 'trans': 'compact'}, {'word': '强大', 'pinyin': 'qiáng dà', 'trans': 'powerful'}]
[13.05.2025 05:13] Renaming previous Chinese page.
[13.05.2025 05:13] Renaming previous data. zh.html to ./d/2025-05-12_zh_reading_task.html
[13.05.2025 05:13] Writing Chinese reading task.
[13.05.2025 05:13] Writing result.
[13.05.2025 05:13] Renaming log file.
[13.05.2025 05:13] Renaming previous data. log.txt to ./logs/2025-05-13_last_log.txt
