[08.10.2025 04:14] Read previous papers.
[08.10.2025 04:14] Generating top page (month).
[08.10.2025 04:14] Writing top page (month).
[08.10.2025 05:12] Read previous papers.
[08.10.2025 05:12] Get feed.
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06217
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26328
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03270
[08.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.06062
[08.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.06036
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05432
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05560
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04081
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05137
[08.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.05122
[08.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.24107
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06182
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06131
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05367
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05342
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05156
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06218
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06139
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04087
[08.10.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.03978
[08.10.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02341
[08.10.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.10.2025 05:12] No deleted papers detected.
[08.10.2025 05:12] Downloading and parsing papers (pdf, html). Total: 21.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.06217.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.06217.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.06217.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.26328.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2509.26328.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2509.26328.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.03270.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.03270.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.03270.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.06062.
[08.10.2025 05:12] Downloading paper 2510.06062 from http://arxiv.org/pdf/2510.06062v1...
[08.10.2025 05:12] Extracting affiliations from text.
[08.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-10-7 ASPO: Asymmetric Importance Sampling Policy Optimization Jiakang Wang1*, Runze Liu1,2*, Lei Lin1, Wenping Hu1, Xiu Li2, Fuzheng Zhang1, Guorui Zhou1 and Kun Gai1 1Kuaishou Technology, 2Tsinghua University 5 2 0 2 7 ] . [ 1 2 6 0 6 0 . 0 1 5 2 : r Abstract: Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of tokenlevel weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0. 1. Introduction Reinforcement Learning from Verifiable Rewards (RLVR) has recently emerged as powerful framework for Large Language Model (LLM) post-training (DeepSeek-AI et al., 2025; Yu et al., 2025). Among RLVR methods, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) has become widely adopted algorithm for Outcome-Supervised RL (OSRL), inspiring numerous variants and follow-up work"
[08.10.2025 05:12] Response: ```python
["Kuaishou Technology", "Tsinghua University"]
```
[08.10.2025 05:12] Deleting PDF ./assets/pdf/2510.06062.pdf.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.06036.
[08.10.2025 05:12] Downloading paper 2510.06036 from http://arxiv.org/pdf/2510.06036v1...
[08.10.2025 05:12] Extracting affiliations from text.
[08.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"REFUSAL FALLS OFF CLIFF: HOW SAFETY ALIGNMENT FAILS IN REASONING? Qingyu Yin1 Chak Tou Leong2 Wenxuan Huang3 Wenjie Li2 Linyi Yang4 Xiting Wang5 Jaehong Yoon6 YunXing7 XingYu7 Jinjin Gu8 5 2 0 2 ] . [ 1 6 3 0 6 0 . 0 1 5 2 : r 1Zhejiang University, 2Hong Kong Polytechnic University, 3East China Normal University, 4Southern University of Science and Technology, 5Renmin University 6Nanyang Technological University 7Xiaohongshu Inc., 8INSAIT "
[08.10.2025 05:12] Response: ```python
[
    "Zhejiang University",
    "Hong Kong Polytechnic University",
    "East China Normal University",
    "Southern University of Science and Technology",
    "Renmin University",
    "Nanyang Technological University",
    "Xiaohongshu Inc.",
    "INSAIT"
]
```
[08.10.2025 05:12] Deleting PDF ./assets/pdf/2510.06036.pdf.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.05432.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.05432.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.05432.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.05560.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.05560.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.05560.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.04081.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.04081.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.04081.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.05137.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.05137.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.05137.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.05122.
[08.10.2025 05:12] Downloading paper 2510.05122 from http://arxiv.org/pdf/2510.05122v1...
[08.10.2025 05:12] Extracting affiliations from text.
[08.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 3 ] . [ 1 2 2 1 5 0 . 0 1 5 2 : r DianJin-CARE CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation Jie Zhu1,2, Yuanchen Zhou2, Shuo Jiang2, Junhui Li1, Lifan Guo2, Feng Chen2, Chi Zhang2, Fang Kong1 1School of Computer Science and Technology, Soochow University 2Qwen DianJin Team, Alibaba Cloud Computing https://huggingface.co/DianJin https://modelscope.cn/organization/tongyi dianjin https://github.com/aliyun/qwen-dianjin https://tongyi.aliyun.com/dianjin "
[08.10.2025 05:12] Response: ```python
[
    "School of Computer Science and Technology, Soochow University",
    "Qwen DianJin Team, Alibaba Cloud Computing"
]
```
[08.10.2025 05:12] Deleting PDF ./assets/pdf/2510.05122.pdf.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24107.
[08.10.2025 05:12] Downloading paper 2509.24107 from http://arxiv.org/pdf/2509.24107v1...
[08.10.2025 05:12] Extracting affiliations from text.
[08.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 7 0 1 4 2 . 9 0 5 2 : r a FATHOM-DEEPRESEARCH: UNLOCKING LONG HORIZON INFORMATION RETRIEVAL AND SYNTHESIS FOR SLMS Shreyas Singh* Fractal AI Research shreyas.singh@fractal.ai Pradeep Moturi* Fractal AI Research pradeep.moturi@fractal.ai Kunal Singh* Fractal AI Research kunal.singh@fractal.ai "
[08.10.2025 05:12] Response: ```python
["Fractal AI Research"]
```
[08.10.2025 05:12] Deleting PDF ./assets/pdf/2509.24107.pdf.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.06182.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.06182.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.06182.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.06131.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.06131.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.06131.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.05367.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.05367.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.05367.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.05342.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.05342.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.05342.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.05156.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.05156.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.05156.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.06218.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.06218.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.06218.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.06139.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.06139.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.06139.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.04087.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.04087.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.04087.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.03978.
[08.10.2025 05:12] Downloading paper 2510.03978 from http://arxiv.org/pdf/2510.03978v1...
[08.10.2025 05:12] Extracting affiliations from text.
[08.10.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 8 7 9 3 0 . 0 1 5 2 : r No Tokens Wasted: Leveraging Long Context in Biomedical VisionLanguage Models Min Woo Sun1 Alejandro Lozano1 Javier Gamazo Tejero2 Vishwesh Nath2 Xiao Xiao Sun1 James Burgess1 Yuhui Zhang1 Kun Yuan1 Robert Tibshirani1 Sean Huver2 Serena Yeung-Levy1 Equal contribution 1Stanford University, USA 2NVIDIA, USA minwoos@stanford.edu lozanoe@stanford.edu javierg@nvidia.com vnath@nvidia.com xxsun@stanford.edu jmhb@stanford.edu yuhuiz@stanford.edu kun@unistra.fr tibs@stanford.edu shuver@nvidia.com syyeung@stanford.edu "
[08.10.2025 05:12] Response: ```python
["Stanford University, USA", "NVIDIA, USA"]
```
[08.10.2025 05:12] Deleting PDF ./assets/pdf/2510.03978.pdf.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2510.02341.
[08.10.2025 05:12] Extra JSON file exists (./assets/json/2510.02341.json), skip PDF parsing.
[08.10.2025 05:12] Paper image links file exists (./assets/img_data/2510.02341.json), skip HTML parsing.
[08.10.2025 05:12] Success.
[08.10.2025 05:12] Enriching papers with extra data.
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 0. TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  					AI-generated summary 				 Process Reward Models (PRMs)...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 1. Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  					AI-generated summary 				 Autoregressive (AR) large language models (LLMs) have achieved remarkable p...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 2. CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  					AI-generated summary 				 Diffusion language models promise bidirectional context and infilling capabilities that autoregressi...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 3. ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  					AI-generated summary 				 Recent Large Language Model (LLM) post-training methods rely on t...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 4. Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  					AI-generated summary 				 Large reasoning m...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 5. AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  					AI-generated summary 				 Large language models (LLMs) demo...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 6. HoloScene is an interactive 3D reconstruction framework that achieves geometry completeness, object interactivity, physical plausibility, photorealistic rendering, and realistic physical properties through an energy-based optimization problem.  					AI-generated summary 				 Digitizing the physical ...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 7. Caco, a code-assisted chain-of-thought framework, automates the generation of high-quality, verifiable, and diverse reasoning data, enhancing the performance of large language models on mathematical reasoning tasks.  					AI-generated summary 				 Reasoning capability is pivotal for Large Language M...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 8. WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  					AI-generated summary 				 RAG (Retrieval-Augm...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 9. CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  					AI-generated summary 				 Emotional Support Conversation (ESC) plays a vital role in al...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 10. Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  					AI-generated summary 				 Tool-integrated reasoning has emerged as a key focus for enab...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 11. Language models use positional, lexical, and reflexive mechanisms to bind and retrieve entities, with a causal model achieving high accuracy in predicting next tokens across various tasks and input lengths.  					AI-generated summary 				 A key component of in-context reasoning is the ability of lan...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 12. MeDiM, a medical discrete diffusion model, integrates multimodal biomedical data by learning shared distributions across images, text, and clinical notes, achieving high-fidelity generation and enhanced downstream performance.  					AI-generated summary 				 Recent advances in generative medical mod...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 13. The paper proposes stage-specific strategies to accelerate diffusion model inference in video generation, reducing memory usage and maintaining quality.  					AI-generated summary 				 Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. ...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 14. MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  					AI-generated summary 				 Direct Preference Optimization (DPO) has emerged as a simple and effective me...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 15. VeriGuard is a framework that ensures formal safety guarantees for LLM-based agents through offline validation and online monitoring.  					AI-generated summary 				 The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and pr...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 16. EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  					AI-generated summary 				 Most existing benchmarks for egocentric vision understand...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 17. FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  					AI-generated summary 				 Referring Video Object Segmentation (RVOS) requires segmenting spec...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 18. A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  					AI-generated summary 				 Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pair...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 19. Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  					AI-generated summary 				 Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 t...
[08.10.2025 05:12] ********************************************************************************
[08.10.2025 05:12] Abstract 20. DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  					AI-generated summary 				 Real-world large language model deployments (e...
[08.10.2025 05:12] Read previous papers.
[08.10.2025 05:12] Generating reviews via LLM API.
[08.10.2025 05:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#data", "#benchmark", "#optimization", "#dataset"], "emoji": "📊", "ru": {"title": "TaTToo: Process Reward Model с инструментами для работы с таблицами", "desc": "Исследователи представили TaTToo — новую Process Reward Model для улучшения рассуждений
[08.10.2025 05:12] Using data from previous issue: {"categories": ["#training", "#inference", "#open_source", "#diffusion", "#optimization", "#architecture"], "emoji": "⚡", "ru": {"title": "Быстрая параллельная генерация текста через блочную диффузию", "desc": "Статья представляет Fast-dLLM v2 — блочную диффузионную языковую модель, которая эффектив
[08.10.2025 05:12] Using data from previous issue: {"categories": ["#inference", "#training", "#open_source", "#diffusion", "#small_models", "#dataset"], "emoji": "🌊", "ru": {"title": "Легковесный диффузионный кодер с направленной генерацией", "desc": "CoDA — это языковая модель на основе диффузии с 1.7 миллиардами параметров, специально обученная д
[08.10.2025 05:12] Querying the API.
[08.10.2025 05:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  					AI-generated summary 				 Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.
[08.10.2025 05:13] Response: ```json
{
  "title": "Асимметричная важность токенов для стабильного обучения LLM",
  "desc": "Исследователи обнаружили фундаментальную проблему в методах обучения с подкреплением для больших языковых моделей: несбалансированное взвешивание токенов с положительными и отрицательными преимуществами. Традиционные подходы подавляют обновление низковероятностных токенов и чрезмерно усиливают высоковероятностные. Предложенный метод ASPO решает эту проблему путем «переворачивания» коэффициентов Importance Sampling для токенов с положительным преимуществом и использования механизма мягкого двойного клиппинга. Эксперименты показали значительное улучшение стабильности обучения и итоговой производительности на задачах программирования и математических рассуждений.",
  "emoji": "⚖️"
}
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  					AI-generated summary 				 Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0."

[08.10.2025 05:13] Response: ```python
['RL', 'TRAINING']
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  					AI-generated summary 				 Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0."

[08.10.2025 05:13] Response: ```python
["OPTIMIZATION"]
```
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Asymmetric Importance Sampling Policy Optimization (ASPO) to improve training in Large Language Models (LLMs) during Outcome-Supervised Reinforcement Learning (OSRL). It identifies a problem with the Importance Sampling ratios, which causes an imbalance in how positive and negative tokens are weighted, leading to ineffective learning. ASPO addresses this by flipping the ratios for positive-advantage tokens and adding a soft dual-clipping mechanism to stabilize updates. Experiments show that ASPO enhances training stability and performance, reducing premature convergence compared to existing methods.","title":"Balancing Token Weighting for Better LLM Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Asymmetric Importance Sampling Policy Optimization (ASPO) to improve training in Large Language Models (LLMs) during Outcome-Supervised Reinforcement Learning (OSRL). It identifies a problem with the Importance Sampling ratios, which causes an imbalance in how positive and negative tokens are weighted, leading to ineffective learning. ASPO addresses this by flipping the ratios for positive-advantage tokens and adding a soft dual-clipping mechanism to stabilize updates. Experiments show that ASPO enhances training stability and performance, reducing premature convergence compared to existing methods.', title='Balancing Token Weighting for Better LLM Training'))
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ASPO（不对称重要性采样策略优化）解决了在结果监督强化学习（OSRL）中令牌加权不平衡的问题。通过翻转正优势令牌的重要性采样比率，ASPO使得正负令牌的更新方向一致，从而提高了训练的稳定性和性能。此外，ASPO还引入了一种软双剪切机制，以稳定极端更新并保持梯度流动。实验结果表明，ASPO在编码和数学推理基准测试中显著改善了训练效果，减少了过早收敛现象。","title":"优化令牌加权，提升训练稳定性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ASPO（不对称重要性采样策略优化）解决了在结果监督强化学习（OSRL）中令牌加权不平衡的问题。通过翻转正优势令牌的重要性采样比率，ASPO使得正负令牌的更新方向一致，从而提高了训练的稳定性和性能。此外，ASPO还引入了一种软双剪切机制，以稳定极端更新并保持梯度流动。实验结果表明，ASPO在编码和数学推理基准测试中显著改善了训练效果，减少了过早收敛现象。', title='优化令牌加权，提升训练稳定性'))
[08.10.2025 05:13] Querying the API.
[08.10.2025 05:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  					AI-generated summary 				 Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\% of these heads can reduce attack success rates below 10\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.
[08.10.2025 05:13] Response: ```json
{
  "title": "Обрыв отказа: почему безопасные модели внезапно становятся опасными",
  "desc": "Исследователи обнаружили феномен «обрыва отказа» в больших reasoning-моделях: модели правильно распознают вредоносные запросы в процессе рассуждения, но резко теряют намерение отказать прямо перед генерацией ответа. Используя методы механистической интерпретируемости, учёные выявили небольшой набор attention heads, которые подавляют безопасное поведение модели. На основе этих находок был разработан метод Cliff-as-a-Judge, который позволяет восстановить безопасность модели, используя всего 1.7% обучающих данных по сравнению с обычным подходом. Абляция всего 3% проблемных attention heads снижает успешность атак до уровня ниже 10%.",
  "emoji": "🧗"
}
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  					AI-generated summary 				 Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\% of these heads can reduce attack success rates below 10\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment."

[08.10.2025 05:13] Response: ```python
["DATA", "TRAINING", "ARCHITECTURE"]
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  					AI-generated summary 				 Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\% of these heads can reduce attack success rates below 10\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment."

[08.10.2025 05:13] Response: ```python
["INTERPRETABILITY", "REASONING", "ALIGNMENT"]
```
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores a phenomenon called the refusal cliff in large reasoning models (LRMs), where the models show a significant drop in their intention to refuse harmful prompts just before generating an output. The authors use mechanistic interpretability to analyze how these models can recognize harmful inputs but fail to maintain their refusal intentions at the final stages of processing. They identify specific attention heads that contribute negatively to this behavior and demonstrate that reducing the influence of just a small percentage of these heads can greatly enhance the models\' safety. Additionally, they introduce a new method called Cliff-as-a-Judge, which selects training examples that highlight the refusal cliff, allowing for effective safety improvements with minimal data.","title":"Understanding and Mitigating the Refusal Cliff in Reasoning Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores a phenomenon called the refusal cliff in large reasoning models (LRMs), where the models show a significant drop in their intention to refuse harmful prompts just before generating an output. The authors use mechanistic interpretability to analyze how these models can recognize harmful inputs but fail to maintain their refusal intentions at the final stages of processing. They identify specific attention heads that contribute negatively to this behavior and demonstrate that reducing the influence of just a small percentage of these heads can greatly enhance the models' safety. Additionally, they introduce a new method called Cliff-as-a-Judge, which selects training examples that highlight the refusal cliff, allowing for effective safety improvements with minimal data.", title='Understanding and Mitigating the Refusal Cliff in Reasoning Models'))
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究探讨了大型推理模型中的拒绝悬崖机制，发现这些模型在生成输出前拒绝意图会急剧下降。通过线性探测方法，我们追踪了拒绝意图在标记位置的变化，发现许多模型在思考过程中能够识别有害提示，但在输出前的最后几个标记处拒绝意图却显著降低。我们通过因果干预分析，识别出少量对拒绝行为产生负面影响的注意力头，去除这些头可以显著降低攻击成功率。基于这些发现，我们提出了一种新的数据选择方法，利用拒绝悬崖的特征来高效修复推理模型的安全对齐。","title":"揭示推理模型的拒绝悬崖机制"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究探讨了大型推理模型中的拒绝悬崖机制，发现这些模型在生成输出前拒绝意图会急剧下降。通过线性探测方法，我们追踪了拒绝意图在标记位置的变化，发现许多模型在思考过程中能够识别有害提示，但在输出前的最后几个标记处拒绝意图却显著降低。我们通过因果干预分析，识别出少量对拒绝行为产生负面影响的注意力头，去除这些头可以显著降低攻击成功率。基于这些发现，我们提出了一种新的数据选择方法，利用拒绝悬崖的特征来高效修复推理模型的安全对齐。', title='揭示推理模型的拒绝悬崖机制'))
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#rlhf", "#agents"], "emoji": "🧪", "ru": {"title": "Может ли AI стать самостоятельным исследователем в машинном обучении?", "desc": "Исследование представляет AInstein — фреймворк для оценки способности больших языковых моделей (LLM) решать исс
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#3d", "#optimization", "#games", "#benchmark"], "emoji": "🏗️", "ru": {"title": "Цифровые двойники с физикой и фотореализмом", "desc": "HoloScene — это фреймворк для интерактивной 3D-реконструкции физического мира в виртуальные среды, готовые к симуляции. Система использует граф сцен
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#math", "#data", "#training"], "emoji": "🔢", "ru": {"title": "Код как основа для автоматической генерации качественных рассуждений", "desc": "Caco — это фреймворк для автоматической генерации высококачественных данных для обучения математическ
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rag", "#benchmark", "#leakage", "#architecture", "#agents"], "emoji": "🔍", "ru": {"title": "WebDetective: Как научить AI-агентов думать самостоятельно, а не следовать подсказкам", "desc": "WebDetective — это новый бенчмарк для оценки многошаговых рассуж
[08.10.2025 05:13] Querying the API.
[08.10.2025 05:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  					AI-generated summary 				 Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.
[08.10.2025 05:13] Response: ```json
{
  "title": "Усиление когнитивного мышления для эмоциональной поддержки",
  "desc": "Статья представляет CARE — фреймворк для улучшения диалоговых систем эмоциональной поддержки. В отличие от существующих подходов, которые фокусируются на синтетических данных, CARE развивает когнитивное мышление модели, используя оригинальный набор данных для обучения. Система применяет reinforcement learning для генерации логически связных и поддерживающих ответов. Эксперименты показывают значительное улучшение качества эмпатии и логической последовательности ответов без необходимости в масштабных синтетических корпусах.",
  "emoji": "🤗"
}
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  					AI-generated summary 				 Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems."

[08.10.2025 05:13] Response: ```python
['RL', 'RLHF']
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  					AI-generated summary 				 Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems."

[08.10.2025 05:13] Response: ```python
['REASONING']
```
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CARE is a framework designed to improve emotional support conversations (ESC) by enhancing cognitive reasoning through reinforcement learning. Unlike previous methods that depend on large amounts of synthetic data, CARE focuses on using existing training data to create more coherent and empathetic responses. The framework emphasizes the importance of logical reasoning in generating supportive dialogue, which is crucial for effective emotional support. Experimental results show that CARE significantly boosts the quality and empathy of responses, making AI systems more human-like in their interactions.","title":"Enhancing Emotional Support with Cognitive Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CARE is a framework designed to improve emotional support conversations (ESC) by enhancing cognitive reasoning through reinforcement learning. Unlike previous methods that depend on large amounts of synthetic data, CARE focuses on using existing training data to create more coherent and empathetic responses. The framework emphasizes the importance of logical reasoning in generating supportive dialogue, which is crucial for effective emotional support. Experimental results show that CARE significantly boosts the quality and empathy of responses, making AI systems more human-like in their interactions.', title='Enhancing Emotional Support with Cognitive Reasoning'))
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CARE是一个框架，通过强化学习增强情感支持对话中的认知推理，提升响应质量和同理心，而不依赖于大规模的合成数据。情感支持对话在缓解心理压力和提供情感价值方面起着重要作用。以往的研究主要集中在数据增强和合成语料库的构建上，忽视了有效情感支持背后的深层认知推理过程。CARE利用原始的情感支持对话训练集，引导模型生成逻辑连贯和支持性的响应，从而显著提升认知推理能力。","title":"CARE：提升情感支持对话的认知推理能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CARE是一个框架，通过强化学习增强情感支持对话中的认知推理，提升响应质量和同理心，而不依赖于大规模的合成数据。情感支持对话在缓解心理压力和提供情感价值方面起着重要作用。以往的研究主要集中在数据增强和合成语料库的构建上，忽视了有效情感支持背后的深层认知推理过程。CARE利用原始的情感支持对话训练集，引导模型生成逻辑连贯和支持性的响应，从而显著提升认知推理能力。', title='CARE：提升情感支持对话的认知推理能力'))
[08.10.2025 05:13] Querying the API.
[08.10.2025 05:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  					AI-generated summary 				 Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.
[08.10.2025 05:13] Response: ```json
{
  "title": "Глубокий веб-поиск с помощью специализированных агентов",
  "desc": "Представлена система Fathom-DeepResearch, состоящая из двух специализированных моделей по 4 миллиарда параметров для сложных информационных задач. Первая модель оптимизирована для веб-поиска и запросов к страницам, обучена с помощью мультиагентного подхода и может делать более 20 последовательных вызовов инструментов. Вторая модель преобразует результаты поиска в структурированные отчёты с цитатами. Система достигает state-of-the-art результатов среди open-weights моделей на бенчмарках для поиска информации и различных задачах рассуждений.",
  "emoji": "🔍",
  "desc": "Представлена система Fathom-DeepResearch, состоящая из двух специализированных моделей по 4 миллиарда параметров для сложных информационных задач. Первая модель оптимизирована для веб-поиска и запросов к страницам, обучена с помощью мультиагентного подхода и может делать более 20 последовательных вызовов инструментов. Вторая модель преобразует результаты поиска в структурированные отчёты с цитатами. Система достигает state-of-the-art результатов среди open-weights моделей на бенчмарках для поиска информации и различных задачах рассуждений."
}
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  					AI-generated summary 				 Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA."

[08.10.2025 05:13] Response: ```python
['AGENTS', 'DATASET', 'RL', 'BENCHMARK']
```
[08.10.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  					AI-generated summary 				 Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA."

[08.10.2025 05:13] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Fathom-DeepResearch is an advanced agentic system designed for effective web search and report synthesis. It consists of two specialized models: Fathom-Search-4B, which excels in evidence-based investigations through live web searches, and Fathom-Synthesizer-4B, which transforms search results into structured reports. The system incorporates innovative techniques like DUETQA for dataset generation and RAPO for enhancing reinforcement learning stability. With its state-of-the-art performance on various benchmarks, Fathom-DeepResearch demonstrates exceptional capabilities in handling complex information-seeking and reasoning tasks.","title":"Revolutionizing Web Search and Report Synthesis with Fathom-DeepResearch"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Fathom-DeepResearch is an advanced agentic system designed for effective web search and report synthesis. It consists of two specialized models: Fathom-Search-4B, which excels in evidence-based investigations through live web searches, and Fathom-Synthesizer-4B, which transforms search results into structured reports. The system incorporates innovative techniques like DUETQA for dataset generation and RAPO for enhancing reinforcement learning stability. With its state-of-the-art performance on various benchmarks, Fathom-DeepResearch demonstrates exceptional capabilities in handling complex information-seeking and reasoning tasks.', title='Revolutionizing Web Search and Report Synthesis with Fathom-DeepResearch'))
[08.10.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Fathom-DeepResearch 是一个智能系统，专门用于网络搜索和报告合成，能够在开放式信息检索任务和多样化推理任务中表现出色。该系统由两个专门模型组成：Fathom-Search-4B 和 Fathom-Synthesizer-4B。Fathom-Search-4B 通过实时网络搜索和针对网页查询进行证据基础调查，采用了多项先进技术来优化其性能。Fathom-Synthesizer-4B 则将多轮 DeepSearch 追踪转换为结构化的、引用密集的 DeepResearch 报告，确保信息的全面合成。","title":"智能搜索与报告合成的未来"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Fathom-DeepResearch 是一个智能系统，专门用于网络搜索和报告合成，能够在开放式信息检索任务和多样化推理任务中表现出色。该系统由两个专门模型组成：Fathom-Search-4B 和 Fathom-Synthesizer-4B。Fathom-Search-4B 通过实时网络搜索和针对网页查询进行证据基础调查，采用了多项先进技术来优化其性能。Fathom-Synthesizer-4B 则将多轮 DeepSearch 追踪转换为结构化的、引用密集的 DeepResearch 报告，确保信息的全面合成。', title='智能搜索与报告合成的未来'))
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#reasoning", "#long_context", "#data", "#multimodal", "#interpretability", "#architecture"], "emoji": "🔗", "ru": {"title": "Три механизма связывания сущностей в языковых моделях", "desc": "Исследование показывает, как языковые модели связывают и извлекают сущности в контексте, испол
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#science", "#healthcare", "#multimodal"], "emoji": "🏥", "ru": {"title": "Единая диффузионная модель для всех медицинских модальностей", "desc": "MeDiM — это первая медицинская модель дискретной диффузии, которая объединяет разные типы биомедицинских данных (изображения
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#video", "#optimization", "#inference"], "emoji": "🎬", "ru": {"title": "Ускорение видеогенерации через управление памятью на разных стадиях", "desc": "Исследователи предложили метод ускорения генерации видео с помощью диффузионных моделей без дополнител
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#rlhf"], "emoji": "⚖️", "ru": {"title": "Адаптивные веса для каждого примера делают обучение по предпочтениям эффективнее", "desc": "MADPO — это новый метод выравнивания больших языковых моделей по предпочтениям, который решает проблему DP
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#inference", "#alignment", "#agents", "#security", "#healthcare"], "emoji": "🛡️", "ru": {"title": "Формальные гарантии безопасности для LLM-агентов", "desc": "VeriGuard — это фреймворк для обеспечения формальных гарантий безопасности AI-агентов на основе LLM в критических областях в
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#multimodal", "#long_context", "#benchmark", "#games", "#transfer_learning", "#synthetic", "#cv"], "emoji": "🌙", "ru": {"title": "Проверка AI-зрения в темноте от первого лица", "desc": "EgoNight — это первый комплексный бенчмарк для оценки egocentric-видения в ночных условиях с фоку
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark", "#games", "#alignment"], "emoji": "🌊", "ru": {"title": "Сегментация видео через непрерывную деформацию под управлением текста", "desc": "FlowRVS решает задачу сегментации объектов в видео по текстовому описанию (RVOS), переформулируя её как проб
[08.10.2025 05:13] Using data from previous issue: {"categories": ["#inference", "#training", "#data", "#rlhf", "#alignment"], "emoji": "🚦", "ru": {"title": "Научить AI понимать, что хорошо, а не только что лучше", "desc": "Исследователи предлагают новый подход к обучению моделей предпочтений, добавляя «внешнюю опцию» в данные сравнений. Традиционны
[08.10.2025 05:13] Querying the API.
[08.10.2025 05:13] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  					AI-generated summary 				 Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.
[08.10.2025 05:14] Response: ```json
{
  "desc": "Исследователи обнаружили, что стандартные vision-language модели с коротким контекстом (до 77 токенов) теряют важную информацию при обработке биомедицинских описаний изображений, которые часто содержат гораздо больше текста. Они создали датасет BIOMEDICA-LongCAP из 1 миллиона пар изображение-текст с расширенными описаниями из научных статей и обучили модель BMC-LongCLIP, поддерживающую до 512 токенов. Новая модель увеличила контекстное окно в 6.6 раз и сократила потери токенов с 55% до 2.2%, что привело к улучшению точности поиска изображений на 30% и классификации на 2%. Результаты показывают, что использование длинного контекста является перспективным направлением для развития мультимодальных моделей в биомедицине.",
  "emoji": "🔬",
  "title": "Длинный контекст для биомедицинских изображений: больше слов — лучше результат"
}
```
[08.10.2025 05:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  					AI-generated summary 				 Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs."

[08.10.2025 05:14] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'MULTIMODAL', 'HEALTHCARE']
```
[08.10.2025 05:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  					AI-generated summary 				 Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs."

[08.10.2025 05:14] Response: ```python
["LONG_CONTEXT"]
```
[08.10.2025 05:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how increasing the context length of text encoders in vision-language models (VLMs) can enhance performance on biomedical captioning tasks. Traditional VLMs are limited to short text windows, which often leads to the loss of important information in longer biomedical captions. By extending the context length to 512 tokens, the authors introduce a new dataset, BIOMEDICA-LongCAP, which includes 1 million image-caption pairs with detailed descriptions. The results show that this approach significantly improves retrieval and classification metrics, highlighting the benefits of long-context modeling in biomedical applications.","title":"Unlocking the Power of Long Contexts in Biomedical Captioning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how increasing the context length of text encoders in vision-language models (VLMs) can enhance performance on biomedical captioning tasks. Traditional VLMs are limited to short text windows, which often leads to the loss of important information in longer biomedical captions. By extending the context length to 512 tokens, the authors introduce a new dataset, BIOMEDICA-LongCAP, which includes 1 million image-caption pairs with detailed descriptions. The results show that this approach significantly improves retrieval and classification metrics, highlighting the benefits of long-context modeling in biomedical applications.', title='Unlocking the Power of Long Contexts in Biomedical Captioning'))
[08.10.2025 05:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了在视觉语言模型中扩展文本编码器的上下文长度对生物医学描述任务的影响。传统的视觉语言模型通常使用较短的文本窗口，这导致长格式的描述被截断。研究发现，使用更长的上下文可以提高检索和分类的性能，因为它允许模型利用更详细的描述。为此，作者引入了一个新的数据集BIOMEDICA-LongCAP，并训练了支持长上下文的生物医学视觉语言模型BMC-LongCLIP，显著提升了模型的性能。","title":"扩展上下文，提升生物医学模型性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文探讨了在视觉语言模型中扩展文本编码器的上下文长度对生物医学描述任务的影响。传统的视觉语言模型通常使用较短的文本窗口，这导致长格式的描述被截断。研究发现，使用更长的上下文可以提高检索和分类的性能，因为它允许模型利用更详细的描述。为此，作者引入了一个新的数据集BIOMEDICA-LongCAP，并训练了支持长上下文的生物医学视觉语言模型BMC-LongCLIP，显著提升了模型的性能。', title='扩展上下文，提升生物医学模型性能'))
[08.10.2025 05:14] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#rlhf"], "emoji": "🔄", "ru": {"title": "Обучение LLM на недовольстве пользователей: превращаем негатив в качество", "desc": "В статье представлен метод DRIFT для обучения больших языковых моделей на основе сигналов недовольства пользовател
[08.10.2025 05:14] Renaming data file.
[08.10.2025 05:14] Renaming previous data. hf_papers.json to ./d/2025-10-08.json
[08.10.2025 05:14] Saving new data file.
[08.10.2025 05:14] Generating page.
[08.10.2025 05:14] Renaming previous page.
[08.10.2025 05:14] Renaming previous data. index.html to ./d/2025-10-08.html
[08.10.2025 05:14] Writing result.
[08.10.2025 05:14] Renaming log file.
[08.10.2025 05:14] Renaming previous data. log.txt to ./logs/2025-10-08_last_log.txt
