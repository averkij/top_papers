[08.10.2025 16:15] Read previous papers.
[08.10.2025 16:15] Generating top page (month).
[08.10.2025 16:15] Writing top page (month).
[08.10.2025 17:10] Read previous papers.
[08.10.2025 17:10] Get feed.
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06217
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04871
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24107
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26328
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03270
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04081
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04162
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06052
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06062
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23379
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06182
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06208
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06131
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05485
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05560
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05432
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06036
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05367
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05342
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03506
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05592
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05318
[08.10.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.05251
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02300
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05137
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06218
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05156
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05122
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06219
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06030
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05571
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03978
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02341
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06213
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06139
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06107
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06071
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06056
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05681
[08.10.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.04506
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04087
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06101
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05934
[08.10.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2510.04514
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00880
[08.10.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21499
[08.10.2025 17:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.10.2025 17:10] No deleted papers detected.
[08.10.2025 17:10] Downloading and parsing papers (pdf, html). Total: 46.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.06217.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.06217.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.06217.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.04871.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.04871.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.04871.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.24107.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2509.24107.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.24107.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.26328.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2509.26328.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.26328.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.03270.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.03270.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.03270.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.04081.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.04081.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.04081.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.04162.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.04162.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.04162.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.06052.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.06052.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.06052.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.06062.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.06062.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.06062.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2509.23379.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2509.23379.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2509.23379.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.06182.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.06182.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.06182.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.06208.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.06208.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.06208.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.06131.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.06131.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.06131.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05485.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.05485.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.05485.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05560.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.05560.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.05560.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05432.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.05432.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.05432.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.06036.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.06036.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.06036.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05367.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.05367.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.05367.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05342.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.05342.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.05342.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.03506.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.03506.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.03506.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05592.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.05592.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.05592.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05318.
[08.10.2025 17:10] Extra JSON file exists (./assets/json/2510.05318.json), skip PDF parsing.
[08.10.2025 17:10] Paper image links file exists (./assets/img_data/2510.05318.json), skip HTML parsing.
[08.10.2025 17:10] Success.
[08.10.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2510.05251.
[08.10.2025 17:10] Downloading paper 2510.05251 from http://arxiv.org/pdf/2510.05251v1...
[08.10.2025 17:11] Extracting affiliations from text.
[08.10.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 1 5 2 5 0 . 0 1 5 2 : r a LET IT CALM: EXPLORATORY ANNEALED DECODING FOR VERIFIABLE REINFORCEMENT LEARNING Lin Gui2*, Lizhu Zhang5, Chenxiao Yang3*, Chenghao Yang1*, Victor Veitch2,4, Zhuokai Zhao5 1Department of Computer Science, University of Chicago 2Department of Statistics, University of Chicago 3Toyota Technological Insitute at Chicago 4Data Science Institute, University of Chicago 5Meta AI {chenghao, glin6}@uchicago.edu, chenxiao@ttic.edu, zhuokai@meta.com *Equal Contribution Codebase Website "
[08.10.2025 17:11] Response: ```python
[
    "Department of Computer Science, University of Chicago",
    "Department of Statistics, University of Chicago",
    "Toyota Technological Institute at Chicago",
    "Data Science Institute, University of Chicago",
    "Meta AI"
]
```
[08.10.2025 17:11] Deleting PDF ./assets/pdf/2510.05251.pdf.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.02300.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.02300.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.02300.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.05137.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.05137.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.05137.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06218.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06218.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06218.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.05156.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.05156.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.05156.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.05122.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.05122.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.05122.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06219.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06219.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06219.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06030.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06030.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06030.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.05571.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.05571.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.05571.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.03978.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.03978.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.03978.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.02341.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.02341.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.02341.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06213.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06213.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06213.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06139.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06139.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06139.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06107.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06107.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06107.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06071.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06071.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06071.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06056.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06056.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06056.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.05681.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.05681.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.05681.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.04506.
[08.10.2025 17:11] Downloading paper 2510.04506 from http://arxiv.org/pdf/2510.04506v1...
[08.10.2025 17:11] Extracting affiliations from text.
[08.10.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"GRACE: GENERATIVE REPRESENTATION LEARNING VIA CONTRASTIVE POLICY OPTIMIZATION Jiashuo Sun1 Shixuan Liu2 Zhaochen Su3 Xianrui Zhong1 Pengcheng Jiang1 Bowen Jin1 Peiran Li4 Weijia Shi5 Jiawei Han1 1University of Illinois UrbanaChampaign 2Australian National University 3Hong Kong University of Science and Technology 4University of WisconsinMadison 5University of Washington "
[08.10.2025 17:11] Response: ```python
["University of Illinois Urbana-Champaign", "Australian National University", "Hong Kong University of Science and Technology", "University of Wisconsin-Madison", "University of Washington"]
```
[08.10.2025 17:11] Deleting PDF ./assets/pdf/2510.04506.pdf.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.04087.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.04087.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.04087.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.06101.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.06101.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.06101.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.05934.
[08.10.2025 17:11] Extra JSON file exists (./assets/json/2510.05934.json), skip PDF parsing.
[08.10.2025 17:11] Paper image links file exists (./assets/img_data/2510.05934.json), skip HTML parsing.
[08.10.2025 17:11] Success.
[08.10.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2510.04514.
[08.10.2025 17:11] Downloading paper 2510.04514 from http://arxiv.org/pdf/2510.04514v1...
[08.10.2025 17:12] Extracting affiliations from text.
[08.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 4 1 5 4 0 . 0 1 5 2 : r ChartAgent: Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering J.P. Morgan AI Research {rachneet.kaur, nishan.srishankar, zhen.zeng}@jpmorgan.com {sumitra.ganesh, manuela.veloso}@jpmorgan.com "
[08.10.2025 17:12] Response: ```python
["J.P. Morgan AI Research"]
```
[08.10.2025 17:12] Deleting PDF ./assets/pdf/2510.04514.pdf.
[08.10.2025 17:12] Success.
[08.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.00880.
[08.10.2025 17:12] Extra JSON file exists (./assets/json/2510.00880.json), skip PDF parsing.
[08.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.00880.json), skip HTML parsing.
[08.10.2025 17:12] Success.
[08.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2509.21499.
[08.10.2025 17:12] Extra JSON file exists (./assets/json/2509.21499.json), skip PDF parsing.
[08.10.2025 17:12] Paper image links file exists (./assets/img_data/2509.21499.json), skip HTML parsing.
[08.10.2025 17:12] Success.
[08.10.2025 17:12] Enriching papers with extra data.
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 0. TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  					AI-generated summary 				 Process Reward Models (PRMs)...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 1. Tiny Recursive Model (TRM) achieves high generalization on complex puzzle tasks using a small, two-layer network with minimal parameters, outperforming larger language models.  					AI-generated summary 				 Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recur...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 2. Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  					AI-generated summary 				 Tool-integrated reasoning has emerged as a key focus for enab...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 3. Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  					AI-generated summary 				 Autoregressive (AR) large language models (LLMs) have achieved remarkable p...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 4. CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  					AI-generated summary 				 Diffusion language models promise bidirectional context and infilling capabilities that autoregressi...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 5. Caco, a code-assisted chain-of-thought framework, automates the generation of high-quality, verifiable, and diverse reasoning data, enhancing the performance of large language models on mathematical reasoning tasks.  					AI-generated summary 				 Reasoning capability is pivotal for Large Language M...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 6. Drax, a discrete flow matching framework for ASR, achieves state-of-the-art recognition accuracy with improved efficiency by constructing an audio-conditioned probability path.  					AI-generated summary 				 Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 7. MixReasoning dynamically adjusts reasoning depth in models to improve efficiency without sacrificing accuracy.  					AI-generated summary 				 Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought b...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 8. ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  					AI-generated summary 				 Recent Large Language Model (LLM) post-training methods rely on t...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 9. Clinical Contrastive Cecoding (CCD) enhances radiology report generation by integrating structured clinical signals, reducing medical hallucinations without altering the base MLLM.  					AI-generated summary 				 Multimodal large language models (MLLMs) have recently achieved remarkable progress in ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 10. Language models use positional, lexical, and reflexive mechanisms to bind and retrieve entities, with a causal model achieving high accuracy in predicting next tokens across various tasks and input lengths.  					AI-generated summary 				 A key component of in-context reasoning is the ability of lan...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 11. A video-to-4D shape generation framework uses temporal attention, time-aware point sampling, and noise sharing to produce dynamic 3D representations from videos, enhancing temporal stability and perceptual fidelity.  					AI-generated summary 				 Video-conditioned 4D shape generation aims to recove...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 12. MeDiM, a medical discrete diffusion model, integrates multimodal biomedical data by learning shared distributions across images, text, and clinical notes, achieving high-fidelity generation and enhanced downstream performance.  					AI-generated summary 				 Recent advances in generative medical mod...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 13. TensorBLEU is a GPU-accelerated BLEU metric implementation for efficient in-training evaluation of natural language processing models, offering significant speedups over CPU-based methods.  					AI-generated summary 				 Modern natural language processing models have achieved unprecedented scale, ye...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 14. HoloScene is an interactive 3D reconstruction framework that achieves geometry completeness, object interactivity, physical plausibility, photorealistic rendering, and realistic physical properties through an energy-based optimization problem.  					AI-generated summary 				 Digitizing the physical ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 15. AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  					AI-generated summary 				 Large language models (LLMs) demo...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 16. Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  					AI-generated summary 				 Large reasoning m...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 17. The paper proposes stage-specific strategies to accelerate diffusion model inference in video generation, reducing memory usage and maintaining quality.  					AI-generated summary 				 Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 18. MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  					AI-generated summary 				 Direct Preference Optimization (DPO) has emerged as a simple and effective me...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 19. OneFlow, a non-autoregressive multimodal model, achieves superior performance in text-image generation and understanding tasks with reduced computational cost compared to autoregressive and diffusion-based models.  					AI-generated summary 				 We present OneFlow, the first non-autoregressive multi...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 20. AgentFlow, a trainable agentic framework with in-the-flow optimization, enhances reasoning in large language models by coordinating specialized modules and outperforms top baselines across various tasks.  					AI-generated summary 				 Outcome-driven reinforcement learning has advanced reasoning in ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 21. BIRD-INTERACT is a benchmark for multi-turn text-to-SQL tasks that simulates realistic database assistant challenges through dynamic interactions, hierarchical knowledge bases, and autonomous decision-making.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable p...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 22. Exploratory Annealed Decoding (EAD) improves sample efficiency in reinforcement learning with verifiable rewards by dynamically adjusting the sampling temperature during generation.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhan...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 23. Equilibrium Matching (EqM) is a generative modeling framework that learns an equilibrium gradient of an implicit energy landscape, enabling efficient sampling and outperforming traditional diffusion and flow models.  					AI-generated summary 				 We introduce Equilibrium Matching (EqM), a generativ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 24. WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  					AI-generated summary 				 RAG (Retrieval-Augm...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 25. EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  					AI-generated summary 				 Most existing benchmarks for egocentric vision understand...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 26. VeriGuard is a framework that ensures formal safety guarantees for LLM-based agents through offline validation and online monitoring.  					AI-generated summary 				 The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and pr...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 27. CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  					AI-generated summary 				 Emotional Support Conversation (ESC) plays a vital role in al...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 28. Human3R is a unified, feed-forward framework for real-time 4D human-scene reconstruction from monocular videos, achieving state-of-the-art performance with a single model and minimal dependencies.  					AI-generated summary 				 We present Human3R, a unified, feed-forward framework for online 4D hum...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 29. Geometry-aware optimal transport and active pruning enhance Gaussian process regression for efficient saddle point searches on high-dimensional energy surfaces.  					AI-generated summary 				 Gaussian process (GP) regression provides a strategy for accelerating saddle point searches on high-dimensi...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 30. EvoPresent, a self-improvement agent framework using multi-task reinforcement learning, enhances academic paper promotion by generating coherent narratives, aesthetically pleasing designs, and realistic presentations, supported by a comprehensive benchmark.  					AI-generated summary 				 The promot...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 31. Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  					AI-generated summary 				 Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 t...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 32. DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  					AI-generated summary 				 Real-world large language model deployments (e...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 33. Quantization robustness in large language models is influenced by learning rate and other hyperparameters, not dataset scale, as demonstrated through controlled training experiments.  					AI-generated summary 				 While post-training quantization is widely adopted for efficient deployment of large ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 34. FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  					AI-generated summary 				 Referring Video Object Segmentation (RVOS) requires segmenting spec...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 35. A framework called Distributional Semantics Tracing identifies the layers and pathways in Transformers where hallucinations occur, revealing a correlation between internal semantic coherence and hallucination rates.  					AI-generated summary 				 Large Language Models (LLMs) are prone to hallucinat...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 36. A benchmark for scatterplot-specific tasks using synthetic datasets evaluates AI models' performance in counting clusters and identifying outliers, with mixed results for localization tasks.  					AI-generated summary 				 AI models are increasingly used for data analysis and visualization, yet benc...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 37. DeepEvolve integrates deep research with algorithm evolution to propose, refine, implement, and test new hypotheses, improving initial algorithms across various scientific domains.  					AI-generated summary 				 Large language models hold promise as scientific assistants, yet existing agents either...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 38. MG-Select, a novel test-time scaling framework for Vision-Language-Action models, improves performance by using KL divergence from a reference distribution generated with masked inputs, achieving significant gains in both in-distribution and out-of-distribution tasks.  					AI-generated summary 				...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 39. GRACE uses contrastive policy optimization to train LLMs as generative agents that produce interpretable rationales, improving embeddings and transparency.  					AI-generated summary 				 Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that tr...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 40. A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  					AI-generated summary 				 Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pair...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 41. Research on distilling coding skills from large language models to smaller ones reveals a "valley of code reasoning" where performance initially decreases with more data before improving sharply, and that small models benefit more from easier questions during distillation.  					AI-generated summary...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 42. Embracing minority ratings, multiple annotators, and multi-emotion predictions in speech emotion recognition improves system robustness and alignment with human perception.  					AI-generated summary 				 Over the past two decades, speech emotion recognition (SER) has received growing attention. To ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 43. ChartAgent, a novel agentic framework, performs visual reasoning directly within charts, achieving state-of-the-art accuracy on ChartBench and ChartX benchmarks by iteratively decomposing queries and using specialized visual actions.  					AI-generated summary 				 Recent multimodal LLMs have shown ...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 44. HalluGuard, a 4B-parameter Small Reasoning Model, effectively mitigates hallucinations in Retrieval-Augmented Generation by classifying document-claim pairs and providing evidence-grounded justifications, achieving high balanced accuracy on the LLM-AggreFact benchmark.  					AI-generated summary 			...
[08.10.2025 17:12] ********************************************************************************
[08.10.2025 17:12] Abstract 45. Systematic investigation reveals that large language models are more sensitive to structural than semantic code perturbations, with implications for training data design.  					AI-generated summary 				 Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), b...
[08.10.2025 17:12] Read previous papers.
[08.10.2025 17:12] Generating reviews via LLM API.
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#data", "#benchmark", "#optimization", "#dataset"], "emoji": "ðŸ“Š", "ru": {"title": "TaTToo: Process Reward Model Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°Ð¼Ð¸", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ TaTToo â€” Ð½Ð¾Ð²ÑƒÑŽ Process Reward Model Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#agi", "#training", "#reasoning", "#small_models"], "emoji": "ðŸ”„", "ru": {"title": "ÐœÐ°Ð»ÐµÐ½ÑŒÐºÐ°Ñ Ñ€ÐµÐºÑƒÑ€ÑÐ¸Ñ Ð¿Ð¾Ð±ÐµÐ¶Ð´Ð°ÐµÑ‚ Ð³Ð¸Ð³Ð°Ð½Ñ‚Ð¾Ð²: 7M Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² LLM", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Tiny Recursive Model (TRM) â€” Ð¼Ð¸Ð½Ð¸Ð°Ñ‚ÑŽÑ€Ð½ÑƒÑŽ Ñ€ÐµÐºÑƒÑ€ÑÐ¸Ð²Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²ÑÐµÐ³Ð¾ Ð¸Ð· 2 ÑÐ»Ð¾Ñ‘Ð² Ð¸ 7 Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agents", "#rl", "#dataset", "#optimization"], "emoji": "ðŸ”", "ru": {"title": "Ð“Ð»ÑƒÐ±Ð¾ÐºÐ¸Ð¹ Ð²ÐµÐ±-Ð¿Ð¾Ð¸ÑÐº Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Fathom-DeepResearch, ÑÐ¾ÑÑ‚Ð¾ÑÑ‰Ð°Ñ Ð¸Ð· Ð´Ð²ÑƒÑ… ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ 4 Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð° Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#training", "#inference", "#open_source", "#diffusion", "#optimization", "#architecture"], "emoji": "âš¡", "ru": {"title": "Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ð±Ð»Ð¾Ñ‡Ð½ÑƒÑŽ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸ÑŽ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Fast-dLLM v2 â€” Ð±Ð»Ð¾Ñ‡Ð½ÑƒÑŽ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½ÑƒÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#inference", "#training", "#open_source", "#diffusion", "#small_models", "#dataset"], "emoji": "ðŸŒŠ", "ru": {"title": "Ð›ÐµÐ³ÐºÐ¾Ð²ÐµÑÐ½Ñ‹Ð¹ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ ÐºÐ¾Ð´ÐµÑ€ Ñ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹", "desc": "CoDA â€” ÑÑ‚Ð¾ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ñ 1.7 Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð°Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð², ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð°Ñ Ð´
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#math", "#data", "#training"], "emoji": "ðŸ”¢", "ru": {"title": "ÐšÐ¾Ð´ ÐºÐ°Ðº Ð¾ÑÐ½Ð¾Ð²Ð° Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹", "desc": "Caco â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ñ‹ÑÐ¾ÐºÐ¾ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐº
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#diffusion", "#audio", "#optimization"], "emoji": "ðŸŽ¤", "ru": {"title": "Drax: ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð´Ð¸ÑÐºÑ€ÐµÑ‚Ð½Ð¾Ðµ flow matching", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Drax â€” Ð½Ð¾Ð²Ñ‹Ð¹ framework Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸ (ASR) Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ discrete flow matching. Ð’Ð¼ÐµÑ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#reasoning", "#math", "#training"], "emoji": "ðŸŽšï¸", "ru": {"title": "ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð³Ð»ÑƒÐ±Ð¸Ð½Ð° Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹: Ð´ÑƒÐ¼Ð°Ð¹ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð°Ð¼, Ð³Ð´Ðµ ÑÑ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ MixReasoning â€” Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð³Ð»ÑƒÐ±Ð¸Ð½Ñƒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ðº
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization"], "emoji": "âš–ï¸", "ru": {"title": "ÐÑÐ¸Ð¼Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡Ð½Ð°Ñ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸ Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð² Ð¼ÐµÑ‚Ð¾Ð´Ð°Ñ… Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: Ð½ÐµÑÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð²Ð·Ð²ÐµÑˆÐ¸Ð²Ð°Ð½
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#healthcare", "#training", "#multimodal", "#hallucinations", "#data", "#science"], "emoji": "ðŸ©»", "ru": {"title": "Ð‘Ð¾Ñ€ÑŒÐ±Ð° Ñ Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ð¼Ð¸ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸ÑÐ¼Ð¸ Ð² AI-Ñ€Ð°Ð´Ð¸Ð¾Ð»Ð¾Ð³Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Clinical Contrastive Decoding (CCD) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡Ñˆ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#reasoning", "#long_context", "#data", "#multimodal", "#interpretability", "#architecture"], "emoji": "ðŸ”—", "ru": {"title": "Ð¢Ñ€Ð¸ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ð° ÑÐ²ÑÐ·Ñ‹Ð²Ð°Ð½Ð¸Ñ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, ÐºÐ°Ðº ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÑÐ²ÑÐ·Ñ‹Ð²Ð°ÑŽÑ‚ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÑŽÑ‚ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ, Ð¸ÑÐ¿Ð¾Ð»
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#video", "#3d"], "emoji": "ðŸŽ¬", "ru": {"title": "ÐžÑ‚ Ð²Ð¸Ð´ÐµÐ¾ Ðº Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ 3D-Ñ„Ð¾Ñ€Ð¼Ðµ: Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ 4D-Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒÑŽ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ 4D-Ñ„Ð¾Ñ€Ð¼ Ð¸Ð· Ð²Ð¸Ð´ÐµÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ 3D-Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð¸Ð· Ð²Ñ…Ð¾Ð´Ð½Ð¾Ð³Ð¾
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#diffusion", "#science", "#healthcare", "#multimodal"], "emoji": "ðŸ¥", "ru": {"title": "Ð•Ð´Ð¸Ð½Ð°Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ñ… Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹", "desc": "MeDiM â€” ÑÑ‚Ð¾ Ð¿ÐµÑ€Ð²Ð°Ñ Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð¸ÑÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ‚Ð¸Ð¿Ñ‹ Ð±Ð¸Ð¾Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#training", "#open_source", "#optimization", "#benchmark"], "emoji": "âš¡", "ru": {"title": "ÐœÐ¾Ð»Ð½Ð¸ÐµÐ½Ð¾ÑÐ½Ð°Ñ BLEU Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ° Ð½Ð° GPU Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "TensorBLEU â€” ÑÑ‚Ð¾ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ BLEU, Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð½Ð° GPU Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ NLP. ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#3d", "#optimization", "#games", "#benchmark"], "emoji": "ðŸ—ï¸", "ru": {"title": "Ð¦Ð¸Ñ„Ñ€Ð¾Ð²Ñ‹Ðµ Ð´Ð²Ð¾Ð¹Ð½Ð¸ÐºÐ¸ Ñ Ñ„Ð¸Ð·Ð¸ÐºÐ¾Ð¹ Ð¸ Ñ„Ð¾Ñ‚Ð¾Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¼Ð¾Ð¼", "desc": "HoloScene â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ 3D-Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¼Ð¸Ñ€Ð° Ð² Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ ÑÑ€ÐµÐ´Ñ‹, Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ðµ Ðº ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸. Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð³Ñ€Ð°Ñ„ ÑÑ†ÐµÐ½
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#rlhf", "#agents"], "emoji": "ðŸ§ª", "ru": {"title": "ÐœÐ¾Ð¶ÐµÑ‚ Ð»Ð¸ AI ÑÑ‚Ð°Ñ‚ÑŒ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ Ð² Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸?", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ AInstein â€” Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ñ€ÐµÑˆÐ°Ñ‚ÑŒ Ð¸ÑÑ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#data", "#alignment", "#interpretability", "#training"], "emoji": "ðŸ§—", "ru": {"title": "ÐžÐ±Ñ€Ñ‹Ð² Ð¾Ñ‚ÐºÐ°Ð·Ð°: Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð²Ð½ÐµÐ·Ð°Ð¿Ð½Ð¾ ÑÑ‚Ð°Ð½Ð¾Ð²ÑÑ‚ÑÑ Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¼Ð¸", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸ Ñ„ÐµÐ½Ð¾Ð¼ÐµÐ½ Â«Ð¾Ð±Ñ€Ñ‹Ð²Ð° Ð¾Ñ‚ÐºÐ°Ð·Ð°Â» Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… reasoning-Ð¼Ð¾Ð´ÐµÐ»ÑÑ…: Ð¼Ð¾Ð´ÐµÐ»Ð¸ 
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#video", "#optimization", "#inference"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÑ‚Ð°Ð´Ð¸ÑÑ…", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#rlhf"], "emoji": "âš–ï¸", "ru": {"title": "ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð²ÐµÑÐ° Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ð´ÐµÐ»Ð°ÑŽÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸ÑÐ¼ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½ÐµÐµ", "desc": "MADPO â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸ÑÐ¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ DP
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#games", "#architecture", "#diffusion", "#multimodal"], "emoji": "ðŸ”„", "ru": {"title": "ÐžÐ´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð±ÐµÐ· Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸", "desc": "OneFlow â€” ÑÑ‚Ð¾ Ð¿ÐµÑ€Ð²Ð°Ñ Ð½ÐµÐ°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¾Ð½Ð½Ð°Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚ÐµÐºÑ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#training", "#agents", "#rl", "#optimization", "#reasoning", "#architecture"], "emoji": "ðŸ”„", "ru": {"title": "ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼Ð°Ñ Ð°Ð³ÐµÐ½Ñ‚Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ð² Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ", "desc": "AgentFlow â€” ÑÑ‚Ð¾ Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð² LLM Ñ‡ÐµÑ€ÐµÐ· ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ†Ð¸ÑŽ Ñ‡ÐµÑ‚Ñ‹
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#agents", "#interpretability", "#benchmark", "#reasoning"], "emoji": "ðŸ—£ï¸", "ru": {"title": "Ð ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¾Ñ…Ð¾Ð´Ð¾Ð²Ñ‹Ñ… SQL-Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð² Ñ Ð±Ð°Ð·Ð°Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ BIRD-INTERACT â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ LLM Ð² Ð¼Ð½Ð¾Ð³Ð¾Ñ…Ð¾Ð´Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð° Ð²
[08.10.2025 17:12] Querying the API.
[08.10.2025 17:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Exploratory Annealed Decoding (EAD) improves sample efficiency in reinforcement learning with verifiable rewards by dynamically adjusting the sampling temperature during generation.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.
[08.10.2025 17:12] Response: ```json
{
  "title": "Ð˜ÑÑÐ»ÐµÐ´ÑƒÐ¹ ÑÐ½Ð°Ñ‡Ð°Ð»Ð°, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¿Ð¾Ñ‚Ð¾Ð¼: Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM",
  "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Exploratory Annealed Decoding (EAD) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…. ÐšÐ»ÑŽÑ‡ÐµÐ²Ð°Ñ Ð¸Ð´ÐµÑ â€” Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¼ÐµÐ½ÑÑ‚ÑŒ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñƒ ÑÑÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸: Ð²Ñ‹ÑÐ¾ÐºÐ°Ñ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð´Ð»Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹, Ð·Ð°Ñ‚ÐµÐ¼ Ð¿Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ð¾Ðµ ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°. Ð­Ñ‚Ð¾ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼ÐµÐ¶Ð´Ñƒ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ Ð¸ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒÑŽ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ð²Ð°Ð¶Ð½Ð¾ Ð´Ð»Ñ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÑŽÑ‚ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð°. EAD Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÑƒÑŽ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð¾Ð¹ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð°Ñ… Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð°Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹.",
  "emoji": "ðŸŒ¡ï¸",
  "desc_length": 4
}
```
[08.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Exploratory Annealed Decoding (EAD) improves sample efficiency in reinforcement learning with verifiable rewards by dynamically adjusting the sampling temperature during generation.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning."

[08.10.2025 17:12] Response: ```python
['RL', 'TRAINING']
```
[08.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Exploratory Annealed Decoding (EAD) improves sample efficiency in reinforcement learning with verifiable rewards by dynamically adjusting the sampling temperature during generation.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning."

[08.10.2025 17:12] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[08.10.2025 17:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Exploratory Annealed Decoding (EAD) enhances sample efficiency in reinforcement learning with verifiable rewards by adjusting the sampling temperature dynamically. This method addresses the challenges of maintaining sample quality while ensuring training stability during exploration. EAD employs a strategy of exploring more at the beginning of the sequence and exploiting at the end, which allows for high-level diversity initially and preserves quality later. The results show that EAD outperforms traditional fixed-temperature sampling methods, making it a valuable tool for improving the reasoning capabilities of large language models.","title":"Dynamic Exploration for Enhanced Learning Efficiency"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Exploratory Annealed Decoding (EAD) enhances sample efficiency in reinforcement learning with verifiable rewards by adjusting the sampling temperature dynamically. This method addresses the challenges of maintaining sample quality while ensuring training stability during exploration. EAD employs a strategy of exploring more at the beginning of the sequence and exploiting at the end, which allows for high-level diversity initially and preserves quality later. The results show that EAD outperforms traditional fixed-temperature sampling methods, making it a valuable tool for improving the reasoning capabilities of large language models.', title='Dynamic Exploration for Enhanced Learning Efficiency'))
[08.10.2025 17:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æŽ¢ç´¢æ€§é€€ç«è§£ç ï¼ˆEADï¼‰æ˜¯ä¸€ç§æé«˜å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±æ ·æœ¬æ•ˆçŽ‡çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¸©åº¦ï¼Œè§£å†³äº†æ ·æœ¬è´¨é‡ä¸Žè®­ç»ƒç¨³å®šæ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚EADé‡‡ç”¨äº†â€œå¼€å§‹æŽ¢ç´¢ï¼Œç»“æŸåˆ©ç”¨â€çš„ç­–ç•¥ï¼ŒåˆæœŸé«˜æ¸©é‡‡æ ·ä¿ƒè¿›å¤šæ ·æ€§ï¼ŒåŽæœŸä½Žæ¸©é‡‡æ ·ä¿æŒæ ·æœ¬è´¨é‡ã€‚å®žéªŒè¡¨æ˜Žï¼ŒEADåœ¨å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡åž‹è§„æ¨¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽå›ºå®šæ¸©åº¦é‡‡æ ·ï¼Œæå‡äº†æ ·æœ¬æ•ˆçŽ‡ã€‚","title":"æŽ¢ç´¢æ€§é€€ç«è§£ç ï¼šæå‡æ ·æœ¬æ•ˆçŽ‡çš„åˆ›æ–°ç­–ç•¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æŽ¢ç´¢æ€§é€€ç«è§£ç ï¼ˆEADï¼‰æ˜¯ä¸€ç§æé«˜å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±æ ·æœ¬æ•ˆçŽ‡çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¸©åº¦ï¼Œè§£å†³äº†æ ·æœ¬è´¨é‡ä¸Žè®­ç»ƒç¨³å®šæ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚EADé‡‡ç”¨äº†â€œå¼€å§‹æŽ¢ç´¢ï¼Œç»“æŸåˆ©ç”¨â€çš„ç­–ç•¥ï¼ŒåˆæœŸé«˜æ¸©é‡‡æ ·ä¿ƒè¿›å¤šæ ·æ€§ï¼ŒåŽæœŸä½Žæ¸©é‡‡æ ·ä¿æŒæ ·æœ¬è´¨é‡ã€‚å®žéªŒè¡¨æ˜Žï¼ŒEADåœ¨å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡åž‹è§„æ¨¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºŽå›ºå®šæ¸©åº¦é‡‡æ ·ï¼Œæå‡äº†æ ·æœ¬æ•ˆçŽ‡ã€‚', title='æŽ¢ç´¢æ€§é€€ç«è§£ç ï¼šæå‡æ ·æœ¬æ•ˆçŽ‡çš„åˆ›æ–°ç­–ç•¥'))
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#inference", "#optimization", "#cv", "#diffusion", "#multimodal"], "emoji": "âš–ï¸", "ru": {"title": "Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· ÑÐ½ÐµÑ€Ð³ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð»Ð°Ð½Ð´ÑˆÐ°Ñ„Ñ‚ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸", "desc": "Equilibrium Matching (EqM) â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¼Ñƒ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÑŽ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ñ‚ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð¾Ñ‚ Ð·Ð°Ð²Ð¸ÑÑÑ‰ÐµÐ¹ Ð¾Ñ‚ Ð²Ñ€Ðµ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rag", "#benchmark", "#leakage", "#architecture", "#agents"], "emoji": "ðŸ”", "ru": {"title": "WebDetective: ÐšÐ°Ðº Ð½Ð°ÑƒÑ‡Ð¸Ñ‚ÑŒ AI-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾, Ð° Ð½Ðµ ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð´ÑÐºÐ°Ð·ÐºÐ°Ð¼", "desc": "WebDetective â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼Ð½Ð¾Ð³Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#multimodal", "#long_context", "#benchmark", "#games", "#transfer_learning", "#synthetic", "#cv"], "emoji": "ðŸŒ™", "ru": {"title": "ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° AI-Ð·Ñ€ÐµÐ½Ð¸Ñ Ð² Ñ‚ÐµÐ¼Ð½Ð¾Ñ‚Ðµ Ð¾Ñ‚ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð»Ð¸Ñ†Ð°", "desc": "EgoNight â€” ÑÑ‚Ð¾ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ egocentric-Ð²Ð¸Ð´ÐµÐ½Ð¸Ñ Ð² Ð½Ð¾Ñ‡Ð½Ñ‹Ñ… ÑƒÑÐ»Ð¾Ð²Ð¸ÑÑ… Ñ Ñ„Ð¾ÐºÑƒ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#inference", "#alignment", "#agents", "#security", "#healthcare"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "Ð¤Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ LLM-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²", "desc": "VeriGuard â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ð¸Ñ Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ð¹ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ AI-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ LLM Ð² ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¾Ð±Ð»Ð°ÑÑ‚ÑÑ… Ð²
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#reasoning"], "emoji": "ðŸ¤—", "ru": {"title": "Ð£ÑÐ¸Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð³Ð½Ð¸Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ CARE â€” Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸. Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ„Ð¾ÐºÑƒÑÐ¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð½
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#cv", "#video", "#3d"], "emoji": "ðŸŽ¥", "ru": {"title": "Ð ÐµÐ°Ð»ÑŒÐ½Ð°Ñ 4D Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ ÑÑ†ÐµÐ½ Ñ Ð»ÑŽÐ´ÑŒÐ¼Ð¸ Ð¸Ð· Ð²Ð¸Ð´ÐµÐ¾", "desc": "Human3R â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ 4D ÑÑ†ÐµÐ½ Ñ Ð»ÑŽÐ´ÑŒÐ¼Ð¸ Ð¸Ð· Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð², Human3R Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ ÑÐ»
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#optimization", "#data", "#science", "#training"], "emoji": "ðŸ”ï¸", "ru": {"title": "Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð¸ÑÐºÐ° ÑÐµÐ´Ð»Ð¾Ð²Ñ‹Ñ… Ñ‚Ð¾Ñ‡ÐµÐº Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð³ÐµÐ¾Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¾ÑÐ²ÐµÐ´Ð¾Ð¼Ð»Ñ‘Ð½Ð½Ð¾Ð¹ Ð³Ð°ÑƒÑÑÐ¾Ð²ÑÐºÐ¾Ð¹ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð° ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸ÑŽ Ð¿Ð¾Ð¸ÑÐºÐ° ÑÐµÐ´Ð»Ð¾Ð²Ñ‹Ñ… Ñ‚Ð¾Ñ‡ÐµÐº Ð½Ð° Ð²Ñ‹ÑÐ¾ÐºÐ¾Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ñ‹Ñ… ÑÐ½ÐµÑ€Ð³ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ð¾Ð²ÐµÑ€Ñ…Ð½Ð¾ÑÑ‚ÑÑ… Ñ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#benchmark", "#story_generation", "#rl", "#multimodal", "#agents", "#optimization", "#games"], "emoji": "ðŸŽ¨", "ru": {"title": "Ð¡Ð°Ð¼Ð¾ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ð¹ÑÑ Ð°Ð³ÐµÐ½Ñ‚ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¿Ñ€ÐµÐ·ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¹ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… ÑÑ‚Ð°Ñ‚ÐµÐ¹", "desc": "EvoPresent â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð°Ð³ÐµÐ½Ñ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¿Ñ€ÐµÐ·ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ 
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#data", "#benchmark", "#long_context", "#healthcare", "#dataset", "#multimodal"], "emoji": "ðŸ”¬", "ru": {"title": "Ð”Ð»Ð¸Ð½Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð±Ð¸Ð¾Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ñ… Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹: Ð±Ð¾Ð»ÑŒÑˆÐµ ÑÐ»Ð¾Ð² â€” Ð»ÑƒÑ‡ÑˆÐµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ðµ vision-language Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¼ ÐºÐ¾Ð½Ñ‚
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#rlhf"], "emoji": "ðŸ”„", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ LLM Ð½Ð° Ð½ÐµÐ´Ð¾Ð²Ð¾Ð»ÑŒÑÑ‚Ð²Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹: Ð¿Ñ€ÐµÐ²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð½ÐµÐ³Ð°Ñ‚Ð¸Ð² Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ DRIFT Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð² Ð½ÐµÐ´Ð¾Ð²Ð¾Ð»ÑŒÑÑ‚Ð²Ð° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "âš–ï¸", "ru": {"title": "Ð£ÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚ÑŒ Ðº ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð¾Ñ‚ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð², Ð° Ð½Ðµ Ð¾Ñ‚ Ð¾Ð±ÑŠÑ‘Ð¼Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð·ÑƒÑ‡Ð¸Ð»Ð¸, ÐºÐ°Ðº post-training ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ð´Ð¾ 32B Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð², Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð½
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark", "#games", "#alignment"], "emoji": "ðŸŒŠ", "ru": {"title": "Ð¡ÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ñ‡ÐµÑ€ÐµÐ· Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½ÑƒÑŽ Ð´ÐµÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾Ð´ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ñ‚ÐµÐºÑÑ‚Ð°", "desc": "FlowRVS Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð·Ð°Ð´Ð°Ñ‡Ñƒ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð² Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð¼Ñƒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÑŽ (RVOS), Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÑ ÐµÑ‘ ÐºÐ°Ðº Ð¿Ñ€Ð¾Ð±
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#hallucinations", "#inference", "#interpretability"], "emoji": "ðŸ”", "ru": {"title": "ÐšÐ°Ñ€Ñ‚Ð° Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹: ÐºÐ°Ðº Ð¸ Ð³Ð´Ðµ LLM Ñ‚ÐµÑ€ÑÑŽÑ‚ ÑÐ²ÑÐ·ÑŒ Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒÑŽ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ Distributional Semantics Tracing (DST), ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¾Ñ‚ÑÐ»Ðµ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#benchmark", "#synthetic", "#dataset", "#optimization"], "emoji": "ðŸ“Š", "ru": {"title": "LLM ÑƒÑ‡Ð°Ñ‚ÑÑ Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸, Ð½Ð¾ Ð½Ðµ Ð²ÑÐµÐ³Ð´Ð° Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑŽÑ‚ Ð³Ð´Ðµ Ñ‡Ñ‚Ð¾ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ AI-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ Ñ€Ð°ÑÑÐµÑÐ½Ð¸Ñ (scatterplots), Ð¸Ñ
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#science", "#benchmark", "#agents", "#optimization", "#data", "#math"], "emoji": "ðŸ§¬", "ru": {"title": "Ð­Ð²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ", "desc": "DeepEvolve â€” ÑÑ‚Ð¾ AI-Ð°Ð³ÐµÐ½Ñ‚, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ ÑÐ²Ð¾Ð»ÑŽÑ†Ð¸ÐµÐ¹ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð¾Ð² Ð´Ð»Ñ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#cv", "#training", "#robotics", "#agents", "#optimization"], "emoji": "ðŸŽ¯", "ru": {"title": "Ð£Ð¼Ð½Ñ‹Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ñ€Ð¾Ð±Ð¾Ñ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ MG-Select â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Vision-Language-Action Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð°Ð¼Ð¸ Ð½
[08.10.2025 17:12] Querying the API.
[08.10.2025 17:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

GRACE uses contrastive policy optimization to train LLMs as generative agents that produce interpretable rationales, improving embeddings and transparency.  					AI-generated summary 				 Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.
[08.10.2025 17:12] Response: ```json
{
  "title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ð¹ Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð½Ñ‹Ð¼ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼",
  "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ GRACE, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (LLM) Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð½ÑÑ‚Ð½Ñ‹Ðµ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÑƒ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ñ ÑÐ²Ð¾Ð¸Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ Ð²Ð¼ÐµÑÑ‚Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð½ÐµÐ¿Ñ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ñ‹Ñ… ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð². ÐœÐµÑ‚Ð¾Ð´ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð½ÑƒÑŽ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸Ðº, Ð³Ð´Ðµ ÑÐ¸Ð³Ð½Ð°Ð»Ñ‹ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÑŽÑ‚ÑÑ ÐºÐ°Ðº Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ Ð´Ð»Ñ reinforcement learning Ð°Ð³ÐµÐ½Ñ‚Ð°. ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¾Ð±Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸Ñ (rationales) ÑÐ²Ð¾ÐµÐ³Ð¾ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð·Ð°Ñ‚ÐµÐ¼ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÑŽÑ‚ÑÑ Ð² ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð³Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°. ÐÐ° Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐµ MTEB Ð¼ÐµÑ‚Ð¾Ð´ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð» ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð½Ð° 11.5% Ð² supervised Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð¸ 6.9% Ð² unsupervised Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ð¿Ñ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹.",
  "emoji": "ðŸŽ­",
  "desc_en": ""
}
```
[08.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GRACE uses contrastive policy optimization to train LLMs as generative agents that produce interpretable rationales, improving embeddings and transparency.  					AI-generated summary 				 Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE."

[08.10.2025 17:12] Response: ```python
['RL', 'AGENTS', 'TRAINING', 'BENCHMARK']
```
[08.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GRACE uses contrastive policy optimization to train LLMs as generative agents that produce interpretable rationales, improving embeddings and transparency.  					AI-generated summary 				 Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE."

[08.10.2025 17:12] Response: ```python
["INTERPRETABILITY", "REASONING", "OPTIMIZATION"]
```
[08.10.2025 17:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GRACE is a new framework that enhances the training of Large Language Models (LLMs) by using contrastive policy optimization. Instead of treating the model as a black box, GRACE allows the LLM to generate clear and interpretable rationales for its decisions, improving transparency. It uses a reward-based approach to guide the model\'s learning, maximizing the similarity of positive examples while minimizing that of negative ones. This results in better embeddings and a more understandable reasoning process, achieving significant performance improvements on benchmark tests.","title":"Transforming LLMs into Interpretable Generative Agents with GRACE"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="GRACE is a new framework that enhances the training of Large Language Models (LLMs) by using contrastive policy optimization. Instead of treating the model as a black box, GRACE allows the LLM to generate clear and interpretable rationales for its decisions, improving transparency. It uses a reward-based approach to guide the model's learning, maximizing the similarity of positive examples while minimizing that of negative ones. This results in better embeddings and a more understandable reasoning process, achieving significant performance improvements on benchmark tests.", title='Transforming LLMs into Interpretable Generative Agents with GRACE'))
[08.10.2025 17:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GRACEæ˜¯ä¸€ç§æ–°çš„æ¡†æž¶ï¼Œé€šè¿‡å¯¹æ¯”ç­–ç•¥ä¼˜åŒ–æ¥è®­ç»ƒå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„æŽ¨ç†ã€‚è¿™ç§æ–¹æ³•å°†å¯¹æ¯”ä¿¡å·è§†ä¸ºå¥–åŠ±ï¼Œè€Œä¸æ˜¯ç®€å•çš„æŸå¤±ï¼Œä»Žè€Œå¼•å¯¼ç”Ÿæˆç­–ç•¥ã€‚GRACEä½¿å¾—LLMèƒ½å¤Ÿç”Ÿæˆç»“æž„åŒ–çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæå‡äº†æ¨¡åž‹çš„é€æ˜Žåº¦å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¤šç»„ä»¶å¥–åŠ±å‡½æ•°çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼ŒGRACEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡åž‹çš„è¡¨çŽ°ã€‚","title":"GRACEï¼šå°†å¯¹æ¯”ä¼˜åŒ–è½¬åŒ–ä¸ºå¯è§£é‡Šçš„ç”Ÿæˆèƒ½åŠ›"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GRACEæ˜¯ä¸€ç§æ–°çš„æ¡†æž¶ï¼Œé€šè¿‡å¯¹æ¯”ç­–ç•¥ä¼˜åŒ–æ¥è®­ç»ƒå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„æŽ¨ç†ã€‚è¿™ç§æ–¹æ³•å°†å¯¹æ¯”ä¿¡å·è§†ä¸ºå¥–åŠ±ï¼Œè€Œä¸æ˜¯ç®€å•çš„æŸå¤±ï¼Œä»Žè€Œå¼•å¯¼ç”Ÿæˆç­–ç•¥ã€‚GRACEä½¿å¾—LLMèƒ½å¤Ÿç”Ÿæˆç»“æž„åŒ–çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæå‡äº†æ¨¡åž‹çš„é€æ˜Žåº¦å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¤šç»„ä»¶å¥–åŠ±å‡½æ•°çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼ŒGRACEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡åž‹çš„è¡¨çŽ°ã€‚', title='GRACEï¼šå°†å¯¹æ¯”ä¼˜åŒ–è½¬åŒ–ä¸ºå¯è§£é‡Šçš„ç”Ÿæˆèƒ½åŠ›'))
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#inference", "#training", "#data", "#rlhf", "#alignment"], "emoji": "ðŸš¦", "ru": {"title": "ÐÐ°ÑƒÑ‡Ð¸Ñ‚ÑŒ AI Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾, Ð° Ð½Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ñ‚Ð¾ Ð»ÑƒÑ‡ÑˆÐµ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ð¹, Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ Â«Ð²Ð½ÐµÑˆÐ½ÑŽÑŽ Ð¾Ð¿Ñ†Ð¸ÑŽÂ» Ð² Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ð¹. Ð¢Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#transfer_learning", "#reasoning", "#small_models", "#training", "#optimization"], "emoji": "ðŸ”ï¸", "ru": {"title": "Ð”Ð¾Ð»Ð¸Ð½Ð° Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹: ÐºÐ°Ðº Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÑƒÑ‡Ð°Ñ‚ÑÑ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸ÑŽ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð·ÑƒÑ‡Ð°Ð»Ð¸, ÐºÐ°Ðº Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‚ÑŒ Ð½Ð°Ð²Ñ‹ÐºÐ¸ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¾Ñ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹Ðº
[08.10.2025 17:12] Using data from previous issue: {"categories": ["#audio", "#alignment", "#interpretability"], "emoji": "ðŸŽ­", "ru": {"title": "Ð¡ÑƒÐ±ÑŠÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ ÑÐ¼Ð¾Ñ†Ð¸Ð¹ ÐºÐ°Ðº Ð¿Ñ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð¾: ÑƒÑ‡Ñ‘Ñ‚ Ð¼Ð½ÐµÐ½Ð¸Ñ Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ð° Ð² Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ð¸ Ñ€ÐµÑ‡Ð¸", "desc": "Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸ÑŽ ÑÐ¼Ð¾Ñ†Ð¸Ð¹ Ð² Ñ€ÐµÑ‡Ð¸ (SER) Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Ð¾Ñ‚ÐºÐ°Ð·Ð°Ñ‚ÑŒÑÑ Ð¾Ñ‚ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°, Ð³Ð´Ðµ Ñ€
[08.10.2025 17:12] Querying the API.
[08.10.2025 17:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ChartAgent, a novel agentic framework, performs visual reasoning directly within charts, achieving state-of-the-art accuracy on ChartBench and ChartX benchmarks by iteratively decomposing queries and using specialized visual actions.  					AI-generated summary 				 Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.
[08.10.2025 17:13] Response: ```json
{
  "desc": "ChartAgent â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð² Ð¸ Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð½ÐµÐ¿Ð¾ÑÑ€ÐµÐ´ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ Ð² Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ. Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð² chain-of-thought, ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¸Ñ‚ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾ Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð´Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ Ð¼Ð°Ð½Ð¸Ð¿ÑƒÐ»Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹: Ñ€Ð¸ÑÑƒÐµÑ‚ Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ð¸, Ð²Ñ‹Ñ€ÐµÐ·Ð°ÐµÑ‚ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸, Ð»Ð¾ÐºÐ°Ð»Ð¸Ð·ÑƒÐµÑ‚ Ð¾ÑÐ¸. ChartAgent Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°ÐµÑ‚ state-of-the-art Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð½Ð° Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€ÐºÐ°Ñ… ChartBench Ð¸ ChartX, Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ñ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð½Ð° 16-17% Ð½Ð° Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ°Ñ… Ð±ÐµÐ· Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð¾Ðº. Ð¤Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ ÐºÐ°Ðº plug-and-play Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ, ÑƒÐ»ÑƒÑ‡ÑˆÐ°Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… LLM Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹.",
  "emoji": "ðŸ“Š",
  "title": "Ð’Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹"
}
```
[08.10.2025 17:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ChartAgent, a novel agentic framework, performs visual reasoning directly within charts, achieving state-of-the-art accuracy on ChartBench and ChartX benchmarks by iteratively decomposing queries and using specialized visual actions.  					AI-generated summary 				 Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents."

[08.10.2025 17:13] Response: ```python
["AGENTS", "CV", "MULTIMODAL", "BENCHMARK"]
```
[08.10.2025 17:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ChartAgent, a novel agentic framework, performs visual reasoning directly within charts, achieving state-of-the-art accuracy on ChartBench and ChartX benchmarks by iteratively decomposing queries and using specialized visual actions.  					AI-generated summary 				 Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents."

[08.10.2025 17:13] Response: ```python
["REASONING", "INTERPRETABILITY"]
```
[08.10.2025 17:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ChartAgent is a new framework designed for visual reasoning in charts, which enhances the ability to answer questions about chart data. It works by breaking down complex queries into smaller visual tasks and using specific actions to interact with the chart, such as drawing or cropping. This method allows ChartAgent to achieve high accuracy on benchmarks like ChartBench and ChartX, outperforming previous models significantly. The framework is versatile, effective across different types of charts, and can improve the performance of various underlying language models.","title":"ChartAgent: Mastering Visual Reasoning in Charts"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ChartAgent is a new framework designed for visual reasoning in charts, which enhances the ability to answer questions about chart data. It works by breaking down complex queries into smaller visual tasks and using specific actions to interact with the chart, such as drawing or cropping. This method allows ChartAgent to achieve high accuracy on benchmarks like ChartBench and ChartX, outperforming previous models significantly. The framework is versatile, effective across different types of charts, and can improve the performance of various underlying language models.', title='ChartAgent: Mastering Visual Reasoning in Charts'))
[08.10.2025 17:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ChartAgentæ˜¯ä¸€ç§æ–°é¢–çš„æ™ºèƒ½æ¡†æž¶ï¼Œèƒ½å¤Ÿç›´æŽ¥åœ¨å›¾è¡¨ä¸­è¿›è¡Œè§†è§‰æŽ¨ç†ã€‚å®ƒé€šè¿‡å°†æŸ¥è¯¢è¿­ä»£åˆ†è§£ä¸ºè§†è§‰å­ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨ä¸“é—¨çš„è§†è§‰æ“ä½œæ¥ä¸Žå›¾è¡¨å›¾åƒè¿›è¡Œäº¤äº’ï¼Œä»Žè€Œå®žçŽ°äº†åœ¨ChartBenchå’ŒChartXåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®çŽ‡ã€‚ä¸Žä¼ ç»Ÿçš„æ–‡æœ¬æŽ¨ç†ä¸åŒï¼ŒChartAgenté‡‡ç”¨äº†æ›´æŽ¥è¿‘äººç±»è®¤çŸ¥ç­–ç•¥çš„æ–¹å¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å„ç§ç±»åž‹çš„å›¾è¡¨ã€‚è¯¥æ¡†æž¶ä¸ä»…åœ¨æœªæ³¨é‡Šçš„å›¾è¡¨ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œè¿˜èƒ½æå‡å¤šç§åŸºç¡€å¤§åž‹è¯­è¨€æ¨¡åž‹çš„æ€§èƒ½ã€‚","title":"ChartAgentï¼šå›¾è¡¨ç†è§£çš„æ–°æ™ºèƒ½æ¡†æž¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ChartAgentæ˜¯ä¸€ç§æ–°é¢–çš„æ™ºèƒ½æ¡†æž¶ï¼Œèƒ½å¤Ÿç›´æŽ¥åœ¨å›¾è¡¨ä¸­è¿›è¡Œè§†è§‰æŽ¨ç†ã€‚å®ƒé€šè¿‡å°†æŸ¥è¯¢è¿­ä»£åˆ†è§£ä¸ºè§†è§‰å­ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨ä¸“é—¨çš„è§†è§‰æ“ä½œæ¥ä¸Žå›¾è¡¨å›¾åƒè¿›è¡Œäº¤äº’ï¼Œä»Žè€Œå®žçŽ°äº†åœ¨ChartBenchå’ŒChartXåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®çŽ‡ã€‚ä¸Žä¼ ç»Ÿçš„æ–‡æœ¬æŽ¨ç†ä¸åŒï¼ŒChartAgenté‡‡ç”¨äº†æ›´æŽ¥è¿‘äººç±»è®¤çŸ¥ç­–ç•¥çš„æ–¹å¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å„ç§ç±»åž‹çš„å›¾è¡¨ã€‚è¯¥æ¡†æž¶ä¸ä»…åœ¨æœªæ³¨é‡Šçš„å›¾è¡¨ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œè¿˜èƒ½æå‡å¤šç§åŸºç¡€å¤§åž‹è¯­è¨€æ¨¡åž‹çš„æ€§èƒ½ã€‚', title='ChartAgentï¼šå›¾è¡¨ç†è§£çš„æ–°æ™ºèƒ½æ¡†æž¶'))
[08.10.2025 17:13] Using data from previous issue: {"categories": ["#training", "#hallucinations", "#dataset", "#rag", "#small_models", "#synthetic", "#benchmark", "#optimization"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "HalluGuard: Ð—Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ Ð³Ð°Ð»Ð»ÑŽÑ†Ð¸Ð½Ð°Ñ†Ð¸Ð¹ Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð°", "desc": "HalluGuard â€” ÑÑ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ 4 Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð°Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ ÑƒÐ¼Ðµ
[08.10.2025 17:13] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#data", "#optimization", "#training", "#plp"], "emoji": "ðŸ—ï¸", "ru": {"title": "Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° ÐºÐ¾Ð´Ð° Ð²Ð°Ð¶Ð½ÐµÐµ ÑÐ¼Ñ‹ÑÐ»Ð° Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÐ»Ð¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ñ‚Ð¾Ð³Ð¾, ÐºÐ°Ðº Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÐ²Ð¾Ð¹ÑÑ‚Ð²Ð° Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð´Ð° Ð²Ð»Ð¸ÑÑŽÑ‚ Ð½Ð° ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ðº Ñ€Ð°ÑÑÑƒ
[08.10.2025 17:13] Renaming data file.
[08.10.2025 17:13] Renaming previous data. hf_papers.json to ./d/2025-10-08.json
[08.10.2025 17:13] Saving new data file.
[08.10.2025 17:13] Generating page.
[08.10.2025 17:13] Renaming previous page.
[08.10.2025 17:13] Renaming previous data. index.html to ./d/2025-10-08.html
[08.10.2025 17:13] Writing result.
[08.10.2025 17:13] Renaming log file.
[08.10.2025 17:13] Renaming previous data. log.txt to ./logs/2025-10-08_last_log.txt
