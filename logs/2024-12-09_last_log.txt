[09.12.2024 08:15] Read previous papers.
[09.12.2024 08:15] Generating top page (month).
[09.12.2024 08:15] Writing top page (month).
[09.12.2024 09:12] Read previous papers.
[09.12.2024 09:12] Get feed.
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05271
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04814
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05237
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04862
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04445
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05270
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04440
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04301
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05263
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03428
[09.12.2024 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04905
[09.12.2024 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.12.2024 09:12] No deleted papers detected.
[09.12.2024 09:12] Downloading and parsing papers (pdf, html). Total: 11.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.05271.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.05271.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.05271.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.04814.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.04814.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.04814.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.05237.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.05237.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.05237.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.04862.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.04862.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.04862.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.04445.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.04445.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.04445.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.05270.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.05270.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.05270.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.04440.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.04440.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.04440.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.04301.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.04301.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.04301.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.05263.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.05263.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.05263.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.03428.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.03428.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.03428.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Downloading and parsing paper https://huggingface.co/papers/2412.04905.
[09.12.2024 09:12] Extra JSON file exists (./assets/json/2412.04905.json), skip PDF parsing.
[09.12.2024 09:12] Paper image links file exists (./assets/img_data/2412.04905.json), skip HTML parsing.
[09.12.2024 09:12] Success.
[09.12.2024 09:12] Enriching papers with extra data.
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 0. We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relations...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 1. Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human pref...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 2. Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, ...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 3. This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction followi...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 4. Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. W...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 5. Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 6. Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and inter...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 7. Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applicat...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 8. Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ign...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 9. The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable perfo...
[09.12.2024 09:12] ********************************************************************************
[09.12.2024 09:12] Abstract 10. Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the E...
[09.12.2024 09:12] Read previous papers.
[09.12.2024 09:12] Generating reviews via LLM API.
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#open_source", "#hallucinations", "#reasoning", "#training", "#multimodal", "#benchmark", "#dataset"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: InternVL 2.5 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹", "desc": "InternVL 2.5 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#training", "#alignment", "#video"], "emoji": "ğŸ¥", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LiFT - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. Ğ
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#reasoning", "#dataset", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#open_source", "#small_models", "#long_context", "#benchmark"], "emoji": "ğŸ§ ", "ru": {"title": "EXAONE 3.5: ĞĞ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ LG AI Research", "desc": "ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ LG AI Research Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ EXAONE 3.5, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#training", "#transfer_learning", "#games", "#robotics", "#multimodal", "#video"], "emoji": "ğŸ¤–", "ru": {"title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization"], "emoji": "ğŸš€", "ru": {"title": "APOLLO: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ APOLLO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). APOLLO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#hallucinations", "#agents", "#games", "#multimodal", "#video"], "emoji": "ğŸ¬", "ru": {"title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenMAC - Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. 
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#cv"], "emoji": "ğŸš€", "ru": {"title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼", "desc": "SwiftEdit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SwiftEdi
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#diffusion", "#video", "#multimodal"], "emoji": "â±ï¸", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MinT - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼. ĞœĞµ
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#3d"], "emoji": "ğŸ ", "ru": {"title": "2DGS-Room: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 2DGS-Room Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ¿Ñ€
[09.12.2024 09:12] Using data from previous issue: {"categories": ["#benchmark", "#dialogue_generation", "#agents"], "emoji": "ğŸ—£ï¸", "ru": {"title": "DEMO: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'ĞœĞ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°' Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DEMO
[09.12.2024 09:12] Trying to get texts in Chinese.
[09.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL
[09.12.2024 09:12] Mistral response. {"id": "616001bce96f477c835e76a9f9441e6c", "object": "chat.completion", "created": 1733735558, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86 InternVL 2.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e InternVL 2.0 \u7684\u5148\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\u3002\u5b83\u4fdd\u7559\u4e86\u6838\u5fc3\u6a21\u578b\u67b6\u6784\uff0c\u4f46\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7b56\u7565\u4ee5\u53ca\u6570\u636e\u8d28\u91cf\u65b9\u9762\u8fdb\u884c\u4e86\u663e\u8457\u6539\u8fdb\u3002\u6211\u4eec\u7814\u7a76\u4e86\u6a21\u578b\u6269\u5c55\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u3001\u8bed\u8a00\u6a21\u578b\u3001\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6d4b\u8bd5\u65f6\u914d\u7f6e\u7684\u6027\u80fd\u8d8b\u52bf\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0cInternVL 2.5 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u591a\u5b66\u79d1\u63a8\u7406\u3001\u6587\u6863\u7406\u89e3\u3001\u591a\u56fe\u50cf/\u89c6\u9891\u7406\u89e3\u3001\u5b9e\u9645\u7406\u89e3\u3001\u591a\u6a21\u6001\u5e7b\u89c9\u68c0\u6d4b\u3001\u89c6\u89c9\u5b9a\u4f4d\u3001\u591a\u8bed\u8a00\u80fd\u529b\u548c\u7eaf\u8bed\u8a00\u5904\u7406\u3002\u6211\u4eec\u7684\u6a21\u578b\u662f\u7b2c\u4e00\u4e2a\u5728 MMMU \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8fc7 70% \u7684\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6f5c\u529b\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u4e2a\u6a21\u578b\u80fd\u4e3a\u5f00\u6e90\u793e\u533a\u8d21\u732e\u65b0\u7684\u6807\u51c6\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u5e94\u7528\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002HuggingFace \u6f14\u793a\u89c1 https://huggingface.co/spaces/OpenGVLab/InternVL\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 328, "total_tokens": 669, "completion_tokens": 341}}
[09.12.2024 09:12] Response: æˆ‘ä»¬ä»‹ç»äº† InternVL 2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº InternVL 2.0 çš„å…ˆè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚å®ƒä¿ç•™äº†æ ¸å¿ƒæ¨¡å‹æ¶æ„ï¼Œä½†åœ¨è®­ç»ƒå’Œæµ‹è¯•ç­–ç•¥ä»¥åŠæ•°æ®è´¨é‡æ–¹é¢è¿›è¡Œäº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹æ‰©å±•ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†è§†è§‰ç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹ã€æ•°æ®é›†å¤§å°å’Œæµ‹è¯•æ—¶é…ç½®çš„æ€§èƒ½è¶‹åŠ¿ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒInternVL 2.5 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šå­¦ç§‘æ¨ç†ã€æ–‡æ¡£ç†è§£ã€å¤šå›¾åƒ/è§†é¢‘ç†è§£ã€å®é™…ç†è§£ã€å¤šæ¨¡æ€å¹»è§‰æ£€æµ‹ã€è§†è§‰å®šä½ã€å¤šè¯­è¨€èƒ½åŠ›å’Œçº¯è¯­è¨€å¤„ç†ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªåœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ 70% çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æµ‹è¯•æ—¶æ‰©å±•æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¨¡å‹èƒ½ä¸ºå¼€æºç¤¾åŒºè´¡çŒ®æ–°çš„æ ‡å‡†ï¼Œç”¨äºå¼€å‘å’Œåº”ç”¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚HuggingFace æ¼”ç¤ºè§ https://huggingface.co/spaces/OpenGVLab/InternVLã€‚
[09.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æˆ‘ä»¬ä»‹ç»äº† InternVL 2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº InternVL 2.0 çš„å…ˆè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚å®ƒä¿ç•™äº†æ ¸å¿ƒæ¨¡å‹æ¶æ„ï¼Œä½†åœ¨è®­ç»ƒå’Œæµ‹è¯•ç­–ç•¥ä»¥åŠæ•°æ®è´¨é‡æ–¹é¢è¿›è¡Œäº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹æ‰©å±•ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†è§†è§‰ç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹ã€æ•°æ®é›†å¤§å°å’Œæµ‹è¯•æ—¶é…ç½®çš„æ€§èƒ½è¶‹åŠ¿ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒInternVL 2.5 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šå­¦ç§‘æ¨ç†ã€æ–‡æ¡£ç†è§£ã€å¤šå›¾åƒ/è§†é¢‘ç†è§£ã€å®é™…ç†è§£ã€å¤šæ¨¡æ€å¹»è§‰æ£€æµ‹ã€è§†è§‰å®šä½ã€å¤šè¯­è¨€èƒ½åŠ›å’Œçº¯è¯­è¨€å¤„ç†ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªåœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ 70% çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æµ‹è¯•æ—¶æ‰©å±•æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¨¡å‹èƒ½ä¸ºå¼€æºç¤¾åŒºè´¡çŒ®æ–°çš„æ ‡å‡†ï¼Œç”¨äºå¼€å‘å’Œåº”ç”¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚HuggingFace æ¼”ç¤ºè§ https://huggingface.co/spaces/OpenGVLab/InternVLã€‚
[09.12.2024 09:13] Mistral response. {"id": "446f17bedc1d413599a9a779eeb7ad66", "object": "chat.completion", "created": 1733735566, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "W\u01d2men ji\u00e8sh\u00e0o le InternVL 2.5, zh\u00e8 sh\u00ec y\u012bg\u00e8 j\u012by\u00fa InternVL 2.0 de xi\u0101nj\u00ecn du\u014d m\u00f3sh\u00ec d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng x\u00ecli\u00e8. T\u0101 b\u01ceoli\u00fale h\u00e9x\u012bn m\u00f3x\u00edng ji\u00e0g\u00f2u, d\u00e0n z\u00e0i x\u00f9nli\u00e0n h\u00e9 c\u00e8sh\u00ec c\u00e8l\u00fc\u00e8 y\u01d0ji\u01ce sh\u00f9j\u00f9 zh\u00ecli\u00e0ng f\u0101ngmi\u00e0n j\u00ecnx\u00edng le xi\u01cenzh\u00f9 g\u01ceij\u00ecn. W\u01d2men y\u00e1nji\u016b le m\u00f3x\u00edng ku\u00f2zh\u01cen y\u01d4 x\u00ecngn\u00e9ng zh\u012bji\u0101n de gu\u0101nx\u00ec, x\u00ect\u01d2ng de t\u00e0nt\u00e0o le sh\u00ecju\u0101n bi\u0101nm\u01ceq\u00ec, y\u01d4y\u00e1n m\u00f3x\u00edng, sh\u00f9j\u00f9j\u00ed d\u00e0x\u00ecng h\u00e9 c\u00e8sh\u00ec sh\u00ed p\u00e8izh\u00ec de x\u00ecngn\u00e9ng q\u016bsh\u00ec. T\u014dnggu\u00f2 gu\u01cengf\u00e0n de p\u00edngg\u016b, InternVL 2.5 z\u00e0i du\u014dg\u00e8 j\u012bzh\u01d4n c\u00e8sh\u00ec zh\u014dng bi\u01ceoxi\u00e0n ch\u016bs\u00e8, b\u0101oku\u00f2 du\u014d xu\u00e9k\u0113 tu\u012bl\u01d0, w\u00e9nji\u00e0n l\u01d0ji\u011b, du\u014d t\u00faxi\u00e0ng/sh\u00ecp\u00edn l\u01d0ji\u011b, sh\u00edj\u00ec l\u01d0ji\u011b, du\u014d m\u00f3sh\u00ec hu\u00e0nju\u00e9 ji\u01cenc\u00e8, sh\u00ecju\u0101n d\u00ecngw\u00e8i, du\u014d y\u01d4y\u00e1n n\u00e9ngl\u00ec h\u00e9 ch\u00fan y\u01d4y\u00e1n ch\u01d4l\u01d0. W\u01d2men de m\u00f3x\u00edng sh\u00ec d\u00ec-y\u012bg\u00e8 z\u00e0i MMMU j\u012bzh\u01d4n c\u00e8sh\u00ec zh\u014dng ch\u0101ogu\u00f2 70% de k\u0101iyu\u00e1n du\u014d m\u00f3sh\u00ec d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng, b\u00ecng zh\u01censh\u00ec le qi\u00e1ngd\u00e0 de c\u00e8sh\u00ec sh\u00ed ku\u00f2zh\u01cen qi\u00e1nl\u00ec. W\u01d2men x\u012bw\u00e0ng zh\u00e8g\u00e8 m\u00f3x\u00edng n\u00e9ng w\u00e8i k\u0101iyu\u00e1n sh\u00e8q\u016b g\u00f2ngxi\u00e0n x\u012bn de bi\u0101ozh\u01d4n, y\u00f2ngy\u00fa k\u0101if\u0101 h\u00e9 y\u00ecngy\u00f2ng du\u014d m\u00f3sh\u00ec r\u00e9ng\u014dng zh\u00ecn\u00e9ng x\u00ect\u01d2ng. HuggingFace y\u01censh\u00ec ji\u00e0n https://huggingface.co/spaces/OpenGVLab/InternVL.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 356, "total_tokens": 993, "completion_tokens": 637}}
[09.12.2024 09:13] Response: WÇ’men jiÃ¨shÃ o le InternVL 2.5, zhÃ¨ shÃ¬ yÄ«gÃ¨ jÄ«yÃº InternVL 2.0 de xiÄnjÃ¬n duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng xÃ¬liÃ¨. TÄ bÇoliÃºle hÃ©xÄ«n mÃ³xÃ­ng jiÃ gÃ²u, dÃ n zÃ i xÃ¹nliÃ n hÃ© cÃ¨shÃ¬ cÃ¨lÃ¼Ã¨ yÇjiÇ shÃ¹jÃ¹ zhÃ¬liÃ ng fÄngmiÃ n jÃ¬nxÃ­ng le xiÇnzhÃ¹ gÇijÃ¬n. WÇ’men yÃ¡njiÅ« le mÃ³xÃ­ng kuÃ²zhÇn yÇ” xÃ¬ngnÃ©ng zhÄ«jiÄn de guÄnxÃ¬, xÃ¬tÇ’ng de tÃ ntÃ o le shÃ¬juÄn biÄnmÇqÃ¬, yÇ”yÃ¡n mÃ³xÃ­ng, shÃ¹jÃ¹jÃ­ dÃ xÃ¬ng hÃ© cÃ¨shÃ¬ shÃ­ pÃ¨izhÃ¬ de xÃ¬ngnÃ©ng qÅ«shÃ¬. TÅngguÃ² guÇngfÃ n de pÃ­nggÅ«, InternVL 2.5 zÃ i duÅgÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨, bÄokuÃ² duÅ xuÃ©kÄ“ tuÄ«lÇ, wÃ©njiÃ n lÇjiÄ›, duÅ tÃºxiÃ ng/shÃ¬pÃ­n lÇjiÄ›, shÃ­jÃ¬ lÇjiÄ›, duÅ mÃ³shÃ¬ huÃ njuÃ© jiÇncÃ¨, shÃ¬juÄn dÃ¬ngwÃ¨i, duÅ yÇ”yÃ¡n nÃ©nglÃ¬ hÃ© chÃºn yÇ”yÃ¡n chÇ”lÇ. WÇ’men de mÃ³xÃ­ng shÃ¬ dÃ¬-yÄ«gÃ¨ zÃ i MMMU jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng chÄoguÃ² 70% de kÄiyuÃ¡n duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng, bÃ¬ng zhÇnshÃ¬ le qiÃ¡ngdÃ  de cÃ¨shÃ¬ shÃ­ kuÃ²zhÇn qiÃ¡nlÃ¬. WÇ’men xÄ«wÃ ng zhÃ¨gÃ¨ mÃ³xÃ­ng nÃ©ng wÃ¨i kÄiyuÃ¡n shÃ¨qÅ« gÃ²ngxiÃ n xÄ«n de biÄozhÇ”n, yÃ²ngyÃº kÄifÄ hÃ© yÃ¬ngyÃ²ng duÅ mÃ³shÃ¬ rÃ©ngÅng zhÃ¬nÃ©ng xÃ¬tÇ’ng. HuggingFace yÇnshÃ¬ jiÃ n https://huggingface.co/spaces/OpenGVLab/InternVL.
[09.12.2024 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æˆ‘ä»¬ä»‹ç»äº† InternVL 2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº InternVL 2.0 çš„å…ˆè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚å®ƒä¿ç•™äº†æ ¸å¿ƒæ¨¡å‹æ¶æ„ï¼Œä½†åœ¨è®­ç»ƒå’Œæµ‹è¯•ç­–ç•¥ä»¥åŠæ•°æ®è´¨é‡æ–¹é¢è¿›è¡Œäº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹æ‰©å±•ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†è§†è§‰ç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹ã€æ•°æ®é›†å¤§å°å’Œæµ‹è¯•æ—¶é…ç½®çš„æ€§èƒ½è¶‹åŠ¿ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒInternVL 2.5 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šå­¦ç§‘æ¨ç†ã€æ–‡æ¡£ç†è§£ã€å¤šå›¾åƒ/è§†é¢‘ç†è§£ã€å®é™…ç†è§£ã€å¤šæ¨¡æ€å¹»è§‰æ£€æµ‹ã€è§†è§‰å®šä½ã€å¤šè¯­è¨€èƒ½åŠ›å’Œçº¯è¯­è¨€å¤„ç†ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªåœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ 70% çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æµ‹è¯•æ—¶æ‰©å±•æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¨¡å‹èƒ½ä¸ºå¼€æºç¤¾åŒºè´¡çŒ®æ–°çš„æ ‡å‡†ï¼Œç”¨äºå¼€å‘å’Œåº”ç”¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚HuggingFace æ¼”ç¤ºè§ https://huggingface.co/spaces/OpenGVLab/InternVLã€‚
[09.12.2024 09:13] Mistral response. {"id": "509f1b8a69754717ae8c85ef5ac3db8a", "object": "chat.completion", "created": 1733735580, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8 sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"\u5148\u8fdb\", \"pinyin\": \"xi\u0101n j\u00ecn\", \"trans\": \"advanced\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3 t\u00e0i\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u4fdd\u7559\", \"pinyin\": \"b\u01ceo li\u00fa\", \"trans\": \"retain\"},\n    {\"word\": \"\u6838\u5fc3\", \"pinyin\": \"h\u00e9 x\u012bn\", \"trans\": \"core\"},\n    {\"word\": \"\u67b6\u6784\", \"pinyin\": \"ji\u00e0 g\u00f2u\", \"trans\": \"architecture\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cen zh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u6539\u8fdb\", \"pinyin\": \"g\u01cei j\u00ecn\", \"trans\": \"improvement\"},\n    {\"word\": \"\u6269\u5c55\", \"pinyin\": \"ku\u00f2 zh\u01cen\", \"trans\": \"expand\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecng n\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u7cfb\u7edf\", \"pinyin\": \"x\u00ec t\u01d2ng\", \"trans\": \"system\"},\n    {\"word\": \"\u63a2\u8ba8\", \"pinyin\": \"t\u00e0n t\u01ceo\", \"trans\": \"explore\"},\n    {\"word\": \"\u89c6\u89c9\", \"pinyin\": \"sh\u00ec ju\u00e9\", \"trans\": \"visual\"},\n    {\"word\": \"\u7f16\u7801\u5668\", \"pinyin\": \"bi\u0101n m\u01ce q\u00ec\", \"trans\": \"encoder\"},\n    {\"word\": \"\u6570\u636e\u96c6\", \"pinyin\": \"sh\u00f9 j\u00f9 j\u00ed\", \"trans\": \"dataset\"},\n    {\"word\": \"\u914d\u7f6e\", \"pinyin\": \"p\u00e8i zh\u00ec\", \"trans\": \"configuration\"},\n    {\"word\": \"\u8d8b\u52bf\", \"pinyin\": \"q\u016b sh\u00ec\", \"trans\": \"trend\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"\u57fa\u51c6\", \"pinyin\": \"j\u012b zh\u01d4n\", \"trans\": \"benchmark\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceo xi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016b s\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u591a\u5b66\u79d1\", \"pinyin\": \"du\u014d xu\u00e9 k\u0113\", \"trans\": \"multidisciplinary\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012b l\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u6587\u6863\", \"pinyin\": \"w\u00e9n d\u00e0ng\", \"trans\": \"document\"},\n    {\"word\": \"\u7406\u89e3\", \"pinyin\": \"l\u01d0 ji\u011b\", \"trans\": \"understanding\"},\n    {\"word\": \"\u591a\u56fe\u50cf\", \"pinyin\": \"du\u014d t\u00fa xi\u00e0ng\", \"trans\": \"multi-image\"},\n    {\"word\": \"\u89c6\u9891\", \"pinyin\": \"sh\u00ec p\u00ecn\", \"trans\": \"video\"},\n    {\"word\": \"\u5b9e\u9645\", \"pinyin\": \"sh\u00ed j\u00ec\", \"trans\": \"practical\"},\n    {\"word\": \"\u5e7b\u89c9\", \"pinyin\": \"hu\u00e0n ju\u00e9\", \"trans\": \"hallucination\"},\n    {\"word\": \"\u68c0\u6d4b\", \"pinyin\": \"ji\u01cen c\u00e8\", \"trans\": \"detection\"},\n    {\"word\": \"\u5b9a\u4f4d\", \"pinyin\": \"d\u00ecng w\u00e8i\", \"trans\": \"localization\"},\n    {\"word\": \"\u591a\u8bed\u8a00\", \"pinyin\": \"du\u014d y\u01d4 y\u00e1n\", \"trans\": \"multilingual\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ng l\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u7eaf\u8bed\u8a00\", \"pinyin\": \"ch\u00fan y\u01d4 y\u00e1n\", \"trans\": \"pure language\"},\n    {\"word\": \"\u5904\u7406\", \"pinyin\": \"ch\u01d4 l\u01d0\", \"trans\": \"process\"},\n    {\"word\": \"\u5f00\u6e90\", \"pinyin\": \"k\u0101i yu\u00e1n\", \"trans\": \"open-source\"},\n    {\"word\": \"\u793e\u533a\", \"pinyin\": \"sh\u00e8 q\u016b\", \"trans\": \"community\"},\n    {\"word\": \"\u8d21\u732e\", \"pinyin\": \"g\u00f2ng xi\u00e0n\", \"trans\": \"contribute\"},\n    {\"word\": \"\u6807\u51c6\", \"pinyin\": \"bi\u0101o zh\u01d4n\", \"trans\": \"standard\"},\n    {\"word\": \"\u5f00\u53d1\", \"pinyin\": \"k\u0101i f\u0101\", \"trans\": \"develop\"},\n    {\"word\": \"\u5e94\u7528\", \"pinyin\": \"y\u00ecng y\u00f2ng\", \"trans\": \"apply\"},\n    {\"word\": \"\u4eba\u5de5\u667a\u80fd\", \"pinyin\": \"r\u00e9n g\u014dng zh\u00ec n\u00e9ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"\u7cfb\u7edf\", \"pinyin\": \"x\u00ec t\u01d2ng\", \"trans\": \"system\"},\n    {\"word\": \"\u6f14\u793a\", \"pinyin\": \"y\u01cen sh\u00ec\", \"trans\": \"demonstration\"},\n    {\"word\": \"\u89c1\", \"pinyin\": \"ji\u00e0n\", \"trans\": \"see\"}\n]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 388, "total_tokens": 1697, "completion_tokens": 1309}}
[09.12.2024 09:13] Response: [
    {"word": "ä»‹ç»", "pinyin": "jiÃ¨ shÃ o", "trans": "introduce"},
    {"word": "å…ˆè¿›", "pinyin": "xiÄn jÃ¬n", "trans": "advanced"},
    {"word": "å¤šæ¨¡æ€", "pinyin": "duÅ mÃ³ tÃ i", "trans": "multimodal"},
    {"word": "ä¿ç•™", "pinyin": "bÇo liÃº", "trans": "retain"},
    {"word": "æ ¸å¿ƒ", "pinyin": "hÃ© xÄ«n", "trans": "core"},
    {"word": "æ¶æ„", "pinyin": "jiÃ  gÃ²u", "trans": "architecture"},
    {"word": "æ˜¾è‘—", "pinyin": "xiÇn zhÃ¹", "trans": "significant"},
    {"word": "æ”¹è¿›", "pinyin": "gÇi jÃ¬n", "trans": "improvement"},
    {"word": "æ‰©å±•", "pinyin": "kuÃ² zhÇn", "trans": "expand"},
    {"word": "æ€§èƒ½", "pinyin": "xÃ¬ng nÃ©ng", "trans": "performance"},
    {"word": "ç³»ç»Ÿ", "pinyin": "xÃ¬ tÇ’ng", "trans": "system"},
    {"word": "æ¢è®¨", "pinyin": "tÃ n tÇo", "trans": "explore"},
    {"word": "è§†è§‰", "pinyin": "shÃ¬ juÃ©", "trans": "visual"},
    {"word": "ç¼–ç å™¨", "pinyin": "biÄn mÇ qÃ¬", "trans": "encoder"},
    {"word": "æ•°æ®é›†", "pinyin": "shÃ¹ jÃ¹ jÃ­", "trans": "dataset"},
    {"word": "é…ç½®", "pinyin": "pÃ¨i zhÃ¬", "trans": "configuration"},
    {"word": "è¶‹åŠ¿", "pinyin": "qÅ« shÃ¬", "trans": "trend"},
    {"word": "è¯„ä¼°", "pinyin": "pÃ­ng gÅ«", "trans": "evaluate"},
    {"word": "åŸºå‡†", "pinyin": "jÄ« zhÇ”n", "trans": "benchmark"},
    {"word": "è¡¨ç°", "pinyin": "biÇo xiÃ n", "trans": "performance"},
    {"word": "å‡ºè‰²", "pinyin": "chÅ« sÃ¨", "trans": "outstanding"},
    {"word": "å¤šå­¦ç§‘", "pinyin": "duÅ xuÃ© kÄ“", "trans": "multidisciplinary"},
    {"word": "æ¨ç†", "pinyin": "tuÄ« lÇ", "trans": "reasoning"},
    {"word": "æ–‡æ¡£", "pinyin": "wÃ©n dÃ ng", "trans": "document"},
    {"word": "ç†è§£", "pinyin": "lÇ jiÄ›", "trans": "understanding"},
    {"word": "å¤šå›¾åƒ", "pinyin": "duÅ tÃº xiÃ ng", "trans": "multi-image"},
    {"word": "è§†é¢‘", "pinyin": "shÃ¬ pÃ¬n", "trans": "video"},
    {"word": "å®é™…", "pinyin": "shÃ­ jÃ¬", "trans": "practical"},
    {"word": "å¹»è§‰", "pinyin": "huÃ n juÃ©", "trans": "hallucination"},
    {"word": "æ£€æµ‹", "pinyin": "jiÇn cÃ¨", "trans": "detection"},
    {"word": "å®šä½", "pinyin": "dÃ¬ng wÃ¨i", "trans": "localization"},
    {"word": "å¤šè¯­è¨€", "pinyin": "duÅ yÇ” yÃ¡n", "trans": "multilingual"},
    {"word": "èƒ½åŠ›", "pinyin": "nÃ©ng lÃ¬", "trans": "ability"},
    {"word": "çº¯è¯­è¨€", "pinyin": "chÃºn yÇ” yÃ¡n", "trans": "pure language"},
    {"word": "å¤„ç†", "pinyin": "chÇ” lÇ", "trans": "process"},
    {"word": "å¼€æº", "pinyin": "kÄi yuÃ¡n", "trans": "open-source"},
    {"word": "ç¤¾åŒº", "pinyin": "shÃ¨ qÅ«", "trans": "community"},
    {"word": "è´¡çŒ®", "pinyin": "gÃ²ng xiÃ n", "trans": "contribute"},
    {"word": "æ ‡å‡†", "pinyin": "biÄo zhÇ”n", "trans": "standard"},
    {"word": "å¼€å‘", "pinyin": "kÄi fÄ", "trans": "develop"},
    {"word": "åº”ç”¨", "pinyin": "yÃ¬ng yÃ²ng", "trans": "apply"},
    {"word": "äººå·¥æ™ºèƒ½", "pinyin": "rÃ©n gÅng zhÃ¬ nÃ©ng", "trans": "artificial intelligence"},
    {"word": "ç³»ç»Ÿ", "pinyin": "xÃ¬ tÇ’ng", "trans": "system"},
    {"word": "æ¼”ç¤º", "pinyin": "yÇn shÃ¬", "trans": "demonstration"},
    {"word": "è§", "pinyin": "jiÃ n", "trans": "see"}
]
[09.12.2024 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æˆ‘ä»¬ä»‹ç»äº† InternVL 2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº InternVL 2.0 çš„å…ˆè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚å®ƒä¿ç•™äº†æ ¸å¿ƒæ¨¡å‹æ¶æ„ï¼Œä½†åœ¨è®­ç»ƒå’Œæµ‹è¯•ç­–ç•¥ä»¥åŠæ•°æ®è´¨é‡æ–¹é¢è¿›è¡Œäº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹æ‰©å±•ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†è§†è§‰ç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹ã€æ•°æ®é›†å¤§å°å’Œæµ‹è¯•æ—¶é…ç½®çš„æ€§èƒ½è¶‹åŠ¿ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒInternVL 2.5 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šå­¦ç§‘æ¨ç†ã€æ–‡æ¡£ç†è§£ã€å¤šå›¾åƒ/è§†é¢‘ç†è§£ã€å®é™…ç†è§£ã€å¤šæ¨¡æ€å¹»è§‰æ£€æµ‹ã€è§†è§‰å®šä½ã€å¤šè¯­è¨€èƒ½åŠ›å’Œçº¯è¯­è¨€å¤„ç†ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªåœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ 70% çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æµ‹è¯•æ—¶æ‰©å±•æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¨¡å‹èƒ½ä¸ºå¼€æºç¤¾åŒºè´¡çŒ®æ–°çš„æ ‡å‡†ï¼Œç”¨äºå¼€å‘å’Œåº”ç”¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚HuggingFace æ¼”ç¤ºè§ https://huggingface.co/spaces/OpenGVLab/InternVLã€‚
[09.12.2024 09:13] Mistral response. {"id": "88a94b48be4b409ebd94b8c11ca43dfc", "object": "chat.completion", "created": 1733735611, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "We introduced InternVL 2.5, an advanced multimodal large language model series based on InternVL 2.0. It retains the core model architecture but features significant improvements in training and testing strategies, as well as data quality. We studied the relationship between model scaling and performance, systematically exploring performance trends in visual encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations, InternVL 2.5 has demonstrated outstanding performance across multiple benchmarks, including multidisciplinary reasoning, document understanding, multi-image/video understanding, practical understanding, multimodal hallucination detection, visual localization, multilingual capabilities, and pure language processing. Our model is the first open-source multimodal large language model to exceed 70% on the MMMU benchmark and has shown strong test-time scaling potential. We hope this model will contribute new standards to the open-source community for developing and applying multimodal AI systems. See the HuggingFace demo at https://huggingface.co/spaces/OpenGVLab/InternVL.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 354, "total_tokens": 592, "completion_tokens": 238}}
[09.12.2024 09:13] Response: We introduced InternVL 2.5, an advanced multimodal large language model series based on InternVL 2.0. It retains the core model architecture but features significant improvements in training and testing strategies, as well as data quality. We studied the relationship between model scaling and performance, systematically exploring performance trends in visual encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations, InternVL 2.5 has demonstrated outstanding performance across multiple benchmarks, including multidisciplinary reasoning, document understanding, multi-image/video understanding, practical understanding, multimodal hallucination detection, visual localization, multilingual capabilities, and pure language processing. Our model is the first open-source multimodal large language model to exceed 70% on the MMMU benchmark and has shown strong test-time scaling potential. We hope this model will contribute new standards to the open-source community for developing and applying multimodal AI systems. See the HuggingFace demo at https://huggingface.co/spaces/OpenGVLab/InternVL.
[09.12.2024 09:13] Renaming data file.
[09.12.2024 09:13] Renaming previous data. hf_papers.json to ./d/2024-12-09.json
[09.12.2024 09:13] Saving new data file.
[09.12.2024 09:13] Generating page.
[09.12.2024 09:13] Renaming previous page.
[09.12.2024 09:13] Renaming previous data. index.html to ./d/2024-12-09.html
[09.12.2024 09:13] [Experimental] Generating Chinese page for reading.
[09.12.2024 09:13] Chinese vocab [{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'}, {'word': 'å…ˆè¿›', 'pinyin': 'xiÄn jÃ¬n', 'trans': 'advanced'}, {'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'ä¿ç•™', 'pinyin': 'bÇo liÃº', 'trans': 'retain'}, {'word': 'æ ¸å¿ƒ', 'pinyin': 'hÃ© xÄ«n', 'trans': 'core'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'æ”¹è¿›', 'pinyin': 'gÇi jÃ¬n', 'trans': 'improvement'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ² zhÇn', 'trans': 'expand'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'}, {'word': 'æ¢è®¨', 'pinyin': 'tÃ n tÇo', 'trans': 'explore'}, {'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'}, {'word': 'ç¼–ç å™¨', 'pinyin': 'biÄn mÇ qÃ¬', 'trans': 'encoder'}, {'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'}, {'word': 'é…ç½®', 'pinyin': 'pÃ¨i zhÃ¬', 'trans': 'configuration'}, {'word': 'è¶‹åŠ¿', 'pinyin': 'qÅ« shÃ¬', 'trans': 'trend'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}, {'word': 'å¤šå­¦ç§‘', 'pinyin': 'duÅ xuÃ© kÄ“', 'trans': 'multidisciplinary'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'æ–‡æ¡£', 'pinyin': 'wÃ©n dÃ ng', 'trans': 'document'}, {'word': 'ç†è§£', 'pinyin': 'lÇ jiÄ›', 'trans': 'understanding'}, {'word': 'å¤šå›¾åƒ', 'pinyin': 'duÅ tÃº xiÃ ng', 'trans': 'multi-image'}, {'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ¬n', 'trans': 'video'}, {'word': 'å®é™…', 'pinyin': 'shÃ­ jÃ¬', 'trans': 'practical'}, {'word': 'å¹»è§‰', 'pinyin': 'huÃ n juÃ©', 'trans': 'hallucination'}, {'word': 'æ£€æµ‹', 'pinyin': 'jiÇn cÃ¨', 'trans': 'detection'}, {'word': 'å®šä½', 'pinyin': 'dÃ¬ng wÃ¨i', 'trans': 'localization'}, {'word': 'å¤šè¯­è¨€', 'pinyin': 'duÅ yÇ” yÃ¡n', 'trans': 'multilingual'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'çº¯è¯­è¨€', 'pinyin': 'chÃºn yÇ” yÃ¡n', 'trans': 'pure language'}, {'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'process'}, {'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open-source'}, {'word': 'ç¤¾åŒº', 'pinyin': 'shÃ¨ qÅ«', 'trans': 'community'}, {'word': 'è´¡çŒ®', 'pinyin': 'gÃ²ng xiÃ n', 'trans': 'contribute'}, {'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'}, {'word': 'å¼€å‘', 'pinyin': 'kÄi fÄ', 'trans': 'develop'}, {'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'apply'}, {'word': 'äººå·¥æ™ºèƒ½', 'pinyin': 'rÃ©n gÅng zhÃ¬ nÃ©ng', 'trans': 'artificial intelligence'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬ tÇ’ng', 'trans': 'system'}, {'word': 'æ¼”ç¤º', 'pinyin': 'yÇn shÃ¬', 'trans': 'demonstration'}, {'word': 'è§', 'pinyin': 'jiÃ n', 'trans': 'see'}]
[09.12.2024 09:13] Renaming previous Chinese page.
[09.12.2024 09:13] Renaming previous data. zh.html to ./d/2024-12-08_zh_reading_task.html
[09.12.2024 09:13] Writing Chinese reading task.
[09.12.2024 09:13] Writing result.
[09.12.2024 09:13] Renaming log file.
[09.12.2024 09:13] Renaming previous data. log.txt to ./logs/2024-12-09_last_log.txt
