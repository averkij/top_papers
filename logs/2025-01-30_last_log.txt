[30.01.2025 06:14] Read previous papers.
[30.01.2025 06:14] Generating top page (month).
[30.01.2025 06:14] Writing top page (month).
[30.01.2025 07:09] Read previous papers.
[30.01.2025 07:09] Get feed.
[30.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17703
[30.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17749
[30.01.2025 07:09] Extract page data from URL. URL: https://huggingface.co/papers/2501.17433
[30.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17195
[30.01.2025 07:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2025 07:09] No deleted papers detected.
[30.01.2025 07:09] Downloading and parsing papers (pdf, html). Total: 4.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17703.
[30.01.2025 07:09] Extra JSON file exists (./assets/json/2501.17703.json), skip PDF parsing.
[30.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.17703.json), skip HTML parsing.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17749.
[30.01.2025 07:09] Extra JSON file exists (./assets/json/2501.17749.json), skip PDF parsing.
[30.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.17749.json), skip HTML parsing.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17433.
[30.01.2025 07:09] Downloading paper 2501.17433 from http://arxiv.org/pdf/2501.17433v1...
[30.01.2025 07:09] Extracting affiliations from text.
[30.01.2025 07:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation Tiansheng Huang 1 Sihao Hu 1 Fatih Ilhan 1 Selim Furkan Tekin 1 Ling Liu 1 5 2 0 2 9 2 ] . [ 1 3 3 4 7 1 . 1 0 5 2 : r a "
[30.01.2025 07:09] Response: []
[30.01.2025 07:09] Extracting affiliations from text.
[30.01.2025 07:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation Tiansheng Huang 1 Sihao Hu 1 Fatih Ilhan 1 Selim Furkan Tekin 1 Ling Liu 1 5 2 0 2 9 2 ] . [ 1 3 3 4 7 1 . 1 0 5 2 : r aRecent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks models lose their safety alignment ability after fine-tuning on few harmful samples. For risk mitigation, guardrail is typically used to filter out harmful samples before fine-tuning. By designing new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/ Virus. 1. Introduction OpenAI hot-sells its reinforcement Fine-Tuning (RFT) service as their day-2 product within their "12 Days of OpenAI" celebration 1. It is expected to be killer app that enables customers to create expert models for narrow set of tasks in their domain. However, recent research shows that the fine-tuning-as-a-service paradigm for large language models (LLMs) exposes some serious safety concern. Several researchers (Qi et al., 2023; Yang et al., 2023; Zhan et al., 2023; Lermen et al., 2023; Yi et al., 2024a) uncover that the safety alignment of fine-tuned model can readily be com1Georgia Institute of Technology, USA. Correspondence to: Tiansheng Huang <thuang374@gatech.edu>. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute. 1See day 2 in https://openai.com/12-days/. 1 Figure 1. three stage pipeline for harmful fine-tuning attack under guardrail moderation. i) At the first stage, the model is safety aligned with alignment data. ii) At the second stage, the service provider applies guardrail moderation to filter out the harmful samples over the uploaded fine-tuning data. iii) At the third stage, the filtered data is used for fine-tuning the aligned LLM. Our attack Virus is concerning how to construct the user dataset that can bypass the guardrail and break the victim LLMs safety alignment. promised by uploading few harmful data for fine-tuning. The red-teaming attack (Qi et al., 2023) shows that one only needs at little as 10 harmful samples at cost of less than $0.20 to break the safety alignment of an LLM. With OpenAI guardrail moderation for the fine-tuning data, this attack attempt is never successful by now. The starndard guardrail moderation technique is to first stream the finetuning data to guardrail model (a specially fine-tuned LLM) to determine whether some of them are harmful, and only those data that are classified as benign can stream to the fine-tuning API. To this end, we aim to address the following research question: Is there harmful fine-tuning attack that can bypass the guardrail moderation and yet effective to degrade the safety alignment of the victim LLMs? We first validate the robustness of the guardrail moderation to show that guardrail moderation indeed can filter out most harmful samples in the user data uploaded for fine tuning, and thereby effectively mitigating the harmful fine-tuning attack to large degree. Then we make red-teaming attempts to bypass the control. We start the first investigation with two attempts by design. Our first attempt is to concatenate benign QA with harmful QA, which failed to successfully bypass the guardrail moderation. Our second attempt is to design an data optimization method to jailbreak the guardrail moderation model. The results show that this attempt can successfully Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation bypass the guardrail. However, such data optimization introduce gradient mis-match, leading to insufficiency to break safety alignment of the victim LLMs. Learning from the lessons of the above failure attempts, we design Virus, dual objective data optimization scheme, to construct the harmful dataset. Virus aims to optimize the harmful data to achieve dual goals: i) the jailbreak loss against guardrail is low such that it can successfully jailbreak the guardrail moderation, and ii) the gradient taken on this data can resemble the harmful gradient, thereby the prompt can still effectively break down the safety alignment of the victim LLM. Our empirical results show that Virus can effectively bypass the moderation, reaching up-to 100% leakage ratio. On the other hand, the gradient of the data optimized Virus can resemble the harmful gradient, effectively breaking down the safety alignment of the victim LLMs, increasing its harmful score by up-to 21.8%. We summarize our contribution as follows: We systematically study the scenario of harmful finetuning attack under guardrail moderation and provide empirical measurement results, justifying the usefulness of guardrail moderation. Learning from the two failure attempts, we propose Virus, dual goal data optimization method aiming to bypass guardrail with superior attack performance. Extensive experiments on different attack settings demonstrate that Virus can successfully bypass guardrail moderation and break down the victim LLMs safety alignment. The dataset optimized by Virus is available at https:// huggingface.co/datasets/anonymous4486/Virus. 2. Related Work Safety alignment. Safety alignment is typically enforced before LLM deployment, in order to align the output of the LLMs with human values. Typical techniques for safety alignment includes Supervised Fine-Tuning (SFT), or more advanced techniques, e.g., Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Griffith et al., 2013; Dai et al., 2023; Bai et al., 2022; Wu et al., 2023; Dong et al., 2023; Rafailov et al., 2023; Yuan et al., 2023; Song et al., 2023), Stable Alignment (Liu et al., 2023), Selfee (Ye et al., 2023), Circuit Breakers (Zou et al., 2024), and 3Fusion (Tekin et al., 2024). Harmful fine-tuning attack. Safety alignmen"
[30.01.2025 07:09] Mistral response. {"id": "f2703cc50b344255b0b60228b140123d", "object": "chat.completion", "created": 1738220982, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Georgia Institute of Technology, USA\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1738, "total_tokens": 1754, "completion_tokens": 16}}
[30.01.2025 07:09] Response: ```python
["Georgia Institute of Technology, USA"]
```
[30.01.2025 07:09] Deleting PDF ./assets/pdf/2501.17433.pdf.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17195.
[30.01.2025 07:09] Extra JSON file exists (./assets/json/2501.17195.json), skip PDF parsing.
[30.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.17195.json), skip HTML parsing.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Enriching papers with extra data.
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 0. Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct...
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 1. Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thor...
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 2. Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By desi...
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 3. We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwis...
[30.01.2025 07:09] Read previous papers.
[30.01.2025 07:09] Generating reviews via LLM API.
[30.01.2025 07:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#dataset", "#math"], "emoji": "🧠", "ru": {"title": "Критика вместо имитации: новый подход к обучению языковых моделей", "desc": "В статье предлагается новый подход к обучению языковых моделей - Critique Fine-Tuning (CFT), который учит моде
[30.01.2025 07:09] Using data from previous issue: {"categories": ["#ethics", "#data", "#inference", "#training", "#security"], "emoji": "🛡️", "ru": {"title": "Автоматизированное тестирование безопасности языковых моделей: ключ к ответственному ИИ", "desc": "Эта статья описывает опыт внешнего тестирования безопасности новой языковой модели OpenAI o3
[30.01.2025 07:09] Querying the API.
[30.01.2025 07:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus
[30.01.2025 07:09] Response: {
  "desc": "Исследование показывает уязвимость больших языковых моделей (LLM) к вредоносной дообучению, что приводит к потере способности соблюдать правила безопасности. Авторы разработали новый метод атаки под названием Virus, который легко обходит защитные механизмы, слегка модифицируя вредоносные данные. Эксперименты показали, что оптимизированные вредоносные данные не обнаруживаются защитными механизмами и при этом достигают высокой эффективности атаки. Исследователи подчеркивают, что полагаться только на фильтрацию данных недостаточно для решения проблем безопасности предобученных LLM.",
  "emoji": "🦠",
  "title": "Вирусная атака: новый вызов безопасности языковых моделей"
}
[30.01.2025 07:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"

[30.01.2025 07:09] Response: ```python
['DATA', 'TRAINING']
```
[30.01.2025 07:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"

[30.01.2025 07:09] Response: ```python
['SECURITY', 'ETHICS']
```
[30.01.2025 07:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the vulnerabilities of Large Language Models (LLMs) to harmful fine-tuning attacks, where models can lose their safety features after being trained on malicious data. The authors introduce a new red-teaming method called Virus, which demonstrates that relying solely on guardrails for filtering harmful samples is ineffective. Their experiments reveal that the Virus method can modify harmful data in a way that evades detection by the guardrail, achieving a 100% leakage ratio while maintaining high attack performance. The study emphasizes that guardrail moderation is not a reliable solution for addressing the fundamental safety issues present in pre-trained LLMs.","title":"Guardrails Can\'t Save LLMs from Harmful Fine-Tuning Attacks!"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper discusses the vulnerabilities of Large Language Models (LLMs) to harmful fine-tuning attacks, where models can lose their safety features after being trained on malicious data. The authors introduce a new red-teaming method called Virus, which demonstrates that relying solely on guardrails for filtering harmful samples is ineffective. Their experiments reveal that the Virus method can modify harmful data in a way that evades detection by the guardrail, achieving a 100% leakage ratio while maintaining high attack performance. The study emphasizes that guardrail moderation is not a reliable solution for addressing the fundamental safety issues present in pre-trained LLMs.', title="Guardrails Can't Save LLMs from Harmful Fine-Tuning Attacks!"))
[30.01.2025 07:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近的研究表明，大型语言模型（LLMs）在有害微调攻击下非常脆弱，经过少量有害样本的微调后，模型的安全性会下降。为了降低风险，通常会使用防护措施来过滤有害样本。然而，我们的研究表明，仅仅依靠这种防护措施进行数据过滤并不可靠。我们提出的攻击方法“Virus”能够轻松绕过防护措施，通过轻微修改有害数据，使其在高达100%的泄漏率下仍然无法被检测到，同时实现了优越的攻击性能。","title":"防护措施并非万无一失，需警惕微调攻击"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='最近的研究表明，大型语言模型（LLMs）在有害微调攻击下非常脆弱，经过少量有害样本的微调后，模型的安全性会下降。为了降低风险，通常会使用防护措施来过滤有害样本。然而，我们的研究表明，仅仅依靠这种防护措施进行数据过滤并不可靠。我们提出的攻击方法“Virus”能够轻松绕过防护措施，通过轻微修改有害数据，使其在高达100%的泄漏率下仍然无法被检测到，同时实现了优越的攻击性能。', title='防护措施并非万无一失，需警惕微调攻击'))
[30.01.2025 07:09] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#agi", "#synthetic", "#data", "#training", "#optimization", "#dataset", "#rlhf"], "emoji": "⚖️", "ru": {"title": "Atla Selene Mini: Малая модель-судья с большими возможностями", "desc": "Статья представляет Atla Selene Mini - передовую малую языковую
[30.01.2025 07:09] Loading Chinese text from previous data.
[30.01.2025 07:09] Renaming data file.
[30.01.2025 07:09] Renaming previous data. hf_papers.json to ./d/2025-01-30.json
[30.01.2025 07:09] Saving new data file.
[30.01.2025 07:09] Generating page.
[30.01.2025 07:09] Renaming previous page.
[30.01.2025 07:09] Renaming previous data. index.html to ./d/2025-01-30.html
[30.01.2025 07:09] [Experimental] Generating Chinese page for reading.
[30.01.2025 07:09] Chinese vocab [{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervised'}, {'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tuning'}, {'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'}, {'word': '基础模型', 'pinyin': 'jī chǔ mó xíng', 'trans': 'foundational model'}, {'word': '作用', 'pinyin': 'zuò yòng', 'trans': 'effect'}, {'word': '泛化', 'pinyin': 'fàn huà', 'trans': 'generalization'}, {'word': '倾向于', 'pinyin': 'qīng xiàng yú', 'trans': 'tend to'}, {'word': '未见过', 'pinyin': 'wèi jiàn guò', 'trans': 'unseen'}, {'word': '变体', 'pinyin': 'biàn tǐ', 'trans': 'variant'}, {'word': '视觉识别', 'pinyin': 'shì jué shí bié', 'trans': 'visual recognition'}, {'word': '不可或缺', 'pinyin': 'bù kě huò quē', 'trans': 'indispensable'}]
[30.01.2025 07:09] Renaming previous Chinese page.
[30.01.2025 07:09] Renaming previous data. zh.html to ./d/2025-01-29_zh_reading_task.html
[30.01.2025 07:09] Writing Chinese reading task.
[30.01.2025 07:09] Writing result.
[30.01.2025 07:09] Renaming log file.
[30.01.2025 07:09] Renaming previous data. log.txt to ./logs/2025-01-30_last_log.txt
