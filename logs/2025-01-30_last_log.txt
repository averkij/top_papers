[30.01.2025 06:14] Read previous papers.
[30.01.2025 06:14] Generating top page (month).
[30.01.2025 06:14] Writing top page (month).
[30.01.2025 07:09] Read previous papers.
[30.01.2025 07:09] Get feed.
[30.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17703
[30.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17749
[30.01.2025 07:09] Extract page data from URL. URL: https://huggingface.co/papers/2501.17433
[30.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17195
[30.01.2025 07:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2025 07:09] No deleted papers detected.
[30.01.2025 07:09] Downloading and parsing papers (pdf, html). Total: 4.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17703.
[30.01.2025 07:09] Extra JSON file exists (./assets/json/2501.17703.json), skip PDF parsing.
[30.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.17703.json), skip HTML parsing.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17749.
[30.01.2025 07:09] Extra JSON file exists (./assets/json/2501.17749.json), skip PDF parsing.
[30.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.17749.json), skip HTML parsing.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17433.
[30.01.2025 07:09] Downloading paper 2501.17433 from http://arxiv.org/pdf/2501.17433v1...
[30.01.2025 07:09] Extracting affiliations from text.
[30.01.2025 07:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation Tiansheng Huang 1 Sihao Hu 1 Fatih Ilhan 1 Selim Furkan Tekin 1 Ling Liu 1 5 2 0 2 9 2 ] . [ 1 3 3 4 7 1 . 1 0 5 2 : r a "
[30.01.2025 07:09] Response: []
[30.01.2025 07:09] Extracting affiliations from text.
[30.01.2025 07:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation Tiansheng Huang 1 Sihao Hu 1 Fatih Ilhan 1 Selim Furkan Tekin 1 Ling Liu 1 5 2 0 2 9 2 ] . [ 1 3 3 4 7 1 . 1 0 5 2 : r aRecent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks models lose their safety alignment ability after fine-tuning on few harmful samples. For risk mitigation, guardrail is typically used to filter out harmful samples before fine-tuning. By designing new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/ Virus. 1. Introduction OpenAI hot-sells its reinforcement Fine-Tuning (RFT) service as their day-2 product within their "12 Days of OpenAI" celebration 1. It is expected to be killer app that enables customers to create expert models for narrow set of tasks in their domain. However, recent research shows that the fine-tuning-as-a-service paradigm for large language models (LLMs) exposes some serious safety concern. Several researchers (Qi et al., 2023; Yang et al., 2023; Zhan et al., 2023; Lermen et al., 2023; Yi et al., 2024a) uncover that the safety alignment of fine-tuned model can readily be com1Georgia Institute of Technology, USA. Correspondence to: Tiansheng Huang <thuang374@gatech.edu>. Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute. 1See day 2 in https://openai.com/12-days/. 1 Figure 1. three stage pipeline for harmful fine-tuning attack under guardrail moderation. i) At the first stage, the model is safety aligned with alignment data. ii) At the second stage, the service provider applies guardrail moderation to filter out the harmful samples over the uploaded fine-tuning data. iii) At the third stage, the filtered data is used for fine-tuning the aligned LLM. Our attack Virus is concerning how to construct the user dataset that can bypass the guardrail and break the victim LLMs safety alignment. promised by uploading few harmful data for fine-tuning. The red-teaming attack (Qi et al., 2023) shows that one only needs at little as 10 harmful samples at cost of less than $0.20 to break the safety alignment of an LLM. With OpenAI guardrail moderation for the fine-tuning data, this attack attempt is never successful by now. The starndard guardrail moderation technique is to first stream the finetuning data to guardrail model (a specially fine-tuned LLM) to determine whether some of them are harmful, and only those data that are classified as benign can stream to the fine-tuning API. To this end, we aim to address the following research question: Is there harmful fine-tuning attack that can bypass the guardrail moderation and yet effective to degrade the safety alignment of the victim LLMs? We first validate the robustness of the guardrail moderation to show that guardrail moderation indeed can filter out most harmful samples in the user data uploaded for fine tuning, and thereby effectively mitigating the harmful fine-tuning attack to large degree. Then we make red-teaming attempts to bypass the control. We start the first investigation with two attempts by design. Our first attempt is to concatenate benign QA with harmful QA, which failed to successfully bypass the guardrail moderation. Our second attempt is to design an data optimization method to jailbreak the guardrail moderation model. The results show that this attempt can successfully Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation bypass the guardrail. However, such data optimization introduce gradient mis-match, leading to insufficiency to break safety alignment of the victim LLMs. Learning from the lessons of the above failure attempts, we design Virus, dual objective data optimization scheme, to construct the harmful dataset. Virus aims to optimize the harmful data to achieve dual goals: i) the jailbreak loss against guardrail is low such that it can successfully jailbreak the guardrail moderation, and ii) the gradient taken on this data can resemble the harmful gradient, thereby the prompt can still effectively break down the safety alignment of the victim LLM. Our empirical results show that Virus can effectively bypass the moderation, reaching up-to 100% leakage ratio. On the other hand, the gradient of the data optimized Virus can resemble the harmful gradient, effectively breaking down the safety alignment of the victim LLMs, increasing its harmful score by up-to 21.8%. We summarize our contribution as follows: We systematically study the scenario of harmful finetuning attack under guardrail moderation and provide empirical measurement results, justifying the usefulness of guardrail moderation. Learning from the two failure attempts, we propose Virus, dual goal data optimization method aiming to bypass guardrail with superior attack performance. Extensive experiments on different attack settings demonstrate that Virus can successfully bypass guardrail moderation and break down the victim LLMs safety alignment. The dataset optimized by Virus is available at https:// huggingface.co/datasets/anonymous4486/Virus. 2. Related Work Safety alignment. Safety alignment is typically enforced before LLM deployment, in order to align the output of the LLMs with human values. Typical techniques for safety alignment includes Supervised Fine-Tuning (SFT), or more advanced techniques, e.g., Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Griffith et al., 2013; Dai et al., 2023; Bai et al., 2022; Wu et al., 2023; Dong et al., 2023; Rafailov et al., 2023; Yuan et al., 2023; Song et al., 2023), Stable Alignment (Liu et al., 2023), Selfee (Ye et al., 2023), Circuit Breakers (Zou et al., 2024), and 3Fusion (Tekin et al., 2024). Harmful fine-tuning attack. Safety alignmen"
[30.01.2025 07:09] Mistral response. {"id": "f2703cc50b344255b0b60228b140123d", "object": "chat.completion", "created": 1738220982, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Georgia Institute of Technology, USA\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1738, "total_tokens": 1754, "completion_tokens": 16}}
[30.01.2025 07:09] Response: ```python
["Georgia Institute of Technology, USA"]
```
[30.01.2025 07:09] Deleting PDF ./assets/pdf/2501.17433.pdf.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.17195.
[30.01.2025 07:09] Extra JSON file exists (./assets/json/2501.17195.json), skip PDF parsing.
[30.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.17195.json), skip HTML parsing.
[30.01.2025 07:09] Success.
[30.01.2025 07:09] Enriching papers with extra data.
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 0. Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct...
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 1. Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thor...
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 2. Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By desi...
[30.01.2025 07:09] ********************************************************************************
[30.01.2025 07:09] Abstract 3. We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwis...
[30.01.2025 07:09] Read previous papers.
[30.01.2025 07:09] Generating reviews via LLM API.
[30.01.2025 07:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#dataset", "#math"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Critique Fine-Tuning (CFT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´Ğµ
[30.01.2025 07:09] Using data from previous issue: {"categories": ["#ethics", "#data", "#inference", "#training", "#security"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ»ÑÑ‡ Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenAI o3
[30.01.2025 07:09] Querying the API.
[30.01.2025 07:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus
[30.01.2025 07:09] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Virus, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ĞµĞ³ĞºĞ¾ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹, ÑĞ»ĞµĞ³ĞºĞ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… LLM.",
  "emoji": "ğŸ¦ ",
  "title": "Ğ’Ğ¸Ñ€ÑƒÑĞ½Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
}
[30.01.2025 07:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"

[30.01.2025 07:09] Response: ```python
['DATA', 'TRAINING']
```
[30.01.2025 07:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100\% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"

[30.01.2025 07:09] Response: ```python
['SECURITY', 'ETHICS']
```
[30.01.2025 07:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the vulnerabilities of Large Language Models (LLMs) to harmful fine-tuning attacks, where models can lose their safety features after being trained on malicious data. The authors introduce a new red-teaming method called Virus, which demonstrates that relying solely on guardrails for filtering harmful samples is ineffective. Their experiments reveal that the Virus method can modify harmful data in a way that evades detection by the guardrail, achieving a 100% leakage ratio while maintaining high attack performance. The study emphasizes that guardrail moderation is not a reliable solution for addressing the fundamental safety issues present in pre-trained LLMs.","title":"Guardrails Can\'t Save LLMs from Harmful Fine-Tuning Attacks!"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper discusses the vulnerabilities of Large Language Models (LLMs) to harmful fine-tuning attacks, where models can lose their safety features after being trained on malicious data. The authors introduce a new red-teaming method called Virus, which demonstrates that relying solely on guardrails for filtering harmful samples is ineffective. Their experiments reveal that the Virus method can modify harmful data in a way that evades detection by the guardrail, achieving a 100% leakage ratio while maintaining high attack performance. The study emphasizes that guardrail moderation is not a reliable solution for addressing the fundamental safety issues present in pre-trained LLMs.', title="Guardrails Can't Save LLMs from Harmful Fine-Tuning Attacks!"))
[30.01.2025 07:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰å®³å¾®è°ƒæ”»å‡»ä¸‹éå¸¸è„†å¼±ï¼Œç»è¿‡å°‘é‡æœ‰å®³æ ·æœ¬çš„å¾®è°ƒåï¼Œæ¨¡å‹çš„å®‰å…¨æ€§ä¼šä¸‹é™ã€‚ä¸ºäº†é™ä½é£é™©ï¼Œé€šå¸¸ä¼šä½¿ç”¨é˜²æŠ¤æªæ–½æ¥è¿‡æ»¤æœ‰å®³æ ·æœ¬ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»…ä»…ä¾é è¿™ç§é˜²æŠ¤æªæ–½è¿›è¡Œæ•°æ®è¿‡æ»¤å¹¶ä¸å¯é ã€‚æˆ‘ä»¬æå‡ºçš„æ”»å‡»æ–¹æ³•â€œVirusâ€èƒ½å¤Ÿè½»æ¾ç»•è¿‡é˜²æŠ¤æªæ–½ï¼Œé€šè¿‡è½»å¾®ä¿®æ”¹æœ‰å®³æ•°æ®ï¼Œä½¿å…¶åœ¨é«˜è¾¾100%çš„æ³„æ¼ç‡ä¸‹ä»ç„¶æ— æ³•è¢«æ£€æµ‹åˆ°ï¼ŒåŒæ—¶å®ç°äº†ä¼˜è¶Šçš„æ”»å‡»æ€§èƒ½ã€‚","title":"é˜²æŠ¤æªæ–½å¹¶éä¸‡æ— ä¸€å¤±ï¼Œéœ€è­¦æƒ•å¾®è°ƒæ”»å‡»"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰å®³å¾®è°ƒæ”»å‡»ä¸‹éå¸¸è„†å¼±ï¼Œç»è¿‡å°‘é‡æœ‰å®³æ ·æœ¬çš„å¾®è°ƒåï¼Œæ¨¡å‹çš„å®‰å…¨æ€§ä¼šä¸‹é™ã€‚ä¸ºäº†é™ä½é£é™©ï¼Œé€šå¸¸ä¼šä½¿ç”¨é˜²æŠ¤æªæ–½æ¥è¿‡æ»¤æœ‰å®³æ ·æœ¬ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»…ä»…ä¾é è¿™ç§é˜²æŠ¤æªæ–½è¿›è¡Œæ•°æ®è¿‡æ»¤å¹¶ä¸å¯é ã€‚æˆ‘ä»¬æå‡ºçš„æ”»å‡»æ–¹æ³•â€œVirusâ€èƒ½å¤Ÿè½»æ¾ç»•è¿‡é˜²æŠ¤æªæ–½ï¼Œé€šè¿‡è½»å¾®ä¿®æ”¹æœ‰å®³æ•°æ®ï¼Œä½¿å…¶åœ¨é«˜è¾¾100%çš„æ³„æ¼ç‡ä¸‹ä»ç„¶æ— æ³•è¢«æ£€æµ‹åˆ°ï¼ŒåŒæ—¶å®ç°äº†ä¼˜è¶Šçš„æ”»å‡»æ€§èƒ½ã€‚', title='é˜²æŠ¤æªæ–½å¹¶éä¸‡æ— ä¸€å¤±ï¼Œéœ€è­¦æƒ•å¾®è°ƒæ”»å‡»'))
[30.01.2025 07:09] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#agi", "#synthetic", "#data", "#training", "#optimization", "#dataset", "#rlhf"], "emoji": "âš–ï¸", "ru": {"title": "Atla Selene Mini: ĞœĞ°Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑƒĞ´ÑŒÑ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Atla Selene Mini - Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼Ğ°Ğ»ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ
[30.01.2025 07:09] Loading Chinese text from previous data.
[30.01.2025 07:09] Renaming data file.
[30.01.2025 07:09] Renaming previous data. hf_papers.json to ./d/2025-01-30.json
[30.01.2025 07:09] Saving new data file.
[30.01.2025 07:09] Generating page.
[30.01.2025 07:09] Renaming previous page.
[30.01.2025 07:09] Renaming previous data. index.html to ./d/2025-01-30.html
[30.01.2025 07:09] [Experimental] Generating Chinese page for reading.
[30.01.2025 07:09] Chinese vocab [{'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervised'}, {'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tuning'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ng huÃ  xuÃ© xÃ­', 'trans': 'reinforcement learning'}, {'word': 'åŸºç¡€æ¨¡å‹', 'pinyin': 'jÄ« chÇ” mÃ³ xÃ­ng', 'trans': 'foundational model'}, {'word': 'ä½œç”¨', 'pinyin': 'zuÃ² yÃ²ng', 'trans': 'effect'}, {'word': 'æ³›åŒ–', 'pinyin': 'fÃ n huÃ ', 'trans': 'generalization'}, {'word': 'å€¾å‘äº', 'pinyin': 'qÄ«ng xiÃ ng yÃº', 'trans': 'tend to'}, {'word': 'æœªè§è¿‡', 'pinyin': 'wÃ¨i jiÃ n guÃ²', 'trans': 'unseen'}, {'word': 'å˜ä½“', 'pinyin': 'biÃ n tÇ', 'trans': 'variant'}, {'word': 'è§†è§‰è¯†åˆ«', 'pinyin': 'shÃ¬ juÃ© shÃ­ biÃ©', 'trans': 'visual recognition'}, {'word': 'ä¸å¯æˆ–ç¼º', 'pinyin': 'bÃ¹ kÄ› huÃ² quÄ“', 'trans': 'indispensable'}]
[30.01.2025 07:09] Renaming previous Chinese page.
[30.01.2025 07:09] Renaming previous data. zh.html to ./d/2025-01-29_zh_reading_task.html
[30.01.2025 07:09] Writing Chinese reading task.
[30.01.2025 07:09] Writing result.
[30.01.2025 07:09] Renaming log file.
[30.01.2025 07:09] Renaming previous data. log.txt to ./logs/2025-01-30_last_log.txt
