[30.01.2025 02:10] Read previous papers.
[30.01.2025 02:10] Generating top page (month).
[30.01.2025 02:10] Writing top page (month).
[30.01.2025 03:12] Read previous papers.
[30.01.2025 03:12] Get feed.
[30.01.2025 03:12] Extract page data from URL. URL: https://huggingface.co/papers/2501.17749
[30.01.2025 03:12] Extract page data from URL. URL: https://huggingface.co/papers/2501.17703
[30.01.2025 03:12] Extract page data from URL. URL: https://huggingface.co/papers/2501.17195
[30.01.2025 03:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2025 03:12] Downloading and parsing papers (pdf, html). Total: 3.
[30.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.17749.
[30.01.2025 03:12] Downloading paper 2501.17749 from http://arxiv.org/pdf/2501.17749v1...
[30.01.2025 03:12] Extracting affiliations from text.
[30.01.2025 03:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 9 4 7 7 1 . 1 0 5 2 : r EARLY EXTERNAL SAFETY TESTING OF OPENAIS O3-MINI: INSIGHTS FROM PRE-DEPLOYMENT EVALUATION Aitor Arrieta Mondragon University Mondragon, Spain aarrieta@mondragon.edu Miriam Ugarte Mondragon University Mondragon, Spain mugarte@mondragon.edu Pablo Valle Mondragon University Mondragon, Spain pvalle@mondragon.edu José Antonio Parejo University of Seville Seville, Spain japarejo@us.es Sergio Segura University of Seville Seville, Spain sergiosegura@us.es "
[30.01.2025 03:12] Response: ```python
["Mondragon University, Mondragon, Spain", "University of Seville, Seville, Spain"]
```
[30.01.2025 03:12] Deleting PDF ./assets/pdf/2501.17749.pdf.
[30.01.2025 03:12] Success.
[30.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.17703.
[30.01.2025 03:12] Downloading paper 2501.17703 from http://arxiv.org/pdf/2501.17703v1...
[30.01.2025 03:12] Extracting affiliations from text.
[30.01.2025 03:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate Yubo Wang 1 Xiang Yue 2 Wenhu Chen 1 3 https://tiger-ai-lab.github.io/CritiqueFineTuning/ 5 2 0 2 9 ] . [ 1 3 0 7 7 1 . 1 0 5 2 : r Figure 1: Comparison between CFT and SFT on 50K samples from WebInstruct (Yue et al., 2024b). SFT-verified means SFT training on the responses validated by GPT-4o, SFT-GPT4o means SFT training on the responses from GPT-4o. CFT is our approach, which trains on the critique provided by GPT-4o. Abstract Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated In this paresponses for given instructions. per, we challenge this paradigm and propose Critique Fine-Tuning (CFT), strategy where models learn to critique noisy responses rather Inspired by than simply imitate correct ones. human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understandingtraits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct 50K-sample dataset 1Department of Computer Science, University of Waterloo 2Carnegie Mellon University, Pittsburgh 3Vector Insitute, Toronto. Correspondence to: Yubo Wang <y726wang@uwaterloo.ca>, Wenhu Chen <wenhuchen@uwaterloo.ca>. from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields consistent 410% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT modeltrained on just 50K samplesmatches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that "
[30.01.2025 03:12] Response: ```python
[
    "Department of Computer Science, University of Waterloo",
    "Carnegie Mellon University, Pittsburgh",
    "Vector Institute, Toronto"
]
```
[30.01.2025 03:12] Deleting PDF ./assets/pdf/2501.17703.pdf.
[30.01.2025 03:12] Success.
[30.01.2025 03:12] Downloading and parsing paper https://huggingface.co/papers/2501.17195.
[30.01.2025 03:12] Downloading paper 2501.17195 from http://arxiv.org/pdf/2501.17195v1...
[30.01.2025 03:12] Extracting affiliations from text.
[30.01.2025 03:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 5 9 1 7 1 . 1 0 5 2 : r Atla Selene Mini: General Purpose Evaluation Model Andrei Alexandru1 Antonia Calvi1 Henry Broomfield1 Jackson Golden1 Kyle Dai1 Mathias Leys1 Maurice Burger1 Max Bartolo2,3 Roman Engeler1 Sashank Pisupati1 Toby Drane1 Young Sun Park1 1atla 2University College London 3Cohere atla-ai.com "
[30.01.2025 03:12] Response: ```python
["atla", "University College London", "Cohere"]
```
[30.01.2025 03:12] Deleting PDF ./assets/pdf/2501.17195.pdf.
[30.01.2025 03:12] Success.
[30.01.2025 03:12] Enriching papers with extra data.
[30.01.2025 03:12] ********************************************************************************
[30.01.2025 03:12] Abstract 0. Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thor...
[30.01.2025 03:12] ********************************************************************************
[30.01.2025 03:12] Abstract 1. Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct...
[30.01.2025 03:12] ********************************************************************************
[30.01.2025 03:12] Abstract 2. We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwis...
[30.01.2025 03:12] Read previous papers.
[30.01.2025 03:12] Generating reviews via LLM API.
[30.01.2025 03:12] Querying the API.
[30.01.2025 03:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM.
[30.01.2025 03:13] Response: {
  "desc": "Эта статья описывает опыт внешнего тестирования безопасности новой языковой модели OpenAI o3-mini. Исследователи использовали инструмент ASTRAL для автоматической генерации небезопасных промптов, чтобы оценить различные аспекты безопасности модели. Было сгенерировано и выполнено 10 080 тестовых входных данных, из которых 87 случаев были идентифицированы как фактически небезопасное поведение модели. Статья подчеркивает важность тщательного тестирования безопасности больших языковых моделей перед их развертыванием.",
  "emoji": "🛡️",
  "title": "Автоматизированное тестирование безопасности языковых моделей: ключ к ответственному ИИ"
}
[30.01.2025 03:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM."

[30.01.2025 03:13] Response: ```python
["DATA", "INFERENCE", "TRAINING"]
```
[30.01.2025 03:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM."

[30.01.2025 03:13] Response: ```python
['ETHICS', 'SECURITY']
```
[30.01.2025 03:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the safety testing of Large Language Models (LLMs), focusing on the o3-mini model from OpenAI. Researchers from Mondragon University and University of Seville used a tool called ASTRAL to automatically create unsafe test inputs to evaluate the model\'s safety. They generated and executed 10,080 test prompts, identifying 87 instances of unsafe behavior after manual verification. The findings emphasize the importance of rigorous safety assessments before deploying LLMs to mitigate risks like privacy violations and misinformation.","title":"Ensuring Safety in Large Language Models: A Testing Approach"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc="This paper discusses the safety testing of Large Language Models (LLMs), focusing on the o3-mini model from OpenAI. Researchers from Mondragon University and University of Seville used a tool called ASTRAL to automatically create unsafe test inputs to evaluate the model's safety. They generated and executed 10,080 test prompts, identifying 87 instances of unsafe behavior after manual verification. The findings emphasize the importance of rigorous safety assessments before deploying LLMs to mitigate risks like privacy violations and misinformation.", title='Ensuring Safety in Large Language Models: A Testing Approach'))
[30.01.2025 03:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"大型语言模型（LLMs）在我们的日常生活中变得不可或缺，但它们也带来了隐私风险、偏见和错误信息传播等问题。这些风险表明需要建立强有力的安全机制和伦理指导，以确保模型的负责任使用。本文报告了蒙德拉贡大学和塞维利亚大学的研究人员对OpenAI的新o3-mini LLM进行的外部安全测试经验。我们使用ASTRAL工具自动生成不安全的测试输入，以评估LLMs的不同安全类别，并发现了87个实际的不安全行为实例。","title":"确保大型语言模型的安全性与责任使用"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='大型语言模型（LLMs）在我们的日常生活中变得不可或缺，但它们也带来了隐私风险、偏见和错误信息传播等问题。这些风险表明需要建立强有力的安全机制和伦理指导，以确保模型的负责任使用。本文报告了蒙德拉贡大学和塞维利亚大学的研究人员对OpenAI的新o3-mini LLM进行的外部安全测试经验。我们使用ASTRAL工具自动生成不安全的测试输入，以评估LLMs的不同安全类别，并发现了87个实际的不安全行为实例。', title='确保大型语言模型的安全性与责任使用'))
[30.01.2025 03:13] Querying the API.
[30.01.2025 03:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models.
[30.01.2025 03:13] Response: {
  "desc": "В статье предлагается новый подход к обучению языковых моделей - Critique Fine-Tuning (CFT), который учит модели критиковать неточные ответы, а не просто имитировать правильные. CFT показывает улучшение на 4-10% по сравнению с традиционным Supervised Fine-Tuning (SFT) на шести математических тестах с различными базовыми моделями. Модель Qwen2.5-Math-CFT, обученная всего на 50 тысячах примеров, показывает результаты на уровне или лучше конкурентных моделей, использующих более 2 миллионов примеров. Авторы утверждают, что обучение на основе критики более эффективно для развития рассуждений языковых моделей.",

  "emoji": "🧠",

  "title": "Критика вместо имитации: новый подход к обучению языковых моделей"
}
[30.01.2025 03:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models."

[30.01.2025 03:13] Response: ```python
['DATASET', 'TRAINING', 'MATH']
```
[30.01.2025 03:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models."

[30.01.2025 03:13] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[30.01.2025 03:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Critique Fine-Tuning (CFT), a novel approach to training language models that focuses on teaching them to critique noisy responses instead of merely imitating correct ones. By mimicking human critical thinking, CFT fosters a deeper understanding and analysis of language, which is often neglected in traditional Supervised Fine-Tuning (SFT). The authors validate CFT\'s effectiveness using a dataset of 50,000 samples generated by GPT-4o, demonstrating consistent performance improvements of 4-10% over SFT across various math benchmarks. The results suggest that critique-based training can significantly enhance the reasoning capabilities of language models, outperforming models trained on much larger datasets.","title":"Critique Fine-Tuning: Enhancing Language Model Reasoning through Critical Analysis"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc="This paper introduces Critique Fine-Tuning (CFT), a novel approach to training language models that focuses on teaching them to critique noisy responses instead of merely imitating correct ones. By mimicking human critical thinking, CFT fosters a deeper understanding and analysis of language, which is often neglected in traditional Supervised Fine-Tuning (SFT). The authors validate CFT's effectiveness using a dataset of 50,000 samples generated by GPT-4o, demonstrating consistent performance improvements of 4-10% over SFT across various math benchmarks. The results suggest that critique-based training can significantly enhance the reasoning capabilities of language models, outperforming models trained on much larger datasets.", title='Critique Fine-Tuning: Enhancing Language Model Reasoning through Critical Analysis'))
[30.01.2025 03:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的训练语言模型的方法，称为批评微调（CFT），与传统的监督微调（SFT）不同，CFT让模型学习如何批评噪声响应，而不是仅仅模仿正确的答案。这种方法受到人类学习过程的启发，强调批判性思维，促进更深层次的分析和细致的理解。通过构建一个包含5万样本的数据集，并使用GPT-4o生成批评，CFT在多个数学基准测试中相较于SFT取得了4-10%的一致性提升。研究结果表明，基于批评的训练方法为提升语言模型的推理能力提供了更有效的替代方案。","title":"批评微调：提升语言模型推理的新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文提出了一种新的训练语言模型的方法，称为批评微调（CFT），与传统的监督微调（SFT）不同，CFT让模型学习如何批评噪声响应，而不是仅仅模仿正确的答案。这种方法受到人类学习过程的启发，强调批判性思维，促进更深层次的分析和细致的理解。通过构建一个包含5万样本的数据集，并使用GPT-4o生成批评，CFT在多个数学基准测试中相较于SFT取得了4-10%的一致性提升。研究结果表明，基于批评的训练方法为提升语言模型的推理能力提供了更有效的替代方案。', title='批评微调：提升语言模型推理的新方法'))
[30.01.2025 03:13] Querying the API.
[30.01.2025 03:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage widespread community adoption.
[30.01.2025 03:13] Response: {
  "desc": "Статья представляет Atla Selene Mini - передовую малую языковую модель-судью (SLMJ). Модель превосходит существующие SLMJ и GPT-4o-mini по общей производительности в 11 различных тестовых наборах. Selene Mini обучена с использованием стратегии курирования данных, сочетающей публичные наборы данных с синтетически сгенерированными критическими оценками. Модель демонстрирует улучшенное согласие с оценками экспертов-людей в финансовых и медицинских задачах без предварительной настройки.",
  "emoji": "⚖️",
  "title": "Atla Selene Mini: Малая модель-судья с большими возможностями"
}
[30.01.2025 03:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage widespread community adoption."

[30.01.2025 03:13] Response: ```python
['DATASET', 'DATA', 'RLHF', 'SMALL_MODELS', 'TRAINING']
```
[30.01.2025 03:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage widespread community adoption."

[30.01.2025 03:13] Response: ```python
['AGI', 'OPTIMIZATION', 'SYNTHETIC', 'OPEN_SOURCE']
```
[30.01.2025 03:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Atla Selene Mini is a cutting-edge small language model designed to evaluate various tasks effectively. It surpasses existing models, including GPT-4o-mini, by achieving superior performance on 11 benchmarks related to scoring, classification, and preference tasks. The model benefits from a unique data curation strategy that enhances public datasets with synthetic critiques, ensuring high-quality training data. With a combination of direct preference optimization and supervised fine-tuning, Selene Mini demonstrates strong alignment with human evaluations, particularly in specialized fields like finance and medicine.","title":"Selene Mini: The Next Level in Language Model Evaluation!"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='Atla Selene Mini is a cutting-edge small language model designed to evaluate various tasks effectively. It surpasses existing models, including GPT-4o-mini, by achieving superior performance on 11 benchmarks related to scoring, classification, and preference tasks. The model benefits from a unique data curation strategy that enhances public datasets with synthetic critiques, ensuring high-quality training data. With a combination of direct preference optimization and supervised fine-tuning, Selene Mini demonstrates strong alignment with human evaluations, particularly in specialized fields like finance and medicine.', title='Selene Mini: The Next Level in Language Model Evaluation!'))
[30.01.2025 03:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们介绍了Atla Selene Mini，这是一种先进的小型语言模型评估器（SLMJ）。Selene Mini在11个不同的基准测试中表现优于其他模型，包括GPT-4o-mini，尤其在绝对评分、分类和成对偏好任务上。通过精心的数据策划策略，我们增强了公共数据集，并确保数据质量，从而训练出一个在真实场景中表现出色的评估器。Selene Mini在金融和医疗行业数据集上与人类专家评估的零-shot一致性显著提高，且对提示格式的变化具有良好的鲁棒性。","title":"Atla Selene Mini：超越传统评估的语言模型"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='我们介绍了Atla Selene Mini，这是一种先进的小型语言模型评估器（SLMJ）。Selene Mini在11个不同的基准测试中表现优于其他模型，包括GPT-4o-mini，尤其在绝对评分、分类和成对偏好任务上。通过精心的数据策划策略，我们增强了公共数据集，并确保数据质量，从而训练出一个在真实场景中表现出色的评估器。Selene Mini在金融和医疗行业数据集上与人类专家评估的零-shot一致性显著提高，且对提示格式的变化具有良好的鲁棒性。', title='Atla Selene Mini：超越传统评估的语言模型'))
[30.01.2025 03:13] Loading Chinese text from previous data.
[30.01.2025 03:13] Renaming data file.
[30.01.2025 03:13] Renaming previous data. hf_papers.json to ./d/2025-01-30.json
[30.01.2025 03:13] Saving new data file.
[30.01.2025 03:13] Generating page.
[30.01.2025 03:13] Renaming previous page.
[30.01.2025 03:13] Renaming previous data. index.html to ./d/2025-01-30.html
[30.01.2025 03:13] [Experimental] Generating Chinese page for reading.
[30.01.2025 03:13] Chinese vocab [{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervised'}, {'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tuning'}, {'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'}, {'word': '基础模型', 'pinyin': 'jī chǔ mó xíng', 'trans': 'foundational model'}, {'word': '作用', 'pinyin': 'zuò yòng', 'trans': 'effect'}, {'word': '泛化', 'pinyin': 'fàn huà', 'trans': 'generalization'}, {'word': '倾向于', 'pinyin': 'qīng xiàng yú', 'trans': 'tend to'}, {'word': '未见过', 'pinyin': 'wèi jiàn guò', 'trans': 'unseen'}, {'word': '变体', 'pinyin': 'biàn tǐ', 'trans': 'variant'}, {'word': '视觉识别', 'pinyin': 'shì jué shí bié', 'trans': 'visual recognition'}, {'word': '不可或缺', 'pinyin': 'bù kě huò quē', 'trans': 'indispensable'}]
[30.01.2025 03:13] Renaming previous Chinese page.
[30.01.2025 03:13] Renaming previous data. zh.html to ./d/2025-01-29_zh_reading_task.html
[30.01.2025 03:13] Writing Chinese reading task.
[30.01.2025 03:13] Writing result.
[30.01.2025 03:13] Renaming log file.
[30.01.2025 03:13] Renaming previous data. log.txt to ./logs/2025-01-30_last_log.txt
