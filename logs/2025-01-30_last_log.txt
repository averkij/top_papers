[30.01.2025 08:13] Read previous papers.
[30.01.2025 08:13] Generating top page (month).
[30.01.2025 08:13] Writing top page (month).
[30.01.2025 09:10] Read previous papers.
[30.01.2025 09:10] Get feed.
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17703
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14334
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17749
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17433
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17195
[30.01.2025 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2025 09:10] No deleted papers detected.
[30.01.2025 09:10] Downloading and parsing papers (pdf, html). Total: 5.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17703.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17703.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17703.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.14334.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.14334.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.14334.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17749.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17749.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17749.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17433.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17433.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17433.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17195.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17195.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17195.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Enriching papers with extra data.
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 0. Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 1. The rapid growth of artificial intelligence (AI), particularly Large Language Models (LLMs), has raised concerns regarding its global environmental impact that extends beyond greenhouse gas emissions to include consideration of hardware fabrication and end-of-life processes. The opacity from major p...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 2. Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thor...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 3. Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By desi...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 4. We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwis...
[30.01.2025 09:10] Read previous papers.
[30.01.2025 09:10] Generating reviews via LLM API.
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#dataset", "#math"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Critique Fine-Tuning (CFT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´Ğµ
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#agents", "#data", "#benchmark", "#ethics"], "emoji": "ğŸŒ±", "ru": {"title": "Ğ—ĞµĞ»ĞµĞ½Ñ‹Ğ¹ Ğ˜Ğ˜: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¼Ñƒ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼Ñƒ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ 
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#ethics", "#data", "#inference", "#training", "#security"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ»ÑÑ‡ Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenAI o3
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#ethics", "#security", "#training", "#data"], "emoji": "ğŸ¦ ", "ru": {"title": "Ğ’Ğ¸Ñ€ÑƒÑĞ½Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ Ğ¿
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#agi", "#synthetic", "#data", "#training", "#optimization", "#dataset", "#rlhf"], "emoji": "âš–ï¸", "ru": {"title": "Atla Selene Mini: ĞœĞ°Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑƒĞ´ÑŒÑ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Atla Selene Mini - Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼Ğ°Ğ»ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ
[30.01.2025 09:10] Trying to get texts in Chinese.
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models.
[30.01.2025 09:10] Mistral response. {"id": "edbd96be8e3d4d118dac4fc021a129e1", "object": "chat.completion", "created": 1738228229, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u6311\u6218\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u6279\u8bc4\u5fae\u8c03\uff08CFT\uff09\u7b56\u7565\u3002CFT\u8ba9\u6a21\u578b\u5b66\u4e60\u6279\u8bc4\u6709\u566a\u97f3\u7684\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u6a21\u4eff\u6b63\u786e\u7684\u54cd\u5e94\u3002\u53d7\u5f3a\u8c03\u6279\u5224\u6027\u601d\u7ef4\u7684\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\u542f\u53d1\uff0cCFT\u9f13\u52b1\u66f4\u6df1\u5165\u7684\u5206\u6790\u548c\u7ec6\u81f4\u7684\u7406\u89e3\u3002\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CFT\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793aCFT\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u4f18\u4e8eSFT\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 359, "total_tokens": 505, "completion_tokens": 146}}
[30.01.2025 09:10] Response: è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚
[30.01.2025 09:10] Mistral response. {"id": "661cb8d0494341fc8a300763d7572db2", "object": "chat.completion", "created": 1738228233, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u6311\u6218\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u6279\u8bc4\u5fae\u8c03\uff08CFT\uff09\u7b56\u7565\u3002CFT\u8ba9\u6a21\u578b\u5b66\u4e60\u6279\u8bc4\u6709\u566a\u97f3\u7684\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u6a21\u4eff\u6b63\u786e\u7684\u54cd\u5e94\u3002\u53d7\u5f3a\u8c03\u6279\u5224\u6027\u601d\u7ef4\u7684\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\u542f\u53d1\uff0cCFT\u9f13\u52b1\u66f4\u6df1\u5165\u7684\u5206\u6790\u548c\u7ec6\u81f4\u7684\u7406\u89e3\u3002\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CFT\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793aCFT\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u4f18\u4e8eSFT\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ti\u01ceozh\u00e0n le ji\u00e0nd\u016b w\u0113iti\u00e1o (SFT) de f\u00e0nsh\u00ec, t\u00edch\u016b le p\u012bp\u00edng w\u0113iti\u00e1o (CFT) c\u00e8l\u00fc\u00e8. CFT r\u00e0ng m\u00f3x\u00edng xu\u00e9x\u00ed p\u012bp\u00edng y\u01d2u z\u00e0oy\u012bn de xi\u01cengy\u00ecng, \u00e9r b\u00f9sh\u00ec ji\u01cend\u0101n de m\u00f3f\u01ceng zh\u00e8ngqu\u00e8 de xi\u01cengy\u00ecng. Sh\u00f2u qi\u00e1ngdi\u00e0o p\u012bp\u00e0n x\u00ecng s\u012bw\u00e9i de r\u00e9nl\u00e8i xu\u00e9x\u00ed gu\u00f2ch\u00e9ng q\u01d0f\u0101, CFT g\u01d4l\u00ec g\u00e8ng sh\u0113nr\u00f9 de f\u0113nxi h\u00e9 x\u00eczh\u00ec de l\u01d0ji\u011b. Zu\u00f2zh\u011b t\u014dnggu\u00f2 sh\u00ecy\u00e0n y\u00e0nzh\u00e8ng le CFT de y\u01d2uxi\u00e0ox\u00ecng, ji\u00e9gu\u01d2 xi\u01censh\u00ec CFT z\u00e0i du\u014dg\u00e8 sh\u00f9xu\u00e9 j\u012bzh\u01d4n sh\u00e0ng y\u014duy\u00fa SFT."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 161, "total_tokens": 586, "completion_tokens": 425}}
[30.01.2025 09:10] Response: è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚

ZhÃ¨ piÄn wÃ©nzhÄng tiÇozhÃ n le jiÃ ndÅ« wÄ“itiÃ¡o (SFT) de fÃ nshÃ¬, tÃ­chÅ« le pÄ«pÃ­ng wÄ“itiÃ¡o (CFT) cÃ¨lÃ¼Ã¨. CFT rÃ ng mÃ³xÃ­ng xuÃ©xÃ­ pÄ«pÃ­ng yÇ’u zÃ oyÄ«n de xiÇngyÃ¬ng, Ã©r bÃ¹shÃ¬ jiÇndÄn de mÃ³fÇng zhÃ¨ngquÃ¨ de xiÇngyÃ¬ng. ShÃ²u qiÃ¡ngdiÃ o pÄ«pÃ n xÃ¬ng sÄ«wÃ©i de rÃ©nlÃ¨i xuÃ©xÃ­ guÃ²chÃ©ng qÇfÄ, CFT gÇ”lÃ¬ gÃ¨ng shÄ“nrÃ¹ de fÄ“nxi hÃ© xÃ¬zhÃ¬ de lÇjiÄ›. ZuÃ²zhÄ› tÅngguÃ² shÃ¬yÃ n yÃ nzhÃ¨ng le CFT de yÇ’uxiÃ oxÃ¬ng, jiÃ©guÇ’ xiÇnshÃ¬ CFT zÃ i duÅgÃ¨ shÃ¹xuÃ© jÄ«zhÇ”n shÃ ng yÅuyÃº SFT.
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚
[30.01.2025 09:10] Mistral response. {"id": "9c9723ab195c4e3da21a834563b18f6e", "object": "chat.completion", "created": 1738228241, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceo zh\u00e0n', 'trans': 'challenge'},\n{'word': '\u76d1\u7763', 'pinyin': 'ji\u00e0n d\u016b', 'trans': 'supervise'},\n{'word': '\u5fae\u8c03', 'pinyin': 'w\u0113i ti\u00e1o', 'trans': 'fine-tune'},\n{'word': '\u8303\u5f0f', 'pinyin': 'f\u00e0n sh\u00ec', 'trans': 'paradigm'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u6279\u8bc4', 'pinyin': 'p\u012b p\u00edng', 'trans': 'criticize'},\n{'word': '\u7b56\u7565', 'pinyin': 'c\u00e8 l\u00fc\u00e8', 'trans': 'strategy'},\n{'word': '\u566a\u97f3', 'pinyin': 'z\u00e0o y\u012bn', 'trans': 'noise'},\n{'word': '\u54cd\u5e94', 'pinyin': 'xi\u01ceng y\u00ecng', 'trans': 'response'},\n{'word': '\u6a21\u4eff', 'pinyin': 'm\u00f3 f\u01ceng', 'trans': 'imitate'},\n{'word': '\u542f\u53d1', 'pinyin': 'q\u01d0 f\u0101', 'trans': 'inspire'},\n{'word': '\u6279\u5224\u6027', 'pinyin': 'p\u012b p\u00e0n x\u00ecng', 'trans': 'critical'},\n{'word': '\u601d\u7ef4', 'pinyin': 's\u012b w\u00e9i', 'trans': 'thinking'},\n{'word': '\u9f13\u52b1', 'pinyin': 'g\u01d4 l\u00ec', 'trans': 'encourage'},\n{'word': '\u6df1\u5165', 'pinyin': 'sh\u0113n r\u00f9', 'trans': 'in-depth'},\n{'word': '\u7ec6\u81f4', 'pinyin': 'x\u00ec zh\u00ec', 'trans': 'detailed'},\n{'word': '\u9a8c\u8bc1', 'pinyin': 'y\u00e0n zh\u00e8ng', 'trans': 'verify'},\n{'word': '\u6709\u6548\u6027', 'pinyin': 'y\u01d2u xi\u00e0o x\u00ecng', 'trans': 'effectiveness'},\n{'word': '\u57fa\u51c6', 'pinyin': 'j\u012b zh\u01d4n', 'trans': 'benchmark'},\n{'word': '\u4f18\u4e8e', 'pinyin': 'y\u014du y\u00fa', 'trans': 'superior to'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 193, "total_tokens": 769, "completion_tokens": 576}}
[30.01.2025 09:10] Response: [{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'},
{'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervise'},
{'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'},
{'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'},
{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},
{'word': 'æ‰¹è¯„', 'pinyin': 'pÄ« pÃ­ng', 'trans': 'criticize'},
{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},
{'word': 'å™ªéŸ³', 'pinyin': 'zÃ o yÄ«n', 'trans': 'noise'},
{'word': 'å“åº”', 'pinyin': 'xiÇng yÃ¬ng', 'trans': 'response'},
{'word': 'æ¨¡ä»¿', 'pinyin': 'mÃ³ fÇng', 'trans': 'imitate'},
{'word': 'å¯å‘', 'pinyin': 'qÇ fÄ', 'trans': 'inspire'},
{'word': 'æ‰¹åˆ¤æ€§', 'pinyin': 'pÄ« pÃ n xÃ¬ng', 'trans': 'critical'},
{'word': 'æ€ç»´', 'pinyin': 'sÄ« wÃ©i', 'trans': 'thinking'},
{'word': 'é¼“åŠ±', 'pinyin': 'gÇ” lÃ¬', 'trans': 'encourage'},
{'word': 'æ·±å…¥', 'pinyin': 'shÄ“n rÃ¹', 'trans': 'in-depth'},
{'word': 'ç»†è‡´', 'pinyin': 'xÃ¬ zhÃ¬', 'trans': 'detailed'},
{'word': 'éªŒè¯', 'pinyin': 'yÃ n zhÃ¨ng', 'trans': 'verify'},
{'word': 'æœ‰æ•ˆæ€§', 'pinyin': 'yÇ’u xiÃ o xÃ¬ng', 'trans': 'effectiveness'},
{'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'},
{'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}]
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« æŒ‘æˆ˜äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†æ‰¹è¯„å¾®è°ƒï¼ˆCFTï¼‰ç­–ç•¥ã€‚CFTè®©æ¨¡å‹å­¦ä¹ æ‰¹è¯„æœ‰å™ªéŸ³çš„å“åº”ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿æ­£ç¡®çš„å“åº”ã€‚å—å¼ºè°ƒæ‰¹åˆ¤æ€§æ€ç»´çš„äººç±»å­¦ä¹ è¿‡ç¨‹å¯å‘ï¼ŒCFTé¼“åŠ±æ›´æ·±å…¥çš„åˆ†æå’Œç»†è‡´çš„ç†è§£ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†CFTçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCFTåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†ä¸Šä¼˜äºSFTã€‚
[30.01.2025 09:10] Mistral response. {"id": "ff5568a13e804300ad9593e0f13c30b1", "object": "chat.completion", "created": 1738228252, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 159, "total_tokens": 265, "completion_tokens": 106}}
[30.01.2025 09:10] Response: This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks.
[30.01.2025 09:10] Renaming data file.
[30.01.2025 09:10] Renaming previous data. hf_papers.json to ./d/2025-01-30.json
[30.01.2025 09:10] Saving new data file.
[30.01.2025 09:10] Generating page.
[30.01.2025 09:10] Renaming previous page.
[30.01.2025 09:10] Renaming previous data. index.html to ./d/2025-01-30.html
[30.01.2025 09:10] [Experimental] Generating Chinese page for reading.
[30.01.2025 09:10] Chinese vocab [{'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'}, {'word': 'ç›‘ç£', 'pinyin': 'jiÃ n dÅ«', 'trans': 'supervise'}, {'word': 'å¾®è°ƒ', 'pinyin': 'wÄ“i tiÃ¡o', 'trans': 'fine-tune'}, {'word': 'èŒƒå¼', 'pinyin': 'fÃ n shÃ¬', 'trans': 'paradigm'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ‰¹è¯„', 'pinyin': 'pÄ« pÃ­ng', 'trans': 'criticize'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'å™ªéŸ³', 'pinyin': 'zÃ o yÄ«n', 'trans': 'noise'}, {'word': 'å“åº”', 'pinyin': 'xiÇng yÃ¬ng', 'trans': 'response'}, {'word': 'æ¨¡ä»¿', 'pinyin': 'mÃ³ fÇng', 'trans': 'imitate'}, {'word': 'å¯å‘', 'pinyin': 'qÇ fÄ', 'trans': 'inspire'}, {'word': 'æ‰¹åˆ¤æ€§', 'pinyin': 'pÄ« pÃ n xÃ¬ng', 'trans': 'critical'}, {'word': 'æ€ç»´', 'pinyin': 'sÄ« wÃ©i', 'trans': 'thinking'}, {'word': 'é¼“åŠ±', 'pinyin': 'gÇ” lÃ¬', 'trans': 'encourage'}, {'word': 'æ·±å…¥', 'pinyin': 'shÄ“n rÃ¹', 'trans': 'in-depth'}, {'word': 'ç»†è‡´', 'pinyin': 'xÃ¬ zhÃ¬', 'trans': 'detailed'}, {'word': 'éªŒè¯', 'pinyin': 'yÃ n zhÃ¨ng', 'trans': 'verify'}, {'word': 'æœ‰æ•ˆæ€§', 'pinyin': 'yÇ’u xiÃ o xÃ¬ng', 'trans': 'effectiveness'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}]
[30.01.2025 09:10] Renaming previous Chinese page.
[30.01.2025 09:10] Renaming previous data. zh.html to ./d/2025-01-29_zh_reading_task.html
[30.01.2025 09:10] Writing Chinese reading task.
[30.01.2025 09:10] Writing result.
[30.01.2025 09:10] Renaming log file.
[30.01.2025 09:10] Renaming previous data. log.txt to ./logs/2025-01-30_last_log.txt
