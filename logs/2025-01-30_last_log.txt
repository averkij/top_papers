[30.01.2025 08:13] Read previous papers.
[30.01.2025 08:13] Generating top page (month).
[30.01.2025 08:13] Writing top page (month).
[30.01.2025 09:10] Read previous papers.
[30.01.2025 09:10] Get feed.
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17703
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14334
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17749
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17433
[30.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.17195
[30.01.2025 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.01.2025 09:10] No deleted papers detected.
[30.01.2025 09:10] Downloading and parsing papers (pdf, html). Total: 5.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17703.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17703.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17703.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.14334.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.14334.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.14334.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17749.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17749.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17749.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17433.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17433.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17433.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.17195.
[30.01.2025 09:10] Extra JSON file exists (./assets/json/2501.17195.json), skip PDF parsing.
[30.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.17195.json), skip HTML parsing.
[30.01.2025 09:10] Success.
[30.01.2025 09:10] Enriching papers with extra data.
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 0. Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 1. The rapid growth of artificial intelligence (AI), particularly Large Language Models (LLMs), has raised concerns regarding its global environmental impact that extends beyond greenhouse gas emissions to include consideration of hardware fabrication and end-of-life processes. The opacity from major p...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 2. Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thor...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 3. Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By desi...
[30.01.2025 09:10] ********************************************************************************
[30.01.2025 09:10] Abstract 4. We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwis...
[30.01.2025 09:10] Read previous papers.
[30.01.2025 09:10] Generating reviews via LLM API.
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#dataset", "#math"], "emoji": "üß†", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫–∞ –≤–º–µ—Å—Ç–æ –∏–º–∏—Ç–∞—Ü–∏–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Critique Fine-Tuning (CFT), –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#agents", "#data", "#benchmark", "#ethics"], "emoji": "üå±", "ru": {"title": "–ó–µ–ª–µ–Ω—ã–π –ò–ò: –ø—É—Ç—å –∫ —É—Å—Ç–æ–π—á–∏–≤–æ–º—É –±—É–¥—É—â–µ–º—É —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é 
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#ethics", "#data", "#inference", "#training", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∫–ª—é—á –∫ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–º—É –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–ø—ã—Ç –≤–Ω–µ—à–Ω–µ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ OpenAI o3
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#ethics", "#security", "#training", "#data"], "emoji": "ü¶†", "ru": {"title": "–í–∏—Ä—É—Å–Ω–∞—è –∞—Ç–∞–∫–∞: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–π –¥–æ–æ–±—É—á–µ–Ω–∏—é, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ—Ä–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–æ–±–ª—é–¥–∞—Ç—å –ø
[30.01.2025 09:10] Using data from previous issue: {"categories": ["#small_models", "#open_source", "#agi", "#synthetic", "#data", "#training", "#optimization", "#dataset", "#rlhf"], "emoji": "‚öñÔ∏è", "ru": {"title": "Atla Selene Mini: –ú–∞–ª–∞—è –º–æ–¥–µ–ª—å-—Å—É–¥—å—è —Å –±–æ–ª—å—à–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Atla Selene Mini - –ø–µ—Ä–µ–¥–æ–≤—É—é –º–∞–ª—É—é —è–∑—ã–∫–æ–≤—É—é
[30.01.2025 09:10] Trying to get texts in Chinese.
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models.
[30.01.2025 09:10] Mistral response. {"id": "edbd96be8e3d4d118dac4fc021a129e1", "object": "chat.completion", "created": 1738228229, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u6311\u6218\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u6279\u8bc4\u5fae\u8c03\uff08CFT\uff09\u7b56\u7565\u3002CFT\u8ba9\u6a21\u578b\u5b66\u4e60\u6279\u8bc4\u6709\u566a\u97f3\u7684\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u6a21\u4eff\u6b63\u786e\u7684\u54cd\u5e94\u3002\u53d7\u5f3a\u8c03\u6279\u5224\u6027\u601d\u7ef4\u7684\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\u542f\u53d1\uff0cCFT\u9f13\u52b1\u66f4\u6df1\u5165\u7684\u5206\u6790\u548c\u7ec6\u81f4\u7684\u7406\u89e3\u3002\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CFT\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793aCFT\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u4f18\u4e8eSFT\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 359, "total_tokens": 505, "completion_tokens": 146}}
[30.01.2025 09:10] Response: ËøôÁØáÊñáÁ´†ÊåëÊàò‰∫ÜÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÁöÑËåÉÂºèÔºåÊèêÂá∫‰∫ÜÊâπËØÑÂæÆË∞ÉÔºàCFTÔºâÁ≠ñÁï•„ÄÇCFTËÆ©Ê®°ÂûãÂ≠¶‰π†ÊâπËØÑÊúâÂô™Èü≥ÁöÑÂìçÂ∫îÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞Ê®°‰ªøÊ≠£Á°ÆÁöÑÂìçÂ∫î„ÄÇÂèóÂº∫Ë∞ÉÊâπÂà§ÊÄßÊÄùÁª¥ÁöÑ‰∫∫Á±ªÂ≠¶‰π†ËøáÁ®ãÂêØÂèëÔºåCFTÈºìÂä±Êõ¥Ê∑±ÂÖ•ÁöÑÂàÜÊûêÂíåÁªÜËá¥ÁöÑÁêÜËß£„ÄÇ‰ΩúËÄÖÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜCFTÁöÑÊúâÊïàÊÄßÔºåÁªìÊûúÊòæÁ§∫CFTÂú®Â§ö‰∏™Êï∞Â≠¶Âü∫ÂáÜ‰∏ä‰ºò‰∫éSFT„ÄÇ
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†ÊåëÊàò‰∫ÜÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÁöÑËåÉÂºèÔºåÊèêÂá∫‰∫ÜÊâπËØÑÂæÆË∞ÉÔºàCFTÔºâÁ≠ñÁï•„ÄÇCFTËÆ©Ê®°ÂûãÂ≠¶‰π†ÊâπËØÑÊúâÂô™Èü≥ÁöÑÂìçÂ∫îÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞Ê®°‰ªøÊ≠£Á°ÆÁöÑÂìçÂ∫î„ÄÇÂèóÂº∫Ë∞ÉÊâπÂà§ÊÄßÊÄùÁª¥ÁöÑ‰∫∫Á±ªÂ≠¶‰π†ËøáÁ®ãÂêØÂèëÔºåCFTÈºìÂä±Êõ¥Ê∑±ÂÖ•ÁöÑÂàÜÊûêÂíåÁªÜËá¥ÁöÑÁêÜËß£„ÄÇ‰ΩúËÄÖÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜCFTÁöÑÊúâÊïàÊÄßÔºåÁªìÊûúÊòæÁ§∫CFTÂú®Â§ö‰∏™Êï∞Â≠¶Âü∫ÂáÜ‰∏ä‰ºò‰∫éSFT„ÄÇ
[30.01.2025 09:10] Mistral response. {"id": "661cb8d0494341fc8a300763d7572db2", "object": "chat.completion", "created": 1738228233, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u6311\u6218\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u6279\u8bc4\u5fae\u8c03\uff08CFT\uff09\u7b56\u7565\u3002CFT\u8ba9\u6a21\u578b\u5b66\u4e60\u6279\u8bc4\u6709\u566a\u97f3\u7684\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u6a21\u4eff\u6b63\u786e\u7684\u54cd\u5e94\u3002\u53d7\u5f3a\u8c03\u6279\u5224\u6027\u601d\u7ef4\u7684\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\u542f\u53d1\uff0cCFT\u9f13\u52b1\u66f4\u6df1\u5165\u7684\u5206\u6790\u548c\u7ec6\u81f4\u7684\u7406\u89e3\u3002\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CFT\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793aCFT\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u4f18\u4e8eSFT\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ti\u01ceozh\u00e0n le ji\u00e0nd\u016b w\u0113iti\u00e1o (SFT) de f\u00e0nsh\u00ec, t\u00edch\u016b le p\u012bp\u00edng w\u0113iti\u00e1o (CFT) c\u00e8l\u00fc\u00e8. CFT r\u00e0ng m\u00f3x\u00edng xu\u00e9x\u00ed p\u012bp\u00edng y\u01d2u z\u00e0oy\u012bn de xi\u01cengy\u00ecng, \u00e9r b\u00f9sh\u00ec ji\u01cend\u0101n de m\u00f3f\u01ceng zh\u00e8ngqu\u00e8 de xi\u01cengy\u00ecng. Sh\u00f2u qi\u00e1ngdi\u00e0o p\u012bp\u00e0n x\u00ecng s\u012bw\u00e9i de r\u00e9nl\u00e8i xu\u00e9x\u00ed gu\u00f2ch\u00e9ng q\u01d0f\u0101, CFT g\u01d4l\u00ec g\u00e8ng sh\u0113nr\u00f9 de f\u0113nxi h\u00e9 x\u00eczh\u00ec de l\u01d0ji\u011b. Zu\u00f2zh\u011b t\u014dnggu\u00f2 sh\u00ecy\u00e0n y\u00e0nzh\u00e8ng le CFT de y\u01d2uxi\u00e0ox\u00ecng, ji\u00e9gu\u01d2 xi\u01censh\u00ec CFT z\u00e0i du\u014dg\u00e8 sh\u00f9xu\u00e9 j\u012bzh\u01d4n sh\u00e0ng y\u014duy\u00fa SFT."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 161, "total_tokens": 586, "completion_tokens": 425}}
[30.01.2025 09:10] Response: ËøôÁØáÊñáÁ´†ÊåëÊàò‰∫ÜÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÁöÑËåÉÂºèÔºåÊèêÂá∫‰∫ÜÊâπËØÑÂæÆË∞ÉÔºàCFTÔºâÁ≠ñÁï•„ÄÇCFTËÆ©Ê®°ÂûãÂ≠¶‰π†ÊâπËØÑÊúâÂô™Èü≥ÁöÑÂìçÂ∫îÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞Ê®°‰ªøÊ≠£Á°ÆÁöÑÂìçÂ∫î„ÄÇÂèóÂº∫Ë∞ÉÊâπÂà§ÊÄßÊÄùÁª¥ÁöÑ‰∫∫Á±ªÂ≠¶‰π†ËøáÁ®ãÂêØÂèëÔºåCFTÈºìÂä±Êõ¥Ê∑±ÂÖ•ÁöÑÂàÜÊûêÂíåÁªÜËá¥ÁöÑÁêÜËß£„ÄÇ‰ΩúËÄÖÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜCFTÁöÑÊúâÊïàÊÄßÔºåÁªìÊûúÊòæÁ§∫CFTÂú®Â§ö‰∏™Êï∞Â≠¶Âü∫ÂáÜ‰∏ä‰ºò‰∫éSFT„ÄÇ

Zh√® piƒÅn w√©nzhƒÅng ti«éozh√†n le ji√†nd≈´ wƒìiti√°o (SFT) de f√†nsh√¨, t√≠ch≈´ le pƒ´p√≠ng wƒìiti√°o (CFT) c√®l√º√®. CFT r√†ng m√≥x√≠ng xu√©x√≠ pƒ´p√≠ng y«íu z√†oyƒ´n de xi«éngy√¨ng, √©r b√πsh√¨ ji«éndƒÅn de m√≥f«éng zh√®ngqu√® de xi«éngy√¨ng. Sh√≤u qi√°ngdi√†o pƒ´p√†n x√¨ng sƒ´w√©i de r√©nl√®i xu√©x√≠ gu√≤ch√©ng q«êfƒÅ, CFT g«îl√¨ g√®ng shƒìnr√π de fƒìnxi h√© x√¨zh√¨ de l«êjiƒõ. Zu√≤zhƒõ t≈çnggu√≤ sh√¨y√†n y√†nzh√®ng le CFT de y«íuxi√†ox√¨ng, ji√©gu«í xi«énsh√¨ CFT z√†i du≈çg√® sh√πxu√© jƒ´zh«în sh√†ng y≈çuy√∫ SFT.
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†ÊåëÊàò‰∫ÜÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÁöÑËåÉÂºèÔºåÊèêÂá∫‰∫ÜÊâπËØÑÂæÆË∞ÉÔºàCFTÔºâÁ≠ñÁï•„ÄÇCFTËÆ©Ê®°ÂûãÂ≠¶‰π†ÊâπËØÑÊúâÂô™Èü≥ÁöÑÂìçÂ∫îÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞Ê®°‰ªøÊ≠£Á°ÆÁöÑÂìçÂ∫î„ÄÇÂèóÂº∫Ë∞ÉÊâπÂà§ÊÄßÊÄùÁª¥ÁöÑ‰∫∫Á±ªÂ≠¶‰π†ËøáÁ®ãÂêØÂèëÔºåCFTÈºìÂä±Êõ¥Ê∑±ÂÖ•ÁöÑÂàÜÊûêÂíåÁªÜËá¥ÁöÑÁêÜËß£„ÄÇ‰ΩúËÄÖÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜCFTÁöÑÊúâÊïàÊÄßÔºåÁªìÊûúÊòæÁ§∫CFTÂú®Â§ö‰∏™Êï∞Â≠¶Âü∫ÂáÜ‰∏ä‰ºò‰∫éSFT„ÄÇ
[30.01.2025 09:10] Mistral response. {"id": "9c9723ab195c4e3da21a834563b18f6e", "object": "chat.completion", "created": 1738228241, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceo zh\u00e0n', 'trans': 'challenge'},\n{'word': '\u76d1\u7763', 'pinyin': 'ji\u00e0n d\u016b', 'trans': 'supervise'},\n{'word': '\u5fae\u8c03', 'pinyin': 'w\u0113i ti\u00e1o', 'trans': 'fine-tune'},\n{'word': '\u8303\u5f0f', 'pinyin': 'f\u00e0n sh\u00ec', 'trans': 'paradigm'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u6279\u8bc4', 'pinyin': 'p\u012b p\u00edng', 'trans': 'criticize'},\n{'word': '\u7b56\u7565', 'pinyin': 'c\u00e8 l\u00fc\u00e8', 'trans': 'strategy'},\n{'word': '\u566a\u97f3', 'pinyin': 'z\u00e0o y\u012bn', 'trans': 'noise'},\n{'word': '\u54cd\u5e94', 'pinyin': 'xi\u01ceng y\u00ecng', 'trans': 'response'},\n{'word': '\u6a21\u4eff', 'pinyin': 'm\u00f3 f\u01ceng', 'trans': 'imitate'},\n{'word': '\u542f\u53d1', 'pinyin': 'q\u01d0 f\u0101', 'trans': 'inspire'},\n{'word': '\u6279\u5224\u6027', 'pinyin': 'p\u012b p\u00e0n x\u00ecng', 'trans': 'critical'},\n{'word': '\u601d\u7ef4', 'pinyin': 's\u012b w\u00e9i', 'trans': 'thinking'},\n{'word': '\u9f13\u52b1', 'pinyin': 'g\u01d4 l\u00ec', 'trans': 'encourage'},\n{'word': '\u6df1\u5165', 'pinyin': 'sh\u0113n r\u00f9', 'trans': 'in-depth'},\n{'word': '\u7ec6\u81f4', 'pinyin': 'x\u00ec zh\u00ec', 'trans': 'detailed'},\n{'word': '\u9a8c\u8bc1', 'pinyin': 'y\u00e0n zh\u00e8ng', 'trans': 'verify'},\n{'word': '\u6709\u6548\u6027', 'pinyin': 'y\u01d2u xi\u00e0o x\u00ecng', 'trans': 'effectiveness'},\n{'word': '\u57fa\u51c6', 'pinyin': 'j\u012b zh\u01d4n', 'trans': 'benchmark'},\n{'word': '\u4f18\u4e8e', 'pinyin': 'y\u014du y\u00fa', 'trans': 'superior to'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 193, "total_tokens": 769, "completion_tokens": 576}}
[30.01.2025 09:10] Response: [{'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'},
{'word': 'ÁõëÁù£', 'pinyin': 'ji√†n d≈´', 'trans': 'supervise'},
{'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'},
{'word': 'ËåÉÂºè', 'pinyin': 'f√†n sh√¨', 'trans': 'paradigm'},
{'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'},
{'word': 'ÊâπËØÑ', 'pinyin': 'pƒ´ p√≠ng', 'trans': 'criticize'},
{'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'},
{'word': 'Âô™Èü≥', 'pinyin': 'z√†o yƒ´n', 'trans': 'noise'},
{'word': 'ÂìçÂ∫î', 'pinyin': 'xi«éng y√¨ng', 'trans': 'response'},
{'word': 'Ê®°‰ªø', 'pinyin': 'm√≥ f«éng', 'trans': 'imitate'},
{'word': 'ÂêØÂèë', 'pinyin': 'q«ê fƒÅ', 'trans': 'inspire'},
{'word': 'ÊâπÂà§ÊÄß', 'pinyin': 'pƒ´ p√†n x√¨ng', 'trans': 'critical'},
{'word': 'ÊÄùÁª¥', 'pinyin': 'sƒ´ w√©i', 'trans': 'thinking'},
{'word': 'ÈºìÂä±', 'pinyin': 'g«î l√¨', 'trans': 'encourage'},
{'word': 'Ê∑±ÂÖ•', 'pinyin': 'shƒìn r√π', 'trans': 'in-depth'},
{'word': 'ÁªÜËá¥', 'pinyin': 'x√¨ zh√¨', 'trans': 'detailed'},
{'word': 'È™åËØÅ', 'pinyin': 'y√†n zh√®ng', 'trans': 'verify'},
{'word': 'ÊúâÊïàÊÄß', 'pinyin': 'y«íu xi√†o x√¨ng', 'trans': 'effectiveness'},
{'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'},
{'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'}]
[30.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†ÊåëÊàò‰∫ÜÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÁöÑËåÉÂºèÔºåÊèêÂá∫‰∫ÜÊâπËØÑÂæÆË∞ÉÔºàCFTÔºâÁ≠ñÁï•„ÄÇCFTËÆ©Ê®°ÂûãÂ≠¶‰π†ÊâπËØÑÊúâÂô™Èü≥ÁöÑÂìçÂ∫îÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞Ê®°‰ªøÊ≠£Á°ÆÁöÑÂìçÂ∫î„ÄÇÂèóÂº∫Ë∞ÉÊâπÂà§ÊÄßÊÄùÁª¥ÁöÑ‰∫∫Á±ªÂ≠¶‰π†ËøáÁ®ãÂêØÂèëÔºåCFTÈºìÂä±Êõ¥Ê∑±ÂÖ•ÁöÑÂàÜÊûêÂíåÁªÜËá¥ÁöÑÁêÜËß£„ÄÇ‰ΩúËÄÖÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜCFTÁöÑÊúâÊïàÊÄßÔºåÁªìÊûúÊòæÁ§∫CFTÂú®Â§ö‰∏™Êï∞Â≠¶Âü∫ÂáÜ‰∏ä‰ºò‰∫éSFT„ÄÇ
[30.01.2025 09:10] Mistral response. {"id": "ff5568a13e804300ad9593e0f13c30b1", "object": "chat.completion", "created": 1738228252, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 159, "total_tokens": 265, "completion_tokens": 106}}
[30.01.2025 09:10] Response: This article challenges the paradigm of Supervised Fine-Tuning (SFT) by proposing the Critical Fine-Tuning (CFT) strategy. CFT allows the model to learn to critique noisy responses rather than simply mimicking correct responses. Inspired by the human learning process that emphasizes critical thinking, CFT encourages deeper analysis and detailed understanding. The authors validated the effectiveness of CFT through experiments, with results showing that CFT outperforms SFT on multiple mathematical benchmarks.
[30.01.2025 09:10] Renaming data file.
[30.01.2025 09:10] Renaming previous data. hf_papers.json to ./d/2025-01-30.json
[30.01.2025 09:10] Saving new data file.
[30.01.2025 09:10] Generating page.
[30.01.2025 09:10] Renaming previous page.
[30.01.2025 09:10] Renaming previous data. index.html to ./d/2025-01-30.html
[30.01.2025 09:10] [Experimental] Generating Chinese page for reading.
[30.01.2025 09:10] Chinese vocab [{'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}, {'word': 'ÁõëÁù£', 'pinyin': 'ji√†n d≈´', 'trans': 'supervise'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'}, {'word': 'ËåÉÂºè', 'pinyin': 'f√†n sh√¨', 'trans': 'paradigm'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ÊâπËØÑ', 'pinyin': 'pƒ´ p√≠ng', 'trans': 'criticize'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Âô™Èü≥', 'pinyin': 'z√†o yƒ´n', 'trans': 'noise'}, {'word': 'ÂìçÂ∫î', 'pinyin': 'xi«éng y√¨ng', 'trans': 'response'}, {'word': 'Ê®°‰ªø', 'pinyin': 'm√≥ f«éng', 'trans': 'imitate'}, {'word': 'ÂêØÂèë', 'pinyin': 'q«ê fƒÅ', 'trans': 'inspire'}, {'word': 'ÊâπÂà§ÊÄß', 'pinyin': 'pƒ´ p√†n x√¨ng', 'trans': 'critical'}, {'word': 'ÊÄùÁª¥', 'pinyin': 'sƒ´ w√©i', 'trans': 'thinking'}, {'word': 'ÈºìÂä±', 'pinyin': 'g«î l√¨', 'trans': 'encourage'}, {'word': 'Ê∑±ÂÖ•', 'pinyin': 'shƒìn r√π', 'trans': 'in-depth'}, {'word': 'ÁªÜËá¥', 'pinyin': 'x√¨ zh√¨', 'trans': 'detailed'}, {'word': 'È™åËØÅ', 'pinyin': 'y√†n zh√®ng', 'trans': 'verify'}, {'word': 'ÊúâÊïàÊÄß', 'pinyin': 'y«íu xi√†o x√¨ng', 'trans': 'effectiveness'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'}]
[30.01.2025 09:10] Renaming previous Chinese page.
[30.01.2025 09:10] Renaming previous data. zh.html to ./d/2025-01-29_zh_reading_task.html
[30.01.2025 09:10] Writing Chinese reading task.
[30.01.2025 09:10] Writing result.
[30.01.2025 09:10] Renaming log file.
[30.01.2025 09:10] Renaming previous data. log.txt to ./logs/2025-01-30_last_log.txt
