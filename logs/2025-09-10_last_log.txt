[10.09.2025 10:12] Read previous papers.
[10.09.2025 10:12] Generating top page (month).
[10.09.2025 10:12] Writing top page (month).
[10.09.2025 11:09] Read previous papers.
[10.09.2025 11:09] Get feed.
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07980
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07969
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07979
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07295
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06818
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06830
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06951
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06923
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07414
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07301
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07968
[10.09.2025 11:09] Extract page data from URL. URL: https://huggingface.co/papers/2509.06942
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01624
[10.09.2025 11:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07558
[10.09.2025 11:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.09.2025 11:09] No deleted papers detected.
[10.09.2025 11:09] Downloading and parsing papers (pdf, html). Total: 14.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.07980.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.07980.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.07980.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.07969.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.07969.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.07969.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.07979.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.07979.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.07979.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.07295.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.07295.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.07295.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.06818.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.06818.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.06818.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.06830.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.06830.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.06830.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.06951.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.06951.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.06951.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.06923.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.06923.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.06923.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.07414.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.07414.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.07414.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.07301.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.07301.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.07301.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.07968.
[10.09.2025 11:09] Extra JSON file exists (./assets/json/2509.07968.json), skip PDF parsing.
[10.09.2025 11:09] Paper image links file exists (./assets/img_data/2509.07968.json), skip HTML parsing.
[10.09.2025 11:09] Success.
[10.09.2025 11:09] Downloading and parsing paper https://huggingface.co/papers/2509.06942.
[10.09.2025 11:09] Downloading paper 2509.06942 from http://arxiv.org/pdf/2509.06942v2...
[10.09.2025 11:10] Extracting affiliations from text.
[10.09.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference Xiangwei Shen1,2,, Zhimin Li1,, Zhantao Yang1, Shiyi Zhang3, Yingfang Zhang1, Donghao Li1, Chunyu Wang1, Qinglin Lu1, Yansong Tang3, 1Hunyuan, Tencent 2School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3Shenzhen International Graduate School, Tsinghua University 5 2 0 2 ] . [ 2 2 4 9 6 0 . 9 0 5 2 : r Figure 1. Images generated by FLUX.1-dev finetuned through our Semantic Relative Preference Optimization (SRPO) Our method substantially improves upon the baseline model, achieving superior photorealism and enhanced fine-grained detail while maintaining remarkable training efficiency-converging in just 10 minutes using 32 NVIDIA H20 GPUs. "
[10.09.2025 11:10] Response: ```python
[
    "Hunyuan, Tencent",
    "School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen",
    "Shenzhen International Graduate School, Tsinghua University"
]
```
[10.09.2025 11:10] Deleting PDF ./assets/pdf/2509.06942.pdf.
[10.09.2025 11:10] Success.
[10.09.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2509.01624.
[10.09.2025 11:10] Extra JSON file exists (./assets/json/2509.01624.json), skip PDF parsing.
[10.09.2025 11:10] Paper image links file exists (./assets/img_data/2509.01624.json), skip HTML parsing.
[10.09.2025 11:10] Success.
[10.09.2025 11:10] Downloading and parsing paper https://huggingface.co/papers/2509.07558.
[10.09.2025 11:10] Extra JSON file exists (./assets/json/2509.07558.json), skip PDF parsing.
[10.09.2025 11:10] Paper image links file exists (./assets/img_data/2509.07558.json), skip HTML parsing.
[10.09.2025 11:10] Success.
[10.09.2025 11:10] Enriching papers with extra data.
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 0. Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.  					AI-generated summary 				 Parallel thinking has emerged as...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 1. Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.  					AI-generated summary 				 Recent advances in large multimodal models have lev...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 2. VIRAL, a regularization strategy, aligns MLLMs' visual representations with pre-trained VFMs, enhancing performance on vision-centric tasks.  					AI-generated summary 				 Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 3. Reconstruction Alignment (RecA) is a post-training method that enhances multimodal models by using visual embeddings as dense prompts, improving image generation and editing fidelity.  					AI-generated summary 				 Unified multimodal models (UMMs) unify visual understanding and generation within a ...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 4. UMO, a Unified Multi-identity Optimization framework, enhances identity consistency and reduces confusion in multi-reference image customization using reinforcement learning on diffusion models.  					AI-generated summary 				 Recent advancements in image customization exhibit a wide range of applic...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 5. Curia, a foundation model trained on extensive cross-sectional imaging data, demonstrates superior performance across multiple radiological tasks and shows emergent properties in cross-modality and low-data settings.  					AI-generated summary 				 AI-assisted radiological interpretation is based on...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 6. F1, a pretrained VLA framework with foresight generation, improves task success and generalization in dynamic environments through a Mixture-of-Transformer architecture and next-scale prediction.  					AI-generated summary 				 Executing language-conditioned tasks in dynamic visual environments rema...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 7. SEELE, a novel RLVR framework, dynamically adjusts problem difficulty using adaptive hint lengths to enhance exploration efficiency and improve performance in math reasoning tasks.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success i...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 8. Language Self-Play (LSP) enhances large language models' performance on instruction-following tasks through self-play, surpassing data-driven methods.  					AI-generated summary 				 Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training d...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 9. CASTLE, an attention mechanism that updates keys with future context while maintaining autoregressive properties, outperforms standard causal attention in language modeling.  					AI-generated summary 				 In standard causal attention, each token's query, key, and value (QKV) are static and encode o...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 10. SimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.  					AI-generated summary 				 We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Langu...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 11. Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.  					AI-generated summary 				 Recent studies have demonstrated the effectiveness of directly aligning diff...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 12. Q-Sched, a novel post-training quantization method for diffusion models, reduces model size by 4x while maintaining full-precision accuracy and improving image quality metrics.  					AI-generated summary 				 Text-to-image diffusion models are computationally intensive, often requiring dozens of for...
[10.09.2025 11:10] ********************************************************************************
[10.09.2025 11:10] Abstract 13. ŒîL Normalization addresses gradient variance in Reinforcement Learning with Verifiable Rewards by providing an unbiased policy loss estimate with minimal variance.  					AI-generated summary 				 We propose Delta L Normalization, a simple yet effective loss aggregation method tailored to the charact...
[10.09.2025 11:10] Read previous papers.
[10.09.2025 11:10] Generating reviews via LLM API.
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#math", "#open_source", "#rl", "#optimization", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –ò–ò: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "Parallel-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#data", "#open_source", "#cv", "#rl", "#training", "#multimodal", "#dataset", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: Mini-o3 —Ä–∞–∑–¥–≤–∏–≥–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°–∏—Å—Ç–µ–º–∞ Mini-o3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–ª—É–±–æ–∫–æ–º—É –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–º
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#alignment", "#training", "#multimodal", "#reasoning"], "emoji": "üîç", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VIRAL - —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#open_source", "#multimodal", "#optimization", "#diffusion", "#training"], "emoji": "üîÑ", "ru": {"title": "RecA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Reconstruction Alignment (RecA) - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#rl", "#open_source", "#optimization", "#diffusion", "#training"], "emoji": "üé≠", "ru": {"title": "UMO: –£–ª—É—á—à–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "UMO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø—É—Ç–∞–Ω–∏—Ü—ã –≤ –∫–∞—Å—Ç
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#healthcare", "#benchmark", "#data", "#low_resource", "#open_source"], "emoji": "üß†", "ru": {"title": "Curia: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏", "desc": "–ú–æ–¥–µ–ª—å Curia - —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –æ–±—à–∏—Ä–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#agi", "#cv", "#reasoning", "#benchmark", "#agents", "#transfer_learning", "#training"], "emoji": "üîÆ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–≤–∏–¥–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –ò–ò", "desc": "F1 - —ç—Ç–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#math", "#rl", "#optimization", "#training", "#rlhf", "#reasoning"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ò–ò –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é", "desc": "SEELE - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR), –∫
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#games", "#rl", "#optimization", "#training", "#rlhf"], "emoji": "üéÆ", "ru": {"title": "–°–∞–º–æ–∏–≥—Ä–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é –±–µ–∑ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Language Self-Play (LSP). LSP –∏—Å–ø–æ–ª—å–∑
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#architecture", "#optimization", "#training"], "emoji": "üè∞", "ru": {"title": "CASTLE: –í–∑–≥–ª—è–¥ –≤ –±—É–¥—É—â–µ–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CASTLE –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –í –æ—Ç–ª–∏
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#interpretability", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SimpleQA Verified - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[10.09.2025 11:10] Querying the API.
[10.09.2025 11:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.  					AI-generated summary 				 Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.
[10.09.2025 11:10] Response: {
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–≤–∞ –º–µ—Ç–æ–¥–∞ —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: Direct-Align –∏ Semantic Relative Preference Optimization (SRPO). Direct-Align –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —à—É–º–æ–≤–æ–π –ø—Ä–∏–æ—Ä –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏–∑–±–µ–≥–∞—è —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –ø–æ–∑–¥–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö. SRPO —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ-–æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã, –ø–æ–∑–≤–æ–ª—è—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –æ–Ω–ª–∞–π–Ω –≤ –æ—Ç–≤–µ—Ç –Ω–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã —Å–Ω–∏–∂–∞—é—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ—Ñ–ª–∞–π–Ω-–∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –º–æ–¥–µ–ª–∏ FLUX.1.dev —É–ª—É—á—à–∏–ª–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å –∏ —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–æ–ª–µ–µ —á–µ–º –≤ 3 —Ä–∞–∑–∞ –ø–æ –æ—Ü–µ–Ω–∫–∞–º –ª—é–¥–µ–π.",
  "emoji": "üñºÔ∏è",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π"
}
[10.09.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.  					AI-generated summary 				 Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x."

[10.09.2025 11:10] Response: ```python
['RLHF', 'TRAINING']
```
[10.09.2025 11:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.  					AI-generated summary 				 Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x."

[10.09.2025 11:10] Response: ```python
["ALIGNMENT", "OPTIMIZATION", "DIFFUSION"]
```
[10.09.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents two innovative methods, Direct-Align and Semantic Relative Preference Optimization (SRPO), to enhance diffusion models\' alignment with human preferences. Direct-Align simplifies the process of recovering original images by using a predefined noise prior, which reduces the need for costly multistep denoising. SRPO allows for real-time adjustments of rewards based on text prompts, minimizing the need for extensive offline reward model adaptations. Together, these methods significantly improve the aesthetic quality and realism of generated images while lowering computational costs.","title":"Enhancing Diffusion Models with Direct-Align and SRPO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents two innovative methods, Direct-Align and Semantic Relative Preference Optimization (SRPO), to enhance diffusion models' alignment with human preferences. Direct-Align simplifies the process of recovering original images by using a predefined noise prior, which reduces the need for costly multistep denoising. SRPO allows for real-time adjustments of rewards based on text prompts, minimizing the need for extensive offline reward model adaptations. Together, these methods significantly improve the aesthetic quality and realism of generated images while lowering computational costs.", title='Enhancing Diffusion Models with Direct-Align and SRPO'))
[10.09.2025 11:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫ÜDirect-AlignÂíåËØ≠‰πâÁõ∏ÂØπÂÅèÂ•Ω‰ºòÂåñÔºàSRPOÔºâ‰∏§ÁßçÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÊâ©Êï£Ê®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩêÂ∫¶ÔºåÂêåÊó∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇDirect-AlignÈÄöËøáÈ¢ÑÂÆö‰πâÂô™Â£∞Êù•ÊúâÊïàÊÅ¢Â§çÂéüÂßãÂõæÂÉèÔºåÈÅøÂÖç‰∫ÜÂú®ÂêéÊúüÊó∂Èó¥Ê≠•ÁöÑËøáÂ∫¶‰ºòÂåñ„ÄÇSRPOÂàôÂ∞ÜÂ•ñÂä±‰ø°Âè∑‰∏éÊñáÊú¨Êù°‰ª∂Áõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫ÜÂú®Á∫øË∞ÉÊï¥Â•ñÂä±Ôºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÂØπÁ¶ªÁ∫øÂ•ñÂä±ÂæÆË∞ÉÁöÑ‰æùËµñ„ÄÇÈÄöËøá‰ºòÂåñÂéªÂô™ÂíåÂú®Á∫øÂ•ñÂä±Ë∞ÉÊï¥ÔºåÊàë‰ª¨ÊòæËëóÊèêÈ´ò‰∫ÜFLUX.1.devÊ®°ÂûãÂú®ÁúüÂÆûÊÑüÂíåÁæéÂ≠¶Ë¥®Èáè‰∏äÁöÑË°®Áé∞„ÄÇ","title":"ÊèêÂçáÊâ©Êï£Ê®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫ÜDirect-AlignÂíåËØ≠‰πâÁõ∏ÂØπÂÅèÂ•Ω‰ºòÂåñÔºàSRPOÔºâ‰∏§ÁßçÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÊâ©Êï£Ê®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩêÂ∫¶ÔºåÂêåÊó∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇDirect-AlignÈÄöËøáÈ¢ÑÂÆö‰πâÂô™Â£∞Êù•ÊúâÊïàÊÅ¢Â§çÂéüÂßãÂõæÂÉèÔºåÈÅøÂÖç‰∫ÜÂú®ÂêéÊúüÊó∂Èó¥Ê≠•ÁöÑËøáÂ∫¶‰ºòÂåñ„ÄÇSRPOÂàôÂ∞ÜÂ•ñÂä±‰ø°Âè∑‰∏éÊñáÊú¨Êù°‰ª∂Áõ∏ÁªìÂêàÔºåÂÆûÁé∞‰∫ÜÂú®Á∫øË∞ÉÊï¥Â•ñÂä±Ôºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÂØπÁ¶ªÁ∫øÂ•ñÂä±ÂæÆË∞ÉÁöÑ‰æùËµñ„ÄÇÈÄöËøá‰ºòÂåñÂéªÂô™ÂíåÂú®Á∫øÂ•ñÂä±Ë∞ÉÊï¥ÔºåÊàë‰ª¨ÊòæËëóÊèêÈ´ò‰∫ÜFLUX.1.devÊ®°ÂûãÂú®ÁúüÂÆûÊÑüÂíåÁæéÂ≠¶Ë¥®Èáè‰∏äÁöÑË°®Áé∞„ÄÇ', title='ÊèêÂçáÊâ©Êï£Ê®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩê'))
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#training", "#diffusion", "#inference", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "Q-Sched: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "Q-Sched - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª
[10.09.2025 11:10] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#reasoning"], "emoji": "üìä", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é Delta L Normalization", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Delta L Normalization –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR). –≠—Ç–æ—Ç –ø–æ–¥—Ö
[10.09.2025 11:10] Renaming data file.
[10.09.2025 11:10] Renaming previous data. hf_papers.json to ./d/2025-09-10.json
[10.09.2025 11:10] Saving new data file.
[10.09.2025 11:10] Generating page.
[10.09.2025 11:10] Renaming previous page.
[10.09.2025 11:10] Renaming previous data. index.html to ./d/2025-09-10.html
[10.09.2025 11:10] Writing result.
[10.09.2025 11:10] Renaming log file.
[10.09.2025 11:10] Renaming previous data. log.txt to ./logs/2025-09-10_last_log.txt
