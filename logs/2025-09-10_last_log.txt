[10.09.2025 05:12] Read previous papers.
[10.09.2025 05:12] Generating top page (month).
[10.09.2025 05:12] Writing top page (month).
[10.09.2025 06:17] Read previous papers.
[10.09.2025 06:17] Get feed.
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07980
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07969
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07979
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07295
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06923
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06951
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06818
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07414
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07301
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07968
[10.09.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01624
[10.09.2025 06:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.09.2025 06:17] No deleted papers detected.
[10.09.2025 06:17] Downloading and parsing papers (pdf, html). Total: 11.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.07980.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.07980.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.07980.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.07969.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.07969.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.07969.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.07979.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.07979.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.07979.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.07295.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.07295.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.07295.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.06923.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.06923.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.06923.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.06951.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.06951.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.06951.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.06818.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.06818.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.06818.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.07414.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.07414.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.07414.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.07301.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.07301.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.07301.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.07968.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.07968.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.07968.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2509.01624.
[10.09.2025 06:17] Extra JSON file exists (./assets/json/2509.01624.json), skip PDF parsing.
[10.09.2025 06:17] Paper image links file exists (./assets/img_data/2509.01624.json), skip HTML parsing.
[10.09.2025 06:17] Success.
[10.09.2025 06:17] Enriching papers with extra data.
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 0. Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.  					AI-generated summary 				 Parallel thinking has emerged as...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 1. Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.  					AI-generated summary 				 Recent advances in large multimodal models have lev...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 2. VIRAL, a regularization strategy, aligns MLLMs' visual representations with pre-trained VFMs, enhancing performance on vision-centric tasks.  					AI-generated summary 				 Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 3. Reconstruction Alignment (RecA) is a post-training method that enhances multimodal models by using visual embeddings as dense prompts, improving image generation and editing fidelity.  					AI-generated summary 				 Unified multimodal models (UMMs) unify visual understanding and generation within a ...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 4. SEELE, a novel RLVR framework, dynamically adjusts problem difficulty using adaptive hint lengths to enhance exploration efficiency and improve performance in math reasoning tasks.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success i...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 5. F1, a pretrained VLA framework with foresight generation, improves task success and generalization in dynamic environments through a Mixture-of-Transformer architecture and next-scale prediction.  					AI-generated summary 				 Executing language-conditioned tasks in dynamic visual environments rema...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 6. UMO, a Unified Multi-identity Optimization framework, enhances identity consistency and reduces confusion in multi-reference image customization using reinforcement learning on diffusion models.  					AI-generated summary 				 Recent advancements in image customization exhibit a wide range of applic...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 7. Language Self-Play (LSP) enhances large language models' performance on instruction-following tasks through self-play, surpassing data-driven methods.  					AI-generated summary 				 Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training d...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 8. CASTLE, an attention mechanism that updates keys with future context while maintaining autoregressive properties, outperforms standard causal attention in language modeling.  					AI-generated summary 				 In standard causal attention, each token's query, key, and value (QKV) are static and encode o...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 9. SimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.  					AI-generated summary 				 We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Langu...
[10.09.2025 06:17] ********************************************************************************
[10.09.2025 06:17] Abstract 10. Q-Sched, a novel post-training quantization method for diffusion models, reduces model size by 4x while maintaining full-precision accuracy and improving image quality metrics.  					AI-generated summary 				 Text-to-image diffusion models are computationally intensive, often requiring dozens of for...
[10.09.2025 06:17] Read previous papers.
[10.09.2025 06:17] Generating reviews via LLM API.
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#math", "#open_source", "#rl", "#optimization", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –ò–ò: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "Parallel-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#data", "#open_source", "#cv", "#rl", "#training", "#multimodal", "#dataset", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –ø–æ–∏—Å–∫–µ: Mini-o3 —Ä–∞–∑–¥–≤–∏–≥–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°–∏—Å—Ç–µ–º–∞ Mini-o3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–ª—É–±–æ–∫–æ–º—É –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–º
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#alignment", "#training", "#multimodal", "#reasoning"], "emoji": "üîç", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VIRAL - —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#open_source", "#multimodal", "#optimization", "#diffusion", "#training"], "emoji": "üîÑ", "ru": {"title": "RecA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Reconstruction Alignment (RecA) - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#math", "#rl", "#optimization", "#training", "#rlhf", "#reasoning"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ò–ò –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é", "desc": "SEELE - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR), –∫
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#agi", "#cv", "#reasoning", "#benchmark", "#agents", "#transfer_learning", "#training"], "emoji": "üîÆ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–≤–∏–¥–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –ò–ò", "desc": "F1 - —ç—Ç–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#dataset", "#rl", "#open_source", "#optimization", "#diffusion", "#training"], "emoji": "üé≠", "ru": {"title": "UMO: –£–ª—É—á—à–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "UMO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø—É—Ç–∞–Ω–∏—Ü—ã –≤ –∫–∞—Å—Ç
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#games", "#rl", "#optimization", "#training", "#rlhf"], "emoji": "üéÆ", "ru": {"title": "–°–∞–º–æ–∏–≥—Ä–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é –±–µ–∑ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Language Self-Play (LSP). LSP –∏—Å–ø–æ–ª—å–∑
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#architecture", "#optimization", "#training"], "emoji": "üè∞", "ru": {"title": "CASTLE: –í–∑–≥–ª—è–¥ –≤ –±—É–¥—É—â–µ–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CASTLE –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –í –æ—Ç–ª–∏
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#dataset", "#hallucinations", "#interpretability", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SimpleQA Verified - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[10.09.2025 06:17] Using data from previous issue: {"categories": ["#training", "#diffusion", "#inference", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "Q-Sched: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "Q-Sched - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª
[10.09.2025 06:17] Renaming data file.
[10.09.2025 06:17] Renaming previous data. hf_papers.json to ./d/2025-09-10.json
[10.09.2025 06:17] Saving new data file.
[10.09.2025 06:17] Generating page.
[10.09.2025 06:17] Renaming previous page.
[10.09.2025 06:17] Renaming previous data. index.html to ./d/2025-09-10.html
[10.09.2025 06:17] Writing result.
[10.09.2025 06:17] Renaming log file.
[10.09.2025 06:17] Renaming previous data. log.txt to ./logs/2025-09-10_last_log.txt
