[12.02.2026 06:02] Read previous papers.
[12.02.2026 06:02] Generating top page (month).
[12.02.2026 06:02] Writing top page (month).
[12.02.2026 07:05] Read previous papers.
[12.02.2026 07:05] Get feed.
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10604
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11124
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10177
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10560
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11089
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08253
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10975
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10224
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10999
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10609
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09713
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08711
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09901
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07106
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04935
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11144
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10179
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08489
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08030
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06008
[12.02.2026 07:05] Extract page data from URL. URL: https://huggingface.co/papers/2602.11008
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10652
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09014
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10699
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08995
[12.02.2026 07:05] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11137
[12.02.2026 07:05] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.02.2026 07:05] No deleted papers detected.
[12.02.2026 07:05] Downloading and parsing papers (pdf, html). Total: 26.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10604.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10604.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10604.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.11124.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.11124.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.11124.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10177.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10177.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10177.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10560.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10560.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10560.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.11089.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.11089.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.11089.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.08253.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.08253.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.08253.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10975.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10975.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10975.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10224.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10224.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10224.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10999.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10999.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10999.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10609.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10609.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10609.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.09713.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.09713.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.09713.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.08711.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.08711.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.08711.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.09901.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.09901.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.09901.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.07106.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.07106.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.07106.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.04935.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.04935.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.04935.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.11144.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.11144.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.11144.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10179.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10179.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10179.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.08489.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.08489.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.08489.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.08030.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.08030.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.08030.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.06008.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.06008.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.06008.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.11008.
[12.02.2026 07:05] Downloading paper 2602.11008 from https://arxiv.org/pdf/2602.11008v1...
[12.02.2026 07:05] Extracting affiliations from text.
[12.02.2026 07:05] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Ammar Ali * 1 2 Baher Mohammad * 1 2 Denis Makhov 2 Dmitriy Shopkhoev 2 Magauiya Zhussip 2 Stamatios Lefkimmiatis 2 6 2 0 2 1 1 ] . [ 1 8 0 0 1 1 . 2 0 6 2 : r a "
[12.02.2026 07:05] Response: ```python
[]
```
[12.02.2026 07:05] Extracting affiliations from text.
[12.02.2026 07:05] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Ammar Ali * 1 2 Baher Mohammad * 1 2 Denis Makhov 2 Dmitriy Shopkhoev 2 Magauiya Zhussip 2 Stamatios Lefkimmiatis 2 6 2 0 2 1 1 ] . [ 1 8 0 0 1 1 . 2 0 6 2 : r aWe present ROCKET, training-free model compression method that achieves state-of-theart performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to target model size. Second, it introduces single-step sparse matrix factorization inspired by dictionary learning: using only small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 2050% compression rates. Notably, it retains over 90% of the original models performance at 30% compression without any finetuning. Moreover, when applying light finetuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8Bparameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code implementing ROCKET. 1. Introduction In recent years, transformers have achieved unprecedented success across wide range of tasks in both computer vision *Equal contribution 1Department of Computer Science, ITMO University, Saint-Petersburg, Russia 2MWS AI, Moscow, Russia. Correspondence to: Ammar Ali <ammarali32@itmo.ru>, Baher Mohammad <b.mohammad@mts.ai>. Preprint. February 12, 2026. and natural language processing. Modern large language models (LLMs) typically scale up to billions of parameters, significantly increasing the computational and memory requirements for both training and inference stages. This substantial resource demand poses critical challenge for their wider practical deployment, especially on edge devices or in latency-sensitive applications. Due to the excessive size of modern LLMs, there has been significant research effort to make such models more efficient and accessible under constrained hardware budgets. Such efforts primarily focus on three key strategies: quantization (Hassibi & Stork, 1992), distillation(Hinton et al., 2015), and weight compression via matrix factorization (Wang et al., 2020). Among these, post-training weight factorization has emerged as particularly promising direction, enabling substantial parameter reduction without the need for costly retraining or fine-tuning. dominant paradigm in this area is low-rank approximation using truncated Singular Value Decomposition (SVD), which approximates each weight matrix as the product of two smaller dense matrices. However, this strategy imposes rigid structural constraint forcing all columns of the weight matrix to lie in single shared low-dimensional subspace. This often limits the representational capacity and leads to significant performance degradation under moderate to high compression. This limitation has spurred the development of methods that go beyond single shared subspace representation , adopting instead union-of-subspaces framework akin to dictionary learning. In such models, weight matrix is expressed as combination of subset of basis matrices (Zhussip et al., 2025), or alternatively, its individual columns are represented as sparse linear combinations of atoms from shared dictionary (Shopkhoev et al., 2025). These formulations provide greater flexibility by capturing the heterogeneous local structures present within the weight matrix. Despite their theoretical appeal, practical adoption of these methods faces severe computational challenges: conventional dictionary learning algorithms rely on iterative alternating minimization between sparse coding and dictionary update steps, which is prohibitively expensive for large-scale LLM weight matrices (Aharon et al., 2006). ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression In this work, we propose ROCKET, fast, training-free compression method that overcomes the representational rigidity of low-rank factorization while avoiding the computational burden of iterative dictionary learning. Our approach introduces two key innovations. First, ROCKET compresses weight matrices via single-step structured sparsification of low-rank basis. This yields factorization that inherits the expressive power of union-of-subspaces models yet operates orders of magnitude faster than alternating minimization schemes. Second, rather than applying uniform compression or relying on heuristic layer-wise sensitivity estimates, ROCKET formulates global compression allocation as multi-choice knapsack problem. For each layer, it selects the optimal compression configuration from set of precomputed candidates to minimize total weight reconstruction error under target model size constraint. Together, these components enable ROCKET to produce compact models that achieve substantially higher accuracy compared to existing post-training compression methods. The contributions of this work are summarized as follows: (1) We propose ROCKET, an efficient, training-free LLM compression method that factorizes weight matrices into sparse dictionary representation computable in single step, eliminating the need for iterative optimization; (2) We introduce calibration-guided criterion for sparsifying the coefficient matrix, operating effectively in both the original and whitened weight spaces to preserve salient directional information; (3) We formulate layer-wise compression allocation as multi-choice knapsack problem, enabling dynamic, performance-aware distribution of the global compression budget across layers; (4) Through extensive experiments, we demonstrate that ROCKET consistently outperforms state-of-the-art compression methods including structured sparsification, low-rank factorization, and adaptive budget allocation techniques across multiple modalities (text, vision, and audio). 2. Related Work This work intersects three primary re"
[12.02.2026 07:05] Mistral response. {"id": "61578167356249aeb676d43e304c1a69", "created": 1770879949, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1303, "total_tokens": 1334, "completion_tokens": 31, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Computer Science, ITMO University, Saint-Petersburg, Russia\", \"MWS AI, Moscow, Russia\"]\n```"}}]}
[12.02.2026 07:05] Response: ```python
["Department of Computer Science, ITMO University, Saint-Petersburg, Russia", "MWS AI, Moscow, Russia"]
```
[12.02.2026 07:05] Deleting PDF ./assets/pdf/2602.11008.pdf.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10652.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10652.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10652.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.09014.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.09014.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.09014.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.10699.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.10699.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.10699.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.08995.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.08995.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.08995.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Downloading and parsing paper https://huggingface.co/papers/2602.11137.
[12.02.2026 07:05] Extra JSON file exists (./assets/json/2602.11137.json), skip PDF parsing.
[12.02.2026 07:05] Paper image links file exists (./assets/img_data/2602.11137.json), skip HTML parsing.
[12.02.2026 07:05] Success.
[12.02.2026 07:05] Enriching papers with extra data.
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 0. Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.  					AI-generated summary 				 We introduce Step 3.5 Flash, ...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 1. PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.  					AI-generated summary 				 With the rapid development of large multimodal models, reliable judge and critic models have become essential f...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 2. Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.  					AI-gen...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 3. GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.  					AI-generated summary 				 While reasoning over long context is crucial for various real-world ...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 4. DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.  					AI-generated summary 				 In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-q...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 5. A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.  					AI-generated summary 				 While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), ex...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 6. FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.  					AI-generated summary 				 Agents powered by large language models (LLMs) are increasingly adopted in the sof...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 7. Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged ...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 8. CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.  					AI-generated summary 				 Agentic coding requires agents to effectively in...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 9. Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.  					AI-generated summary 				 Reinforcement learning...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 10. Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.  					AI-generated summary 				 Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 11. Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.  					AI-generated summary 				 This paper proposes Omni Dense Captioning, a novel task d...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 12. A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.  					AI-generated summary 				 Query Processing (QP) bridges user in...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 13. Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.  					AI-generated summary 				 Omni-modal large language models (O...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 14. A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.  					AI-generated summary 				 Adapting LLM agents to domain-specific tool calling remai...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 15. GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.  					AI-generated summary 				 Unified Multimodal Models (UMMs) have...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 16. Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.  					AI-generated summary 				 Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vis...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 17. Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) ha...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 18. Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.  					AI-generated summary 				 Reasoning models enhance problem-solving by scaling test-time comput...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 19. AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 20. ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free mod...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 21. A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.  					AI-generated summary 				 Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-base...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 22. ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.  					AI-generated summary 				 Diffusion models have achieved remarkable generation q...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 23. V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.  					AI-generated summary 				 Generative recommendation via autoregressive models has unified retrieval and ran...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 24. Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.  					AI-generated summary 				 Computer-use agents (CUAs) have made tremendou...
[12.02.2026 07:05] ********************************************************************************
[12.02.2026 07:05] Abstract 25. Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.  					AI-generated summary 				 The prevailing paradigm in large language model (LLM) development is to pretrain a ...
[12.02.2026 07:05] Read previous papers.
[12.02.2026 07:05] Generating reviews via LLM API.
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#architecture", "#agents", "#benchmark", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –∞–≥–µ–Ω—Ç–Ω—ã–π –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Step 3.5 Flash, –æ—Ç–Ω–æ—Å—è—â–∞—è—Å—è –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts, –∫–æ—Ç–æ—Ä–∞
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#rlhf", "#robotics", "#multimodal", "#alignment", "#benchmark", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —á—É—Ç—å—ë–º: —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", "desc": "PhyCritic ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#science", "#training", "#reasoning", "#agents", "#math", "#open_source"], "emoji": "üßÆ", "ru": {"title": "–û—Ç –æ–ª–∏–º–ø–∏–∞–¥ –∫ –Ω–∞—É—á–Ω—ã–º –æ—Ç–∫—Ä—ã—Ç–∏—è–º: AI-–∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "Aletheia ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–µ—Ä—Å–∏—é 
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–∞—è –ø–∞–º—è—Ç—å —Å —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º–∏ –∑–∞—Ç–≤–æ—Ä–∞–º–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "GRU-Mem —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –≤ –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ç–µ–∫—Å—Ç–æ–º –∑–∞—Ç–≤–æ—Ä—ã, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#synthetic", "#data", "#training", "#optimization", "#benchmark", "#rl"], "emoji": "üë®‚Äçüç≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫—É–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è DataChef-32B ‚Äî —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—Ü–µ–ø—Ç–æ
[12.02.2026 07:05] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è G-LNS, –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#leakage", "#plp", "#open_source", "#benchmark", "#agents", "#optimization", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –±–∞–≥–æ–≤ –∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "FeatureBench ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–º—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö: –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏", "desc": "Meta-Experience Learning ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[12.02.2026 07:05] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é –∏—Å—Ç–æ—Ä–∏–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLI-Gym, —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π, –ø—É—Ç–µ–º –∏–º–∏—Ç–∞—Ü–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üéØ", "ru": {"title": "–ö–∞–ª–º–∞–Ω–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Online Causal Kalman Filtering –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –ø—Ä–∏ importance samplin
[12.02.2026 07:05] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–û—Ç —à—Ç—Ä–∏—Ö–∞ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π", "desc": "Stroke3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –∏–∑ –¥–≤—É–º–µ—Ä–Ω—ã—Ö —Ä–∏—Å—É–Ω–∫–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π pipeline. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∞—Ä
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#video", "#multimodal", "#training", "#reasoning", "#benchmark", "#audio", "#dataset", "#open_source"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –æ–ø–∏—Å—ã–≤–∞—Ç—å –≤–∏–¥–µ–æ –∫–∞–∫ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—Å—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Omni Dense Captioning ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞
[12.02.2026 07:05] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QP-OneModel ‚Äî –µ–¥–∏–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥–∑
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#audio", "#3d", "#architecture", "#dataset"], "emoji": "üé≠", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ—á—å –∏ –º–∏–º–∏–∫–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –æ—Ç –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "Ex-Omni ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, 
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#small_models", "#inference", "#training", "#agents"], "emoji": "üéØ", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è—é—â–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Activation Steering Adapter (ASA) –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ç –ø–∞–º—è—Ç–∏ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –æ—Ü–µ–Ω–æ—á–Ω–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ GENIUS –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞–¥
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#security", "#ethics", "#benchmark"], "emoji": "üé®", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏: –∑–∞—â–∏—Ç–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç –∞—Ç–∞–∫ —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –∞—Ç–∞–∫ –Ω–∞ –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–ö—Ä–µ–ø–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RLTR, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#training", "#long_context", "#reasoning", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –∑–∞–±—ã–≤–∞—Ç—å –Ω–µ–Ω—É–∂–Ω–æ–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Free()LM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–≤–æ–¥—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∑–∞–±—ã–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä Free-Module 
[12.02.2026 07:05] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents"], "emoji": "ü§ù", "ru": {"title": "LLM-–∞–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –≤–µ—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã —á–µ—Ä–µ–∑ —è–∑—ã–∫", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫-—Å–∏—Å—Ç–µ–º–∞ AgenticPay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –°–∏—Å—Ç–µ
[12.02.2026 07:05] Querying the API.
[12.02.2026 07:05] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.
[12.02.2026 07:05] Response: ```json
{
  "desc": "ROCKET ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –ø–æ —Å–ª–æ—è–º –∫–∞–∫ –∑–∞–¥–∞—á—É –º–Ω–æ–≥–æ–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ —Ä—é–∫–∑–∞–∫–∞ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –æ—à–∏–±–∫–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–Ω–æ—à–∞–≥–æ–≤—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—á–Ω—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—É—é —Å–ª–æ–≤–∞—Ä–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –ø—Ä–∏–º–µ–Ω—è—è —Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö. –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–µ—Å–æ–≤ –∫ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤ –∑–∞–∫—Ä—ã—Ç–æ–π —Ñ–æ—Ä–º–µ —á–µ—Ä–µ–∑ –Ω–∞–∏–º–µ–Ω—å—à–∏–µ –∫–≤–∞–¥—Ä–∞—Ç—ã, –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑–±–µ–≥–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è. ROCKET –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –±–æ–ª–µ–µ 90% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å–∂–∞—Ç–∏–∏ –Ω–∞ 30% –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.",
  "emoji": "üöÄ",
  "title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é"
}
```
[12.02.2026 07:05] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main."

[12.02.2026 07:05] Response: ```python
["INFERENCE", "TRAINING"]
```
[12.02.2026 07:05] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main."

[12.02.2026 07:05] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper presents ROCKET, a method for training-free model compression that optimizes weight sparsification and layer-wise compression allocation. It addresses optimization of model efficiency through sparse matrix factorization and closed-form solutions, which are core optimization techniques.

- **OPEN_SOURCE**: The paper explicitly states "The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main," indicating the authors are releasing their code publicly as an open-source contribution.
[12.02.2026 07:05] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper presents ROCKET, a method for training-free model compression that optimizes weight sparsification and layer-wise compression allocation. It addresses optimization of model efficiency through sparse matrix factorization and closed-form solutions, which are core optimization techniques.

- **OPEN_SOURCE**: The paper explicitly states "The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main," indicating the authors are releasing their code publicly as an open-source contribution.
[12.02.2026 07:06] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ROCKET is a novel model compression technique that does not require training, making it efficient and straightforward. It treats the problem of compressing each layer of a neural network as a multi-choice knapsack problem, optimizing the compression level for each layer to minimize errors while staying within a size limit. The method employs a unique sparse matrix factorization approach that uses a small calibration dataset to adjust weight coefficients based on their importance, avoiding the need for complex iterative processes. As a result, ROCKET achieves significant compression rates while maintaining high performance, outperforming traditional methods without the need for fine-tuning.","title":"ROCKET: Efficient Model Compression Without Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ROCKET is a novel model compression technique that does not require training, making it efficient and straightforward. It treats the problem of compressing each layer of a neural network as a multi-choice knapsack problem, optimizing the compression level for each layer to minimize errors while staying within a size limit. The method employs a unique sparse matrix factorization approach that uses a small calibration dataset to adjust weight coefficients based on their importance, avoiding the need for complex iterative processes. As a result, ROCKET achieves significant compression rates while maintaining high performance, outperforming traditional methods without the need for fine-tuning.', title='ROCKET: Efficient Model Compression Without Training'))
[12.02.2026 07:06] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ROCKETÊòØ‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂéãÁº©ÊñπÊ≥ïÔºåÂÆÉÂ∞ÜÈÄêÂ±ÇÂéãÁº©ÈóÆÈ¢òËßÜ‰∏∫Â§öÈÄâËÉåÂåÖÈóÆÈ¢òÔºåÂπ∂Âà©Áî®Á®ÄÁñèÁü©ÈòµÂàÜËß£ÂÆûÁé∞È´òÊïàÁöÑÊùÉÈáçÁ®ÄÁñèÂåñÔºåËÄåÊó†ÈúÄËø≠‰ª£‰ºòÂåñ„ÄÇËØ•ÊñπÊ≥ïÂú®ÂÖ®ÁêÉÂéãÁº©È¢ÑÁÆó‰∏ãÔºåÈÄöËøáÈÄâÊã©ÊØèÂ±ÇÁöÑÊúÄ‰Ω≥ÂéãÁº©Á∫ßÂà´ÔºåÊúÄÂ∞èÂåñÊÄªÈáçÂª∫ËØØÂ∑ÆÔºåÂêåÊó∂Êª°Ë∂≥ÁõÆÊ†áÊ®°ÂûãÂ§ßÂ∞è„ÄÇROCKETÂú®‰∏çÂêåÊ®°ÂûãÊû∂ÊûÑ‰∏ãÁöÑÂéãÁº©ÁéáËææÂà∞20-50%ÔºåÂπ∂‰∏îÂú®30%ÁöÑÂéãÁº©‰∏ã‰øùÁïô‰∫ÜË∂ÖËøá90%ÁöÑÂéüÂßãÊ®°ÂûãÊÄßËÉΩ„ÄÇÈÄöËøáËΩªÂæÆÁöÑÂæÆË∞ÉÈò∂ÊÆµÔºåÂéãÁº©ÂêéÁöÑÊ®°ÂûãÊÄßËÉΩÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçáÔºåÂá†‰πéÂèØ‰ª•‰∏éÂéüÂßãÊ®°ÂûãÁõ∏Â™≤Áæé„ÄÇ","title":"ROCKETÔºöÊó†ËÆ≠ÁªÉÁöÑÈ´òÊïàÊ®°ÂûãÂéãÁº©ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ROCKETÊòØ‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊ®°ÂûãÂéãÁº©ÊñπÊ≥ïÔºåÂÆÉÂ∞ÜÈÄêÂ±ÇÂéãÁº©ÈóÆÈ¢òËßÜ‰∏∫Â§öÈÄâËÉåÂåÖÈóÆÈ¢òÔºåÂπ∂Âà©Áî®Á®ÄÁñèÁü©ÈòµÂàÜËß£ÂÆûÁé∞È´òÊïàÁöÑÊùÉÈáçÁ®ÄÁñèÂåñÔºåËÄåÊó†ÈúÄËø≠‰ª£‰ºòÂåñ„ÄÇËØ•ÊñπÊ≥ïÂú®ÂÖ®ÁêÉÂéãÁº©È¢ÑÁÆó‰∏ãÔºåÈÄöËøáÈÄâÊã©ÊØèÂ±ÇÁöÑÊúÄ‰Ω≥ÂéãÁº©Á∫ßÂà´ÔºåÊúÄÂ∞èÂåñÊÄªÈáçÂª∫ËØØÂ∑ÆÔºåÂêåÊó∂Êª°Ë∂≥ÁõÆÊ†áÊ®°ÂûãÂ§ßÂ∞è„ÄÇROCKETÂú®‰∏çÂêåÊ®°ÂûãÊû∂ÊûÑ‰∏ãÁöÑÂéãÁº©ÁéáËææÂà∞20-50%ÔºåÂπ∂‰∏îÂú®30%ÁöÑÂéãÁº©‰∏ã‰øùÁïô‰∫ÜË∂ÖËøá90%ÁöÑÂéüÂßãÊ®°ÂûãÊÄßËÉΩ„ÄÇÈÄöËøáËΩªÂæÆÁöÑÂæÆË∞ÉÈò∂ÊÆµÔºåÂéãÁº©ÂêéÁöÑÊ®°ÂûãÊÄßËÉΩÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçáÔºåÂá†‰πéÂèØ‰ª•‰∏éÂéüÂßãÊ®°ÂûãÁõ∏Â™≤Áæé„ÄÇ', title='ROCKETÔºöÊó†ËÆ≠ÁªÉÁöÑÈ´òÊïàÊ®°ÂûãÂéãÁº©ÊñπÊ≥ï'))
[12.02.2026 07:06] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∞—è—Å—è –ø–∞–º—è—Ç—å: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è UMEM ‚Äî –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –≤ –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é
[12.02.2026 07:06] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#diffusion", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArcFlow ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Ç–æ
[12.02.2026 07:06] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ-–Ω–∞–≥—Ä–∞–¥–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", "desc": "V-STAR ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–≥—Ä–∞–¥–æ–π –≤ –º–æ–¥–µ–ª—è—Ö —Å —É—Å–∏–ª–µ–Ω–Ω—ã–º –æ
[12.02.2026 07:06] Using data from previous issue: {"categories": ["#agents", "#security", "#alignment", "#benchmark", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ DeAction –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –≤
[12.02.2026 07:06] Using data from previous issue: {"categories": ["#training"], "emoji": "üéØ", "ru": {"title": "–ë–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî –ª—É—á—à–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Ä–æ–ª—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏. 
[12.02.2026 07:06] Renaming data file.
[12.02.2026 07:06] Renaming previous data. hf_papers.json to ./d/2026-02-12.json
[12.02.2026 07:06] Saving new data file.
[12.02.2026 07:06] Generating page.
[12.02.2026 07:06] Renaming previous page.
[12.02.2026 07:06] Renaming previous data. index.html to ./d/2026-02-12.html
[12.02.2026 07:06] Writing result.
[12.02.2026 07:06] Renaming log file.
[12.02.2026 07:06] Renaming previous data. log.txt to ./logs/2026-02-12_last_log.txt
