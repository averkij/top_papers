[12.02.2026 10:38] Read previous papers.
[12.02.2026 10:38] Generating top page (month).
[12.02.2026 10:38] Writing top page (month).
[12.02.2026 11:35] Read previous papers.
[12.02.2026 11:35] Get feed.
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10604
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11144
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11124
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04935
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10560
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10177
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08711
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08253
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11089
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10975
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11008
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10224
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07106
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10999
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10609
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09713
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09514
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09901
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10179
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11149
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10748
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08489
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08030
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07954
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09014
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06008
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10652
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10231
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10870
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10699
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08995
[12.02.2026 11:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11137
[12.02.2026 11:35] Extract page data from URL. URL: https://huggingface.co/papers/2602.10778
[12.02.2026 11:35] Extract page data from URL. URL: https://huggingface.co/papers/2602.08741
[12.02.2026 11:35] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.02.2026 11:35] No deleted papers detected.
[12.02.2026 11:35] Downloading and parsing papers (pdf, html). Total: 34.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10604.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10604.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10604.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.11144.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.11144.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.11144.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.11124.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.11124.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.11124.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.04935.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.04935.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.04935.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10560.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10560.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10560.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10177.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10177.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10177.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.08711.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.08711.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.08711.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.08253.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.08253.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.08253.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.11089.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.11089.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.11089.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10975.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10975.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10975.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.11008.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.11008.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.11008.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10224.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10224.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10224.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.07106.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.07106.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.07106.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10999.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10999.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10999.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10609.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10609.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10609.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.09713.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.09713.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.09713.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.09514.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.09514.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.09514.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.09901.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.09901.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.09901.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10179.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10179.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10179.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.11149.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.11149.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.11149.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10748.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10748.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10748.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.08489.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.08489.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.08489.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.08030.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.08030.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.08030.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.07954.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.07954.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.07954.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.09014.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.09014.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.09014.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.06008.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.06008.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.06008.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10652.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10652.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10652.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10231.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10231.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10231.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10870.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10870.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10870.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10699.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.10699.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.10699.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.08995.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.08995.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.08995.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.11137.
[12.02.2026 11:35] Extra JSON file exists (./assets/json/2602.11137.json), skip PDF parsing.
[12.02.2026 11:35] Paper image links file exists (./assets/img_data/2602.11137.json), skip HTML parsing.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.10778.
[12.02.2026 11:35] Downloading paper 2602.10778 from https://arxiv.org/pdf/2602.10778v1...
[12.02.2026 11:35] Extracting affiliations from text.
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"GoodVibe: Security-by-Vibe for LLM-Based Code Generation 6 2 0 2 1 1 ] . [ 1 8 7 7 0 1 . 2 0 6 2 : r a Stjepan Picek University of Zagreb & Radboud University Ahmad-Reza Sadeghi Technical University of Darmstadt "
[12.02.2026 11:35] Response: ```python
[
    "University of Zagreb",
    "Radboud University",
    "Technical University of Darmstadt"
]
```
[12.02.2026 11:35] Deleting PDF ./assets/pdf/2602.10778.pdf.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Downloading and parsing paper https://huggingface.co/papers/2602.08741.
[12.02.2026 11:35] Downloading paper 2602.08741 from https://arxiv.org/pdf/2602.08741v1...
[12.02.2026 11:35] Extracting affiliations from text.
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 ] . [ 1 1 4 7 8 0 . 2 0 6 2 : r Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing Jona te Lintelo Radboud University, The Netherlands Lichao Wu Technical University of Darmstadt, Germany Stjepan Picek Faculty of Electrical Engineering and Computing University of Zagreb, Croatia & Radboud University, The Netherlands "
[12.02.2026 11:35] Response: ```python
[
    "Radboud University, The Netherlands",
    "Technical University of Darmstadt, Germany",
    "Faculty of Electrical Engineering and Computing University of Zagreb, Croatia",
    "Radboud University, The Netherlands"
]
```
[12.02.2026 11:35] Deleting PDF ./assets/pdf/2602.08741.pdf.
[12.02.2026 11:35] Success.
[12.02.2026 11:35] Enriching papers with extra data.
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 0. Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.  					AI-generated summary 				 We introduce Step 3.5 Flash, ...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 1. GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.  					AI-generated summary 				 Unified Multimodal Models (UMMs) have...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 2. PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.  					AI-generated summary 				 With the rapid development of large multimodal models, reliable judge and critic models have become essential f...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 3. A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.  					AI-generated summary 				 Adapting LLM agents to domain-specific tool calling remai...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 4. GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.  					AI-generated summary 				 While reasoning over long context is crucial for various real-world ...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 5. Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.  					AI-gen...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 6. Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.  					AI-generated summary 				 This paper proposes Omni Dense Captioning, a novel task d...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 7. A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.  					AI-generated summary 				 While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), ex...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 8. DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.  					AI-generated summary 				 In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-q...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 9. FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.  					AI-generated summary 				 Agents powered by large language models (LLMs) are increasingly adopted in the sof...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 10. ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free mod...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 11. Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged ...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 12. Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.  					AI-generated summary 				 Omni-modal large language models (O...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 13. CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.  					AI-generated summary 				 Agentic coding requires agents to effectively in...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 14. Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.  					AI-generated summary 				 Reinforcement learning...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 15. Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.  					AI-generated summary 				 Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 16. EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.  					AI-generated summary 				 Long-horizon planning is widely recognized as a core capability...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 17. A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.  					AI-generated summary 				 Query Processing (QP) bridges user in...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 18. Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.  					AI-generated summary 				 Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vis...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 19. Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.  					AI-generated summary 				 Supervised fine-tuning (SFT) on cha...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 20. Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.  					AI-generated summary 				 Knowledge Graphs (KGs) store structured factual know...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 21. Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) ha...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 22. Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.  					AI-generated summary 				 Reasoning models enhance problem-solving by scaling test-time comput...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 23. Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.  					AI-generated summary 				 As Large Language Models (LLMs) become increasingly deployed in Polish l...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 24. ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.  					AI-generated summary 				 Diffusion models have achieved remarkable generation q...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 25. AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 26. A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.  					AI-generated summary 				 Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-base...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 27. Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.  					AI-generated summary 				 Group Relative Policy Optimization (GRPO) assigns ...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 28. FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.  					AI-generated summary 				 Federated Learning (FL) enables ...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 29. V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.  					AI-generated summary 				 Generative recommendation via autoregressive models has unified retrieval and ran...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 30. Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.  					AI-generated summary 				 Computer-use agents (CUAs) have made tremendou...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 31. Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.  					AI-generated summary 				 The prevailing paradigm in large language model (LLM) development is to pretrain a ...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 32. GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for ...
[12.02.2026 11:35] ********************************************************************************
[12.02.2026 11:35] Abstract 33. Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language...
[12.02.2026 11:35] Read previous papers.
[12.02.2026 11:35] Generating reviews via LLM API.
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#architecture", "#agents", "#benchmark", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –∞–≥–µ–Ω—Ç–Ω—ã–π –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Step 3.5 Flash, –æ—Ç–Ω–æ—Å—è—â–∞—è—Å—è –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts, –∫–æ—Ç–æ—Ä–∞
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ç –ø–∞–º—è—Ç–∏ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –æ—Ü–µ–Ω–æ—á–Ω–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ GENIUS –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞–¥
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#rlhf", "#robotics", "#multimodal", "#alignment", "#benchmark", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —á—É—Ç—å—ë–º: —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", "desc": "PhyCritic ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#small_models", "#inference", "#training", "#agents"], "emoji": "üéØ", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è—é—â–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Activation Steering Adapter (ASA) –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–∞—è –ø–∞–º—è—Ç—å —Å —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º–∏ –∑–∞—Ç–≤–æ—Ä–∞–º–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "GRU-Mem —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –≤ –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ç–µ–∫—Å—Ç–æ–º –∑–∞—Ç–≤–æ—Ä—ã, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#science", "#training", "#reasoning", "#agents", "#math", "#open_source"], "emoji": "üßÆ", "ru": {"title": "–û—Ç –æ–ª–∏–º–ø–∏–∞–¥ –∫ –Ω–∞—É—á–Ω—ã–º –æ—Ç–∫—Ä—ã—Ç–∏—è–º: AI-–∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "Aletheia ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–µ—Ä—Å–∏—é 
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#video", "#multimodal", "#training", "#reasoning", "#benchmark", "#audio", "#dataset", "#open_source"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –æ–ø–∏—Å—ã–≤–∞—Ç—å –≤–∏–¥–µ–æ –∫–∞–∫ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—Å—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Omni Dense Captioning ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞
[12.02.2026 11:35] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è G-LNS, –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#synthetic", "#data", "#training", "#optimization", "#benchmark", "#rl"], "emoji": "üë®‚Äçüç≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫—É–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è DataChef-32B ‚Äî —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—Ü–µ–ø—Ç–æ
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#leakage", "#plp", "#open_source", "#benchmark", "#agents", "#optimization", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –±–∞–≥–æ–≤ –∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "FeatureBench ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–º—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üöÄ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é", "desc": "ROCKET ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –ø–æ —Å–ª–æ—è–º –∫–∞–∫ –∑–∞–¥–∞—á—É –º–Ω–æ–≥–æ–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ —Ä—é–∫–∑–∞–∫
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö: –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏", "desc": "Meta-Experience Learning ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#audio", "#3d", "#architecture", "#dataset"], "emoji": "üé≠", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ—á—å –∏ –º–∏–º–∏–∫–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –æ—Ç –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "Ex-Omni ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, 
[12.02.2026 11:35] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é –∏—Å—Ç–æ—Ä–∏–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLI-Gym, —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π, –ø—É—Ç–µ–º –∏–º–∏—Ç–∞—Ü–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üéØ", "ru": {"title": "–ö–∞–ª–º–∞–Ω–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Online Causal Kalman Filtering –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –ø—Ä–∏ importance samplin
[12.02.2026 11:35] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–û—Ç —à—Ç—Ä–∏—Ö–∞ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π", "desc": "Stroke3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –∏–∑ –¥–≤—É–º–µ—Ä–Ω—ã—Ö —Ä–∏—Å—É–Ω–∫–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π pipeline. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∞—Ä
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#agents", "#reasoning"], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EcoGym ‚Äî —ç—Ç–æ –æ–±–æ–±—â—ë–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-based –∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏
[12.02.2026 11:35] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QP-OneModel ‚Äî –µ–¥–∏–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥–∑
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#security", "#ethics", "#benchmark"], "emoji": "üé®", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏: –∑–∞—â–∏—Ç–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç –∞—Ç–∞–∫ —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –∞—Ç–∞–∫ –Ω–∞ –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#reasoning", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: –∫–∞–∫ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–µ–ª–∞—é—Ç —É–º–Ω–µ–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–µ—Ç–æ–¥–æ–º supervised fine-tuning –Ω–∞
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#rag"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π: –ø–æ—á–µ–º—É LLM —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ FactCheck –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∞–∫—Ç—ã –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π –ø–æ —Ç—Ä—ë–º –Ω–∞–ø—Ä–∞–≤
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–ö—Ä–µ–ø–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RLTR, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#training", "#long_context", "#reasoning", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –∑–∞–±—ã–≤–∞—Ç—å –Ω–µ–Ω—É–∂–Ω–æ–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Free()LM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–≤–æ–¥—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∑–∞–±—ã–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä Free-Module 
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#dataset", "#benchmark", "#ethics", "#small_models", "#multilingual"], "emoji": "üõ°Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è LLM –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Bielik Guard ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#diffusion", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArcFlow ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Ç–æ
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents"], "emoji": "ü§ù", "ru": {"title": "LLM-–∞–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –≤–µ—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã —á–µ—Ä–µ–∑ —è–∑—ã–∫", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫-—Å–∏—Å—Ç–µ–º–∞ AgenticPay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –°–∏—Å—Ç–µ
[12.02.2026 11:35] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∞—è—Å—è –ø–∞–º—è—Ç—å: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è UMEM ‚Äî –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –≤ –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rlhf"], "emoji": "üß©", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Blockwise Advantage Estimation - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä
[12.02.2026 11:35] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–ü—Ä–∏–≤–∞—Ç–Ω–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "FedPS ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –º–µ—Ç–æ–¥—ã —ç—Å–∫–∏–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã
[12.02.2026 11:35] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ-–Ω–∞–≥—Ä–∞–¥–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", "desc": "V-STAR ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–≥—Ä–∞–¥–æ–π –≤ –º–æ–¥–µ–ª—è—Ö —Å —É—Å–∏–ª–µ–Ω–Ω—ã–º –æ
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#agents", "#security", "#alignment", "#benchmark", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ DeAction –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –≤
[12.02.2026 11:35] Using data from previous issue: {"categories": ["#training"], "emoji": "üéØ", "ru": {"title": "–ë–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî –ª—É—á—à–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Ä–æ–ª—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏. 
[12.02.2026 11:35] Querying the API.
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.   We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.
[12.02.2026 11:35] Response: ```json
{
  "desc": "GoodVibe ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞ –∫–æ–¥–µ, —á–µ—Ä–µ–∑ —Ü–µ–ª–µ–≤—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –Ω–µ–±–æ–ª—å—à–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—É—é –∞—Ç—Ä–∏–±—É—Ü–∏—é –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤ –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ç–æ–ª—å–∫–æ —ç—Ç–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏–∑–¥–µ—Ä–∂–∫–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —à–µ—Å—Ç–∏ LLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞ –≤ 2,5 —Ä–∞–∑–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ —Ç—Ä–µ–±—É–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ 4700 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π.",
  "emoji": "üîí",
  "title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–¥ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω–æ-–∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ LLM"
}
```
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.   We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality."

[12.02.2026 11:35] Response: ```python
["PLP", "TRAINING", "INFERENCE"]
```
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.   We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality."

[12.02.2026 11:35] Response: ```python
['SECURITY', 'INTERPRETABILITY', 'OPTIMIZATION']
```
[12.02.2026 11:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GoodVibe is a novel framework designed to enhance the security of code language models by focusing on specific neurons that are critical for security reasoning. Instead of fine-tuning the entire model, GoodVibe selectively updates only the neurons identified as security-relevant, which significantly reduces training costs and avoids issues like catastrophic forgetting. The framework employs activation-driven neuron clustering to optimize updates, ensuring that the model remains efficient while improving the security of generated code. Evaluations show that GoodVibe can achieve substantial security improvements with far fewer parameters and less computational expense compared to traditional fine-tuning methods.","title":"Enhancing Code Security with Neuron-Level Fine-Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GoodVibe is a novel framework designed to enhance the security of code language models by focusing on specific neurons that are critical for security reasoning. Instead of fine-tuning the entire model, GoodVibe selectively updates only the neurons identified as security-relevant, which significantly reduces training costs and avoids issues like catastrophic forgetting. The framework employs activation-driven neuron clustering to optimize updates, ensuring that the model remains efficient while improving the security of generated code. Evaluations show that GoodVibe can achieve substantial security improvements with far fewer parameters and less computational expense compared to traditional fine-tuning methods.', title='Enhancing Code Security with Neuron-Level Fine-Tuning'))
[12.02.2026 11:35] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GoodVibeÊòØ‰∏Ä‰∏™Á•ûÁªèÂÖÉÁ∫ßÊ°ÜÊû∂ÔºåÈÄöËøáÈíàÂØπÊÄßÂú∞ÂæÆË∞É‰∏éÂÆâÂÖ®Áõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÊù•Â¢ûÂº∫‰ª£Á†ÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãÁöÑÂÆûÁî®ÊÄßÂπ∂ÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÇËØ•Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂÆâÂÖ®Áõ∏ÂÖ≥ÁöÑÊé®ÁêÜ‰ªÖÈõÜ‰∏≠Âú®Â∞ëÈáèÁ•ûÁªèÂÖÉ‰∏ä„ÄÇÈÄöËøá‰ΩøÁî®Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑÂΩíÂõ†ÊñπÊ≥ïËØÜÂà´Ëøô‰∫õÁ•ûÁªèÂÖÉÔºåGoodVibe‰ªÖÂØπËøô‰∫õÂÆâÂÖ®ÂÖ≥ÈîÆÁöÑÂ≠êÁ©∫Èó¥ËøõË°åÈÄâÊã©ÊÄßÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGoodVibeÂú®Â§ö‰∏™ÂÆâÂÖ®ÂÖ≥ÈîÆÁºñÁ®ãËØ≠Ë®Ä‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàê‰ª£Á†ÅÁöÑÂÆâÂÖ®ÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÊï¥‰ΩìÊïàÁî®„ÄÇ","title":"GoodVibeÔºöÊèêÂçá‰ª£Á†ÅÂÆâÂÖ®ÊÄßÁöÑÁ•ûÁªèÂÖÉÁ∫ßÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GoodVibeÊòØ‰∏Ä‰∏™Á•ûÁªèÂÖÉÁ∫ßÊ°ÜÊû∂ÔºåÈÄöËøáÈíàÂØπÊÄßÂú∞ÂæÆË∞É‰∏éÂÆâÂÖ®Áõ∏ÂÖ≥ÁöÑÁ•ûÁªèÂÖÉÊù•Â¢ûÂº∫‰ª£Á†ÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãÁöÑÂÆûÁî®ÊÄßÂπ∂ÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÇËØ•Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂÆâÂÖ®Áõ∏ÂÖ≥ÁöÑÊé®ÁêÜ‰ªÖÈõÜ‰∏≠Âú®Â∞ëÈáèÁ•ûÁªèÂÖÉ‰∏ä„ÄÇÈÄöËøá‰ΩøÁî®Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑÂΩíÂõ†ÊñπÊ≥ïËØÜÂà´Ëøô‰∫õÁ•ûÁªèÂÖÉÔºåGoodVibe‰ªÖÂØπËøô‰∫õÂÆâÂÖ®ÂÖ≥ÈîÆÁöÑÂ≠êÁ©∫Èó¥ËøõË°åÈÄâÊã©ÊÄßÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGoodVibeÂú®Â§ö‰∏™ÂÆâÂÖ®ÂÖ≥ÈîÆÁºñÁ®ãËØ≠Ë®Ä‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàê‰ª£Á†ÅÁöÑÂÆâÂÖ®ÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÊï¥‰ΩìÊïàÁî®„ÄÇ', title='GoodVibeÔºöÊèêÂçá‰ª£Á†ÅÂÆâÂÖ®ÊÄßÁöÑÁ•ûÁªèÂÖÉÁ∫ßÊ°ÜÊû∂'))
[12.02.2026 11:35] Querying the API.
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L^3), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L^3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L^3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.
[12.02.2026 11:35] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —É—è–∑–≤–∏–º–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä Mixture-of-Experts (MoE) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –≥–¥–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ –Ω–µ–±–æ–ª—å—à–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ Large Language Lobotomy (L¬≥), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤, –æ—Ç–≤–µ—á–∞—é—â–∏—Ö –∑–∞ –æ—Ç–∫–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ú–µ—Ç–æ–¥ —è–≤–ª—è–µ—Ç—Å—è –±–µ–∑—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω—ã–º, –¥–æ—Å—Ç–∏–≥–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫ –¥–æ 86,3% –ø—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ –º–µ–Ω–µ–µ 20% —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ —Å–ª–æ–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é MoE –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö.",
  "emoji": "‚ö†Ô∏è",
  "title": "–≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –≤ MoE –º–æ–¥–µ–ª—è—Ö"
}
```
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L^3), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L^3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L^3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods."

[12.02.2026 11:35] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[12.02.2026 11:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L^3), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L^3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L^3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods."

[12.02.2026 11:35] Response: ```python
["SECURITY", "ALIGNMENT"]
```
[12.02.2026 11:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses a new attack method called Large Language Lobotomy (L^3) that targets Mixture-of-Experts (MoE) language models. MoE models are efficient because they activate only a few parameters for each input, but this creates vulnerabilities in their safety mechanisms. The authors demonstrate that harmful behaviors in these models are linked to specific experts, allowing L^3 to selectively silence them to produce unsafe outputs. Their experiments show that this method significantly increases the success rate of attacks while maintaining the overall language performance of the models.","title":"Exploiting Expert Dynamics: A New Threat to MoE Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses a new attack method called Large Language Lobotomy (L^3) that targets Mixture-of-Experts (MoE) language models. MoE models are efficient because they activate only a few parameters for each input, but this creates vulnerabilities in their safety mechanisms. The authors demonstrate that harmful behaviors in these models are linked to specific experts, allowing L^3 to selectively silence them to produce unsafe outputs. Their experiments show that this method significantly increases the success rate of attacks while maintaining the overall language performance of the models.', title='Exploiting Expert Dynamics: A New Threat to MoE Language Models'))
[12.02.2026 11:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÆâÂÖ®ÊÄßÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåMoEÊ®°Âûã‰∏≠ÁöÑÂÆâÂÖ®ÂÖ≥ÈîÆË°å‰∏∫‰∏ªË¶ÅÈõÜ‰∏≠Âú®Â∞ëÊï∞‰∏ìÂÆ∂‰∏äÔºåËÄå‰∏çÊòØÂùáÂåÄÂàÜÂ∏É„ÄÇÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫L^3ÁöÑÊîªÂáªÊñπÊ≥ïÔºåÈÄöËøáÂà©Áî®‰∏ìÂÆ∂Ë∑ØÁî±Âä®ÊÄÅÊù•Á†¥ÂùèÂÆâÂÖ®ÂØπÈΩêÔºåÂêåÊó∂‰øùÊåÅËØ≠Ë®ÄÂÆûÁî®ÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåL^3Âú®Â§ö‰∏™ÂºÄÊ∫êMoE LLM‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊîªÂáªÊàêÂäüÁéáÔºåÊè≠Á§∫‰∫ÜÊïàÁéáÈ©±Âä®ÁöÑMoEËÆæËÆ°‰∏éÂÆâÂÖ®ÂØπÈΩê‰πãÈó¥ÁöÑÊ†πÊú¨ÁüõÁõæ„ÄÇ","title":"Âà©Áî®‰∏ìÂÆ∂Ë∑ØÁî±Âä®ÊÄÅÁ†¥Ëß£ÂÆâÂÖ®ÂØπÈΩê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÆâÂÖ®ÊÄßÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåMoEÊ®°Âûã‰∏≠ÁöÑÂÆâÂÖ®ÂÖ≥ÈîÆË°å‰∏∫‰∏ªË¶ÅÈõÜ‰∏≠Âú®Â∞ëÊï∞‰∏ìÂÆ∂‰∏äÔºåËÄå‰∏çÊòØÂùáÂåÄÂàÜÂ∏É„ÄÇÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫L^3ÁöÑÊîªÂáªÊñπÊ≥ïÔºåÈÄöËøáÂà©Áî®‰∏ìÂÆ∂Ë∑ØÁî±Âä®ÊÄÅÊù•Á†¥ÂùèÂÆâÂÖ®ÂØπÈΩêÔºåÂêåÊó∂‰øùÊåÅËØ≠Ë®ÄÂÆûÁî®ÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåL^3Âú®Â§ö‰∏™ÂºÄÊ∫êMoE LLM‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊîªÂáªÊàêÂäüÁéáÔºåÊè≠Á§∫‰∫ÜÊïàÁéáÈ©±Âä®ÁöÑMoEËÆæËÆ°‰∏éÂÆâÂÖ®ÂØπÈΩê‰πãÈó¥ÁöÑÊ†πÊú¨ÁüõÁõæ„ÄÇ', title='Âà©Áî®‰∏ìÂÆ∂Ë∑ØÁî±Âä®ÊÄÅÁ†¥Ëß£ÂÆâÂÖ®ÂØπÈΩê'))
[12.02.2026 11:36] Renaming data file.
[12.02.2026 11:36] Renaming previous data. hf_papers.json to ./d/2026-02-12.json
[12.02.2026 11:36] Saving new data file.
[12.02.2026 11:36] Generating page.
[12.02.2026 11:36] Renaming previous page.
[12.02.2026 11:36] Renaming previous data. index.html to ./d/2026-02-12.html
[12.02.2026 11:36] Writing result.
[12.02.2026 11:36] Renaming log file.
[12.02.2026 11:36] Renaming previous data. log.txt to ./logs/2026-02-12_last_log.txt
