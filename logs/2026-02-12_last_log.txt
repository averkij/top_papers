[12.02.2026 16:51] Read previous papers.
[12.02.2026 16:51] Generating top page (month).
[12.02.2026 16:51] Writing top page (month).
[12.02.2026 17:53] Read previous papers.
[12.02.2026 17:53] Get feed.
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10604
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11144
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04935
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11124
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10560
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10177
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08253
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10622
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08711
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10975
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11008
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11089
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10224
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11149
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10609
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07106
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09514
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08099
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10999
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09713
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10231
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09901
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10179
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08030
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02192
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10748
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08489
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07954
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09014
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07900
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06008
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10652
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08995
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11137
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10870
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10778
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10699
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08741
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08052
[12.02.2026 17:53] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03773
[12.02.2026 17:53] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.02.2026 17:53] No deleted papers detected.
[12.02.2026 17:53] Downloading and parsing papers (pdf, html). Total: 40.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10604.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10604.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10604.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.11144.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.11144.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.11144.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.04935.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.04935.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.04935.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.11124.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.11124.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.11124.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10560.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10560.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10560.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10177.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10177.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10177.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08253.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08253.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08253.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10622.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10622.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10622.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08711.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08711.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08711.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10975.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10975.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10975.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.11008.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.11008.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.11008.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.11089.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.11089.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.11089.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10224.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10224.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10224.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.11149.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.11149.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.11149.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10609.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10609.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10609.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.07106.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.07106.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.07106.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.09514.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.09514.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.09514.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08099.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08099.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08099.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10999.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10999.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10999.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.09713.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.09713.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.09713.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10231.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10231.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10231.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.09901.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.09901.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.09901.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10179.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10179.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10179.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08030.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08030.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08030.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.02192.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.02192.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.02192.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10748.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10748.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10748.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08489.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08489.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08489.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.07954.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.07954.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.07954.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.09014.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.09014.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.09014.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.07900.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.07900.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.07900.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.06008.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.06008.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.06008.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10652.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10652.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10652.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08995.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08995.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08995.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.11137.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.11137.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.11137.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10870.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10870.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10870.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10778.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10778.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10778.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.10699.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.10699.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.10699.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08741.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08741.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08741.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.08052.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.08052.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.08052.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Downloading and parsing paper https://huggingface.co/papers/2602.03773.
[12.02.2026 17:53] Extra JSON file exists (./assets/json/2602.03773.json), skip PDF parsing.
[12.02.2026 17:53] Paper image links file exists (./assets/img_data/2602.03773.json), skip HTML parsing.
[12.02.2026 17:53] Success.
[12.02.2026 17:53] Enriching papers with extra data.
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 0. Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.  					AI-generated summary 				 We introduce Step 3.5 Flash, ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 1. GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.  					AI-generated summary 				 Unified Multimodal Models (UMMs) have...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 2. A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.  					AI-generated summary 				 Adapting LLM agents to domain-specific tool calling remai...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 3. PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.  					AI-generated summary 				 With the rapid development of large multimodal models, reliable judge and critic models have become essential f...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 4. GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.  					AI-generated summary 				 While reasoning over long context is crucial for various real-world ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 5. Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.  					AI-gen...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 6. A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.  					AI-generated summary 				 While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), ex...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 7. Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  					AI-generated summary 			...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 8. Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.  					AI-generated summary 				 This paper proposes Omni Dense Captioning, a novel task d...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 9. FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.  					AI-generated summary 				 Agents powered by large language models (LLMs) are increasingly adopted in the sof...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 10. ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free mod...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 11. DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.  					AI-generated summary 				 In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-q...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 12. Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 13. Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.  					AI-generated summary 				 Supervised fine-tuning (SFT) on cha...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 14. Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.  					AI-generated summary 				 Reinforcement learning...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 15. Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.  					AI-generated summary 				 Omni-modal large language models (O...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 16. EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.  					AI-generated summary 				 Long-horizon planning is widely recognized as a core capability...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 17. Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  					AI-generated summary 				 Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 18. CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.  					AI-generated summary 				 Agentic coding requires agents to effectively in...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 19. Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.  					AI-generated summary 				 Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 20. Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.  					AI-generated summary 				 Group Relative Policy Optimization (GRPO) assigns ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 21. A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.  					AI-generated summary 				 Query Processing (QP) bridges user in...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 22. Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.  					AI-generated summary 				 Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vis...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 23. Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.  					AI-generated summary 				 Reasoning models enhance problem-solving by scaling test-time comput...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 24. ECHO-2 is a distributed reinforcement learning framework that enables efficient post-training of large language models by overlapping rollout generation, dissemination, and training while managing policy staleness and network latency.  					AI-generated summary 				 Reinforcement learning (RL) is a ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 25. Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.  					AI-generated summary 				 Knowledge Graphs (KGs) store structured factual know...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 26. Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) ha...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 27. Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.  					AI-generated summary 				 As Large Language Models (LLMs) become increasingly deployed in Polish l...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 28. ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.  					AI-generated summary 				 Diffusion models have achieved remarkable generation q...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 29. Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods.  					AI-generated summary 				 Large Language Model (LLM) code agents increasingly resolve repository-level issues by iterat...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 30. AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 31. A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.  					AI-generated summary 				 Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-base...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 32. Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.  					AI-generated summary 				 Computer-use agents (CUAs) have made tremendou...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 33. Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.  					AI-generated summary 				 The prevailing paradigm in large language model (LLM) development is to pretrain a ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 34. FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.  					AI-generated summary 				 Federated Learning (FL) enables ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 35. GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for ...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 36. V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.  					AI-generated summary 				 Generative recommendation via autoregressive models has unified retrieval and ran...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 37. Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 38. A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  					AI-generated summary 				 The Unrelated Parallel Machine Scheduling P...
[12.02.2026 17:53] ********************************************************************************
[12.02.2026 17:53] Abstract 39. RC, an iterative decoding algorithm, enables large language models to extrapolate and continuously improve beyond training budgets by constructing reasoning chains that enhance across iterations, achieving superior performance on long-horizon tasks.  					AI-generated summary 				 Large Language Mod...
[12.02.2026 17:53] Read previous papers.
[12.02.2026 17:53] Generating reviews via LLM API.
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#architecture", "#agents", "#benchmark", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –∞–≥–µ–Ω—Ç–Ω—ã–π –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Step 3.5 Flash, –æ—Ç–Ω–æ—Å—è—â–∞—è—Å—è –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts, –∫–æ—Ç–æ—Ä–∞
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ç –ø–∞–º—è—Ç–∏ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –æ—Ü–µ–Ω–æ—á–Ω–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ GENIUS –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞–¥
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#small_models", "#inference", "#training", "#agents"], "emoji": "üéØ", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è—é—â–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Activation Steering Adapter (ASA) –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#rlhf", "#robotics", "#multimodal", "#alignment", "#benchmark", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —á—É—Ç—å—ë–º: —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", "desc": "PhyCritic ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–∞—è –ø–∞–º—è—Ç—å —Å —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º–∏ –∑–∞—Ç–≤–æ—Ä–∞–º–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "GRU-Mem —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –≤ –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ç–µ–∫—Å—Ç–æ–º –∑–∞—Ç–≤–æ—Ä—ã, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#science", "#training", "#reasoning", "#agents", "#math", "#open_source"], "emoji": "üßÆ", "ru": {"title": "–û—Ç –æ–ª–∏–º–ø–∏–∞–¥ –∫ –Ω–∞—É—á–Ω—ã–º –æ—Ç–∫—Ä—ã—Ç–∏—è–º: AI-–∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "Aletheia ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–µ—Ä—Å–∏—é 
[12.02.2026 17:53] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è G-LNS, –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#open_source", "#optimization"], "emoji": "üë§", "ru": {"title": "–ú—è–≥–∫–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –∫–∞—á–µ—Å—Ç
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#video", "#multimodal", "#training", "#reasoning", "#benchmark", "#audio", "#dataset", "#open_source"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –æ–ø–∏—Å—ã–≤–∞—Ç—å –≤–∏–¥–µ–æ –∫–∞–∫ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—Å—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Omni Dense Captioning ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#leakage", "#plp", "#open_source", "#benchmark", "#agents", "#optimization", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –±–∞–≥–æ–≤ –∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "FeatureBench ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–º—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üöÄ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é", "desc": "ROCKET ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –ø–æ —Å–ª–æ—è–º –∫–∞–∫ –∑–∞–¥–∞—á—É –º–Ω–æ–≥–æ–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ —Ä—é–∫–∑–∞–∫
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#synthetic", "#data", "#training", "#optimization", "#benchmark", "#rl"], "emoji": "üë®‚Äçüç≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫—É–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è DataChef-32B ‚Äî —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—Ü–µ–ø—Ç–æ
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö: –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏", "desc": "Meta-Experience Learning ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#reasoning", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: –∫–∞–∫ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–µ–ª–∞—é—Ç —É–º–Ω–µ–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–µ—Ç–æ–¥–æ–º supervised fine-tuning –Ω–∞
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üéØ", "ru": {"title": "–ö–∞–ª–º–∞–Ω–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Online Causal Kalman Filtering –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –ø—Ä–∏ importance samplin
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#audio", "#3d", "#architecture", "#dataset"], "emoji": "üé≠", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ—á—å –∏ –º–∏–º–∏–∫–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –æ—Ç –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "Ex-Omni ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, 
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#agents", "#reasoning"], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EcoGym ‚Äî —ç—Ç–æ –æ–±–æ–±—â—ë–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-based –∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#rag", "#video", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ–ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏
[12.02.2026 17:53] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é –∏—Å—Ç–æ—Ä–∏–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLI-Gym, —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π, –ø—É—Ç–µ–º –∏–º–∏—Ç–∞—Ü–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ
[12.02.2026 17:53] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–û—Ç —à—Ç—Ä–∏—Ö–∞ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π", "desc": "Stroke3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –∏–∑ –¥–≤—É–º–µ—Ä–Ω—ã—Ö —Ä–∏—Å—É–Ω–∫–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π pipeline. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∞—Ä
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rlhf"], "emoji": "üß©", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Blockwise Advantage Estimation - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä
[12.02.2026 17:53] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QP-OneModel ‚Äî –µ–¥–∏–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥–∑
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#security", "#ethics", "#benchmark"], "emoji": "üé®", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏: –∑–∞—â–∏—Ç–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç –∞—Ç–∞–∫ —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –∞—Ç–∞–∫ –Ω–∞ –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#training", "#long_context", "#reasoning", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –∑–∞–±—ã–≤–∞—Ç—å –Ω–µ–Ω—É–∂–Ω–æ–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Free()LM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–≤–æ–¥—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∑–∞–±—ã–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä Free-Module 
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "ECHO-2 ‚Äî —ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#rag"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π: –ø–æ—á–µ–º—É LLM —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ FactCheck –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∞–∫—Ç—ã –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π –ø–æ —Ç—Ä—ë–º –Ω–∞–ø—Ä–∞–≤
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–ö—Ä–µ–ø–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RLTR, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#dataset", "#benchmark", "#ethics", "#small_models", "#multilingual"], "emoji": "üõ°Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è LLM –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Bielik Guard ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#diffusion", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArcFlow ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Ç–æ
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#agents", "#plp", "#benchmark", "#reasoning"], "emoji": "üêõ", "ru": {"title": "–¢–µ—Å—Ç—ã –≤ –∫–æ–¥–µ –Ω–µ –≤—Å–µ–≥–¥–∞ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã ‚Äî –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ", "desc": "–í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –∫–æ–¥–∞, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –∏—Ç–µ—Ä–∞—Ç–∏–≤
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents"], "emoji": "ü§ù", "ru": {"title": "LLM-–∞–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –≤–µ—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã —á–µ—Ä–µ–∑ —è–∑—ã–∫", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫-—Å–∏—Å—Ç–µ–º–∞ AgenticPay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –°–∏—Å—Ç–µ
[12.02.2026 17:53] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∞—è—Å—è –ø–∞–º—è—Ç—å: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è UMEM ‚Äî –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –≤ –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#agents", "#security", "#alignment", "#benchmark", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ DeAction –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –≤
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#training"], "emoji": "üéØ", "ru": {"title": "–ë–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî –ª—É—á—à–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Ä–æ–ª—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏. 
[12.02.2026 17:53] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–ü—Ä–∏–≤–∞—Ç–Ω–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "FedPS ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –º–µ—Ç–æ–¥—ã —ç—Å–∫–∏–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#optimization", "#inference", "#security", "#plp", "#interpretability", "#training"], "emoji": "üîí", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–¥ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω–æ-–∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ LLM", "desc": "GoodVibe ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞ –∫–æ–¥–µ, —á–µ—Ä–µ–∑ —Ü
[12.02.2026 17:53] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ-–Ω–∞–≥—Ä–∞–¥–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", "desc": "V-STAR ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–≥—Ä–∞–¥–æ–π –≤ –º–æ–¥–µ–ª—è—Ö —Å —É—Å–∏–ª–µ–Ω–Ω—ã–º –æ
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#training", "#security", "#architecture", "#alignment"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –≤ MoE –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —É—è–∑–≤–∏–º–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä Mixture-of-Experts (MoE) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –≥–¥–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–µ–∑–æ–ø–∞
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#graphs", "#optimization", "#architecture", "#benchmark", "#rlhf"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –∏
[12.02.2026 17:53] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#rl", "#small_models"], "emoji": "üîÑ", "ru": {"title": "–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RC ‚Äî —ç—Ç–æ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —É–ª—É—á—à
[12.02.2026 17:53] Renaming data file.
[12.02.2026 17:53] Renaming previous data. hf_papers.json to ./d/2026-02-12.json
[12.02.2026 17:53] Saving new data file.
[12.02.2026 17:53] Generating page.
[12.02.2026 17:53] Renaming previous page.
[12.02.2026 17:53] Renaming previous data. index.html to ./d/2026-02-12.html
[12.02.2026 17:53] Writing result.
[12.02.2026 17:53] Renaming log file.
[12.02.2026 17:53] Renaming previous data. log.txt to ./logs/2026-02-12_last_log.txt
