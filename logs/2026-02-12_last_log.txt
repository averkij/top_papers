[12.02.2026 14:07] Read previous papers.
[12.02.2026 14:07] Generating top page (month).
[12.02.2026 14:07] Writing top page (month).
[12.02.2026 15:44] Read previous papers.
[12.02.2026 15:44] Get feed.
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10604
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11144
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04935
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11124
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10560
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08253
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10177
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08711
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10975
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11008
[12.02.2026 15:44] Extract page data from URL. URL: https://huggingface.co/papers/2602.10622
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11089
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11149
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10609
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10224
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07106
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09514
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10999
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09713
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10231
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09901
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10179
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08030
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10748
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08489
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07954
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09014
[12.02.2026 15:44] Extract page data from URL. URL: https://huggingface.co/papers/2602.08099
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06008
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10652
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07900
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11137
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10870
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10778
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10699
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08995
[12.02.2026 15:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08741
[12.02.2026 15:44] Extract page data from URL. URL: https://huggingface.co/papers/2602.08052
[12.02.2026 15:44] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.02.2026 15:44] No deleted papers detected.
[12.02.2026 15:44] Downloading and parsing papers (pdf, html). Total: 38.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10604.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10604.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10604.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.11144.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.11144.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.11144.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.04935.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.04935.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.04935.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.11124.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.11124.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.11124.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10560.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10560.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10560.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08253.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.08253.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.08253.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10177.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10177.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10177.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08711.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.08711.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.08711.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10975.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10975.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10975.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.11008.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.11008.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.11008.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10622.
[12.02.2026 15:44] Downloading paper 2602.10622 from https://arxiv.org/pdf/2602.10622v1...
[12.02.2026 15:44] Extracting affiliations from text.
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 1 1 ] . [ 1 2 2 6 0 1 . 2 0 6 2 : r How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning Jiahao Yuan1,2, Yike Xu1, Jinyong Wen1, Baokun Wang1,, Yang Chen1, Xiaotong Lin1, Wuliang Huang1, Ziyi Gao1, Xing Fu1, Yu Cheng1, Weiqiang Wang1 1DeepFind Team, Ant Group, 2East China Normal University Corresponding author, Jiahao Yuan and Yike Xu contributed equally to this work. The work was completed during Jiahao(ECNU)s internship at Ant Group. jhyuan.cs@gmail.com Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user In this work, we conduct systematic study of embeddings remains underexplored. causal, hybrid, and bidirectional attention masks within unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, gradient-based pre-warmup applied before linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM. Correspondence: yike.wbk@antgroup.com User embeddings integrate large-scale heterogeneous signals including textual profiles, interaction histories, and tabular attributes into compact representations that enable robus"
[12.02.2026 15:44] Response: ```python
["Ant Group", "East China Normal University"]
```
[12.02.2026 15:44] Deleting PDF ./assets/pdf/2602.10622.pdf.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.11089.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.11089.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.11089.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.11149.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.11149.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.11149.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10609.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10609.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10609.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10224.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10224.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10224.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.07106.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.07106.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.07106.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.09514.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.09514.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.09514.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10999.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10999.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10999.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.09713.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.09713.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.09713.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10231.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10231.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10231.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.09901.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.09901.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.09901.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10179.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10179.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10179.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08030.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.08030.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.08030.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10748.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10748.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10748.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08489.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.08489.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.08489.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.07954.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.07954.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.07954.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.09014.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.09014.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.09014.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08099.
[12.02.2026 15:44] Downloading paper 2602.08099 from https://arxiv.org/pdf/2602.08099v1...
[12.02.2026 15:44] Extracting affiliations from text.
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 ] . [ 1 9 9 0 8 0 . 2 0 6 2 : r VidVec: Unlocking Video MLLM Embeddings for VideoText Retrieval Issar Tzachor 1 Dvir Samuel 1 Rami Ben-Ari "
[12.02.2026 15:44] Response: ```python
[]
```
[12.02.2026 15:44] Extracting affiliations from text.
[12.02.2026 15:44] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 ] . [ 1 9 9 0 8 0 . 2 0 6 2 : r VidVec: Unlocking Video MLLM Embeddings for VideoText Retrieval Issar Tzachor 1 Dvir Samuel 1 Rami Ben-AriRecent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for videotext embedding and retrieval. We first conduct systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related videotext embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by substantial margin, achieving state-of-the-art results across common video retrieval benchmarks. 1. Introduction Multimodal Large Language Models (MLLMs) have recently emerged as dominant paradigm in visionlanguage understanding, demonstrating strong performance across tasks such as captioning (Jia et al., 2024), visual question answering (Awadalla et al., 2023), and even visual mathematical reasoning (Chen et al., 2025). common design represents visual input as sequences of visual tokens that are fed into generative Large Language Model (LLM) which, after joint fine-tuning, enables open-world reasoning and instruction following over multimodal content. Extending this paradigm from images to videos has led to rapid progress in video-based MLLMs (Zhang et al., 2024b; Cheng et al., 1OriginAI, Israel. Correspondence to: Issar Tzachor <iyttor@gmail.com>. Preprint. February 10, 2026. 1 2024; Zhang et al., 2025a; Tang et al., 2025; Yang et al., 2025). Compared to images, videos introduce richer content and temporal dynamics, substantially increasing the challenge of representation learning, reasoning, and retrieval. As MLLMs become increasingly widespread, multimodal representation learning has attracted growing research interest. Early vision-language models (VLMs) such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), and BLIP (Li et al., 2022) established powerful dual-encoder recipe for text-image representations by aligning image and text embeddings with contrastive learning on large-scale imagetext data. Follow-up studies suggest extensions of these models to video embedding and retrieval (Luo et al., 2021; Ma et al., 2022; Xu et al., 2021). Recent large Video Foundation Models (VFMs) have pushed zero-shot and transfer performance by scaling videotext pretraining. InternVideo2 (Wang et al., 2024) has pretrained 100M videotext pairs to reach superior performance, while VideoPrism (Zhao et al., 2024) contrastively trained with 600M pairs. Recent studies suggest that simply scaling videotext pretraining does not uniformly improve performance (Feichtenhofer et al., 2022; Tong et al., 2022; Uselis et al., 2025) motivating alternative approaches that focus on representation quality and task-related alignment, rather than relying solely on ever-larger corpora. Recently, an emerging line of work investigates whether MLLMs can serve as representation learners across visiontext modalities, motivated by the state-of-the-art performance of LLM-based embedding models on MTEB (Muennighoff et al., 2023). E5-V (Jiang et al., 2024) suggested refining MLLMs language component using textual supervision to learn image-text aligned embeddings. Subsequent methods (Jiang et al., 2025; Lin et al., 2025) convert MLLMs into embedding models by incorporating visiontext paired data into contrastive training. To overcome the limited scale of curated embedding datasets, MegaPairs (Zhou et al., 2024) and UniIR (Wei et al., 2024) propose large-scale training datasets. Although recent methods report impressive results on image retrieval (Zhang et al., 2025b; Thirukovalluru et al., 2025; Kong et al., 2025; Liu et al., 2025; Gu et al., 2026), video is often treated as an auxiliary modality rather than primary focus, and performance on video retrieval remains VidVec: Unlocking Video MLLM Embeddings for VideoText Retrieval Figure 1. An overview of VidVec. (a) Zero-shot retrieval: extract video and text embeddings from an intermediate MLLM layer for initial ranking. (b) Zero-shot reranking: leverage the calibrated MLLM head for pairwise scoring to rerank top-K candidates. (c) In-context optimization: lightweight model alignment using only 60K text-only pairs for embedding extraction via text-to-text mapping from dense video captions to short summaries, designed to mirror the videotext inference setup. behind that of dedicated Video Foundation Models (VFMs) (Wang et al., 2024; Liu et al., 2022; Zhu et al., 2023; Lan et al., 2025b). The few efforts that explicitly target video have yet to achieve strong results on standard videotext retrieval benchmarks. CARE (Xu et al., 2024) emphasizes fine-grained captioning and retrieval but underperforms in conventional retrieval settings, while VLM2Vec-V2 (Meng et al., 2025) incorporates videotext pairs during training yet reports substantially lower performance, even compared to earlier MLLM-based embedders (Kong et al., 2025). Overview of our approach. In this work, we study MLLMs as embedding extractors specifically for videotext retrieval. We show that off-the-shelf MLLMs encode substantial retrieval-relevant information in their hidden representations. systematic layer-wise analysis reveals that selecting appropriate intermediate layers already yield strong zero-shot retrieval performance (Fig. 1a). Further gains are obtained by re-ranking with calibrated MLLM head (Fig. 1b). Finally, we propose an efficient in-context optimization scheme that maps dense video captions to short summaries, enabling task-related embedding learning without visual inputs (Fig. 1c). Using only 60K text-only in-context pairs, we outperform trained MLLM embedders and video foundation models trained on orders of magnitude more videotext data. The key contributions of this work are: 1. We propose methodology to assess and exploit hidden representations o"
[12.02.2026 15:44] Mistral response. {"id": "4c1ed6688c98490ab281d2c463b25847", "created": 1770911066, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1612, "total_tokens": 1623, "completion_tokens": 11, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"OriginAI, Israel\"]\n```"}}]}
[12.02.2026 15:44] Response: ```python
["OriginAI, Israel"]
```
[12.02.2026 15:44] Deleting PDF ./assets/pdf/2602.08099.pdf.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.06008.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.06008.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.06008.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10652.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10652.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10652.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.07900.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.07900.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.07900.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.11137.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.11137.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.11137.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10870.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10870.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10870.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10778.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10778.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10778.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.10699.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.10699.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.10699.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08995.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.08995.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.08995.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08741.
[12.02.2026 15:44] Extra JSON file exists (./assets/json/2602.08741.json), skip PDF parsing.
[12.02.2026 15:44] Paper image links file exists (./assets/img_data/2602.08741.json), skip HTML parsing.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Downloading and parsing paper https://huggingface.co/papers/2602.08052.
[12.02.2026 15:44] Downloading paper 2602.08052 from https://arxiv.org/pdf/2602.08052v1...
[12.02.2026 15:44] Extracting affiliations from text.
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"GRAPH-ENHANCED DEEP REINFORCEMENT LEARNING FOR MULTI-OBJECTIVE UNRELATED PARALLEL MACHINE SCHEDULING 6 2 0 2 8 ] . [ 1 2 5 0 8 0 . 2 0 6 2 : r Bulent Soykan Institute for Simulation and Training University of Central Florida Orlando, FL, USA Bulent.Soykan@ucf.edu Sean Mondesire, Ghaith Rabadi, Grace Bochenek School of Modeling, Simulation, and Training University of Central Florida Orlando, FL, USA {Sean.Mondesire, Ghaith.Rabadi, Grace.Bochenek}@ucf.edu "
[12.02.2026 15:44] Response: ```python
["University of Central Florida"]
```
[12.02.2026 15:44] Deleting PDF ./assets/pdf/2602.08052.pdf.
[12.02.2026 15:44] Success.
[12.02.2026 15:44] Enriching papers with extra data.
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 0. Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.  					AI-generated summary 				 We introduce Step 3.5 Flash, ...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 1. GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.  					AI-generated summary 				 Unified Multimodal Models (UMMs) have...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 2. A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.  					AI-generated summary 				 Adapting LLM agents to domain-specific tool calling remai...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 3. PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.  					AI-generated summary 				 With the rapid development of large multimodal models, reliable judge and critic models have become essential f...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 4. GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.  					AI-generated summary 				 While reasoning over long context is crucial for various real-world ...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 5. A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.  					AI-generated summary 				 While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), ex...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 6. Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.  					AI-gen...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 7. Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.  					AI-generated summary 				 This paper proposes Omni Dense Captioning, a novel task d...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 8. FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.  					AI-generated summary 				 Agents powered by large language models (LLMs) are increasingly adopted in the sof...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 9. ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free mod...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 10. Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  					AI-generated summary 			...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 11. DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.  					AI-generated summary 				 In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-q...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 12. Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.  					AI-generated summary 				 Supervised fine-tuning (SFT) on cha...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 13. Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.  					AI-generated summary 				 Reinforcement learning...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 14. Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged ...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 15. Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.  					AI-generated summary 				 Omni-modal large language models (O...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 16. EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.  					AI-generated summary 				 Long-horizon planning is widely recognized as a core capability...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 17. CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.  					AI-generated summary 				 Agentic coding requires agents to effectively in...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 18. Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.  					AI-generated summary 				 Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 19. Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.  					AI-generated summary 				 Group Relative Policy Optimization (GRPO) assigns ...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 20. A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.  					AI-generated summary 				 Query Processing (QP) bridges user in...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 21. Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.  					AI-generated summary 				 Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vis...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 22. Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.  					AI-generated summary 				 Reasoning models enhance problem-solving by scaling test-time comput...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 23. Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.  					AI-generated summary 				 Knowledge Graphs (KGs) store structured factual know...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 24. Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) ha...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 25. Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.  					AI-generated summary 				 As Large Language Models (LLMs) become increasingly deployed in Polish l...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 26. ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.  					AI-generated summary 				 Diffusion models have achieved remarkable generation q...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 27. Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  					AI-generated summary 				 Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 28. AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 29. A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.  					AI-generated summary 				 Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-base...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 30. Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods.  					AI-generated summary 				 Large Language Model (LLM) code agents increasingly resolve repository-level issues by iterat...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 31. Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.  					AI-generated summary 				 The prevailing paradigm in large language model (LLM) development is to pretrain a ...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 32. FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.  					AI-generated summary 				 Federated Learning (FL) enables ...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 33. GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for ...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 34. V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.  					AI-generated summary 				 Generative recommendation via autoregressive models has unified retrieval and ran...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 35. Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.  					AI-generated summary 				 Computer-use agents (CUAs) have made tremendou...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 36. Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language...
[12.02.2026 15:44] ********************************************************************************
[12.02.2026 15:44] Abstract 37. A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  					AI-generated summary 				 The Unrelated Parallel Machine Scheduling P...
[12.02.2026 15:44] Read previous papers.
[12.02.2026 15:44] Generating reviews via LLM API.
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#architecture", "#agents", "#benchmark", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –∞–≥–µ–Ω—Ç–Ω—ã–π –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Step 3.5 Flash, –æ—Ç–Ω–æ—Å—è—â–∞—è—Å—è –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts, –∫–æ—Ç–æ—Ä–∞
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ç –ø–∞–º—è—Ç–∏ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –æ—Ü–µ–Ω–æ—á–Ω–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ GENIUS –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞–¥
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#small_models", "#inference", "#training", "#agents"], "emoji": "üéØ", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è—é—â–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Activation Steering Adapter (ASA) –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#rlhf", "#robotics", "#multimodal", "#alignment", "#benchmark", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —á—É—Ç—å—ë–º: —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", "desc": "PhyCritic ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–∞—è –ø–∞–º—è—Ç—å —Å —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º–∏ –∑–∞—Ç–≤–æ—Ä–∞–º–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "GRU-Mem —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –≤ –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ç–µ–∫—Å—Ç–æ–º –∑–∞—Ç–≤–æ—Ä—ã, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã
[12.02.2026 15:44] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è G-LNS, –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#science", "#training", "#reasoning", "#agents", "#math", "#open_source"], "emoji": "üßÆ", "ru": {"title": "–û—Ç –æ–ª–∏–º–ø–∏–∞–¥ –∫ –Ω–∞—É—á–Ω—ã–º –æ—Ç–∫—Ä—ã—Ç–∏—è–º: AI-–∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "Aletheia ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–µ—Ä—Å–∏—é 
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#video", "#multimodal", "#training", "#reasoning", "#benchmark", "#audio", "#dataset", "#open_source"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –æ–ø–∏—Å—ã–≤–∞—Ç—å –≤–∏–¥–µ–æ –∫–∞–∫ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—Å—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Omni Dense Captioning ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#leakage", "#plp", "#open_source", "#benchmark", "#agents", "#optimization", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –±–∞–≥–æ–≤ –∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "FeatureBench ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–º—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üöÄ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é", "desc": "ROCKET ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –ø–æ —Å–ª–æ—è–º –∫–∞–∫ –∑–∞–¥–∞—á—É –º–Ω–æ–≥–æ–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ —Ä—é–∫–∑–∞–∫
[12.02.2026 15:44] Querying the API.
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  					AI-generated summary 				 Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.
[12.02.2026 15:44] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ decoder-only —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Gradient-Guided Soft Masking, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥a –æ—Ç –ø—Ä–∏—á–∏–Ω–Ω–æ–≥–æ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –∫ –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–æ–º—É –≤–Ω–∏–º–∞–Ω–∏—é, —É–ª—É—á—à–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö Alipay —Å –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –º–µ–∂–¥—É —Ç–∏–ø–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á.",
  "emoji": "üë§",
  "title": "–ú—è–≥–∫–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
```
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  					AI-generated summary 				 Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM."

[12.02.2026 15:44] Response: ```python
["TRAINING", "ARCHITECTURE", "BENCHMARK"]
```
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  					AI-generated summary 				 Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM."

[12.02.2026 15:44] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[12.02.2026 15:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how different attention masking strategies affect the quality of user embeddings in decoder-only language models. It introduces a new technique called Gradient-Guided Soft Masking, which enhances training stability and representation quality by gradually allowing future attention during optimization. The study evaluates various attention masks within a contrastive learning framework using real-world user behavior data. Results show that the proposed method outperforms traditional masking strategies, leading to better user representation for tasks like prediction and marketing sensitivity.","title":"Enhancing User Embeddings with Gradient-Guided Soft Masking"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how different attention masking strategies affect the quality of user embeddings in decoder-only language models. It introduces a new technique called Gradient-Guided Soft Masking, which enhances training stability and representation quality by gradually allowing future attention during optimization. The study evaluates various attention masks within a contrastive learning framework using real-world user behavior data. Results show that the proposed method outperforms traditional masking strategies, leading to better user representation for tasks like prediction and marketing sensitivity.', title='Enhancing User Embeddings with Gradient-Guided Soft Masking'))
[12.02.2026 15:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫Ü‰∏çÂêåÊ≥®ÊÑèÂäõÊé©ËîΩÁ≠ñÁï•ÂØπËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°Âûã‰∏≠Áî®Êà∑ÂµåÂÖ•Ë¥®ÈáèÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑËΩØÊé©ËîΩÊäÄÊúØÔºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÁî®Êà∑Ë°å‰∏∫ÂàÜÊûêÁöÑË°®Á§∫Ë¥®Èáè„ÄÇÈÄöËøáÂú®Â§ßËßÑÊ®°ÁúüÂÆûÊï∞ÊçÆ‰∏äËøõË°åÂØπÊØîÂ≠¶‰π†ÔºåÊàë‰ª¨Á≥ªÁªüÂú∞Á†îÁ©∂‰∫ÜÂõ†Êûú„ÄÅÊ∑∑ÂêàÂíåÂèåÂêëÊ≥®ÊÑèÂäõÊé©ËîΩÁöÑÊïàÊûú„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®Â§ö‰∏™Áî®Êà∑ËÆ§Áü•Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Êõ¥È´òÁöÑÁ®≥ÂÆöÊÄßÂíåË¥®ÈáèÔºåÂº∫Ë∞É‰∫ÜÊé©ËîΩËÆæËÆ°ÂíåËÆ≠ÁªÉËøáÊ∏°ÁöÑÈáçË¶ÅÊÄß„ÄÇ","title":"‰ºòÂåñÁî®Êà∑ÂµåÂÖ•Ë¥®ÈáèÁöÑÊ≥®ÊÑèÂäõÊé©ËîΩÁ≠ñÁï•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫Ü‰∏çÂêåÊ≥®ÊÑèÂäõÊé©ËîΩÁ≠ñÁï•ÂØπËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°Âûã‰∏≠Áî®Êà∑ÂµåÂÖ•Ë¥®ÈáèÁöÑÂΩ±Âìç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑËΩØÊé©ËîΩÊäÄÊúØÔºå‰ª•ÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÁî®Êà∑Ë°å‰∏∫ÂàÜÊûêÁöÑË°®Á§∫Ë¥®Èáè„ÄÇÈÄöËøáÂú®Â§ßËßÑÊ®°ÁúüÂÆûÊï∞ÊçÆ‰∏äËøõË°åÂØπÊØîÂ≠¶‰π†ÔºåÊàë‰ª¨Á≥ªÁªüÂú∞Á†îÁ©∂‰∫ÜÂõ†Êûú„ÄÅÊ∑∑ÂêàÂíåÂèåÂêëÊ≥®ÊÑèÂäõÊé©ËîΩÁöÑÊïàÊûú„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÊñπÊ≥ïÂú®Â§ö‰∏™Áî®Êà∑ËÆ§Áü•Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Êõ¥È´òÁöÑÁ®≥ÂÆöÊÄßÂíåË¥®ÈáèÔºåÂº∫Ë∞É‰∫ÜÊé©ËîΩËÆæËÆ°ÂíåËÆ≠ÁªÉËøáÊ∏°ÁöÑÈáçË¶ÅÊÄß„ÄÇ', title='‰ºòÂåñÁî®Êà∑ÂµåÂÖ•Ë¥®ÈáèÁöÑÊ≥®ÊÑèÂäõÊé©ËîΩÁ≠ñÁï•'))
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#synthetic", "#data", "#training", "#optimization", "#benchmark", "#rl"], "emoji": "üë®‚Äçüç≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫—É–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è DataChef-32B ‚Äî —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—Ü–µ–ø—Ç–æ
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#reasoning", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: –∫–∞–∫ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–µ–ª–∞—é—Ç —É–º–Ω–µ–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–µ—Ç–æ–¥–æ–º supervised fine-tuning –Ω–∞
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üéØ", "ru": {"title": "–ö–∞–ª–º–∞–Ω–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Online Causal Kalman Filtering –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –ø—Ä–∏ importance samplin
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö: –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏", "desc": "Meta-Experience Learning ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#audio", "#3d", "#architecture", "#dataset"], "emoji": "üé≠", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ—á—å –∏ –º–∏–º–∏–∫–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –æ—Ç –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "Ex-Omni ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, 
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#agents", "#reasoning"], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EcoGym ‚Äî —ç—Ç–æ –æ–±–æ–±—â—ë–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-based –∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏
[12.02.2026 15:44] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é –∏—Å—Ç–æ—Ä–∏–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLI-Gym, —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π, –ø—É—Ç–µ–º –∏–º–∏—Ç–∞—Ü–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ
[12.02.2026 15:44] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–û—Ç —à—Ç—Ä–∏—Ö–∞ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π", "desc": "Stroke3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –∏–∑ –¥–≤—É–º–µ—Ä–Ω—ã—Ö —Ä–∏—Å—É–Ω–∫–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π pipeline. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∞—Ä
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rlhf"], "emoji": "üß©", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Blockwise Advantage Estimation - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä
[12.02.2026 15:44] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QP-OneModel ‚Äî –µ–¥–∏–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥–∑
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#security", "#ethics", "#benchmark"], "emoji": "üé®", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏: –∑–∞—â–∏—Ç–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç –∞—Ç–∞–∫ —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –∞—Ç–∞–∫ –Ω–∞ –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#training", "#long_context", "#reasoning", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –∑–∞–±—ã–≤–∞—Ç—å –Ω–µ–Ω—É–∂–Ω–æ–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Free()LM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–≤–æ–¥—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∑–∞–±—ã–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä Free-Module 
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#rag"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π: –ø–æ—á–µ–º—É LLM —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ FactCheck –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∞–∫—Ç—ã –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π –ø–æ —Ç—Ä—ë–º –Ω–∞–ø—Ä–∞–≤
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–ö—Ä–µ–ø–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RLTR, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#dataset", "#benchmark", "#ethics", "#small_models", "#multilingual"], "emoji": "üõ°Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è LLM –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Bielik Guard ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#diffusion", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArcFlow ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Ç–æ
[12.02.2026 15:44] Querying the API.
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  					AI-generated summary 				 Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.
[12.02.2026 15:44] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –ø–æ—Å–ª–æ–π–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—ë–≤ MLLM –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–Ω–∏ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞—á–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –ª—ë–≥–∫–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ –≤–∏–¥–µ–æ–∫–∞–ø—Ç–∏–æ–Ω—ã –≤ –∫—Ä–∞—Ç–∫–∏–µ —Ä–µ–∑—é–º–µ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –≤–∏–¥–µ–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞ –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –≤–∏–¥–µ–æ–ø–æ–∏—Å–∫–∞, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
  "emoji": "üé¨",
  "title": "–í–∏–¥–µ–æ–ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è"
}
```
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  					AI-generated summary 				 Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks."

[12.02.2026 15:44] Response: ```python
['MULTIMODAL', 'VIDEO', 'RAG', 'BENCHMARK']
```
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  					AI-generated summary 				 Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks."

[12.02.2026 15:44] Response: ```python
['INTERPRETABILITY', 'TRANSFER_LEARNING']
```

**Justification:**

1. **INTERPRETABILITY**: The paper conducts "a systematic layer-wise analysis" of MLLMs to understand which layers encode task-relevant information. This directly involves analyzing model behavior and understanding how different components contribute to the model's performance.

2. **TRANSFER_LEARNING**: The paper adapts pre-trained generative MLLMs for a new task (video-text embedding and retrieval) without visual supervision, demonstrating knowledge transfer from the original pre-training to a new domain/task.
[12.02.2026 15:44] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "TRANSFER_LEARNING"]


**Justification:**

1. **INTERPRETABILITY**: The paper conducts "a systematic layer-wise analysis" of MLLMs to understand which layers encode task-relevant information. This directly involves analyzing model behavior and understanding how different components contribute to the model"s performance.

2. **TRANSFER_LEARNING**: The paper adapts pre-trained generative MLLMs for a new task (video-text embedding and retrieval) without visual supervision, demonstrating knowledge transfer from the original pre-training to a new domain/task.
[12.02.2026 15:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how generative multimodal large language models (MLLMs) can be used for video-text embedding and retrieval without needing visual data. The authors analyze the intermediate layers of MLLMs, revealing that they contain valuable information for video tasks. By combining these intermediate embeddings with a calibrated MLLM head, the researchers achieve impressive zero-shot retrieval performance. Additionally, they propose a text-based alignment method that enhances video-text learning, leading to superior results compared to existing techniques, all without fine-tuning on visual data.","title":"Unlocking Video-Text Retrieval with MLLMs: No Visuals Needed!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how generative multimodal large language models (MLLMs) can be used for video-text embedding and retrieval without needing visual data. The authors analyze the intermediate layers of MLLMs, revealing that they contain valuable information for video tasks. By combining these intermediate embeddings with a calibrated MLLM head, the researchers achieve impressive zero-shot retrieval performance. Additionally, they propose a text-based alignment method that enhances video-text learning, leading to superior results compared to existing techniques, all without fine-tuning on visual data.', title='Unlocking Video-Text Retrieval with MLLMs: No Visuals Needed!'))
[12.02.2026 15:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®ÁîüÊàêÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâËøõË°åËßÜÈ¢ë-ÊñáÊú¨ÂµåÂÖ•ÂíåÊ£ÄÁ¥¢„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåMLLMÁöÑ‰∏≠Èó¥Â±ÇÂ∑≤ÁªèÁºñÁ†Å‰∫ÜÂ§ßÈáè‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåÂõ†Ê≠§ÂèØ‰ª•ÈÄöËøáÁªìÂêàËøô‰∫õ‰∏≠Èó¥Â±ÇÁöÑÂµåÂÖ•‰∏éÁªèËøáÊ†°ÂáÜÁöÑMLLMÂ§¥Êù•ÂÆûÁé∞Âº∫Â§ßÁöÑÈõ∂-shotÊ£ÄÁ¥¢ÊÄßËÉΩÔºåËÄåÊó†ÈúÄ‰ªª‰ΩïËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑÂü∫‰∫éÊñáÊú¨ÁöÑÂØπÈΩêÁ≠ñÁï•ÔºåÂ∞ÜÂØÜÈõÜÁöÑËßÜÈ¢ëÊ†áÈ¢òÊò†Â∞ÑÂà∞ÁÆÄÁü≠ÁöÑÊëòË¶ÅÔºå‰ªéËÄåÂú®Ê≤°ÊúâËßÜËßâÁõëÁù£ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ËßÜÈ¢ë-ÊñáÊú¨ÂµåÂÖ•Â≠¶‰π†„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â∏∏ËßÅÁöÑËßÜÈ¢ëÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ","title":"Âà©Áî®Â§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÊèêÂçáËßÜÈ¢ëÊ£ÄÁ¥¢ÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®ÁîüÊàêÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâËøõË°åËßÜÈ¢ë-ÊñáÊú¨ÂµåÂÖ•ÂíåÊ£ÄÁ¥¢„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåMLLMÁöÑ‰∏≠Èó¥Â±ÇÂ∑≤ÁªèÁºñÁ†Å‰∫ÜÂ§ßÈáè‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåÂõ†Ê≠§ÂèØ‰ª•ÈÄöËøáÁªìÂêàËøô‰∫õ‰∏≠Èó¥Â±ÇÁöÑÂµåÂÖ•‰∏éÁªèËøáÊ†°ÂáÜÁöÑMLLMÂ§¥Êù•ÂÆûÁé∞Âº∫Â§ßÁöÑÈõ∂-shotÊ£ÄÁ¥¢ÊÄßËÉΩÔºåËÄåÊó†ÈúÄ‰ªª‰ΩïËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑÂü∫‰∫éÊñáÊú¨ÁöÑÂØπÈΩêÁ≠ñÁï•ÔºåÂ∞ÜÂØÜÈõÜÁöÑËßÜÈ¢ëÊ†áÈ¢òÊò†Â∞ÑÂà∞ÁÆÄÁü≠ÁöÑÊëòË¶ÅÔºå‰ªéËÄåÂú®Ê≤°ÊúâËßÜËßâÁõëÁù£ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ËßÜÈ¢ë-ÊñáÊú¨ÂµåÂÖ•Â≠¶‰π†„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â∏∏ËßÅÁöÑËßÜÈ¢ëÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ', title='Âà©Áî®Â§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÊèêÂçáËßÜÈ¢ëÊ£ÄÁ¥¢ÊÄßËÉΩ'))
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents"], "emoji": "ü§ù", "ru": {"title": "LLM-–∞–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –≤–µ—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã —á–µ—Ä–µ–∑ —è–∑—ã–∫", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫-—Å–∏—Å—Ç–µ–º–∞ AgenticPay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –°–∏—Å—Ç–µ
[12.02.2026 15:44] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∞—è—Å—è –ø–∞–º—è—Ç—å: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è UMEM ‚Äî –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –≤ –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#agents", "#plp", "#benchmark", "#reasoning"], "emoji": "üêõ", "ru": {"title": "–¢–µ—Å—Ç—ã –≤ –∫–æ–¥–µ –Ω–µ –≤—Å–µ–≥–¥–∞ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã ‚Äî –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ", "desc": "–í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –∫–æ–¥–∞, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –∏—Ç–µ—Ä–∞—Ç–∏–≤
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#training"], "emoji": "üéØ", "ru": {"title": "–ë–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî –ª—É—á—à–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Ä–æ–ª—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏. 
[12.02.2026 15:44] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–ü—Ä–∏–≤–∞—Ç–Ω–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "FedPS ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –º–µ—Ç–æ–¥—ã —ç—Å–∫–∏–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#optimization", "#inference", "#security", "#plp", "#interpretability", "#training"], "emoji": "üîí", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–¥ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω–æ-–∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ LLM", "desc": "GoodVibe ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞ –∫–æ–¥–µ, —á–µ—Ä–µ–∑ —Ü
[12.02.2026 15:44] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ-–Ω–∞–≥—Ä–∞–¥–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", "desc": "V-STAR ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–≥—Ä–∞–¥–æ–π –≤ –º–æ–¥–µ–ª—è—Ö —Å —É—Å–∏–ª–µ–Ω–Ω—ã–º –æ
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#agents", "#security", "#alignment", "#benchmark", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ DeAction –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –≤
[12.02.2026 15:44] Using data from previous issue: {"categories": ["#training", "#security", "#architecture", "#alignment"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –≤ MoE –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —É—è–∑–≤–∏–º–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä Mixture-of-Experts (MoE) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –≥–¥–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–µ–∑–æ–ø–∞
[12.02.2026 15:44] Querying the API.
[12.02.2026 15:44] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  					AI-generated summary 				 The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.
[12.02.2026 15:45] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Proximal Policy Optimization (PPO) –∏ –≥—Ä–∞—Ñ–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (GNN) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω–∞—Ö. –ì—Ä–∞—Ñ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∫–æ–¥–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–¥–∞—á, –º–∞—à–∏–Ω –∏ –ø–µ—Ä–µ–Ω–∞–ª–∞–¥–æ–∫, –ø–æ–∑–≤–æ–ª—è—è –∞–≥–µ–Ω—Ç—É PPO –æ–±—É—á–∏—Ç—å—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—é –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–µ–Ω–∞–ª–∞–¥–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –º–µ—Ç–∞—ç–≤—Ä–∏—Å—Ç–∏–∫–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.",
  "emoji": "‚öôÔ∏è",
  "title": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞"
}
```
[12.02.2026 15:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  					AI-generated summary 				 The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling."

[12.02.2026 15:45] Response: ```python
["RL", "RLHF", "ARCHITECTURE", "BENCHMARK"]
```
[12.02.2026 15:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  					AI-generated summary 				 The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling."

[12.02.2026 15:45] Response: ```python
['GRAPHS', 'OPTIMIZATION', 'REASONING']
```
[12.02.2026 15:45] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a Deep Reinforcement Learning framework that integrates Proximal Policy Optimization (PPO) with Graph Neural Networks (GNN) to tackle the Unrelated Parallel Machine Scheduling Problem (UPMSP). The challenge lies in effectively minimizing two conflicting objectives: Total Weighted Tardiness (TWT) and Total Setup Time (TST). By utilizing a GNN, the framework captures the intricate relationships between jobs, machines, and setups, enabling the PPO agent to develop an efficient scheduling policy. Experimental results indicate that this approach significantly outperforms traditional methods, offering a better balance between TWT and TST in complex manufacturing environments.","title":"Balancing Tardiness and Setup Time with Deep Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a Deep Reinforcement Learning framework that integrates Proximal Policy Optimization (PPO) with Graph Neural Networks (GNN) to tackle the Unrelated Parallel Machine Scheduling Problem (UPMSP). The challenge lies in effectively minimizing two conflicting objectives: Total Weighted Tardiness (TWT) and Total Setup Time (TST). By utilizing a GNN, the framework captures the intricate relationships between jobs, machines, and setups, enabling the PPO agent to develop an efficient scheduling policy. Experimental results indicate that this approach significantly outperforms traditional methods, offering a better balance between TWT and TST in complex manufacturing environments.', title='Balancing Tardiness and Setup Time with Deep Reinforcement Learning'))
[12.02.2026 15:45] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàPPOÔºâÂíåÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Â§öÁõÆÊ†áË∞ÉÂ∫¶ÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÊúâÊïàÂπ≥Ë°°‰∫ÜÊÄªÂä†ÊùÉÂª∂ËøüÔºàTWTÔºâÂíåÊÄªËÆæÁΩÆÊó∂Èó¥ÔºàTSTÔºâÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôê„ÄÇGNNËÉΩÂ§üÊúâÊïàË°®Á§∫‰Ωú‰∏ö„ÄÅÊú∫Âô®ÂíåËÆæÁΩÆÁöÑÂ§çÊùÇÁä∂ÊÄÅÔºå‰ΩøÂæóPPO‰ª£ÁêÜËÉΩÂ§üÂ≠¶‰π†Áõ¥Êé•ÁöÑË∞ÉÂ∫¶Á≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPPO-GNN‰ª£ÁêÜÂú®Âü∫ÂáÜÂÆû‰æã‰∏äÊòæËëó‰ºò‰∫éÊ†áÂáÜË∞ÉÂ∫¶ËßÑÂàôÂíåÂÖÉÂêØÂèëÂºèÁÆóÊ≥ïÔºåÊèê‰æõ‰∫ÜÂ§çÊùÇÂà∂ÈÄ†Ë∞ÉÂ∫¶ÁöÑÂº∫Â§ß‰∏îÂèØÊâ©Â±ïÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ","title":"Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÂ§öÁõÆÊ†áË∞ÉÂ∫¶‰ºòÂåñ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËøëÁ´ØÁ≠ñÁï•‰ºòÂåñÔºàPPOÔºâÂíåÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Â§öÁõÆÊ†áË∞ÉÂ∫¶ÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÊúâÊïàÂπ≥Ë°°‰∫ÜÊÄªÂä†ÊùÉÂª∂ËøüÔºàTWTÔºâÂíåÊÄªËÆæÁΩÆÊó∂Èó¥ÔºàTSTÔºâÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôê„ÄÇGNNËÉΩÂ§üÊúâÊïàË°®Á§∫‰Ωú‰∏ö„ÄÅÊú∫Âô®ÂíåËÆæÁΩÆÁöÑÂ§çÊùÇÁä∂ÊÄÅÔºå‰ΩøÂæóPPO‰ª£ÁêÜËÉΩÂ§üÂ≠¶‰π†Áõ¥Êé•ÁöÑË∞ÉÂ∫¶Á≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPPO-GNN‰ª£ÁêÜÂú®Âü∫ÂáÜÂÆû‰æã‰∏äÊòæËëó‰ºò‰∫éÊ†áÂáÜË∞ÉÂ∫¶ËßÑÂàôÂíåÂÖÉÂêØÂèëÂºèÁÆóÊ≥ïÔºåÊèê‰æõ‰∫ÜÂ§çÊùÇÂà∂ÈÄ†Ë∞ÉÂ∫¶ÁöÑÂº∫Â§ß‰∏îÂèØÊâ©Â±ïÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ', title='Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÂ§öÁõÆÊ†áË∞ÉÂ∫¶‰ºòÂåñ'))
[12.02.2026 15:45] Renaming data file.
[12.02.2026 15:45] Renaming previous data. hf_papers.json to ./d/2026-02-12.json
[12.02.2026 15:45] Saving new data file.
[12.02.2026 15:45] Generating page.
[12.02.2026 15:45] Renaming previous page.
[12.02.2026 15:45] Renaming previous data. index.html to ./d/2026-02-12.html
[12.02.2026 15:45] Writing result.
[12.02.2026 15:45] Renaming log file.
[12.02.2026 15:45] Renaming previous data. log.txt to ./logs/2026-02-12_last_log.txt
