[12.02.2026 21:28] Read previous papers.
[12.02.2026 21:28] Generating top page (month).
[12.02.2026 21:28] Writing top page (month).
[12.02.2026 22:24] Read previous papers.
[12.02.2026 22:24] Get feed.
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10604
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11124
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11144
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04935
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10177
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10560
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08253
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10622
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08711
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10975
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11008
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10224
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11089
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11149
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10609
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07106
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09514
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08099
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11103
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10999
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09713
[12.02.2026 22:24] Extract page data from URL. URL: https://huggingface.co/papers/2602.10367
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10231
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09901
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02192
[12.02.2026 22:24] Extract page data from URL. URL: https://huggingface.co/papers/2602.10229
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10179
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08030
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10748
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08489
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07954
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09014
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07900
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06008
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10652
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08995
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11137
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10870
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10778
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10699
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08741
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08052
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03773
[12.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08934
[12.02.2026 22:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.02.2026 22:24] No deleted papers detected.
[12.02.2026 22:24] Downloading and parsing papers (pdf, html). Total: 44.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10604.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10604.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10604.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.11124.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.11124.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.11124.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.11144.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.11144.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.11144.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.04935.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.04935.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.04935.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10177.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10177.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10177.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10560.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10560.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10560.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.08253.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.08253.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.08253.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10622.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10622.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10622.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.08711.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.08711.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.08711.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10975.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10975.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10975.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.11008.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.11008.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.11008.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10224.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10224.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10224.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.11089.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.11089.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.11089.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.11149.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.11149.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.11149.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10609.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10609.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10609.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.07106.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.07106.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.07106.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.09514.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.09514.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.09514.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.08099.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.08099.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.08099.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.11103.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.11103.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.11103.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10999.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10999.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10999.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.09713.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.09713.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.09713.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10367.
[12.02.2026 22:24] Downloading paper 2602.10367 from https://arxiv.org/pdf/2602.10367v1...
[12.02.2026 22:24] Extracting affiliations from text.
[12.02.2026 22:24] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LiveMedBench: Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation Dingjie Song Lehigh University Bethlehem, PA, USA Zhiling Yan Lehigh University Bethlehem, PA, USA Zhe Fang Harvard University Boston, MA, USA 6 2 0 2 0 1 ] A . [ 1 7 6 3 0 1 . 2 0 6 2 : r Yisheng Ji Imperial College London London, United Kingdom Xiang Li Massachusetts General Hospital and Harvard Medical School Boston, MA, USA Quanzheng Li Massachusetts General Hospital and Harvard Medical School Boston, MA, USA Lichao Sun Lehigh University Bethlehem, PA, USA Abstract The deployment of Large Language Models (LLMs) in highstakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose MultiAgent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702"
[12.02.2026 22:24] Response: ```python
[
    "Lehigh University",
    "Harvard University",
    "Imperial College London",
    "Massachusetts General Hospital",
    "Harvard Medical School"
]
```
[12.02.2026 22:24] Deleting PDF ./assets/pdf/2602.10367.pdf.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10231.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.10231.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.10231.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.09901.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.09901.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.09901.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02192.
[12.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02192.json), skip PDF parsing.
[12.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02192.json), skip HTML parsing.
[12.02.2026 22:24] Success.
[12.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.10229.
[12.02.2026 22:24] Downloading paper 2602.10229 from https://arxiv.org/pdf/2602.10229v1...
[12.02.2026 22:25] Extracting affiliations from text.
[12.02.2026 22:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Weihao Liu 1 Dehai Min 1 Lu Cheng "
[12.02.2026 22:25] Response: ```python
[]
```
[12.02.2026 22:25] Extracting affiliations from text.
[12.02.2026 22:25] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens Weihao Liu 1 Dehai Min 1 Lu ChengWhile explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces ContextPrediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy. 2 6 2 0 2 0 1 ] . [ 1 9 2 2 0 1 . 2 0 6 2 : r 1. Introduction The capability of Large Language Models (LLMs) to perform multi-step reasoning has largely depended on generating explicit text steps, known as Chain-of-Thought (CoT) 1Department of Computer Science, University of Illinois Chicago, Chicago, IL, USA. Correspondence to: Weihao Liu <wliu681@uic.edu>, Lu Cheng <lucheng@uic.edu>. Preprint. February 12, 2026. 2Repository URL. 1 Figure 1. Comparison of reasoning paradigms. Explicit CoT verbalizes all steps as text tokens. Coconut uses fixed number of latent tokens from hidden states. Soft-Thinking constructs latent tokens via probability-weighted interpolation with entropy-based stopping. Assistant-based methods rely on external models. Our LT-Tuning dynamically interleaves text and latent tokens through confidence-driven insertion and Context-Prediction Fusion. (Wei et al., 2022; Chen et al., 2023). Although effective, this approach requires the model to perform reasoning in discrete token sequence, which means the model can not think twice before acting, or they demand extra cost on extremely long text output (Jaech et al., 2024; Yeo et al., 2025; Guo et al., 2025; Seed et al., 2025) and self-reflection (Renze & Guven, 2024; Kang et al., 2025; Yu et al., 2025). Motivated by these limitations, recent work has explored reasoning in continuous latent spaces as an alternative (Zhu et al., 2025a; Chen et al., 2025). By allowing models to reason directly in high-dimensional hidden states rather than explicit tokens (Hao et al., 2024; Shen et al., 2025; Wei Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens et al., 2025), this line of research aims to decouple internal reasoning from explicit text generation. While promising, latent-space reasoning methods face two fundamental challenges: Constructing well-aligned latent representations. Latent tokens must be semantically expressive while remaining compatible with the models internal embedding space. Methods relying on external assistant models (Xu et al., 2025; He et al., 2025) struggle with representational misalignment, whereas purely intrinsic approaches (Chen et al., 2025) risk distribution mismatch between input embeddings and output hidden statesparticularly in models with untied input and output embeddingswhich can lead to instability or feature collapse. Adapting reasoning cost dynamically. Most existing methods employ static reasoning schedules, ignoring the fact that step difficulty varies. This fixed allocation is often inefficient, as it wastes computation on trivial steps while failing to provide sufficient depth for complex reasoning. To address these challenges, we propose Latent Thoughts Tuning (LT-Tuning), framework that enables LLMs to perform robust latent reasoning without external assistants. An illustration of the difference between our method and mainstream methods in latent reasoning is visualized in Figure 1. Our core innovation is Context-Prediction Fusion mechanism that constructs latent tokens by combining two complementary sources: the contextual history encoded in hidden states, and the predictive semantic guidance from probability-weighted vocabulary embeddings. This fusion bridges the gap between the models output space and input embedding manifold, mitigating feature collapse in larger models. Additionally, we introduce confidence-driven strategy that allows the model to dynamically determine when to engage latent reasoning, avoiding the inefficiency of static allocation. The entire framework is trained through three-stage curriculum learning that progressively transitions from purely explicit CoT to reasoning with latent thoughts. Our main contributions can be summarized as follows. (1) unified latent reasoning method. We introduce LTTuning, latent-space reasoning framework that enables adaptive and stable continuous reasoning without architectural modifications. The method integrates (i) confidencedriven dynamic decision on explicit CoT or latent reasoning, (ii) contextprediction fusion to construct well-aligned latent tokens by combining contextual hidden states with predictive semantic guidance, and (iii) progressive curriculum learning strategy that stabilizes latent-space optimization and mitigates feature collapse. 2 (2) Comprehensive empirical evaluation and scaling analysis. We conduct extensive experiments on mathematical reasoning benchmarks across model scales from 1B to 8B parameters. Results show that LT-Tuning consistently outperforms existing latent reasoning baselines at all scales, achieving up to 4.3% average improvement over the strongest prior method. Notably, while prior approaches such as Coconut (Hao et al., 2024) degrade severely on larger models due to feature collapse, LT-Tuning exhibits robust and healthy scaling behavior across benchmarks. 2. Related Work Explicit Reasoning. The emergence of Chain-of-Thought (CoT) prompting (Wei et al., 2022) marked paradigm shift in how"
[12.02.2026 22:25] Mistral response. {"id": "07a02fd3bdf840ada75c9b65cdc251e4", "created": 1770935103, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1398, "total_tokens": 1420, "completion_tokens": 22, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Department of Computer Science, University of Illinois Chicago, Chicago, IL, USA\"]\n```"}}]}
[12.02.2026 22:25] Response: ```python
["Department of Computer Science, University of Illinois Chicago, Chicago, IL, USA"]
```
[12.02.2026 22:25] Deleting PDF ./assets/pdf/2602.10229.pdf.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.10179.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.10179.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.10179.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.08030.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.08030.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.08030.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.10748.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.10748.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.10748.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.08489.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.08489.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.08489.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.07954.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.07954.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.07954.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.09014.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.09014.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.09014.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.07900.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.07900.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.07900.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.06008.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.06008.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.06008.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.10652.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.10652.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.10652.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.08995.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.08995.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.08995.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.11137.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.11137.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.11137.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.10870.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.10870.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.10870.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.10778.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.10778.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.10778.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.10699.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.10699.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.10699.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.08741.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.08741.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.08741.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.08052.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.08052.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.08052.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.03773.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.03773.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.03773.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2602.08934.
[12.02.2026 22:25] Extra JSON file exists (./assets/json/2602.08934.json), skip PDF parsing.
[12.02.2026 22:25] Paper image links file exists (./assets/img_data/2602.08934.json), skip HTML parsing.
[12.02.2026 22:25] Success.
[12.02.2026 22:25] Enriching papers with extra data.
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 0. Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.  					AI-generated summary 				 We introduce Step 3.5 Flash, ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 1. PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.  					AI-generated summary 				 With the rapid development of large multimodal models, reliable judge and critic models have become essential f...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 2. GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.  					AI-generated summary 				 Unified Multimodal Models (UMMs) have...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 3. A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.  					AI-generated summary 				 Adapting LLM agents to domain-specific tool calling remai...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 4. Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.  					AI-gen...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 5. GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.  					AI-generated summary 				 While reasoning over long context is crucial for various real-world ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 6. A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.  					AI-generated summary 				 While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), ex...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 7. Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  					AI-generated summary 			...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 8. Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.  					AI-generated summary 				 This paper proposes Omni Dense Captioning, a novel task d...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 9. FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.  					AI-generated summary 				 Agents powered by large language models (LLMs) are increasingly adopted in the sof...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 10. ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free mod...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 11. Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 12. DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.  					AI-generated summary 				 In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-q...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 13. Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.  					AI-generated summary 				 Supervised fine-tuning (SFT) on cha...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 14. Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.  					AI-generated summary 				 Reinforcement learning...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 15. Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.  					AI-generated summary 				 Omni-modal large language models (O...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 16. EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.  					AI-generated summary 				 Long-horizon planning is widely recognized as a core capability...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 17. Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  					AI-generated summary 				 Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 18. GameDevBench is introduced as the first benchmark for evaluating agents on game development tasks that combine software development complexity with deep multimodal understanding requirements.  					AI-generated summary 				 Despite rapid progress on coding agents, progress on their multimodal counte...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 19. CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.  					AI-generated summary 				 Agentic coding requires agents to effectively in...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 20. Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.  					AI-generated summary 				 Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 21. LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning.  					AI-generated summary 				 The deployment of Large Language Models (LLMs) in high-stakes...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 22. Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.  					AI-generated summary 				 Group Relative Policy Optimization (GRPO) assigns ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 23. A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.  					AI-generated summary 				 Query Processing (QP) bridges user in...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 24. ECHO-2 is a distributed reinforcement learning framework that enables efficient post-training of large language models by overlapping rollout generation, dissemination, and training while managing policy staleness and network latency.  					AI-generated summary 				 Reinforcement learning (RL) is a ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 25. Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach.  					AI-generated summary 				 While explicit Chain-of-...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 26. Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.  					AI-generated summary 				 Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vis...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 27. Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.  					AI-generated summary 				 Reasoning models enhance problem-solving by scaling test-time comput...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 28. Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.  					AI-generated summary 				 Knowledge Graphs (KGs) store structured factual know...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 29. Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) ha...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 30. Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.  					AI-generated summary 				 As Large Language Models (LLMs) become increasingly deployed in Polish l...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 31. ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.  					AI-generated summary 				 Diffusion models have achieved remarkable generation q...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 32. Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods.  					AI-generated summary 				 Large Language Model (LLM) code agents increasingly resolve repository-level issues by iterat...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 33. AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 34. A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.  					AI-generated summary 				 Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-base...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 35. Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.  					AI-generated summary 				 Computer-use agents (CUAs) have made tremendou...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 36. Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.  					AI-generated summary 				 The prevailing paradigm in large language model (LLM) development is to pretrain a ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 37. FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.  					AI-generated summary 				 Federated Learning (FL) enables ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 38. GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for ...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 39. V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.  					AI-generated summary 				 Generative recommendation via autoregressive models has unified retrieval and ran...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 40. Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 41. A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  					AI-generated summary 				 The Unrelated Parallel Machine Scheduling P...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 42. RC, an iterative decoding algorithm, enables large language models to extrapolate and continuously improve beyond training budgets by constructing reasoning chains that enhance across iterations, achieving superior performance on long-horizon tasks.  					AI-generated summary 				 Large Language Mod...
[12.02.2026 22:25] ********************************************************************************
[12.02.2026 22:25] Abstract 43. StealthRL uses reinforcement learning with LoRA adapters to create adversarial paraphrases that evade multiple AI text detectors while preserving meaning, demonstrating significant robustness gaps in current detection systems.  					AI-generated summary 				 AI-text detectors face a critical robustn...
[12.02.2026 22:25] Read previous papers.
[12.02.2026 22:25] Generating reviews via LLM API.
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#architecture", "#agents", "#benchmark", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –∞–≥–µ–Ω—Ç–Ω—ã–π –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Step 3.5 Flash, –æ—Ç–Ω–æ—Å—è—â–∞—è—Å—è –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture-of-Experts, –∫–æ—Ç–æ—Ä–∞
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#rlhf", "#robotics", "#multimodal", "#alignment", "#benchmark", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ö—Ä–∏—Ç–∏–∫ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —á—É—Ç—å—ë–º: —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", "desc": "PhyCritic ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ç –ø–∞–º—è—Ç–∏ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –æ—Ü–µ–Ω–æ—á–Ω–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ GENIUS –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ñ–ª—é–∏–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∑–∞–¥
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#small_models", "#inference", "#training", "#agents"], "emoji": "üéØ", "ru": {"title": "–ù–∞–ø—Ä–∞–≤–ª—è—é—â–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Activation Steering Adapter (ASA) –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#science", "#training", "#reasoning", "#agents", "#math", "#open_source"], "emoji": "üßÆ", "ru": {"title": "–û—Ç –æ–ª–∏–º–ø–∏–∞–¥ –∫ –Ω–∞—É—á–Ω—ã–º –æ—Ç–∫—Ä—ã—Ç–∏—è–º: AI-–∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "Aletheia ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–µ—Ä—Å–∏—é 
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–∞—è –ø–∞–º—è—Ç—å —Å —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º–∏ –∑–∞—Ç–≤–æ—Ä–∞–º–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "GRU-Mem —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –≤ –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ç–µ–∫—Å—Ç–æ–º –∑–∞—Ç–≤–æ—Ä—ã, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã
[12.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è G-LNS, –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#open_source", "#optimization"], "emoji": "üë§", "ru": {"title": "–ú—è–≥–∫–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –∫–∞—á–µ—Å—Ç
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#video", "#multimodal", "#training", "#reasoning", "#benchmark", "#audio", "#dataset", "#open_source"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –æ–ø–∏—Å—ã–≤–∞—Ç—å –≤–∏–¥–µ–æ –∫–∞–∫ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—Å—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Omni Dense Captioning ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#leakage", "#plp", "#open_source", "#benchmark", "#agents", "#optimization", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –±–∞–≥–æ–≤ –∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "FeatureBench ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–º—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "üöÄ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é", "desc": "ROCKET ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –ø–æ —Å–ª–æ—è–º –∫–∞–∫ –∑–∞–¥–∞—á—É –º–Ω–æ–≥–æ–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–≥–æ —Ä—é–∫–∑–∞–∫
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö: –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏", "desc": "Meta-Experience Learning ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#synthetic", "#data", "#training", "#optimization", "#benchmark", "#rl"], "emoji": "üë®‚Äçüç≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫—É–ª–∏–Ω–∞—Ä–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è DataChef-32B ‚Äî —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—Ü–µ–ø—Ç–æ
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#reasoning", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: –∫–∞–∫ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–µ–ª–∞—é—Ç —É–º–Ω–µ–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ä–∞—Å—Å—É–∂–¥–∞—é—â–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–µ—Ç–æ–¥–æ–º supervised fine-tuning –Ω–∞
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "üéØ", "ru": {"title": "–ö–∞–ª–º–∞–Ω–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Online Causal Kalman Filtering –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –ø—Ä–∏ importance samplin
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#audio", "#3d", "#architecture", "#dataset"], "emoji": "üé≠", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ—á—å –∏ –º–∏–º–∏–∫–∞: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –æ—Ç –¥–∏–Ω–∞–º–∏–∫–∏", "desc": "Ex-Omni ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–º–Ω–∏-–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, 
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#agents", "#reasoning"], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EcoGym ‚Äî —ç—Ç–æ –æ–±–æ–±—â—ë–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-based –∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#rag", "#video", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ–ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#plp", "#multimodal", "#games", "#dataset", "#benchmark", "#agents", "#open_source"], "emoji": "üéÆ", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–≥—Ä—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ GameDevBench ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏–≥—Ä, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—á–µ—Ç–∞—é—Ç
[12.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é –∏—Å—Ç–æ—Ä–∏–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLI-Gym, —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π, –ø—É—Ç–µ–º –∏–º–∏—Ç–∞—Ü–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ
[12.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–û—Ç —à—Ç—Ä–∏—Ö–∞ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π", "desc": "Stroke3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∏–≥–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –∏–∑ –¥–≤—É–º–µ—Ä–Ω—ã—Ö —Ä–∏—Å—É–Ω–∫–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π pipeline. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∞—Ä
[12.02.2026 22:25] Querying the API.
[12.02.2026 22:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning.  					AI-generated summary 				 The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.
[12.02.2026 22:25] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LiveMedBench ‚Äî –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö LLM, —Å–≤–æ–±–æ–¥–Ω—ã–π –æ—Ç –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ –ø–æ–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º–∏ —Å–ª—É—á–∞—è–º–∏ –∏–∑ –æ–Ω–ª–∞–π–Ω-—Å–æ–æ–±—â–µ—Å—Ç–≤ –∏ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä—ë—Ö–º–µ—Ä–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–π –∫—É—Ä–∞—Ü–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä—É–±—Ä–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –≤—Ä–∞—á–µ–π –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ —Å—É–∂–¥–µ–Ω–∏—è–º–∏, —á–µ–º –º–µ—Ç–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM-—Å—É–¥–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 38 –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–∏–ª–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã: –¥–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ–ª—å–∫–æ 39,2% —Ç–æ—á–Ω–æ—Å—Ç–∏, –∏ 84% –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—É—é –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏—é —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤.",
  "emoji": "üè•",
  "title": "–ñ–∏–≤–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫: –æ–±—É–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤ –æ—Ü–µ–Ω–∫–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö LLM"
}
```
[12.02.2026 22:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning.  					AI-generated summary 				 The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints."

[12.02.2026 22:25] Response: ```python
['BENCHMARK', 'HEALTHCARE', 'DATASET', 'MULTILINGUAL', 'AGENTS']
```
[12.02.2026 22:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning.  					AI-generated summary 				 The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints."

[12.02.2026 22:25] Response: ```python
['LEAKAGE', 'SCIENCE', 'ALIGNMENT']
```
[12.02.2026 22:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LiveMedBench is a new benchmark designed to improve the evaluation of Large Language Models (LLMs) in medical settings. It addresses issues like data contamination and outdated knowledge by continuously updating its dataset with real-world clinical cases. The benchmark uses a Multi-Agent Clinical Curation Framework to ensure the integrity of the data and an Automated Rubric-based Evaluation Framework to assess LLM performance against expert criteria. This approach reveals that most LLMs struggle with contextual application, highlighting the need for better alignment with clinical reasoning.","title":"Revolutionizing Medical LLM Evaluation with LiveMedBench"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LiveMedBench is a new benchmark designed to improve the evaluation of Large Language Models (LLMs) in medical settings. It addresses issues like data contamination and outdated knowledge by continuously updating its dataset with real-world clinical cases. The benchmark uses a Multi-Agent Clinical Curation Framework to ensure the integrity of the data and an Automated Rubric-based Evaluation Framework to assess LLM performance against expert criteria. This approach reveals that most LLMs struggle with contextual application, highlighting the need for better alignment with clinical reasoning.', title='Revolutionizing Medical LLM Evaluation with LiveMedBench'))
[12.02.2026 22:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LiveMedBench ÊòØ‰∏Ä‰∏™ÈíàÂØπÂåªÁñóÈ¢ÜÂüüÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËØÑ‰º∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËØÑ‰º∞‰∏≠ÁöÑÊï∞ÊçÆÊ±°ÊüìÂíåÊó∂Èó¥‰∏çÂØπÈΩêÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáÊØèÂë®Êõ¥Êñ∞ÁöÑÁúüÂÆû‰∏¥Â∫äÊ°à‰æãÔºåÁ°Æ‰øùËØÑ‰º∞Êï∞ÊçÆ‰∏éÊ®°ÂûãËÆ≠ÁªÉÊï∞ÊçÆ‰∏•Ê†ºÂàÜÁ¶ªÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÊï∞ÊçÆÊ≥ÑÊºè„ÄÇËØ•Âü∫ÂáÜÈááÁî®Âü∫‰∫éËØÑÂàÜÊ†áÂáÜÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞‰∏é‰∏ìÂÆ∂ÁöÑ‰∏¥Â∫äÊé®ÁêÜÂØπÈΩê„ÄÇÈÄöËøáÂØπ38ÁßçÂåªÁñó‰∏ì‰∏öÁöÑ2756‰∏™ÁúüÂÆûÊ°à‰æãËøõË°åËØÑ‰º∞ÔºåÂèëÁé∞Áé∞ÊúâÊ®°ÂûãÁöÑË°®Áé∞‰ªçÁÑ∂ÊúâÈôêÔºåÂº∫Ë∞É‰∫ÜÂú®‰∏¥Â∫äÂ∫îÁî®‰∏≠ÂÆöÂà∂ÂåªÁñóÁü•ËØÜÁöÑÈáçË¶ÅÊÄß„ÄÇ","title":"LiveMedBenchÔºöÊèêÂçáÂåªÁñóLLMËØÑ‰º∞ÁöÑÊ†áÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LiveMedBench ÊòØ‰∏Ä‰∏™ÈíàÂØπÂåªÁñóÈ¢ÜÂüüÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËØÑ‰º∞ÁöÑÂü∫ÂáÜÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËØÑ‰º∞‰∏≠ÁöÑÊï∞ÊçÆÊ±°ÊüìÂíåÊó∂Èó¥‰∏çÂØπÈΩêÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáÊØèÂë®Êõ¥Êñ∞ÁöÑÁúüÂÆû‰∏¥Â∫äÊ°à‰æãÔºåÁ°Æ‰øùËØÑ‰º∞Êï∞ÊçÆ‰∏éÊ®°ÂûãËÆ≠ÁªÉÊï∞ÊçÆ‰∏•Ê†ºÂàÜÁ¶ªÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÊï∞ÊçÆÊ≥ÑÊºè„ÄÇËØ•Âü∫ÂáÜÈááÁî®Âü∫‰∫éËØÑÂàÜÊ†áÂáÜÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞‰∏é‰∏ìÂÆ∂ÁöÑ‰∏¥Â∫äÊé®ÁêÜÂØπÈΩê„ÄÇÈÄöËøáÂØπ38ÁßçÂåªÁñó‰∏ì‰∏öÁöÑ2756‰∏™ÁúüÂÆûÊ°à‰æãËøõË°åËØÑ‰º∞ÔºåÂèëÁé∞Áé∞ÊúâÊ®°ÂûãÁöÑË°®Áé∞‰ªçÁÑ∂ÊúâÈôêÔºåÂº∫Ë∞É‰∫ÜÂú®‰∏¥Â∫äÂ∫îÁî®‰∏≠ÂÆöÂà∂ÂåªÁñóÁü•ËØÜÁöÑÈáçË¶ÅÊÄß„ÄÇ', title='LiveMedBenchÔºöÊèêÂçáÂåªÁñóLLMËØÑ‰º∞ÁöÑÊ†áÂáÜ'))
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rlhf"], "emoji": "üß©", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Blockwise Advantage Estimation - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä
[12.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QP-OneModel ‚Äî –µ–¥–∏–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥–∑
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "ECHO-2 ‚Äî —ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º
[12.02.2026 22:25] Querying the API.
[12.02.2026 22:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach.  					AI-generated summary 				 While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.
[12.02.2026 22:25] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Latent Thoughts Tuning (LT-Tuning) ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ Context-Prediction-Fusion, —Ä–µ—à–∞—é—â–µ–º –ø—Ä–æ–±–ª–µ–º—É –∫–æ–ª–ª–∞–ø—Å–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –≤–æ–∑–Ω–∏–∫–∞—é—â–µ–π –ø—Ä–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π curriculum learning —Å —Ç—Ä–µ–º—è —ç—Ç–∞–ø–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É —Å–∫—Ä—ã—Ç—ã–º –∏ —è–≤–Ω—ã–º —Ä–µ–∂–∏–º–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LT-Tuning –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.",
  "emoji": "üß†",
  "title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ —Å–ª–æ–≤: –º—ã—à–ª–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ"
}
```
[12.02.2026 22:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach.  					AI-generated summary 				 While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy."

[12.02.2026 22:25] Response: ```python
["TRAINING", "ARCHITECTURE"]
```
[12.02.2026 22:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach.  					AI-generated summary 				 While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy."

[12.02.2026 22:25] Response: ```python
["REASONING"]
```
[12.02.2026 22:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Latent Thoughts Tuning (LT-Tuning) is a new framework designed to enhance reasoning in continuous latent spaces by integrating contextual hidden states with semantic predictions. This approach allows for more flexible and robust inference compared to traditional methods that rely on discrete token outputs. LT-Tuning addresses common issues like feature collapse and instability by using a Context-Prediction-Fusion mechanism, which combines hidden states with guidance from vocabulary embeddings. Additionally, it employs a progressive curriculum learning strategy that allows models to switch between latent and explicit reasoning modes, leading to improved performance in reasoning tasks.","title":"Empowering Continuous Reasoning with Latent Thoughts Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Latent Thoughts Tuning (LT-Tuning) is a new framework designed to enhance reasoning in continuous latent spaces by integrating contextual hidden states with semantic predictions. This approach allows for more flexible and robust inference compared to traditional methods that rely on discrete token outputs. LT-Tuning addresses common issues like feature collapse and instability by using a Context-Prediction-Fusion mechanism, which combines hidden states with guidance from vocabulary embeddings. Additionally, it employs a progressive curriculum learning strategy that allows models to switch between latent and explicit reasoning modes, leading to improved performance in reasoning tasks.', title='Empowering Continuous Reasoning with Latent Thoughts Tuning'))
[12.02.2026 22:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Latent Thoughts TuningÔºàLT-TuningÔºâÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÂú®ËøûÁª≠ÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫Ü‰∏ä‰∏ãÊñáÈöêËóèÁä∂ÊÄÅÂíåÈ¢ÑÊµãËØ≠‰πâÊåáÂØºÔºåÈÄöËøáÊ∏êËøõÂºèËØæÁ®ãÂ≠¶‰π†ÊñπÊ≥ïÂÆûÁé∞Á®≥ÂÅ•ÁöÑÊé®ÁêÜ„ÄÇ‰∏é‰º†ÁªüÁöÑÊòæÂºèÊÄùÁª¥Èìæ‰∏çÂêåÔºåLT-TuningËÉΩÂ§üÁÅµÊ¥ªÂú∞Âú®ÊΩúÂú®ÂíåÊòæÂºèÊÄùÁª¥Ê®°Âºè‰πãÈó¥ÂàáÊç¢ÔºåÂÖãÊúç‰∫ÜÁâπÂæÅÂ¥©Ê∫ÉÂíå‰∏çÁ®≥ÂÆöÊÄßÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄß‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊΩúÂú®Êé®ÁêÜÂü∫Á∫ø„ÄÇ","title":"ÊΩúÂú®ÊÄùÁª¥Ë∞É‰ºòÔºöÁÅµÊ¥ªÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Latent Thoughts TuningÔºàLT-TuningÔºâÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÂú®ËøûÁª≠ÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÊé®ÁêÜ„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫Ü‰∏ä‰∏ãÊñáÈöêËóèÁä∂ÊÄÅÂíåÈ¢ÑÊµãËØ≠‰πâÊåáÂØºÔºåÈÄöËøáÊ∏êËøõÂºèËØæÁ®ãÂ≠¶‰π†ÊñπÊ≥ïÂÆûÁé∞Á®≥ÂÅ•ÁöÑÊé®ÁêÜ„ÄÇ‰∏é‰º†ÁªüÁöÑÊòæÂºèÊÄùÁª¥Èìæ‰∏çÂêåÔºåLT-TuningËÉΩÂ§üÁÅµÊ¥ªÂú∞Âú®ÊΩúÂú®ÂíåÊòæÂºèÊÄùÁª¥Ê®°Âºè‰πãÈó¥ÂàáÊç¢ÔºåÂÖãÊúç‰∫ÜÁâπÂæÅÂ¥©Ê∫ÉÂíå‰∏çÁ®≥ÂÆöÊÄßÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Êé®ÁêÜÂáÜÁ°ÆÊÄß‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊΩúÂú®Êé®ÁêÜÂü∫Á∫ø„ÄÇ', title='ÊΩúÂú®ÊÄùÁª¥Ë∞É‰ºòÔºöÁÅµÊ¥ªÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï'))
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#security", "#ethics", "#benchmark"], "emoji": "üé®", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏: –∑–∞—â–∏—Ç–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç –∞—Ç–∞–∫ —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –∞—Ç–∞–∫ –Ω–∞ –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#training", "#long_context", "#reasoning", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—Å—è –∑–∞–±—ã–≤–∞—Ç—å –Ω–µ–Ω—É–∂–Ω–æ–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Free()LM —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–≤–æ–¥—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∑–∞–±—ã–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä Free-Module 
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#rag"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π: –ø–æ—á–µ–º—É LLM —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ FactCheck –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∞–∫—Ç—ã –≤ –≥—Ä–∞—Ñ–∞—Ö –∑–Ω–∞–Ω–∏–π –ø–æ —Ç—Ä—ë–º –Ω–∞–ø—Ä–∞–≤
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–ö—Ä–µ–ø–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RLTR, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#dataset", "#benchmark", "#ethics", "#small_models", "#multilingual"], "emoji": "üõ°Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è LLM –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Bielik Guard ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#diffusion", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ArcFlow ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Ç–æ
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#agents", "#plp", "#benchmark", "#reasoning"], "emoji": "üêõ", "ru": {"title": "–¢–µ—Å—Ç—ã –≤ –∫–æ–¥–µ –Ω–µ –≤—Å–µ–≥–¥–∞ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã ‚Äî –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ", "desc": "–í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö –∫–æ–¥–∞, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –∏—Ç–µ—Ä–∞—Ç–∏–≤
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents"], "emoji": "ü§ù", "ru": {"title": "LLM-–∞–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –≤–µ—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã —á–µ—Ä–µ–∑ —è–∑—ã–∫", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫-—Å–∏—Å—Ç–µ–º–∞ AgenticPay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –°–∏—Å—Ç–µ
[12.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ—Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∞—è—Å—è –ø–∞–º—è—Ç—å: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è UMEM ‚Äî –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –≤ –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#agents", "#security", "#alignment", "#benchmark", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ DeAction –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤ –∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –≤
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#training"], "emoji": "üéØ", "ru": {"title": "–ë–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî –ª—É—á—à–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Ä–æ–ª—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏. 
[12.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–ü—Ä–∏–≤–∞—Ç–Ω–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "FedPS ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –º–µ—Ç–æ–¥—ã —ç—Å–∫–∏–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#optimization", "#inference", "#security", "#plp", "#interpretability", "#training"], "emoji": "üîí", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–¥ —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω–æ-–∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ LLM", "desc": "GoodVibe ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞ –∫–æ–¥–µ, —á–µ—Ä–µ–∑ —Ü
[12.02.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ-–Ω–∞–≥—Ä–∞–¥–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", "desc": "V-STAR ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–≥—Ä–∞–¥–æ–π –≤ –º–æ–¥–µ–ª—è—Ö —Å —É—Å–∏–ª–µ–Ω–Ω—ã–º –æ
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#training", "#security", "#architecture", "#alignment"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –≤ MoE –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —É—è–∑–≤–∏–º–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä Mixture-of-Experts (MoE) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –≥–¥–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±–µ–∑–æ–ø–∞
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#graphs", "#optimization", "#architecture", "#benchmark", "#rlhf"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –∏
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#rl", "#small_models"], "emoji": "üîÑ", "ru": {"title": "–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "RC ‚Äî —ç—Ç–æ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —É–ª—É—á—à
[12.02.2026 22:25] Using data from previous issue: {"categories": ["#security", "#open_source", "#training", "#benchmark", "#rl"], "emoji": "üõ°Ô∏è", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–¥–≤–µ—Ä—Å–∞—Ä–∏–∞–ª—å–Ω—ã–µ –∞—Ç–∞–∫–∏ –Ω–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –ò–ò-—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è StealthRL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ
[12.02.2026 22:25] Renaming data file.
[12.02.2026 22:25] Renaming previous data. hf_papers.json to ./d/2026-02-12.json
[12.02.2026 22:25] Saving new data file.
[12.02.2026 22:25] Generating page.
[12.02.2026 22:25] Renaming previous page.
[12.02.2026 22:25] Renaming previous data. index.html to ./d/2026-02-12.html
[12.02.2026 22:25] Writing result.
[12.02.2026 22:25] Renaming log file.
[12.02.2026 22:25] Renaming previous data. log.txt to ./logs/2026-02-12_last_log.txt
