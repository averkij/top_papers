[03.02.2025 21:09] Read previous papers.
[03.02.2025 21:09] Generating top page (month).
[03.02.2025 21:09] Writing top page (month).
[03.02.2025 22:09] Read previous papers.
[03.02.2025 22:09] Get feed.
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.19393
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.19324
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18119
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.19339
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14677
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.19399
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2411.04983
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18837
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18841
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18052
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18804
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18965
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18753
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18128
[03.02.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2404.07097
[03.02.2025 22:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.02.2025 22:09] No deleted papers detected.
[03.02.2025 22:09] Downloading and parsing papers (pdf, html). Total: 15.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.19393.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.19393.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.19393.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.19324.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.19324.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.19324.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18119.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18119.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18119.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.19339.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.19339.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.19339.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.14677.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.14677.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.14677.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.19399.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.19399.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.19399.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2411.04983.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2411.04983.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2411.04983.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18837.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18837.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18837.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18841.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18841.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18841.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18052.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18052.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18052.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18804.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18804.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18804.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18965.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18965.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18965.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18753.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18753.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18753.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2501.18128.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2501.18128.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2501.18128.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2404.07097.
[03.02.2025 22:09] Extra JSON file exists (./assets/json/2404.07097.json), skip PDF parsing.
[03.02.2025 22:09] Paper image links file exists (./assets/img_data/2404.07097.json), skip HTML parsing.
[03.02.2025 22:09] Success.
[03.02.2025 22:09] Enriching papers with extra data.
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 0. Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve te...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 1. ...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 2. Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 3. Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the nee...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 4. Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 5. The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's abilit...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 6. The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 7. Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, ...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 8. We conduct experiments on the impact of increasing inference-time compute in reasoning models (specifically OpenAI o1-preview and o1-mini) on their robustness to adversarial attacks. We find that across a variety of attacks, increased inference-time compute leads to improved robustness. In many case...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 9. Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to th...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 10. Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of ...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 11. We show that learning-rate schedules for large model training behave surprisingly similar to a performance bound from non-smooth convex optimization theory. We provide a bound for the constant schedule with linear cooldown; in particular, the practical benefit of cooldown is reflected in the bound d...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 12. Task-generic promptable image segmentation aims to achieve segmentation of diverse samples under a single task description by utilizing only one task-generic prompt. Current methods leverage the generalization capabilities of Vision-Language Models (VLMs) to infer instance-specific prompts from thes...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 13. Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this...
[03.02.2025 22:09] ********************************************************************************
[03.02.2025 22:09] Abstract 14. This paper addresses the long-standing challenge of reconstructing 3D structures from videos with dynamic content. Current approaches to this problem were not designed to operate on casual videos recorded by standard cameras or require a long optimization time.   Aiming to significantly improve the ...
[03.02.2025 22:09] Read previous papers.
[03.02.2025 22:09] Generating reviews via LLM API.
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#reasoning", "#training", "#math", "#optimization", "#data"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —è–∑—ã–∫–æ–≤–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π —Ç–µ—Å—Ç–æ–≤—ã–º –º–∞—Å—à
[03.02.2025 22:09] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–ù–æ–≤—ã–π —à–∞–≥ –≤ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —É–ª—É—á—à–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#inference", "#graphs", "#transfer_learning", "#training", "#multimodal", "#data"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–¥—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π —Å –±–æ–ª—å—à–∏–º–∏ 
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#agi", "#benchmark", "#optimization", "#dataset", "#open_source", "#reasoning", "#multimodal", "#interpretability", "#cv"], "emoji": "üß†", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø–∏–∫—Å–µ–ª—å–Ω—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–∏—Ä: –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è –ò–ò", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#training", "#dataset", "#video"], "emoji": "‚úÇÔ∏è", "ru": {"title": "MatAnyone: –¢–æ—á–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ø–∞–º—è—Ç–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "MatAnyone - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–¥–µ–ª–µ–Ω–∏—é –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ –±–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å –ø–∞–º—è—Ç–∏ –¥–ª—è 
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#long_context", "#optimization"], "emoji": "üîç", "ru": {"title": "SSMax: —É–ª—É—á—à–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é Scalable-Softmax (SSMax) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏.
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#cv", "#agents", "#reasoning", "#optimization", "#rl"], "emoji": "üß†", "ru": {"title": "DINO-WM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–µ–∑ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –±–µ–∑ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ - DI
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#synthetic", "#training", "#architecture", "#dataset", "#security", "#inference"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–µ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã: –Ω–∞–¥–µ–∂–Ω–∞—è –∑–∞—â–∏—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã—Ö –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ - –∑–∞—â–∏—Ç–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –¥–ª
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#security", "#reasoning", "#inference"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π - –≤—ã—à–µ –∑–∞—â–∏—Ç–∞: –ø–æ–≤—ã—à–µ–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –ò–ò –∫ –∞—Ç–∞–∫–∞–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –≤–ª–∏—è–Ω–∏—é —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∫ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#dataset", "#interpretability", "#security", "#architecture", "#ethics", "#training", "#benchmark"], "emoji": "üßπ", "ru": {"title": "–ß–∏—Å—Ç–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: SAeUron —É–¥–∞–ª—è–µ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SAeUron - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#training", "#3d", "#diffusion", "#architecture", "#benchmark", "#optimization"], "emoji": "üé≠", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3D —Å—Ü–µ–Ω –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MVGD - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∫–∞—Ä—Ç –≥
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#transfer_learning", "#math", "#training", "#optimization"], "emoji": "üìà", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –Ω–µ–æ–∂–∏–¥
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#dataset", "#healthcare", "#optimization", "#cv"], "emoji": "üîç", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –ø—Ä–æ–º–ø—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º INT (Instance-specific Negative Mining fo
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#survey", "#multilingual", "#small_models", "#benchmark", "#transfer_learning"], "emoji": "üì∞", "ru": {"title": "–ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –±—Ä–æ—Å–∞—é—Ç –≤—ã–∑–æ–≤ –≥–∏–≥–∞–Ω—Ç–∞–º –≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ 20 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –º–µ
[03.02.2025 22:09] Using data from previous issue: {"categories": ["#3d", "#training", "#cv", "#architecture"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TracksTo4D - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D —Å—Ç—Ä—É–∫—Ç—É—Ä –∏–∑ –≤–∏–¥–µ–æ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ
[03.02.2025 22:09] Loading Chinese text from previous data.
[03.02.2025 22:09] Renaming data file.
[03.02.2025 22:09] Renaming previous data. hf_papers.json to ./d/2025-02-03.json
[03.02.2025 22:09] Saving new data file.
[03.02.2025 22:09] Generating page.
[03.02.2025 22:09] Renaming previous page.
[03.02.2025 22:09] Renaming previous data. index.html to ./d/2025-02-03.html
[03.02.2025 22:09] [Experimental] Generating Chinese page for reading.
[03.02.2025 22:09] Chinese vocab [{'word': 'Âª∫Ê®°', 'pinyin': 'ji√†n m√≥', 'trans': 'modeling'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìng w√©i', 'trans': 'called'}, {'word': 'Áº©Êîæ', 'pinyin': 'su≈ç f√†ng', 'trans': 'scaling'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«én sh√¨', 'trans': 'demonstrate'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'capability'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'Â§çÂà∂', 'pinyin': 'f√π zh√¨', 'trans': 'replicate'}, {'word': 'Âä™Âäõ', 'pinyin': 'n«î l√¨', 'trans': 'efforts'}, {'word': 'Êï¥ÁêÜ', 'pinyin': 'zhƒõng l«ê', 'trans': 'organize'}, {'word': 'È¢ÑÁÆó', 'pinyin': 'y√π su√†n', 'trans': 'budget'}, {'word': 'Âº∫Âà∂', 'pinyin': 'qi√°ng zh√¨', 'trans': 'enforcement'}, {'word': 'ÈÄîÂæÑ', 'pinyin': 't√∫ j√¨ng', 'trans': 'means'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open-source'}]
[03.02.2025 22:09] Renaming previous Chinese page.
[03.02.2025 22:09] Renaming previous data. zh.html to ./d/2025-02-02_zh_reading_task.html
[03.02.2025 22:09] Writing Chinese reading task.
[03.02.2025 22:09] Writing result.
[03.02.2025 22:09] Renaming log file.
[03.02.2025 22:09] Renaming previous data. log.txt to ./logs/2025-02-03_last_log.txt
