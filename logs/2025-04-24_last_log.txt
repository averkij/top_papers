[24.04.2025 08:15] Read previous papers.
[24.04.2025 08:15] Generating top page (month).
[24.04.2025 08:15] Writing top page (month).
[24.04.2025 09:12] Read previous papers.
[24.04.2025 09:12] Get feed.
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15279
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14509
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15431
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16801
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15843
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16929
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.16915
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15585
[24.04.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2504.15777
[24.04.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2504.15707
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15254
[24.04.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.10419
[24.04.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.04.2025 09:12] No deleted papers detected.
[24.04.2025 09:12] Downloading and parsing papers (pdf, html). Total: 12.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.15279.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.15279.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.15279.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.14509.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.14509.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.14509.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.15431.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.15431.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.15431.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.16801.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.16801.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.16801.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.15843.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.15843.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.15843.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.16929.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.16929.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.16929.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.16915.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.16915.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.16915.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.15585.
[24.04.2025 09:12] Extra JSON file exists (./assets/json/2504.15585.json), skip PDF parsing.
[24.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.15585.json), skip HTML parsing.
[24.04.2025 09:12] Success.
[24.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.15777.
[24.04.2025 09:12] Downloading paper 2504.15777 from http://arxiv.org/pdf/2504.15777v1...
[24.04.2025 09:13] Extracting affiliations from text.
[24.04.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 7 7 7 5 1 . 4 0 5 2 : r Tina: Tiny Reasoning Models via LoRA Shangshang Wang1, Julian Asilis1, Ömer Faruk Akgül1, Enes Burak Bilgin1, Ollie Liu1, and Willie Neiswanger 1University of Southern California How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, at only $9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base models underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights & checkpoints. Notion Blog: https://shangshangwang.notion.site/tina Code Repository: https://github.com/shangshang-wang/Tina Training Logs: https://wandb.ai/upup-ashton-wang-usc/Tina Model Weights & Checkpoints: https://huggingfac"
[24.04.2025 09:13] Response: ```python
["University of Southern California"]
```
[24.04.2025 09:13] Deleting PDF ./assets/pdf/2504.15777.pdf.
[24.04.2025 09:13] Success.
[24.04.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2504.15707.
[24.04.2025 09:13] Downloading paper 2504.15707 from http://arxiv.org/pdf/2504.15707v1...
[24.04.2025 09:13] Extracting affiliations from text.
[24.04.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RePOPE: Impact of Annotation Errors on the POPE Benchmark Tubingen AI Center University of Tubingen 5 2 0 2 2 2 ] . [ 1 7 0 7 5 1 . 4 0 5 2 : r a "
[24.04.2025 09:13] Response: ```python
["Tubingen AI Center, University of Tubingen"]
```
[24.04.2025 09:13] Deleting PDF ./assets/pdf/2504.15707.pdf.
[24.04.2025 09:13] Success.
[24.04.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2504.15254.
[24.04.2025 09:13] Extra JSON file exists (./assets/json/2504.15254.json), skip PDF parsing.
[24.04.2025 09:13] Paper image links file exists (./assets/img_data/2504.15254.json), skip HTML parsing.
[24.04.2025 09:13] Success.
[24.04.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2504.10419.
[24.04.2025 09:13] Extra JSON file exists (./assets/json/2504.10419.json), skip PDF parsing.
[24.04.2025 09:13] Paper image links file exists (./assets/img_data/2504.10419.json), skip HTML parsing.
[24.04.2025 09:13] Success.
[24.04.2025 09:13] Enriching papers with extra data.
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 0. Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine ...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 1. In this paper, we introduce DreamID, a diffusion-based face swapping model that achieves high levels of ID similarity, attribute preservation, image fidelity, and fast inference speed. Unlike the typical face swapping training process, which often relies on implicit supervision and struggles to achi...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 2. We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data ...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 3. Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent stu...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 4. Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 5. As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we i...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 6. Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different ty...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 7. The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both ...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 8. How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using onl...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 9. Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in anno...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 10. C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a...
[24.04.2025 09:13] ********************************************************************************
[24.04.2025 09:13] Abstract 11. Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting check...
[24.04.2025 09:13] Read previous papers.
[24.04.2025 09:13] Generating reviews via LLM API.
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#rl", "#multimodal", "#dataset"], "emoji": "🧠", "ru": {"title": "VisuLogic: преодолевая разрыв в визуальном мышлении ИИ", "desc": "Статья представляет VisuLogic - новый бенчмарк для оценки визуального мышления мультимодальных больших языковых моделей (MLL
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#architecture"], "emoji": "🎭", "ru": {"title": "DreamID: Быстрый и качественный обмен лиц с помощью диффузионных моделей", "desc": "DreamID - это модель обмена лиц на основе диффузии, которая достигает высокого уровня сходства ID, сохранения атрибут
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#low_resource", "#training", "#transfer_learning", "#architecture", "#data"], "emoji": "🌏", "ru": {"title": "Эффективный перенос знаний между языками в компактной мультиязычной модели", "desc": "Trillion-7B - это мультиязычная языковая модель с акценто
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#benchmark", "#training", "#alignment", "#multimodal", "#optimization"], "emoji": "🧠", "ru": {"title": "DeGLA: Улучшение композиционного понимания без потери общих возможностей", "desc": "Данная статья представляет новый подход под названием DeGLA (Decoupled Global-Local Alignment) 
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#rlhf", "#alignment", "#benchmark"], "emoji": "🧠", "ru": {"title": "Pre-DPO: Эффективная оптимизация языковых моделей с учетом предпочтений", "desc": "Эта статья представляет Pre-DPO - новый подход к обучению языковых моделей на основе предпочтений чело
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#training", "#math", "#interpretability", "#cv", "#optimization"], "emoji": "🧠", "ru": {"title": "Единая теория функций потерь в машинном обучении", "desc": "Статья представляет единое информационно-теоретическое уравнение, обобщающее множество современных функций потерь в машинном 
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#dataset", "#optimization", "#training"], "emoji": "🎨", "ru": {"title": "DreamO: универсальная кастомизация изображений с помощью диффузионных трансформеров", "desc": "DreamO - это унифицированная система для кастомизации изображений, использующая архитектуру ди
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#survey", "#alignment", "#agi", "#agents", "#security", "#data"], "emoji": "🛡️", "ru": {"title": "Полностековая безопасность LLM: от данных до применения", "desc": "Эта статья представляет концепцию 'полностековой' безопасности для больших языковых моделей (
[24.04.2025 09:13] Querying the API.
[24.04.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints.
[24.04.2025 09:13] Response: {
  "desc": "Статья представляет семейство моделей Tina, демонстрирующих сильные способности к рассуждениям при минимальных вычислительных затратах. Используя параметрически-эффективные обновления во время обучения с подкреплением (RL) и низкоранговую адаптацию (LoRA), авторы достигают производительности, конкурирующей с современными моделями рассуждений. Лучшая модель Tina показывает более чем 20% увеличение производительности рассуждений при затратах всего $9 на дообучение и оценку. Исследователи предполагают, что эффективность связана с быстрой адаптацией модели к структурному формату рассуждений, вознаграждаемому RL, при сохранении базовых знаний модели.",
  "emoji": "🧠",
  "title": "Эффективное обучение рассуждениям с минимальными ресурсами"
}
[24.04.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints."

[24.04.2025 09:13] Response: ```python
['RL', 'TRAINING', 'SMALL_MODELS']
```
[24.04.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints."

[24.04.2025 09:13] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[24.04.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Tina, a series of compact reasoning models that achieve strong reasoning capabilities with minimal resources. By utilizing low-rank adaptation (LoRA) during reinforcement learning (RL), Tina enhances a small 1.5B parameter base model, demonstrating that effective reasoning can be achieved cost-effectively. The models not only compete with but sometimes outperform state-of-the-art (SOTA) RL reasoning models, all while significantly reducing post-training costs. The findings suggest that LoRA effectively tailors the model for reasoning tasks, maintaining the base model\'s knowledge while improving performance.","title":"Tiny Models, Big Reasoning: Cost-Effective AI with Tina"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces Tina, a series of compact reasoning models that achieve strong reasoning capabilities with minimal resources. By utilizing low-rank adaptation (LoRA) during reinforcement learning (RL), Tina enhances a small 1.5B parameter base model, demonstrating that effective reasoning can be achieved cost-effectively. The models not only compete with but sometimes outperform state-of-the-art (SOTA) RL reasoning models, all while significantly reducing post-training costs. The findings suggest that LoRA effectively tailors the model for reasoning tasks, maintaining the base model's knowledge while improving performance.", title='Tiny Models, Big Reasoning: Cost-Effective AI with Tina'))
[24.04.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种名为Tina的小型推理模型系列，旨在以高性价比实现强大的推理能力。Tina通过在一个仅有1.5亿参数的基础模型上，采用低秩适应（LoRA）和强化学习（RL）进行参数高效更新，展示了在资源有限的情况下仍能获得显著的推理性能。与现有的最先进（SOTA）模型相比，Tina在推理性能上具有竞争力，甚至在某些情况下超越了这些模型，同时其后期训练和评估成本仅为9美元，节省了约260倍的成本。我们的研究表明，LoRA在快速适应推理结构的同时，能够有效保留基础模型的知识，从而实现高效的推理能力。","title":"高效推理，低成本实现"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种名为Tina的小型推理模型系列，旨在以高性价比实现强大的推理能力。Tina通过在一个仅有1.5亿参数的基础模型上，采用低秩适应（LoRA）和强化学习（RL）进行参数高效更新，展示了在资源有限的情况下仍能获得显著的推理性能。与现有的最先进（SOTA）模型相比，Tina在推理性能上具有竞争力，甚至在某些情况下超越了这些模型，同时其后期训练和评估成本仅为9美元，节省了约260倍的成本。我们的研究表明，LoRA在快速适应推理结构的同时，能够有效保留基础模型的知识，从而实现高效的推理能力。', title='高效推理，低成本实现'))
[24.04.2025 09:13] Querying the API.
[24.04.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE .
[24.04.2025 09:13] Response: {
  "desc": "Исследование анализирует влияние ошибок в разметке набора данных MSCOCO на бенчмарк объектных галлюцинаций POPE. Авторы провели повторную аннотацию изображений и обнаружили несбалансированность ошибок разметки в разных подмножествах. При оценке нескольких моделей на исправленных метках (названных RePOPE) были выявлены значительные изменения в рейтинге моделей. Это подчеркивает важность качества разметки для корректной оценки эффективности моделей машинного обучения.",
  "emoji": "🏷️",
  "title": "Качество разметки определяет точность оценки моделей"
}
[24.04.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE ."

[24.04.2025 09:13] Response: ```python
['DATASET', 'BENCHMARK', 'DATA']
```
[24.04.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE ."

[24.04.2025 09:13] Response: ```python
["HALLUCINATIONS", "OPEN_SOURCE"]
```
[24.04.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the effects of label errors in the MSCOCO dataset on the object hallucination benchmark known as POPE. The authors re-annotate the images in the benchmark to identify inconsistencies in the annotation quality across various subsets. By evaluating several models using the newly revised labels, referred to as RePOPE, they find significant changes in model performance rankings. This study emphasizes the critical importance of label accuracy in machine learning benchmarks and its influence on model evaluation outcomes.","title":"Revising Labels, Revising Rankings: The Impact of Annotation Quality"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the effects of label errors in the MSCOCO dataset on the object hallucination benchmark known as POPE. The authors re-annotate the images in the benchmark to identify inconsistencies in the annotation quality across various subsets. By evaluating several models using the newly revised labels, referred to as RePOPE, they find significant changes in model performance rankings. This study emphasizes the critical importance of label accuracy in machine learning benchmarks and its influence on model evaluation outcomes.', title='Revising Labels, Revising Rankings: The Impact of Annotation Quality'))
[24.04.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"在这项研究中，我们评估了MSCOCO数据集中标签错误对常用的物体幻觉基准POPE的影响。我们重新标注了基准图像，并发现不同子集之间的标注错误存在不平衡。通过在修订后的标签上评估多个模型，我们观察到模型排名发生了显著变化，突显了标签质量的重要性。此研究的代码和数据可在https://github.com/YanNeu/RePOPE获取。","title":"标签质量影响模型表现"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='在这项研究中，我们评估了MSCOCO数据集中标签错误对常用的物体幻觉基准POPE的影响。我们重新标注了基准图像，并发现不同子集之间的标注错误存在不平衡。通过在修订后的标签上评估多个模型，我们观察到模型排名发生了显著变化，突显了标签质量的重要性。此研究的代码和数据可在https://github.com/YanNeu/RePOPE获取。', title='标签质量影响模型表现'))
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#data", "#benchmark", "#optimization", "#dataset", "#reasoning"], "emoji": "🔄", "ru": {"title": "CRUST-Bench: новый бенчмарк для оценки транспиляции C в безопасный Rust", "desc": "CRUST-Bench - это новый набор данных для оценки транспиляции кода с C на безопасный Rust. Он включает 1
[24.04.2025 09:13] Using data from previous issue: {"categories": ["#data", "#interpretability", "#dataset", "#science"], "emoji": "☑️", "ru": {"title": "CheckboxQA: новый датасет для улучшения распознавания флажков в документах", "desc": "Статья представляет набор данных CheckboxQA, созданный для оценки и улучшения работы моделей машинного обучения
[24.04.2025 09:13] Trying to get texts in Chinese.
[24.04.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Visual reasoning is a core component of human intelligence and a critical capability for advanced multimodal models. Yet current reasoning evaluations of multimodal large language models (MLLMs) often rely on text descriptions and allow language-based reasoning shortcuts, failing to measure genuine vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark of 1,000 human-verified problems across six categories (e.g., quantitative shifts, spatial relations, attribute comparisons). These various types of questions can be evaluated to assess the visual reasoning capabilities of MLLMs from multiple perspectives. We evaluate leading MLLMs on this benchmark and analyze their results to identify common failure modes. Most models score below 30% accuracy-only slightly above the 25% random baseline and far below the 51.4% achieved by humans-revealing significant gaps in visual reasoning. Furthermore, we provide a supplementary training dataset and a reinforcement-learning baseline to support further progress.
[24.04.2025 09:13] Mistral response. {"id": "c3166c3903744c50934bab49c94b897e", "object": "chat.completion", "created": 1745486015, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u89c6\u89c9\u63a8\u7406\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u4e5f\u662f\u5148\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u5173\u952e\u80fd\u529b\u3002\u7136\u800c\uff0c\u5f53\u524d\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u8bc4\u4f30\u5f80\u5f80\u4f9d\u8d56\u6587\u672c\u63cf\u8ff0\uff0c\u5141\u8bb8\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u6377\u5f84\uff0c\u65e0\u6cd5\u8861\u91cf\u771f\u6b63\u7684\u89c6\u89c9\u4e2d\u5fc3\u63a8\u7406\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86VisuLogic\uff1a\u4e00\u4e2a\u5305\u542b1,000\u4e2a\u4eba\u7c7b\u9a8c\u8bc1\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u516d\u4e2a\u7c7b\u522b\uff08\u5982\u91cf\u5316\u8f6c\u6362\u3001\u7a7a\u95f4\u5173\u7cfb\u3001\u5c5e\u6027\u6bd4\u8f83\uff09\u3002\u8fd9\u4e9b\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\u53ef\u4ee5\u4ece\u591a\u4e2a\u89d2\u5ea6\u8bc4\u4f30MLLMs\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002\u6211\u4eec\u5bf9\u8be5\u57fa\u51c6\u8bc4\u4f30\u4e86\u9886\u5148\u7684MLLMs\uff0c\u5e76\u5206\u6790\u5176\u7ed3\u679c\uff0c\u4ee5\u8bc6\u522b\u5e38\u89c1\u7684\u5931\u8d25\u6a21\u5f0f\u3002\u5927\u591a\u6570\u6a21\u578b\u7684\u51c6\u786e\u7387\u4f4e\u4e8e30%\uff0c\u4ec5\u7565\u9ad8\u4e8e25%\u7684\u968f\u673a\u57fa\u7ebf\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768451.4%\uff0c\u663e\u793a\u51fa\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8865\u5145\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u8fdb\u5c55\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 244, "total_tokens": 558, "completion_tokens": 314}}
[24.04.2025 09:13] Response: 视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。
[24.04.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。
[24.04.2025 09:13] Mistral response. {"id": "52a9f0b7a8de41908180d068721f6327", "object": "chat.completion", "created": 1745486019, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u89c6\u89c9\u63a8\u7406\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u4e5f\u662f\u5148\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u5173\u952e\u80fd\u529b\u3002\u7136\u800c\uff0c\u5f53\u524d\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u8bc4\u4f30\u5f80\u5f80\u4f9d\u8d56\u6587\u672c\u63cf\u8ff0\uff0c\u5141\u8bb8\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u6377\u5f84\uff0c\u65e0\u6cd5\u8861\u91cf\u771f\u6b63\u7684\u89c6\u89c9\u4e2d\u5fc3\u63a8\u7406\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86VisuLogic\uff1a\u4e00\u4e2a\u5305\u542b1,000\u4e2a\u4eba\u7c7b\u9a8c\u8bc1\u95ee\u9898\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u516d\u4e2a\u7c7b\u522b\uff08\u5982\u91cf\u5316\u8f6c\u6362\u3001\u7a7a\u95f4\u5173\u7cfb\u3001\u5c5e\u6027\u6bd4\u8f83\uff09\u3002\u8fd9\u4e9b\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\u53ef\u4ee5\u4ece\u591a\u4e2a\u89d2\u5ea6\u8bc4\u4f30MLLMs\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002\u6211\u4eec\u5bf9\u8be5\u57fa\u51c6\u8bc4\u4f30\u4e86\u9886\u5148\u7684MLLMs\uff0c\u5e76\u5206\u6790\u5176\u7ed3\u679c\uff0c\u4ee5\u8bc6\u522b\u5e38\u89c1\u7684\u5931\u8d25\u6a21\u5f0f\u3002\u5927\u591a\u6570\u6a21\u578b\u7684\u51c6\u786e\u7387\u4f4e\u4e8e30%\uff0c\u4ec5\u7565\u9ad8\u4e8e25%\u7684\u968f\u673a\u57fa\u7ebf\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768451.4%\uff0c\u663e\u793a\u51fa\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8865\u5145\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u8fdb\u5c55\u3002\n\nSh\u00ecju\u00e9 tu\u012bl\u01d0 sh\u00ec r\u00e9nl\u00e8i zh\u00ecn\u00e9ng de h\u00e9x\u012bn z\u01d4ch\u00e9ng b\u00f9f\u0113n, y\u011b sh\u00ec xi\u0101nj\u00ecn du\u014d m\u00f3sh\u00ec du\u014d m\u00f3sh\u00ec m\u00f3x\u00edng de gu\u01cenji\u00e0n n\u00e9ngl\u00ec. R\u00e1n'\u00e9r, d\u0101ngqi\u00e1n du\u00ec du\u014d m\u00f3sh\u00ec d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (MLLMs) de tu\u012bl\u01d0 p\u00edngji\u00e0 w\u01cengw\u01ceng y\u012bl\u00e0i w\u00e9nb\u011bn mi\u00e1osh\u00f9, y\u01d4n x\u01d4 j\u012by\u00fa y\u01d4y\u00e1n de tu\u012bl\u01d0 ji\u00e9j\u00ecng, w\u00faf\u01ce h\u00e9ngli\u00e1ng zh\u0113nzh\u00e8ng de sh\u00ecju\u00e9 zh\u014dngx\u012bn tu\u012bl\u01d0. W\u00e8i ji\u011bju\u00e9 zh\u00e8 y\u012b w\u00e8nt\u00ed, w\u01d2men y\u01d0n r\u00f9le VisuLogic: y\u012bg\u00e8 b\u0101oh\u00e1n 1,000 g\u00e8 r\u00e9nl\u00e8i y\u00e0nzh\u00e8ng w\u00e8nt\u00ed de j\u012bzh\u01d4n, h\u00e1nji\u0113 li\u00f9 g\u00e8 l\u00e8ibi\u00e9 (r\u00fa li\u00e0ng hu\u00e0 zhu\u01cenhu\u00e0n, k\u014dngji\u0101n gu\u0101nx\u00ec, sh\u01d4x\u00ecng b\u01d0ji\u00e0o). Zh\u00e8xi\u0113 b\u00f9t\u00f3ng l\u00e8ix\u00edng de w\u00e8nt\u00ed k\u011by\u01d0 c\u00f3ng du\u014d g\u00e8 ji\u01ceod\u00f9 p\u00edngji\u00e0 MLLMs de sh\u00ecju\u00e9 tu\u012bl\u01d0 n\u00e9ngl\u00ec. W\u01d2men du\u00ec g\u0101i j\u012bzh\u01d4n p\u00edngji\u00e0 le l\u01d0ngxi\u0101n de MLLMs, b\u00ecng f\u0113nxi q\u00ed ji\u00e9gu\u01d2, y\u01d0 sh\u00edbi\u00e9 ch\u00e1ngji\u00e0n de sh\u012bb\u00e0i m\u00f3sh\u00ec. D\u00e0du\u014dsh\u00f9 m\u00f3x\u00edng de zh\u01d4nqu\u00e8l\u01dc d\u012by\u00fa 30%, j\u01d0n l\u00fc\u00e8 g\u0101oy\u00fa 25% de su\u00edj\u012b j\u012bzh\u01d4n, yu\u01cen d\u012by\u00fa r\u00e9nl\u00e8i de 51.4%, xi\u01censh\u00ec ch\u016b sh\u00ecju\u00e9 tu\u012bl\u01d0 f\u0101ngmi\u00e0n de xi\u01cenzh\u00f9 ch\u0101j\u00f9. C\u01d0w\u00e0i, w\u01d2men t\u00edg\u014dngle y\u012bg\u00e8 b\u01d4ch\u014dng x\u00f9nli\u00e0n sh\u00f9j\u00f9j\u00ed h\u00e9 y\u012bg\u00e8 qi\u00e1ngzh\u00f9 xu\u00e9x\u00ed j\u012bzh\u01d4n, y\u01d0 zh\u012bch\u00ed j\u00ecnf\u0101 de j\u00ecnb\u00f9."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 328, "total_tokens": 1270, "completion_tokens": 942}}
[24.04.2025 09:13] Response: 视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。

Shìjué tuīlǐ shì rénlèi zhìnéng de héxīn zǔchéng bùfēn, yě shì xiānjìn duō móshì duō móshì móxíng de guǎnjiàn nénglì. Rán'ér, dāngqián duì duō móshì dà yǔyán móxíng (MLLMs) de tuīlǐ píngjià wǎngwǎng yīlài wénběn miáoshù, yǔn xǔ jīyú yǔyán de tuīlǐ jiéjìng, wúfǎ héngliáng zhēnzhèng de shìjué zhōngxīn tuīlǐ. Wèi jiějué zhè yī wèntí, wǒmen yǐn rùle VisuLogic: yīgè bāohán 1,000 gè rénlèi yànzhèng wèntí de jīzhǔn, hánjiē liù gè lèibié (rú liàng huà zhuǎnhuàn, kōngjiān guānxì, shǔxìng bǐjiào). Zhèxiē bùtóng lèixíng de wèntí kěyǐ cóng duō gè jiǎodù píngjià MLLMs de shìjué tuīlǐ nénglì. Wǒmen duì gāi jīzhǔn píngjià le lǐngxiān de MLLMs, bìng fēnxi qí jiéguǒ, yǐ shíbié chángjiàn de shībài móshì. Dàduōshù móxíng de zhǔnquèlǜ dīyú 30%, jǐn lüè gāoyú 25% de suíjī jīzhǔn, yuǎn dīyú rénlèi de 51.4%, xiǎnshì chū shìjué tuīlǐ fāngmiàn de xiǎnzhù chājù. Cǐwài, wǒmen tígōngle yīgè bǔchōng xùnliàn shùjùjí hé yīgè qiángzhù xuéxí jīzhǔn, yǐ zhīchí jìnfā de jìnbù.
[24.04.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。
[24.04.2025 09:14] Mistral response. {"id": "6717b58236114906b231ae9870793f0f", "object": "chat.completion", "created": 1745486036, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u89c6\u89c9\", \"pinyin\": \"sh\u00ecju\u00e9\", \"trans\": \"vision\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012bl\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u6838\u5fc3\", \"pinyin\": \"h\u00e9x\u012bn\", \"trans\": \"core\"},\n    {\"word\": \"\u7ec4\u6210\u90e8\u5206\", \"pinyin\": \"z\u01d4ch\u00e9ng b\u00f9f\u0113n\", \"trans\": \"component\"},\n    {\"word\": \"\u5148\u8fdb\", \"pinyin\": \"xi\u0101nj\u00ecn\", \"trans\": \"advanced\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3sh\u00ec\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u5173\u952e\", \"pinyin\": \"gu\u01cenji\u00e0n\", \"trans\": \"key\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ngl\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u4f9d\u8d56\", \"pinyin\": \"y\u012bl\u00e0i\", \"trans\": \"rely on\"},\n    {\"word\": \"\u63cf\u8ff0\", \"pinyin\": \"mi\u00e1osh\u00f9\", \"trans\": \"description\"},\n    {\"word\": \"\u5141\u8bb8\", \"pinyin\": \"y\u01d4nx\u01d4\", \"trans\": \"allow\"},\n    {\"word\": \"\u57fa\u4e8e\", \"pinyin\": \"j\u012by\u00fa\", \"trans\": \"based on\"},\n    {\"word\": \"\u6377\u5f84\", \"pinyin\": \"ji\u00e9j\u00ecng\", \"trans\": \"shortcut\"},\n    {\"word\": \"\u8861\u91cf\", \"pinyin\": \"h\u00e9ngli\u00e1ng\", \"trans\": \"measure\"},\n    {\"word\": \"\u771f\u6b63\", \"pinyin\": \"zh\u0113nzh\u00e8ng\", \"trans\": \"genuine\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0nr\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u57fa\u51c6\", \"pinyin\": \"j\u012bzh\u01d4n\", \"trans\": \"benchmark\"},\n    {\"word\": \"\u9a8c\u8bc1\", \"pinyin\": \"y\u00e0nzh\u00e8ng\", \"trans\": \"verification\"},\n    {\"word\": \"\u95ee\u9898\", \"pinyin\": \"w\u00e8nt\u00ed\", \"trans\": \"question\"},\n    {\"word\": \"\u6db5\u76d6\", \"pinyin\": \"h\u00e1ng\u00e0i\", \"trans\": \"cover\"},\n    {\"word\": \"\u7c7b\u522b\", \"pinyin\": \"l\u00e8ibi\u00e9\", \"trans\": \"category\"},\n    {\"word\": \"\u91cf\u5316\", \"pinyin\": \"li\u00e0nghu\u00e0\", \"trans\": \"quantification\"},\n    {\"word\": \"\u8f6c\u6362\", \"pinyin\": \"zhu\u01cenhu\u00e0n\", \"trans\": \"conversion\"},\n    {\"word\": \"\u7a7a\u95f4\", \"pinyin\": \"k\u014dngji\u0101n\", \"trans\": \"space\"},\n    {\"word\": \"\u5173\u7cfb\", \"pinyin\": \"gu\u0101nx\u00ec\", \"trans\": \"relationship\"},\n    {\"word\": \"\u5c5e\u6027\", \"pinyin\": \"sh\u01d4x\u00ecng\", \"trans\": \"attribute\"},\n    {\"word\": \"\u6bd4\u8f83\", \"pinyin\": \"b\u01d0ji\u00e0o\", \"trans\": \"comparison\"},\n    {\"word\": \"\u89d2\u5ea6\", \"pinyin\": \"ji\u01ceod\u00f9\", \"trans\": \"angle\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edngg\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"\u9886\u5148\", \"pinyin\": \"l\u01d0ngxi\u0101n\", \"trans\": \"leading\"},\n    {\"word\": \"\u5206\u6790\", \"pinyin\": \"f\u0113nx\u012b\", \"trans\": \"analyze\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u8bc6\u522b\", \"pinyin\": \"sh\u00edbi\u00e9\", \"trans\": \"identify\"},\n    {\"word\": \"\u6a21\u5f0f\", \"pinyin\": \"m\u00f3sh\u00ec\", \"trans\": \"pattern\"},\n    {\"word\": \"\u51c6\u786e\u7387\", \"pinyin\": \"zh\u01d4nqu\u00e8l\u01dc\", \"trans\": \"accuracy\"},\n    {\"word\": \"\u968f\u673a\", \"pinyin\": \"su\u00edj\u012b\", \"trans\": \"random\"},\n    {\"word\": \"\u57fa\u7ebf\", \"pinyin\": \"j\u012bxi\u00e0n\", \"trans\": \"baseline\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cenzh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u5dee\u8ddd\", \"pinyin\": \"ch\u0101j\u00f9\", \"trans\": \"gap\"},\n    {\"word\": \"\u8865\u5145\", \"pinyin\": \"b\u01d4ch\u014dng\", \"trans\": \"supplement\"},\n    {\"word\": \"\u6570\u636e\u96c6\", \"pinyin\": \"sh\u00f9j\u00f9j\u00ed\", \"trans\": \"dataset\"},\n    {\"word\": \"\u5f3a\u5316\", \"pinyin\": \"qi\u00e1nghu\u00e0\", \"trans\": \"reinforce\"},\n    {\"word\": \"\u5b66\u4e60\", \"pinyin\": \"xu\u00e9x\u00ed\", \"trans\": \"learning\"},\n    {\"word\": \"\u652f\u6301\", \"pinyin\": \"zh\u012bch\u00ed\", \"trans\": \"support\"},\n    {\"word\": \"\u8fdb\u5c55\", \"pinyin\": \"j\u00ecnzh\u01cen\", \"trans\": \"progress\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 360, "total_tokens": 1647, "completion_tokens": 1287}}
[24.04.2025 09:14] Response: [
    {"word": "视觉", "pinyin": "shìjué", "trans": "vision"},
    {"word": "推理", "pinyin": "tuīlǐ", "trans": "reasoning"},
    {"word": "核心", "pinyin": "héxīn", "trans": "core"},
    {"word": "组成部分", "pinyin": "zǔchéng bùfēn", "trans": "component"},
    {"word": "先进", "pinyin": "xiānjìn", "trans": "advanced"},
    {"word": "多模态", "pinyin": "duō móshì", "trans": "multimodal"},
    {"word": "模型", "pinyin": "móxíng", "trans": "model"},
    {"word": "关键", "pinyin": "guǎnjiàn", "trans": "key"},
    {"word": "能力", "pinyin": "nénglì", "trans": "ability"},
    {"word": "依赖", "pinyin": "yīlài", "trans": "rely on"},
    {"word": "描述", "pinyin": "miáoshù", "trans": "description"},
    {"word": "允许", "pinyin": "yǔnxǔ", "trans": "allow"},
    {"word": "基于", "pinyin": "jīyú", "trans": "based on"},
    {"word": "捷径", "pinyin": "jiéjìng", "trans": "shortcut"},
    {"word": "衡量", "pinyin": "héngliáng", "trans": "measure"},
    {"word": "真正", "pinyin": "zhēnzhèng", "trans": "genuine"},
    {"word": "引入", "pinyin": "yǐnrù", "trans": "introduce"},
    {"word": "基准", "pinyin": "jīzhǔn", "trans": "benchmark"},
    {"word": "验证", "pinyin": "yànzhèng", "trans": "verification"},
    {"word": "问题", "pinyin": "wèntí", "trans": "question"},
    {"word": "涵盖", "pinyin": "hángài", "trans": "cover"},
    {"word": "类别", "pinyin": "lèibié", "trans": "category"},
    {"word": "量化", "pinyin": "liànghuà", "trans": "quantification"},
    {"word": "转换", "pinyin": "zhuǎnhuàn", "trans": "conversion"},
    {"word": "空间", "pinyin": "kōngjiān", "trans": "space"},
    {"word": "关系", "pinyin": "guānxì", "trans": "relationship"},
    {"word": "属性", "pinyin": "shǔxìng", "trans": "attribute"},
    {"word": "比较", "pinyin": "bǐjiào", "trans": "comparison"},
    {"word": "角度", "pinyin": "jiǎodù", "trans": "angle"},
    {"word": "评估", "pinyin": "pínggū", "trans": "evaluate"},
    {"word": "领先", "pinyin": "lǐngxiān", "trans": "leading"},
    {"word": "分析", "pinyin": "fēnxī", "trans": "analyze"},
    {"word": "结果", "pinyin": "jiéguǒ", "trans": "result"},
    {"word": "识别", "pinyin": "shíbié", "trans": "identify"},
    {"word": "模式", "pinyin": "móshì", "trans": "pattern"},
    {"word": "准确率", "pinyin": "zhǔnquèlǜ", "trans": "accuracy"},
    {"word": "随机", "pinyin": "suíjī", "trans": "random"},
    {"word": "基线", "pinyin": "jīxiàn", "trans": "baseline"},
    {"word": "显著", "pinyin": "xiǎnzhù", "trans": "significant"},
    {"word": "差距", "pinyin": "chājù", "trans": "gap"},
    {"word": "补充", "pinyin": "bǔchōng", "trans": "supplement"},
    {"word": "数据集", "pinyin": "shùjùjí", "trans": "dataset"},
    {"word": "强化", "pinyin": "qiánghuà", "trans": "reinforce"},
    {"word": "学习", "pinyin": "xuéxí", "trans": "learning"},
    {"word": "支持", "pinyin": "zhīchí", "trans": "support"},
    {"word": "进展", "pinyin": "jìnzhǎn", "trans": "progress"}
]
[24.04.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。
[24.04.2025 09:14] Mistral response. {"id": "05149538b593489d93a802656291c3fe", "object": "chat.completion", "created": 1745486051, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Visual reasoning is a core component of human intelligence and a key capability of advanced multimodal models. However, current evaluations of reasoning in multimodal large language models (MLLMs) often rely on textual descriptions, allowing for language-based reasoning shortcuts that fail to measure true vision-centric reasoning. To address this issue, we introduce VisuLogic: a benchmark containing 1,000 human-verified questions covering six categories (such as quantitative transformations, spatial relationships, and attribute comparisons). These different types of questions can evaluate the visual reasoning capabilities of MLLMs from multiple angles. We evaluated leading MLLMs on this benchmark and analyzed the results to identify common failure modes. Most models achieved an accuracy of less than 30%, only slightly above the 25% random baseline and significantly lower than the human accuracy of 51.4%, indicating a notable gap in visual reasoning. Additionally, we provide a supplementary training dataset and a reinforcement learning baseline to support further advancements."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 326, "total_tokens": 545, "completion_tokens": 219}}
[24.04.2025 09:14] Response: Visual reasoning is a core component of human intelligence and a key capability of advanced multimodal models. However, current evaluations of reasoning in multimodal large language models (MLLMs) often rely on textual descriptions, allowing for language-based reasoning shortcuts that fail to measure true vision-centric reasoning. To address this issue, we introduce VisuLogic: a benchmark containing 1,000 human-verified questions covering six categories (such as quantitative transformations, spatial relationships, and attribute comparisons). These different types of questions can evaluate the visual reasoning capabilities of MLLMs from multiple angles. We evaluated leading MLLMs on this benchmark and analyzed the results to identify common failure modes. Most models achieved an accuracy of less than 30%, only slightly above the 25% random baseline and significantly lower than the human accuracy of 51.4%, indicating a notable gap in visual reasoning. Additionally, we provide a supplementary training dataset and a reinforcement learning baseline to support further advancements.
[24.04.2025 09:14] Renaming data file.
[24.04.2025 09:14] Renaming previous data. hf_papers.json to ./d/2025-04-24.json
[24.04.2025 09:14] Saving new data file.
[24.04.2025 09:14] Generating page.
[24.04.2025 09:14] Renaming previous page.
[24.04.2025 09:14] Renaming previous data. index.html to ./d/2025-04-24.html
[24.04.2025 09:14] [Experimental] Generating Chinese page for reading.
[24.04.2025 09:14] Chinese vocab [{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'vision'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'}, {'word': '核心', 'pinyin': 'héxīn', 'trans': 'core'}, {'word': '组成部分', 'pinyin': 'zǔchéng bùfēn', 'trans': 'component'}, {'word': '先进', 'pinyin': 'xiānjìn', 'trans': 'advanced'}, {'word': '多模态', 'pinyin': 'duō móshì', 'trans': 'multimodal'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '依赖', 'pinyin': 'yīlài', 'trans': 'rely on'}, {'word': '描述', 'pinyin': 'miáoshù', 'trans': 'description'}, {'word': '允许', 'pinyin': 'yǔnxǔ', 'trans': 'allow'}, {'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'}, {'word': '捷径', 'pinyin': 'jiéjìng', 'trans': 'shortcut'}, {'word': '衡量', 'pinyin': 'héngliáng', 'trans': 'measure'}, {'word': '真正', 'pinyin': 'zhēnzhèng', 'trans': 'genuine'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'}, {'word': '验证', 'pinyin': 'yànzhèng', 'trans': 'verification'}, {'word': '问题', 'pinyin': 'wèntí', 'trans': 'question'}, {'word': '涵盖', 'pinyin': 'hángài', 'trans': 'cover'}, {'word': '类别', 'pinyin': 'lèibié', 'trans': 'category'}, {'word': '量化', 'pinyin': 'liànghuà', 'trans': 'quantification'}, {'word': '转换', 'pinyin': 'zhuǎnhuàn', 'trans': 'conversion'}, {'word': '空间', 'pinyin': 'kōngjiān', 'trans': 'space'}, {'word': '关系', 'pinyin': 'guānxì', 'trans': 'relationship'}, {'word': '属性', 'pinyin': 'shǔxìng', 'trans': 'attribute'}, {'word': '比较', 'pinyin': 'bǐjiào', 'trans': 'comparison'}, {'word': '角度', 'pinyin': 'jiǎodù', 'trans': 'angle'}, {'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'}, {'word': '领先', 'pinyin': 'lǐngxiān', 'trans': 'leading'}, {'word': '分析', 'pinyin': 'fēnxī', 'trans': 'analyze'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '识别', 'pinyin': 'shíbié', 'trans': 'identify'}, {'word': '模式', 'pinyin': 'móshì', 'trans': 'pattern'}, {'word': '准确率', 'pinyin': 'zhǔnquèlǜ', 'trans': 'accuracy'}, {'word': '随机', 'pinyin': 'suíjī', 'trans': 'random'}, {'word': '基线', 'pinyin': 'jīxiàn', 'trans': 'baseline'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '差距', 'pinyin': 'chājù', 'trans': 'gap'}, {'word': '补充', 'pinyin': 'bǔchōng', 'trans': 'supplement'}, {'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'}, {'word': '强化', 'pinyin': 'qiánghuà', 'trans': 'reinforce'}, {'word': '学习', 'pinyin': 'xuéxí', 'trans': 'learning'}, {'word': '支持', 'pinyin': 'zhīchí', 'trans': 'support'}, {'word': '进展', 'pinyin': 'jìnzhǎn', 'trans': 'progress'}]
[24.04.2025 09:14] Renaming previous Chinese page.
[24.04.2025 09:14] Renaming previous data. zh.html to ./d/2025-04-23_zh_reading_task.html
[24.04.2025 09:14] Writing Chinese reading task.
[24.04.2025 09:14] Writing result.
[24.04.2025 09:14] Renaming log file.
[24.04.2025 09:14] Renaming previous data. log.txt to ./logs/2025-04-24_last_log.txt
