[25.02.2025 15:12] Read previous papers.
[25.02.2025 15:12] Generating top page (month).
[25.02.2025 15:12] Writing top page (month).
[25.02.2025 16:13] Read previous papers.
[25.02.2025 16:13] Get feed.
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17129
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17258
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17157
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15814
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16584
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17435
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16614
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17407
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16894
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16033
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15894
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17110
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16707
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16922
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17055
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14132
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17414
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15987
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16701
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15425
[25.02.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2502.14429
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15122
[25.02.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2502.16622
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17237
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13074
[25.02.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15167
[25.02.2025 16:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.02.2025 16:13] No deleted papers detected.
[25.02.2025 16:13] Downloading and parsing papers (pdf, html). Total: 26.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17129.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17129.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17129.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17258.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17258.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17258.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17157.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17157.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17157.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.15814.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.15814.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.15814.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16584.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.16584.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.16584.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17435.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17435.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17435.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16614.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.16614.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.16614.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17407.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17407.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17407.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16894.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.16894.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.16894.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16033.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.16033.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.16033.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.15894.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.15894.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.15894.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17110.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17110.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17110.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16707.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.16707.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.16707.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16922.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.16922.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.16922.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17055.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17055.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17055.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.14132.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.14132.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.14132.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17414.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17414.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17414.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.15987.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.15987.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.15987.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16701.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.16701.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.16701.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.15425.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.15425.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.15425.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.14429.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.14429.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.14429.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.15122.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.15122.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.15122.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.16622.
[25.02.2025 16:13] Downloading paper 2502.16622 from http://arxiv.org/pdf/2502.16622v1...
[25.02.2025 16:13] Extracting affiliations from text.
[25.02.2025 16:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures Luis Lara 1 2 Lucia Eve Berger 1 2 Rajesh Raju 1 2 Shawn Whitfield 1 2 5 2 0 2 3 2 ] I . e [ 1 2 2 6 6 1 . 2 0 5 2 : r a "
[25.02.2025 16:13] Response: ```python
[]
```
[25.02.2025 16:13] Extracting affiliations from text.
[25.02.2025 16:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures Luis Lara 1 2 Lucia Eve Berger 1 2 Rajesh Raju 1 2 Shawn Whitfield 1 2 5 2 0 2 3 2 ] I . e [ 1 2 2 6 6 1 . 2 0 5 2 : r aThe COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of patients condition from CXRs. In this study, we produce large COVID severity dataset by merging three sources and investigate the efficacy of transfer learning using ImageNetand CXR-pretrained models and vision transformers (ViTs) in both severity regression and classification tasks. pretrained DenseNet161 model performed the best on the three class severity prediction problem, reaching 80% accuracy overall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases, respectively. The ViT had the best regression results1, with mean absolute error of 0.5676 compared to radiologist-predicted severity scores. The projects source code is publicly available2. 1. Introduction The SARS-CoV2 (COVID-19) pandemic that began in late 2019 had seismic effect on social patterns, technologies and health-care. COVID-19 often presents as pneumonia (COVID-pneumonia) and can quickly become lifethreatening. Medical establishments across the world are still feeling the effects of increased disease burdens and shortages of qualified personnel. For healthcare practitioners needing to triage and decide where best to allocate their resources, cheap, scalable and easy-to-administer tests are essential. Molecular tests indicate whether or not patient has COVID-19 but do not give an indication of its sever1Universite de Montreal, Canada 2Mila Quebec AI Institute, Canada. Correspondence to: Luis Lara <luis.lara@mila.quebec>. Copyright 2023 by the author(s). 1https://huggingface.co/ludolara/vit-COV ID-19-severity 2https://github.com/stwhitfield/covid-sev erity ity. Chest X-rays (CXRs) are favoured when diagnosing suspected COVID-19 (Rubin et al., 2020) because they are relatively cheap, can be taken bedside and can give an indication of severity. Machine Learning (ML) technologies have been suggested since the beginning of the pandemic, and applied in emergency rooms (Shamout et al., 2021), as means of reducing the burden on healthcare workers by increasing accuracy, accelerating diagnosis and suggesting outcomes. At this point, many ML studies have classified COVID-19 vs. nonCOVID-19 patients with high accuracy (for example, (Khan et al., 2020; Minaee et al., 2020; Gupta et al., 2021; Jin et al., 2020; Oh et al., 2020; Tabik et al., 2020; Chakraborty et al., 2022; Wang et al., 2020a; Elkorany & Elsharkawy, 2021; Bhattacharyya et al., 2022; Nasir et al., 2023; Wang et al., 2021a)) but far fewer studies stratify patients by disease severity (Cohen et al., 2020a; Le Dinh et al., 2022; Shamout et al., 2021; Signoroni et al., 2021; Wong et al., 2021; Zandehshahvar et al., 2021) despite it being identified early as critical task (Cohen et al., 2020c). This is partly because obtaining radiologist scores of severity adds an additional cost barrier, so fewer COVID CXR datasets contain this knowledge and are suitable for the task. The contributions of the current study are: 1) Building large COVID severity dataset by combining datasets with different severity scores into single score. 2) Improving on state-of-the-art predictions of COVID severity for the regression task. 3) Comparing the performance of models pretrained on general images (e.g. ImageNet) with models specifically trained on CXRs. We obtain competitive results in the classification (80% overall accuracy) and regression (0.5676 mean absolute error) tasks using pretrained models fine-tuned on our dataset. Development of ML models that accurately predict severity for COVID-19, while providing graphical explanations of its decisions, is useful for physicians to prioritize patients with the best chance of recovery or to intervene at an earlier time point, saving the health system money, helping patients and unburdening health care professionals. Diagnosing Medical Images for COVID-19 2. Related work Severity scoring X-rays of patients with COVID-19 show opacity correlating with the degree of severity of the disease (Wong et al., 2020). Fluid build-up in lungs with COVID-pneumonia reduces the transmittance of the x-ray, resulting in cloudier/more opaque lung images Figure 1. Different scoring systems have been developed to attempt to quantify this severity; the most relevant to the current study are the Brixia score (Borghesi & Maroldi, 2020; Signoroni et al., 2021) and the opacity score (Cohen et al., 2020c;a). Figure 1. Lung opacity often increases with COVID severity Left to right: normal, moderate and severe levels of opacity in lung CXRs of patients with COVID. Early in the pandemic, medical services in Brescia, Italy adopted the Brixia score (Borghesi & Maroldi, 2020), semi-quantitative metric where lungs are divided into six regions, with each region assigned rating from 0 to 3 based on severity as assessed by radiologist (Borghesi & Maroldi, 2020; Signoroni et al., 2021). modified version of the Brixia score, with higher spatial resolution, is additionally associated with severity (Jensen et al., 2022). Cohen et al. compiled an early COVID-19 CXR dataset (Cohen et al., 2020c) that has been hugely influential in the CXR COVID-19 prediction space (Garcia Santa Cruz et al., 2021). subset of their data (93 images) is accompanied by score for degree of opacity (DO) (scored 0-6 for both lungs together) reflects the intensity of opacity: 0 = no opacity; 1 = ground glass opacity; 2 = consolidation; 3 = white-out (Cohen et al., 2020a). Since the degree of opacity can be mapped to the Brixia score by linear regression (Signoroni et al., 2021), we chose to use the opacity score as unifying score for our study. Prediction tasks Authors generally take one of two approaches to severity prediction: treat the problem 1) as classification task, and predict whether patients fall into mild, moderate or severe categories (Le Dinh et al., 2022; Tabik et al., 2020; Zandehshahvar et al., 2021; Gourdeau et al., 2022a), or 2) as regression task and aim to match as closely as possible the scores of professional radiologists (Cohen et al., 2020a; Signoroni et al., 2021). Due to the evolving nature of the COVID pandemic,"
[25.02.2025 16:13] Mistral response. {"id": "b499f24331c2448087a3ce5b4d1f6744", "object": "chat.completion", "created": 1740499994, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Universite de Montreal, Canada\", \"Mila Quebec AI Institute, Canada\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1908, "total_tokens": 1931, "completion_tokens": 23}}
[25.02.2025 16:13] Response: ```python
["Universite de Montreal, Canada", "Mila Quebec AI Institute, Canada"]
```
[25.02.2025 16:13] Deleting PDF ./assets/pdf/2502.16622.pdf.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.17237.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.17237.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.17237.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.13074.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.13074.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.13074.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2502.15167.
[25.02.2025 16:13] Extra JSON file exists (./assets/json/2502.15167.json), skip PDF parsing.
[25.02.2025 16:13] Paper image links file exists (./assets/img_data/2502.15167.json), skip HTML parsing.
[25.02.2025 16:13] Success.
[25.02.2025 16:13] Enriching papers with extra data.
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 0. Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 1. Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained ed...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 2. Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonst...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 3. We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other componen...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 4. Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While inst...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 5. Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic ...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 6. The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks ...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 7. Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward M...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 8. While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leve...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 9. Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 10. Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this ...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 11. The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we int...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 12. Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a fra...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 13. Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 14. This paper comprehensively evaluates several recently proposed optimizers for 4-bit training, revealing that low-bit precision amplifies sensitivity to learning rates and often causes unstable gradient norms, leading to divergence at higher learning rates. Among these, SPAM, a recent optimizer featu...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 15. Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking or...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 16. We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize ...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 17. As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific li...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 18. Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access r...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 19. Hierarchical organization is fundamental to biological systems and human societies, yet artificial intelligence systems often rely on monolithic architectures that limit adaptability and scalability. Current hierarchical reinforcement learning (HRL) approaches typically restrict hierarchies to two l...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 20. Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) re...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 21. We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the dataset...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 22. The COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of a patient's condition from CXRs. In this...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 23. Retrieving images from the same location as a given query is an important component of multiple computer vision tasks, like Visual Place Recognition, Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However, existing solutions are built to specifically work for one of these task...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 24. The Brownian sphere is a random metric space, homeomorphic to the two-dimensional sphere, which arises as the universal scaling limit of many types of random planar maps. The direct construction of the Brownian sphere is via a continuous analogue of the Cori--Vauquelin--Schaeffer (CVS) bijection. Th...
[25.02.2025 16:13] ********************************************************************************
[25.02.2025 16:13] Abstract 25. The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehe...
[25.02.2025 16:13] Read previous papers.
[25.02.2025 16:13] Generating reviews via LLM API.
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#survey", "#training", "#architecture"], "emoji": "üîç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –≥—Ä–∞–Ω–∏—Ü—ã: –ø—É—Ç—å –∫ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#diffusion", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "VideoGrain - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–º—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion", "#cv", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DICEPTION - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#audio", "#synthetic", "#data", "#optimization", "#architecture", "#open_source", "#training"], "emoji": "üó£Ô∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Slam - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#audio", "#dataset"], "emoji": "üéµ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Audio-FLAN - –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–µ —Å –∞—É–¥–∏–æ. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 80 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–µ—á–∏, –º—É–∑—ã–∫–∏ –∏ –∑–≤—É–∫–∞, —Å–æ–¥–µ—Ä–∂
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#inference", "#cv"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è –¥–ª—è –ª—é–±—ã—Ö –∫–∞–º–µ—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ GCC –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. GCC –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning"], "emoji": "üî¨", "ru": {"title": "CodeCriticBench: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CodeCriticBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫—Ä–∏—Ç–∏
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#long_context", "#multilingual", "#benchmark", "#math"], "emoji": "üåê", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ø–ú –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞: –≤—ã–∑–æ–≤—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MCLM - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ —Å –∑–∞–¥–∞—á–∞–º–∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#reasoning"], "emoji": "üêê", "ru": {"title": "GOAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", "desc": "–≠—Ça —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GOAT (Great LoRA Mixture-of-Expert) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: –æ—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMIR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "RIFLEx: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ RIFLEx –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–æ
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#video", "#agents"], "emoji": "üì±", "ru": {"title": "–í–∏–¥–µ–æ-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ò–ò: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "Mobile-Agent-V - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–∏–¥–µ–æ–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ 
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ VLM –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ 
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#science", "#reasoning", "#multilingual", "#benchmark"], "emoji": "‚è≥", "ru": {"title": "CTM: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –Ω–∏–∑–∫–æ–π –±–∏—Ç–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –æ—Ü–µ–Ω–∫—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è 4-–±–∏—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –Ω–∏–∑–∫–æ–±–∏—Ç–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —É—Å–∏–ª–∏–≤–∞–µ—Ç —á—É
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#multimodal", "#science", "#ethics", "#data", "#dataset"], "emoji": "üîç", "ru": {"title": "–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ—Ä–∞—Ü–∏–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–æ–º –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ—Ä–∞—Ü–∏–µ–π –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#architecture", "#video", "#multimodal"], "emoji": "üíÉ", "ru": {"title": "–¢–∞–Ω—Ü—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –ò–ò –æ–∂–∏–≤–ª—è–µ—Ç —Ñ–æ—Ç–æ –ø–æ–¥ –º—É–∑—ã–∫—É", "desc": "X-Dancer - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Ç–∞–Ω—Ü–µ–≤ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–¥ –º—É–∑—ã–∫—É –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#science", "#benchmark", "#open_source", "#training"], "emoji": "üìà", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –≤–ª–∏—è–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –Ω–∞—É—á–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç framework –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è open-weight –º–æ–¥–µ–ª–µ–π –≤ —Å—Ñ–µ—Ä–µ –ò–ò. –ê–≤—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç –º–æ–¥–µ–ª—å –í–∞–Ω–≥–∞ 
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#ethics", "#open_source"], "emoji": "üîì", "ru": {"title": "–î–æ—Å—Ç—É–ø –∫ –ò–ò: –±–æ–ª—å—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ —Ä–µ–ª–∏–∑", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –≤—ã—Ö–æ–¥—è—â–∏–µ –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –æ —Ä–µ–ª–∏–∑–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#optimization", "#rl", "#games", "#architecture", "#agents", "#benchmark", "#training"], "emoji": "üå≥", "ru": {"title": "–î–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TAME Agent Framework (TAG) - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ø–æ–ª–Ω–æ—Å—Ç—å—é 
[25.02.2025 16:13] Querying the API.
[25.02.2025 16:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) reducing the cost of quality estimation at scale, and (2) developing an inexpensive uncertainty estimation method for quality estimation. To address the latter, we introduce Instant Confidence COMET, an uncertainty-aware quality estimation model that matches the performance of previous approaches at a fraction of their costs. We extend this to Early-Exit COMET, a quality estimation model that can compute quality scores and associated confidences already at early model layers, allowing us to early-exit computations and reduce evaluation costs. We also apply our model to machine translation reranking. We combine Early-Exit COMET with an upper confidence bound bandit algorithm to find the best candidate from a large pool without having to run the full evaluation model on all candidates. In both cases (evaluation and reranking) our methods reduce the required compute by 50% with very little degradation in performance.
[25.02.2025 16:13] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#benchmark", "#dataset"], "emoji": "ü¶ñ", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "MONSTER - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∫—Ä—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. –û–Ω —Å–æ–∑–¥–∞–Ω –≤ –ø—Ä–æ—Ç–∏–≤–æ–≤–µ—Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º UCR –∏ UEA, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ
[25.02.2025 16:13] Querying the API.
[25.02.2025 16:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of a patient's condition from CXRs. In this study, we produce a large COVID severity dataset by merging three sources and investigate the efficacy of transfer learning using ImageNet- and CXR-pretrained models and vision transformers (ViTs) in both severity regression and classification tasks. A pretrained DenseNet161 model performed the best on the three class severity prediction problem, reaching 80% accuracy overall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases, respectively. The ViT had the best regression results, with a mean absolute error of 0.5676 compared to radiologist-predicted severity scores. The project's source code is publicly available.
[25.02.2025 16:13] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "üó∫Ô∏è", "ru": {"title": "MegaLoc: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "MegaLoc - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#math"], "emoji": "üåê", "ru": {"title": "–û–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±—Ä–æ—É–Ω–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä—ã –≤ —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ—Ä–µ–≤–æ", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –±–∏–µ–∫—Ü–∏–∏ –ö–æ—Ä–∏-–í–æ–∫–µ–ª–µ–Ω–∞-–®–µ—Ñ—Ñ–µ—Ä–∞ (CVS) –¥–ª—è –±—Ä–æ—É–Ω–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä—ã. –ë—Ä–æ—É–Ω–æ–≤—Å–∫–∞—è —Å—Ñ–µ—Ä–∞ - —ç—Ç–æ —Å–ª—É—á–∞–π–Ω–æ–µ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
[25.02.2025 16:13] Using data from previous issue: {"categories": ["#training", "#multimodal", "#optimization", "#benchmark", "#interpretability", "#agi"], "emoji": "üñºÔ∏è", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ò–ò-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö
[25.02.2025 16:13] Loading Chinese text from previous data.
[25.02.2025 16:13] Renaming data file.
[25.02.2025 16:13] Renaming previous data. hf_papers.json to ./d/2025-02-25.json
[25.02.2025 16:13] Saving new data file.
[25.02.2025 16:13] Generating page.
[25.02.2025 16:13] Renaming previous page.
[25.02.2025 16:13] Renaming previous data. index.html to ./d/2025-02-25.html
[25.02.2025 16:13] [Experimental] Generating Chinese page for reading.
[25.02.2025 16:13] Chinese vocab [{'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'ËÆ°ÁÆóËµÑÊ∫ê', 'pinyin': 'j√¨ su√†n zƒ´ yu√°n', 'trans': 'computational resources'}, {'word': 'ËÆ≠ÁªÉÊï∞ÊçÆ', 'pinyin': 'x√πn li√†n sh√π j√π', 'trans': 'training data'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íu xi√†n', 'trans': 'limited'}, {'word': 'Â§ö‰ªªÂä°', 'pinyin': 'du≈ç r√®n w√π', 'trans': 'multi-task'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çng y√≤ng', 'trans': 'general-purpose'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perception'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-trained'}, {'word': 'ÊñáÊú¨Âà∞ÂõæÂÉè', 'pinyin': 'w√©n bƒõn d√†o t√∫ xi√†ng', 'trans': 'text-to-image'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': '‰ªÖ', 'pinyin': 'j«ên', 'trans': 'only'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unify'}, {'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'encoding'}, {'word': 'ÂÖÖÂàÜÂà©Áî®', 'pinyin': 'ch≈çng f√®n l√¨ y√≤ng', 'trans': 'fully utilize'}, {'word': 'ÈÄÇÂ∫î', 'pinyin': 'sh√¨ y√¨ng', 'trans': 'adapt'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'}]
[25.02.2025 16:13] Renaming previous Chinese page.
[25.02.2025 16:13] Renaming previous data. zh.html to ./d/2025-02-24_zh_reading_task.html
[25.02.2025 16:13] Writing Chinese reading task.
[25.02.2025 16:13] Writing result.
[25.02.2025 16:13] Renaming log file.
[25.02.2025 16:13] Renaming previous data. log.txt to ./logs/2025-02-25_last_log.txt
