[25.02.2025 09:12] Read previous papers.
[25.02.2025 09:12] Generating top page (month).
[25.02.2025 09:12] Writing top page (month).
[25.02.2025 10:11] Read previous papers.
[25.02.2025 10:11] Get feed.
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17157
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17129
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15814
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16584
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17435
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16614
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16033
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16894
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17407
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17110
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16922
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15894
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16707
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15987
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16701
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17258
[25.02.2025 10:11] Extract page data from URL. URL: https://huggingface.co/papers/2502.14132
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15122
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17414
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13074
[25.02.2025 10:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15167
[25.02.2025 10:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.02.2025 10:11] No deleted papers detected.
[25.02.2025 10:11] Downloading and parsing papers (pdf, html). Total: 21.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.17157.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.17157.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.17157.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.17129.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.17129.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.17129.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.15814.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.15814.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.15814.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.16584.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.16584.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.16584.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.17435.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.17435.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.17435.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.16614.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.16614.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.16614.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.16033.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.16033.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.16033.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.16894.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.16894.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.16894.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.17407.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.17407.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.17407.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.17110.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.17110.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.17110.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.16922.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.16922.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.16922.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.15894.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.15894.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.15894.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.16707.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.16707.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.16707.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.15987.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.15987.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.15987.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.16701.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.16701.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.16701.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.17258.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.17258.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.17258.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.14132.
[25.02.2025 10:11] Downloading paper 2502.14132 from http://arxiv.org/pdf/2502.14132v1...
[25.02.2025 10:11] Extracting affiliations from text.
[25.02.2025 10:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Can Community Notes Replace Professional Fact-Checkers? nb@di.ku.dk grwa@di.ku.dk augenstein@di.ku.dk 5 2 0 2 9 1 ] . [ 1 2 3 1 4 1 . 2 0 5 2 : r de@di.ku.dk "
[25.02.2025 10:11] Response: ```python
[]
```
[25.02.2025 10:11] Extracting affiliations from text.
[25.02.2025 10:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Can Community Notes Replace Professional Fact-Checkers?nb@di.ku.dk grwa@di.ku.dk augenstein@di.ku.dk 5 2 0 2 9 1 ] . [ 1 2 3 1 4 1 . 2 0 5 2 : r de@di.ku.dkTwo commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite factchecking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking.The proliferation of misinformation on social media (Arnold, 2020; Diakopoulos, 2020), along with the rise of generative AI (Augenstein et al., 2024) have led to increasing concerns about its current and future potential harms, (e.g., to health (Clemente et al., 2022)) and threats to democracy and political stability (Reglitz, 2022). Fact-checkers play crucial role in combatting misinformation (Graves, 2017), and in recent years, have partnered with social media platforms, e.g., Meta, YouTube, and TikTok, to tackle its spread on these platforms. However, due to the scale of misleading content shared online, community moderation (e.g., options to flag potential misinformation, group/server moderators) is often employed Figure 1: An example of community note. Notice the fact-checking link and rating. in parallel (Morrow et al., 2022), as complementary approach (e.g., (Google, 2025); see also the practice of snoping (Pilarski et al., 2024)). The expansion of fact-checking projects in the last decade (Lauer and Graves, 2024), alongside their broader initiatives to curb misinformation (e.g., citizen media literacy programmes (Juneja and Mitra, 2022)) have been aided by partnerships with social media platforms such as Meta and Google (Graves and Anderson, 2020), which fund independent fact-checking agencies to fact-check potentially false claims on their platform.1 However, political pressure and accusations of bias and censorship, and most recently, Metas announcement of its plans to end its partnerships with fact-checkers in the U.S. and implement community moderation model (Meta, 2025), threatens the financial stability of fact-checking organisations, and hence, their ability to keep up with the increasing volume and sophistication of misinformation spread (Stencel et al., 2024; IFCN, 2024). Metas recent policy shift also implies that these two strategies (fact-checking and community notes) are independent and in opposition, rather than two 1Fact-checkers provide judgment of claim veracity and exert no influence on the platforms content moderation policies (Catalanello and Sanders, 2025). complementary strategies of tackling online misinformation. In this paper, we examine Twitter/X community notes as case study to understand how fact-checking is used in community notes. Specifically, we investigate the following two questions: (RQ1) To what extent do community notes rely on the work of professional fact-checkers? and (RQ2) What are the traits of posts and notes that rely on fact-checking sources? Studying the relationship between fact-checking and community notes is vital for understanding the shared role of expert and community-driven fact-checking in the global information ecosystem. We find that at least 1 in 20 community notes rely explicitly on the work of professional fact-checkers, while this reliance is higher still for high-stakes topics such as health and politics. Our experiments also show that fact-checking is vital for debunking misleading content linked to broader narratives or conspiracy theories. These findings imply that high-quality community notes cannot be produced independently of professional fact-checking. They further suggest that the pressure on fact-checkers exerted by platforms and politicians by defunding and discrediting fact-checking organisations will have corrosive effects on the quality of notes and destructive implications for information integrity more widely.Community moderation has been proposed as means of addressing the scalability (Martel et al., 2024) and cross-partisanship trust (Flamini, 2019) challenges associated with fact-checking. Twitter/Xs Community Notes programme (piloted in 2021 and publicly launched in October 2022 (Twitter/X, 2021)) is notable example of such system. Any platform user may volunteer as Community Notes contributor, although they must achieve particular rating impact score before they can write notes (Twitter/X, 2024b). Notes that achieve helpful rating appear underneath the post, explaining why the post is misleading (see Fig. 1). To be rated helpful, note must receive similar levels of helpfulness rating from users with diverse viewpoints (Twitter/X, 2024a)."
[25.02.2025 10:11] Mistral response. {"id": "99c37cfaef434d6f8c0b008085fc7ba7", "object": "chat.completion", "created": 1740478280, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"University of Copenhagen\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1386, "total_tokens": 1401, "completion_tokens": 15}}
[25.02.2025 10:11] Response: ```python
["University of Copenhagen"]
```
[25.02.2025 10:11] Deleting PDF ./assets/pdf/2502.14132.pdf.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.15122.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.15122.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.15122.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.17414.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.17414.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.17414.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.13074.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.13074.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.13074.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Downloading and parsing paper https://huggingface.co/papers/2502.15167.
[25.02.2025 10:11] Extra JSON file exists (./assets/json/2502.15167.json), skip PDF parsing.
[25.02.2025 10:11] Paper image links file exists (./assets/img_data/2502.15167.json), skip HTML parsing.
[25.02.2025 10:11] Success.
[25.02.2025 10:11] Enriching papers with extra data.
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 0. Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonst...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 1. Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 2. We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other componen...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 3. Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While inst...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 4. Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic ...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 5. The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks ...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 6. Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 7. While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leve...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 8. Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward M...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 9. The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we int...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 10. Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 11. Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge, and existing length extrapolation methods lead to temporal repetition or motion deceleration. In this ...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 12. Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a fra...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 13. As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific li...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 14. Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access r...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 15. Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained ed...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 16. Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking or...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 17. We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the dataset...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 18. We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize ...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 19. The Brownian sphere is a random metric space, homeomorphic to the two-dimensional sphere, which arises as the universal scaling limit of many types of random planar maps. The direct construction of the Brownian sphere is via a continuous analogue of the Cori--Vauquelin--Schaeffer (CVS) bijection. Th...
[25.02.2025 10:11] ********************************************************************************
[25.02.2025 10:11] Abstract 20. The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehe...
[25.02.2025 10:11] Read previous papers.
[25.02.2025 10:11] Generating reviews via LLM API.
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion", "#cv", "#dataset", "#training"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DICEPTION - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#survey", "#training", "#architecture"], "emoji": "üîç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –≥—Ä–∞–Ω–∏—Ü—ã: –ø—É—Ç—å –∫ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#audio", "#synthetic", "#data", "#optimization", "#architecture", "#open_source", "#training"], "emoji": "üó£Ô∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Slam - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#audio", "#dataset"], "emoji": "üéµ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Audio-FLAN - –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–µ —Å –∞—É–¥–∏–æ. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 80 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–µ—á–∏, –º—É–∑—ã–∫–∏ –∏ –∑–≤—É–∫–∞, —Å–æ–¥–µ—Ä–∂
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#inference", "#cv"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Å–≤–µ—â–µ–Ω–∏—è –¥–ª—è –ª—é–±—ã—Ö –∫–∞–º–µ—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ GCC –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. GCC –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning"], "emoji": "üî¨", "ru": {"title": "CodeCriticBench: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CodeCriticBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫—Ä–∏—Ç–∏
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: –æ—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MMIR –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#reasoning"], "emoji": "üêê", "ru": {"title": "GOAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", "desc": "–≠—Ça —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GOAT (Great LoRA Mixture-of-Expert) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#long_context", "#multilingual", "#benchmark", "#math"], "emoji": "üåê", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ø–ú –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞: –≤—ã–∑–æ–≤—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MCLM - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ —Å –∑–∞–¥–∞—á–∞–º–∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#video", "#agents"], "emoji": "üì±", "ru": {"title": "–í–∏–¥–µ–æ-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ò–ò: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "Mobile-Agent-V - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–∏–¥–µ–æ–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ 
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#science", "#reasoning", "#multilingual", "#benchmark"], "emoji": "‚è≥", "ru": {"title": "CTM: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "RIFLEx: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ RIFLEx –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–æ
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ VLM –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ 
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#science", "#benchmark", "#open_source", "#training"], "emoji": "üìà", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –≤–ª–∏—è–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –Ω–∞—É—á–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç framework –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è open-weight –º–æ–¥–µ–ª–µ–π –≤ —Å—Ñ–µ—Ä–µ –ò–ò. –ê–≤—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç –º–æ–¥–µ–ª—å –í–∞–Ω–≥–∞ 
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#ethics", "#open_source"], "emoji": "üîì", "ru": {"title": "–î–æ—Å—Ç—É–ø –∫ –ò–ò: –±–æ–ª—å—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ —Ä–µ–ª–∏–∑", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –≤—ã—Ö–æ–¥—è—â–∏–µ –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –æ —Ä–µ–ª–∏–∑–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#diffusion", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "VideoGrain - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–º—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫
[25.02.2025 10:11] Querying the API.
[25.02.2025 10:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking.
[25.02.2025 10:11] Response: {
  "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–æ–º –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ—Ä–∞—Ü–∏–µ–π –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö. –ò—Å–ø–æ–ª—å–∑—É—è —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ—Ä–ø—É—Å–∞ –∑–∞–º–µ—Ç–æ–∫ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ Twitter/X, –∞–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞ –¥–æ –ø—è—Ç–∏ —Ä–∞–∑ —á–∞—â–µ, —á–µ–º –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ. –û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–µ–Ω —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –¥–ª—è –∑–∞–º–µ—Ç–æ–∫ –æ –ø–æ—Å—Ç–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–º–∏ –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —É—Å–ø–µ—à–Ω–∞—è –º–æ–¥–µ—Ä–∞—Ü–∏—è —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞.",
  "emoji": "üîç",
  "title": "–§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ—Ä–∞—Ü–∏–∏"
}
[25.02.2025 10:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking."

[25.02.2025 10:11] Response: ```python
["DATASET", "DATA", "MULTIMODAL"]
```
[25.02.2025 10:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking."

[25.02.2025 10:11] Response: ```python
['ETHICS', 'SCIENCE']
```
[25.02.2025 10:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the relationship between community moderation and professional fact-checking in combating misinformation on social media platforms like Twitter/X and Meta. Using language models, the authors analyze community notes to identify their attributes, including topics and cited sources. The findings indicate that community notes frequently reference fact-checking sources, particularly when addressing broader misinformation narratives. Ultimately, the study highlights the importance of integrating professional fact-checking into community-driven moderation efforts to enhance their effectiveness.","title":"Community Moderation Thrives on Fact-Checking Support"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the relationship between community moderation and professional fact-checking in combating misinformation on social media platforms like Twitter/X and Meta. Using language models, the authors analyze community notes to identify their attributes, including topics and cited sources. The findings indicate that community notes frequently reference fact-checking sources, particularly when addressing broader misinformation narratives. Ultimately, the study highlights the importance of integrating professional fact-checking into community-driven moderation efforts to enhance their effectiveness.', title='Community Moderation Thrives on Fact-Checking Support'))
[25.02.2025 10:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÁ§æ‰∫§Â™í‰Ωì‰∏äÂØπÊäóËôöÂÅá‰ø°ÊÅØÁöÑ‰∏§ÁßçÁ≠ñÁï•Ôºö‰∏ì‰∏öÊú∫ÊûÑÁöÑ‰∫ãÂÆûÊ†∏Êü•ÂíåÁî®Êà∑ÁöÑÁ§æÂå∫ÁÆ°ÁêÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁ§æÂå∫Á¨îËÆ∞Âú®ÂºïÁî®‰∫ãÂÆûÊ†∏Êü•Êù•Ê∫êÊó∂ÔºåÈ¢ëÁéáÊØî‰πãÂâçÊä•ÂëäÁöÑÈ´òÂá∫‰∫îÂÄçÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏éÊõ¥ÂπøÊ≥õÁöÑËôöÂÅáÂèô‰∫ãÁõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠ê‰∏≠„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêË°®ÊòéÔºåÊàêÂäüÁöÑÁ§æÂå∫ÁÆ°ÁêÜÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùËµñ‰∫é‰∏ì‰∏öÁöÑ‰∫ãÂÆûÊ†∏Êü•„ÄÇÊîøÁ≠ñÂèòÂåñÊòæÁ§∫ÔºåÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞Ê≠£Âú®ÈÄêÊ∏êËΩ¨Âêë‰æùËµñÁî®Êà∑ÁîüÊàêÁöÑÂÜÖÂÆπÔºå‰ΩÜ‰∫ãÂÆûÊ†∏Êü•‰ªçÁÑ∂Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ","title":"Á§æÂå∫ÁÆ°ÁêÜ‰æùËµñ‰∫é‰∏ì‰∏ö‰∫ãÂÆûÊ†∏Êü•"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÁ§æ‰∫§Â™í‰Ωì‰∏äÂØπÊäóËôöÂÅá‰ø°ÊÅØÁöÑ‰∏§ÁßçÁ≠ñÁï•Ôºö‰∏ì‰∏öÊú∫ÊûÑÁöÑ‰∫ãÂÆûÊ†∏Êü•ÂíåÁî®Êà∑ÁöÑÁ§æÂå∫ÁÆ°ÁêÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁ§æÂå∫Á¨îËÆ∞Âú®ÂºïÁî®‰∫ãÂÆûÊ†∏Êü•Êù•Ê∫êÊó∂ÔºåÈ¢ëÁéáÊØî‰πãÂâçÊä•ÂëäÁöÑÈ´òÂá∫‰∫îÂÄçÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏éÊõ¥ÂπøÊ≥õÁöÑËôöÂÅáÂèô‰∫ãÁõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠ê‰∏≠„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêË°®ÊòéÔºåÊàêÂäüÁöÑÁ§æÂå∫ÁÆ°ÁêÜÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùËµñ‰∫é‰∏ì‰∏öÁöÑ‰∫ãÂÆûÊ†∏Êü•„ÄÇÊîøÁ≠ñÂèòÂåñÊòæÁ§∫ÔºåÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞Ê≠£Âú®ÈÄêÊ∏êËΩ¨Âêë‰æùËµñÁî®Êà∑ÁîüÊàêÁöÑÂÜÖÂÆπÔºå‰ΩÜ‰∫ãÂÆûÊ†∏Êü•‰ªçÁÑ∂Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ', title='Á§æÂå∫ÁÆ°ÁêÜ‰æùËµñ‰∫é‰∏ì‰∏ö‰∫ãÂÆûÊ†∏Êü•'))
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#benchmark", "#dataset"], "emoji": "ü¶ñ", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "MONSTER - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∫—Ä—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤. –û–Ω —Å–æ–∑–¥–∞–Ω –≤ –ø—Ä–æ—Ç–∏–≤–æ–≤–µ—Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º UCR –∏ UEA, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#architecture", "#video", "#multimodal"], "emoji": "üíÉ", "ru": {"title": "–¢–∞–Ω—Ü—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –ò–ò –æ–∂–∏–≤–ª—è–µ—Ç —Ñ–æ—Ç–æ –ø–æ–¥ –º—É–∑—ã–∫—É", "desc": "X-Dancer - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Ç–∞–Ω—Ü–µ–≤ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–¥ –º—É–∑—ã–∫—É –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#math"], "emoji": "üåê", "ru": {"title": "–û–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±—Ä–æ—É–Ω–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä—ã –≤ —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ—Ä–µ–≤–æ", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –±–∏–µ–∫—Ü–∏–∏ –ö–æ—Ä–∏-–í–æ–∫–µ–ª–µ–Ω–∞-–®–µ—Ñ—Ñ–µ—Ä–∞ (CVS) –¥–ª—è –±—Ä–æ—É–Ω–æ–≤—Å–∫–æ–π —Å—Ñ–µ—Ä—ã. –ë—Ä–æ—É–Ω–æ–≤—Å–∫–∞—è —Å—Ñ–µ—Ä–∞ - —ç—Ç–æ —Å–ª—É—á–∞–π–Ω–æ–µ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
[25.02.2025 10:11] Using data from previous issue: {"categories": ["#training", "#multimodal", "#optimization", "#benchmark", "#interpretability", "#agi"], "emoji": "üñºÔ∏è", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ò–ò-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö
[25.02.2025 10:11] Loading Chinese text from previous data.
[25.02.2025 10:11] Renaming data file.
[25.02.2025 10:11] Renaming previous data. hf_papers.json to ./d/2025-02-25.json
[25.02.2025 10:11] Saving new data file.
[25.02.2025 10:11] Generating page.
[25.02.2025 10:11] Renaming previous page.
[25.02.2025 10:11] Renaming previous data. index.html to ./d/2025-02-25.html
[25.02.2025 10:11] [Experimental] Generating Chinese page for reading.
[25.02.2025 10:11] Chinese vocab [{'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'ËÆ°ÁÆóËµÑÊ∫ê', 'pinyin': 'j√¨ su√†n zƒ´ yu√°n', 'trans': 'computational resources'}, {'word': 'ËÆ≠ÁªÉÊï∞ÊçÆ', 'pinyin': 'x√πn li√†n sh√π j√π', 'trans': 'training data'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íu xi√†n', 'trans': 'limited'}, {'word': 'Â§ö‰ªªÂä°', 'pinyin': 'du≈ç r√®n w√π', 'trans': 'multi-task'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çng y√≤ng', 'trans': 'general-purpose'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perception'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-trained'}, {'word': 'ÊñáÊú¨Âà∞ÂõæÂÉè', 'pinyin': 'w√©n bƒõn d√†o t√∫ xi√†ng', 'trans': 'text-to-image'}, {'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}, {'word': '‰ªÖ', 'pinyin': 'j«ên', 'trans': 'only'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unify'}, {'word': 'ÁºñÁ†Å', 'pinyin': 'biƒÅn m«é', 'trans': 'encoding'}, {'word': 'ÂÖÖÂàÜÂà©Áî®', 'pinyin': 'ch≈çng f√®n l√¨ y√≤ng', 'trans': 'fully utilize'}, {'word': 'ÈÄÇÂ∫î', 'pinyin': 'sh√¨ y√¨ng', 'trans': 'adapt'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìi ti√°o', 'trans': 'fine-tune'}]
[25.02.2025 10:11] Renaming previous Chinese page.
[25.02.2025 10:11] Renaming previous data. zh.html to ./d/2025-02-24_zh_reading_task.html
[25.02.2025 10:11] Writing Chinese reading task.
[25.02.2025 10:11] Writing result.
[25.02.2025 10:11] Renaming log file.
[25.02.2025 10:11] Renaming previous data. log.txt to ./logs/2025-02-25_last_log.txt
