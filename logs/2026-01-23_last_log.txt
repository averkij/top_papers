[23.01.2026 05:30] Read previous papers.
[23.01.2026 05:30] Generating top page (month).
[23.01.2026 05:30] Writing top page (month).
[23.01.2026 06:38] Read previous papers.
[23.01.2026 06:38] Get feed.
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15165
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16206
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15197
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16093
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16208
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16175
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16125
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15892
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14724
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15621
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15369
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16163
[23.01.2026 06:38] Extract page data from URL. URL: https://huggingface.co/papers/2601.15549
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11868
[23.01.2026 06:38] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16004
[23.01.2026 06:38] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.01.2026 06:38] No deleted papers detected.
[23.01.2026 06:38] Downloading and parsing papers (pdf, html). Total: 15.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.15165.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.15165.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.15165.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.16206.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.16206.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.16206.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.15197.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.15197.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.15197.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.16093.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.16093.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.16093.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.16208.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.16208.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.16208.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.16175.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.16175.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.16175.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.16125.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.16125.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.16125.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.15892.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.15892.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.15892.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.14724.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.14724.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.14724.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.15621.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.15621.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.15621.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.15369.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.15369.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.15369.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.16163.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.16163.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.16163.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.15549.
[23.01.2026 06:38] Downloading paper 2601.15549 from https://arxiv.org/pdf/2601.15549v1...
[23.01.2026 06:38] Extracting affiliations from text.
[23.01.2026 06:38] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 9 4 5 5 1 . 1 0 6 2 : r VIOLA: Towards Video In-Context Learning with Minimal Annotations Ryo Fujii1,2, Hideo Saito1,2, and Ryo Hachiuma3 1 Keio University 2 Keio AI Research Center 3 NVIDIA Abstract. Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs. Keywords: In-context learning Multimodal large language models Active learning Pseudo annotation Recent advances in Multimodal Large Language Models (MLLMs) [38,58,74] have notably enhan"
[23.01.2026 06:38] Response: ```python
[
    "Keio University",
    "Keio AI Research Center",
    "NVIDIA"
]
```
[23.01.2026 06:38] Deleting PDF ./assets/pdf/2601.15549.pdf.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.11868.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.11868.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.11868.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Downloading and parsing paper https://huggingface.co/papers/2601.16004.
[23.01.2026 06:38] Extra JSON file exists (./assets/json/2601.16004.json), skip PDF parsing.
[23.01.2026 06:38] Paper image links file exists (./assets/img_data/2601.16004.json), skip HTML parsing.
[23.01.2026 06:38] Success.
[23.01.2026 06:38] Enriching papers with extra data.
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 0. Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 1. LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 2. BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation bu...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 3. SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.  					AI-generated summary 				 Pixel-wise capabilities are essential for building interactive intelli...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 4. Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Repre...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 5. Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? ...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 6. A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and ...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 7. Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation a...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 8. HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.  					AI-generated summary 				 Recent advanceme...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 9. The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we p...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 10. An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper p...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 11. A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture comple...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 12. VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  					AI-ge...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 13. Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmar...
[23.01.2026 06:38] ********************************************************************************
[23.01.2026 06:38] Abstract 14. Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM ...
[23.01.2026 06:38] Read previous papers.
[23.01.2026 06:38] Generating reviews via LLM API.
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#math", "#optimization", "#diffusion", "#reasoning", "#rl", "#rlhf", "#training"], "emoji": "üîÑ", "ru": {"title": "–ì–∏–±–∫–æ—Å—Ç—å, —Å—Ç–∞–≤—à–∞—è –ª–æ–≤—É—à–∫–æ–π: –ø–æ—á–µ–º—É —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ª—É—á—à–µ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ
[23.01.2026 06:38] Using data from previous issue: {"categories": [], "emoji": "üèúÔ∏è", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä —Ä–∞–∑—É–º–∞ –º–æ–¥–µ–ª–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LLM-in-Sandbox ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –∫–æ–¥–æ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–Ω—ã—Ö –æ
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#robotics", "#reasoning", "#training"], "emoji": "ü§ñ", "ru": {"title": "–Ø–∑—ã–∫ –≤ –¥–µ–π—Å—Ç–≤–∏–∏: –±–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω BayesianVLA ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è vision-language-action –º–æ–¥–µ–ª–µ–π —Ä–æ–±–æ—Ç–æ
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#dataset", "#rl", "#training", "#benchmark", "#cv", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–ü–∏–∫—Å–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –º–∞—Å–æ–∫", "desc": "SAMTok ‚Äî —ç—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–∞—Å–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–∞—Å–∫–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#diffusion", "#synthetic", "#open_source", "#architecture", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç VAE –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã Representation Autoencoders (RAE) ‚Äî –∞—Ä—Ö–∏—Ç
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#math", "#optimization", "#science", "#rl", "#open_source", "#training"], "emoji": "üéØ", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Test-Time Training to Discover (TTT-Discover), –∫–æ—Ç–æ—Ä—ã
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#cv"], "emoji": "üîç", "ru": {"title": "–¢–æ–Ω–∫–æ–∑–µ—Ä–Ω–∏—Å—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–ª–∞–±–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö EDIR –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–æ–∏—Å–∫–∞ —Å–æ—Å—Ç–∞–≤–Ω
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#low_resource", "#optimization", "#plp", "#diffusion", "#architecture", "#training"], "emoji": "üß©", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–≤—Ç–æ—Ä¬≠–µ–≥—Ä–µ—Å—Å–∏—é –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫–æ–¥–∞", "desc": "Stable-DiffCoder ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#video"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "HERMES ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º–µ –ø–∞–º—è—Ç–∏ —Å –ø–µ—Ä–µ–∏
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#multilingual", "#multimodal", "#audio", "#dataset", "#open_source", "#architecture"], "emoji": "üé§", "ru": {"title": "–¢—Ä—ë—Ö—Å–µ–∫—É–Ω–¥–Ω–æ–µ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞ —Å —É–ª—å—Ç—Ä–∞–Ω–∏–∑–∫–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π", "desc": "Qwen3-TTS ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#architecture", "#cv"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenVision 3 ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –µ–¥–∏–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#video", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ–ø—Ä–µ–¥–∏–∫—Ü–∏–∏ –∫ —Ä–æ–±–æ—Ç-–ø–æ–ª–∏—Ç–∏–∫–µ –≤ –æ–¥–∏–Ω —ç—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Cosmos Policy ‚Äî –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏ –≤ –ø–æ–ª–∏—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç—Ç–∞–ø –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è 
[23.01.2026 06:38] Querying the API.
[23.01.2026 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  					AI-generated summary 				 Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.
[23.01.2026 06:38] Response: ```json
{
  "desc": "VIOLA ‚Äî —ç—Ç–æ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –Ω–æ–≤—ã–º –≤–∏–¥–µ–æ–¥–æ–º–µ–Ω–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º –Ω–µ–º–µ—á–µ–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∫–∏ –æ–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–º–µ—á–µ–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ–≥–æ –æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥—Å–∫–∞–∑–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑–ª–∏—á–∞—é—Ç –∏—Å—Ç–∏–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –¥–µ–≤—è—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ VIOLA –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤ —É—Å–ª–æ–≤–∏—è—Ö –¥–µ—Ñ–∏—Ü–∏—Ç–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
  "emoji": "üé¨",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π"
}
```
[23.01.2026 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  					AI-generated summary 				 Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs."

[23.01.2026 06:38] Response: ```python
['VIDEO', 'MULTIMODAL', 'DATA', 'TRAINING']
```
[23.01.2026 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  					AI-generated summary 				 Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs."

[23.01.2026 06:38] Response: ```python
['LOW_RESOURCE', 'TRANSFER_LEARNING']
```
[23.01.2026 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VIOLA is a novel framework designed to enhance the adaptation of multimodal large language models (MLLMs) in video domains with limited labeled data. It combines minimal expert supervision with a wealth of unlabeled data, utilizing density-uncertainty-weighted sampling to select the most informative samples for annotation. Additionally, it employs confidence-aware retrieval mechanisms to effectively manage and utilize unlabeled data, ensuring that the model can distinguish between reliable labels and noisy pseudo-labels. Through extensive testing, VIOLA has shown to significantly improve performance in low-resource environments, making it a valuable tool for real-world applications.","title":"Efficient Video Adaptation with Minimal Labels"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VIOLA is a novel framework designed to enhance the adaptation of multimodal large language models (MLLMs) in video domains with limited labeled data. It combines minimal expert supervision with a wealth of unlabeled data, utilizing density-uncertainty-weighted sampling to select the most informative samples for annotation. Additionally, it employs confidence-aware retrieval mechanisms to effectively manage and utilize unlabeled data, ensuring that the model can distinguish between reliable labels and noisy pseudo-labels. Through extensive testing, VIOLA has shown to significantly improve performance in low-resource environments, making it a valuable tool for real-world applications.', title='Efficient Video Adaptation with Minimal Labels'))
[23.01.2026 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VIOLAÊòØ‰∏Ä‰∏™Ê†áÁ≠æÈ´òÊïàÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊúÄÂ∞èÂåñ‰∏ìÂÆ∂ÁõëÁù£ÂíåÂà©Áî®‰∏∞ÂØåÁöÑÊú™Ê†áËÆ∞Êï∞ÊçÆÔºåÊù•ÂÆûÁé∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®‰ΩéËµÑÊ∫êËßÜÈ¢ëÈ¢ÜÂüüÁöÑÊúâÊïàÈÄÇÂ∫î„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®ÂØÜÂ∫¶-‰∏çÁ°ÆÂÆöÊÄßÂä†ÊùÉÈááÊ†∑ÂíåÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÊ£ÄÁ¥¢Êú∫Âà∂Ôºå‰ª•ÊúÄÂ§ßÂåñÊ≥®ÈáäÈ¢ÑÁÆóÁöÑÊïàÁéá„ÄÇÈÄöËøáËØÜÂà´Â§öÊ†∑ÊÄß„ÄÅ‰ª£Ë°®ÊÄßÂíå‰ø°ÊÅØÈáèÂÖºÂÖ∑ÁöÑÊ†∑Êú¨ÔºåVIOLAËÉΩÂ§üÊúâÊïàÈÄâÊã©Ê†∑Êú¨ÔºåÂêåÊó∂ÈÅøÂÖçÈÄâÊã©ËßÜËßâÂºÇÂ∏∏ÂÄº„ÄÇÊ≠§Â§ñÔºåVIOLAËøòÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ê∑∑ÂêàÊ±†ÔºåÂà©Áî®ÁΩÆ‰ø°Â∫¶ÊÑüÁü•ÁöÑÊ£ÄÁ¥¢ÂíåÊèêÁ§∫ÔºåÂ∏ÆÂä©Ê®°ÂûãÂú®ÈÄÇÂ∫îËøáÁ®ã‰∏≠Âå∫ÂàÜÁúüÂÆûÊ†áÁ≠æÂíåÂô™Â£∞‰º™Ê†áÁ≠æ„ÄÇ","title":"VIOLAÔºö‰ΩéËµÑÊ∫êËßÜÈ¢ëÈ¢ÜÂüüÁöÑÈ´òÊïàÈÄÇÂ∫îÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VIOLAÊòØ‰∏Ä‰∏™Ê†áÁ≠æÈ´òÊïàÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊúÄÂ∞èÂåñ‰∏ìÂÆ∂ÁõëÁù£ÂíåÂà©Áî®‰∏∞ÂØåÁöÑÊú™Ê†áËÆ∞Êï∞ÊçÆÔºåÊù•ÂÆûÁé∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®‰ΩéËµÑÊ∫êËßÜÈ¢ëÈ¢ÜÂüüÁöÑÊúâÊïàÈÄÇÂ∫î„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®ÂØÜÂ∫¶-‰∏çÁ°ÆÂÆöÊÄßÂä†ÊùÉÈááÊ†∑ÂíåÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÊ£ÄÁ¥¢Êú∫Âà∂Ôºå‰ª•ÊúÄÂ§ßÂåñÊ≥®ÈáäÈ¢ÑÁÆóÁöÑÊïàÁéá„ÄÇÈÄöËøáËØÜÂà´Â§öÊ†∑ÊÄß„ÄÅ‰ª£Ë°®ÊÄßÂíå‰ø°ÊÅØÈáèÂÖºÂÖ∑ÁöÑÊ†∑Êú¨ÔºåVIOLAËÉΩÂ§üÊúâÊïàÈÄâÊã©Ê†∑Êú¨ÔºåÂêåÊó∂ÈÅøÂÖçÈÄâÊã©ËßÜËßâÂºÇÂ∏∏ÂÄº„ÄÇÊ≠§Â§ñÔºåVIOLAËøòÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ê∑∑ÂêàÊ±†ÔºåÂà©Áî®ÁΩÆ‰ø°Â∫¶ÊÑüÁü•ÁöÑÊ£ÄÁ¥¢ÂíåÊèêÁ§∫ÔºåÂ∏ÆÂä©Ê®°ÂûãÂú®ÈÄÇÂ∫îËøáÁ®ã‰∏≠Âå∫ÂàÜÁúüÂÆûÊ†áÁ≠æÂíåÂô™Â£∞‰º™Ê†áÁ≠æ„ÄÇ', title='VIOLAÔºö‰ΩéËµÑÊ∫êËßÜÈ¢ëÈ¢ÜÂüüÁöÑÈ´òÊïàÈÄÇÂ∫îÊ°ÜÊû∂'))
[23.01.2026 06:38] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents"], "emoji": "üñ•Ô∏è", "ru": {"title": "–†–µ–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞–±–æ—Ç", "desc": "Terminal-Bench 2.0 ‚Äî —ç—Ç–æ —Å–ª–æ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 89 –∑–∞–¥–∞—á –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI –∞
[23.01.2026 06:38] Using data from previous issue: {"categories": [], "emoji": "‚öõÔ∏è", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ —Å–≤–∏–¥–µ—Ç–µ–ª–∏ –º–µ–∂–≤–µ—Ç–≤–µ–≤—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã —Ä–µ–∞–ª–∏–∑—É—é—Ç –∏ —Ç–µ—Å—Ç–∏—Ä—É—é—Ç –Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ IBM —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö —Å—Ö–µ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–≤–∏–¥–µ—Ç–µ–ª–µ–π –º–µ–∂–≤–µ—Ç–≤–µ–≤—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏—è—Ö, –ø–æ
[23.01.2026 06:38] Renaming data file.
[23.01.2026 06:38] Renaming previous data. hf_papers.json to ./d/2026-01-23.json
[23.01.2026 06:38] Saving new data file.
[23.01.2026 06:38] Generating page.
[23.01.2026 06:38] Renaming previous page.
[23.01.2026 06:38] Renaming previous data. index.html to ./d/2026-01-23.html
[23.01.2026 06:38] Writing result.
[23.01.2026 06:38] Renaming log file.
[23.01.2026 06:38] Renaming previous data. log.txt to ./logs/2026-01-23_last_log.txt
