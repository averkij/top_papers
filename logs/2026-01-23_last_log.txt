[23.01.2026 01:50] Read previous papers.
[23.01.2026 01:50] Generating top page (month).
[23.01.2026 01:50] Writing top page (month).
[23.01.2026 03:44] Read previous papers.
[23.01.2026 03:44] Get feed.
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.15165
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.15197
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.16206
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.16125
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.16175
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.15892
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.15369
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.16208
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.16163
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.15621
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.11868
[23.01.2026 03:44] Extract page data from URL. URL: https://huggingface.co/papers/2601.16004
[23.01.2026 03:44] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.01.2026 03:44] Downloading and parsing papers (pdf, html). Total: 12.
[23.01.2026 03:44] Downloading and parsing paper https://huggingface.co/papers/2601.15165.
[23.01.2026 03:44] Downloading paper 2601.15165 from https://arxiv.org/pdf/2601.15165v1...
[23.01.2026 03:44] Extracting affiliations from text.
[23.01.2026 03:44] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models January 22, 2026 Zanlin Ni 1 , Shenzhi Wang 1, Yang Yue 1, Tianyu Yu 2, Weilin Zhao 2, Yeguo Hua 3, Tianyi Chen 3, Jun Song 4, Cheng Yu 4, Bo Zheng 4, Gao Huang 1 (cid:66) 1 LeapLab, Tsinghua University 3 Tsinghua University 2 NLPLab, Tsinghua University 4 Alibaba Group nzl22@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL)1 to elicit the reasoning capability of dLLMs. In this paper, we reveal counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap 6 2 0 2 1 2 ] . [ 1 5 6 1 5 1 . 1 0 6 2 : r Figure 1: Less flexibility unlocks better reasoning potential. Left: We observe coun"
[23.01.2026 03:44] Response: ```python
[
    "LeapLab, Tsinghua University",
    "Tsinghua University",
    "NLPLab, Tsinghua University",
    "Alibaba Group"
]
```
[23.01.2026 03:44] Deleting PDF ./assets/pdf/2601.15165.pdf.
[23.01.2026 03:44] Success.
[23.01.2026 03:44] Downloading and parsing paper https://huggingface.co/papers/2601.15197.
[23.01.2026 03:44] Downloading paper 2601.15197 from https://arxiv.org/pdf/2601.15197v2...
[23.01.2026 03:44] Extracting affiliations from text.
[23.01.2026 03:44] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 2 7 9 1 5 1 . 1 0 6 2 : r 2026-01-23 Work in progress. BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries Shijie Lian1,2,* Bin Yu2,4,* Xiaopeng Lin2,5,* Laurence T. Yang6,1, Zhaolong Shen2,7 Changti Wu2,8 Yuzhuo Miao1,2 Cong Huang2,3 Kai Chen2,3,9, 1HUST 2ZGCA 3ZGCI 4HIT 5HKUST(GZ) 6ZZU 7BUAA 8ECNU 9DeepCybo https://github.com/ZGC-EmbodyAI/BayesianVLA "
[23.01.2026 03:44] Response: ```python
['HUST', 'ZGCA', 'ZGCI', 'HIT', 'HKUST(GZ)', 'ZZU', 'BUAA', 'ECNU', 'DeepCybo']
```
[23.01.2026 03:44] Deleting PDF ./assets/pdf/2601.15197.pdf.
[23.01.2026 03:44] Success.
[23.01.2026 03:44] Downloading and parsing paper https://huggingface.co/papers/2601.16206.
[23.01.2026 03:44] Downloading paper 2601.16206 from https://arxiv.org/pdf/2601.16206v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LLM-in-Sandbox Elicits General Agentic Intelligence Daixuan Chengαβ Shaohan Huangβ Yuxian Guγ Huatong Songα Guoxin Chenα Li Dongβ Wayne Xin Zhaoα Ji-Rong Wenα Furu Weiβ αGSAI, Renmin University of China βMicrosoft Research γTsinghua University https://llm-in-sandbox.github.io "
[23.01.2026 03:45] Response: ```python
[
    "GSAI, Renmin University of China",
    "Microsoft Research",
    "Tsinghua University"
]
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.16206.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.16125.
[23.01.2026 03:45] Downloading paper 2601.16125 from https://arxiv.org/pdf/2601.16125v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 5 2 1 6 1 . 1 0 6 2 : r Rethinking Composed Image Retrieval Evaluation: Fine-Grained Benchmark from Image Editing Tingyu Song 123 Yanzhao Zhang 2 Mingxin Li 2 Zhuoning Guo42 Dingkun Long 2 Pengjun Xie 2 Siyue Zhang 5 Yilun Zhao6 Shu Wu13 1CASIA 2Tongyi Lab, Alibaba Group 3UCAS 4HKUST(GZ) 5NTU 6Yale "
[23.01.2026 03:45] Response: ```python
[
    "CASIA",
    "Tongyi Lab, Alibaba Group",
    "UCAS",
    "HKUST(GZ)",
    "NTU",
    "Yale"
]
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.16125.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.16175.
[23.01.2026 03:45] Downloading paper 2601.16175 from https://arxiv.org/pdf/2601.16175v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"1, Daniel Koceja 1, Xinhao Li 4, Federico Bianchi 5 Jed McCaleb3, Xiaolong Wang4, Jan Kautz2, Yejin Choi2, James Zou 1,5, Carlos Guestrin 1, Yu Sun 1, "
[23.01.2026 03:45] Response: ```python
[]
```
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"1, Daniel Koceja 1, Xinhao Li 4, Federico Bianchi 5 Jed McCaleb3, Xiaolong Wang4, Jan Kautz2, Yejin Choi2, James Zou 1,5, Carlos Guestrin 1, Yu Sun 1,How can we use AI to discover new state of the art for scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős minimum overlap problem and an autocorrelation inequality; (ii) GPUMode kernel competition (up to 2 faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with cost of only few hundred dollars per problem.Kernel Eng. (TriMul) Algorithms (AtCoder)Best Human Prev. Best AI TTT-Discover Erdős Min. Overlap () A100 () 4531 µs N/A 2198 µs 0.380927 [20] 0.380924 [49] 0.380876 H100 () Heuristic Contest 39 () Denoising () 1371 µs N/A 1161 µs 566, 997 [1] 558,026 [37] 567,6 2 0 2 2 2 ] . [ 1 5 7 1 6 1 . 1 0 6 2 : r Figure 1. TTT-Discover continues to train an LLM on single problem at test time. πθi denotes the policy with the updates weights at test-time training step i. We plot the reward distribution at step 0, 9, 24, and 49 (final), recorded while test-time training for the GPUMode TriMul competition. We generate 512 solutions at each step. As training progresses, the LLM generates better solutions that ultimately surpass the prior art (best human). For comparison, we plot the reward distribution of best-of-N with the same total sampling budget. Core contributors. Joint advising. Correspondence to: merty@stanford.edu, yusun@cs.stanford.edu.To solve hard problems, humans often need to try, fail, stumble upon partial successes, and then learn from their experiences. Consider your first really hard programming assignment. You read the textbook and trained yourself on the book exercises, but this assignment just asked for so much beyond the basics in the book. You tried to guess the solution, but these attempts merely produced small signs of life. So you had to take deep breath and learn from your failed attempts, which made your future attempts more intelligent. Finally, after hours of trying and learning, you understood the new ideas behind the assignment. And indeed, the next attempt worked! In this example, the assignment was hard because it required new ideas beyond your training data (the text and exercises in the book). Now consider using AI to solve scientific discovery problems. This goal is even harder: By definition, discovery problems require ideas not only beyond the models training data but also all existing knowledge of humanity. And out-of-distribution generalization is no easier for AI than for humans [47, 22, 52, 34]. To offset this hardness, prior work has focused on test-time search in the solution space by prompting frozen LLM to make many attempts, similar to how we tried to guess the solution to the assignment. In particular, evolutionary search methods, such as AlphaEvolve, store past attempts in buffer and use them to generate new prompts via hand-crafted and domain-specific heuristics [49, 37, 54, 80]. While these prompts can help the LLM improve previous solutions, the LLM itself cannot improve, similar to student who can never internalize the new ideas behind the assignm"
[23.01.2026 03:45] Mistral response. {"id": "be69f17a70744ba69a1feb97b882f560", "created": 1769139928, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1138, "total_tokens": 1178, "completion_tokens": 40}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Stanford University\",\n    \"NVIDIA\",\n    \"University of Washington\",\n    \"University of California, San Diego\",\n    \"Thinking Machines\"\n]\n```"}}]}
[23.01.2026 03:45] Response: ```python
[
    "Stanford University",
    "NVIDIA",
    "University of Washington",
    "University of California, San Diego",
    "Thinking Machines"
]
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.16175.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.15892.
[23.01.2026 03:45] Downloading paper 2601.15892 from https://arxiv.org/pdf/2601.15892v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model Chenghao Fan1,2, Wen Heng2, Bo Li2, Sichen Liu1, Yuxuan Song2, Jing Su2, Xiaoye Qu1, Kai Shen2, Wei Wei1 1Huazhong University of Science and Technology, 2ByteDance Seed "
[23.01.2026 03:45] Response: ```python
[
    "Huazhong University of Science and Technology",
    "ByteDance"
]
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.15892.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.15369.
[23.01.2026 03:45] Downloading paper 2601.15369 from https://arxiv.org/pdf/2601.15369v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 1 2 ] . e [ 1 9 6 3 5 1 . 1 0 6 2 : r OpenVision 3 : Family of Unified Visual Encoder for Letian Zhang* 1 Huaxiu Yao3 Sucheng Ren* 2 Zeyu Zheng4 Yanqing Liu1 Xianhang Li1 Zeyu Wang1 Yuyin Zhou Weili Nie5 Guilin Liu5 Zhiding Yu5 Cihang Xie1 1UC Santa Cruz 2JHU 3UNC-Chapel Hill 4UC Berkeley 5NVIDIA Project Page: https://ucsc-vlaa.github.io/OpenVision3/ Figure 1. An overview of OpenVision 3s architecture design and performance highlight. Left panel: The architecture of OpenVision 3. We employ frozen VAE and trainable ViT as the unified tokenizer, which produces tokens that are fed simultaneously into both the generation and understanding branches. Middle panel: The learning objectives of the generation branch and the understanding branch. For the generation branch, we focus on high-quality, pixel-level image reconstruction; concurrently, the understanding branch is optimized via joint contrastive learning and captioning objectives. Right panel: The performance summarization shows that OpenVision 3 outperforms other unified tokenizers and semantics-based encoders in rFID and gFID, while remaining competitive with CLIP in multimodal understanding ability. Abstract This paper presents family of advanced vision encoder, named OpenVision 3, that learns single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstructionand semantics-driven signals in shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validat"
[23.01.2026 03:45] Response: ```python
[
    "UC Santa Cruz",
    "JHU",
    "UNC-Chapel Hill",
    "UC Berkeley",
    "NVIDIA"
]
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.15369.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.16208.
[23.01.2026 03:45] Downloading paper 2601.16208 from https://arxiv.org/pdf/2601.16208v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders Shengbang Tong*, Boyang Zheng*, Ziteng Wang*, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie New York University Website Code Models Data 6 2 0 2 2 2 ] . [ 1 8 0 2 6 1 . 1 0 6 2 : r a "
[23.01.2026 03:45] Response: ```python
["New York University"]
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.16208.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.16163.
[23.01.2026 03:45] Downloading paper 2601.16163 from https://arxiv.org/pdf/2601.16163v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 3 6 1 6 1 . 1 0 6 2 : r COSMOS POLICY: FINE-TUNING VIDEO MODELS FOR VISUOMOTOR CONTROL AND PLANNING Moo Jin Kim1,2 Yihuai Gao1,2 Tsung-Yi Lin1 Yen-Chen Lin1 Yunhao Ge1 Grace Lam1 Percy Liang2 Shuran Song1,2 Ming-Yu Liu1 Chelsea Finn2 Jinwei Gu1 1NVIDIA 2Stanford University https://research.nvidia.com/labs/dir/cosmos-policy/ Figure 1: We present Cosmos Policy, state-of-the-art robot policy fine-tuned from the NVIDIA CosmosPredict2-2B video foundation model. Cosmos Policy handles multimodal inputs and multi-view camera images and predicts (1) robot action chunk, (2) future state (represented by robot proprioception and image observations), and (3) value (expected rewards-to-go at the future state). No architectural changes are made to the base video model, and all modalities are jointly modeled through the video diffusion learning objective. "
[23.01.2026 03:45] Response: ```python
['NVIDIA', 'Stanford University']
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.16163.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.15621.
[23.01.2026 03:45] Downloading paper 2601.15621 from https://arxiv.org/pdf/2601.15621v1...
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 1 2 6 5 1 . 1 0 6 2 : r 2026-01-23 Qwen3-TTS Technical Report Qwen Team https://huggingface.co/collections/Qwen/qwen3-tts https://modelscope.cn/collections/Qwen/Qwen3-TTS https://github.com/QwenLM/Qwen3-TTS "
[23.01.2026 03:45] Response: ```python
[]
```
[23.01.2026 03:45] Extracting affiliations from text.
[23.01.2026 03:45] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 1 2 6 5 1 . 1 0 6 2 : r 2026-01-23 Qwen3-TTS Technical Report Qwen Team https://huggingface.co/collections/Qwen/qwen3-tts https://modelscope.cn/collections/Qwen/Qwen3-TTS https://github.com/QwenLM/Qwen3-TTSIn this report, we present the Qwen3-TTS series, family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-ofthe-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97 ms) through its 12.5 Hz, 16-layer multi-codebook design and lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.Figure 1: Qwen3-TTS is multilingual, controllable, robust, and streaming text-to-speech model. Based on these features, Qwen3-TTS supports wide range of tasks, including but not limited to cloning, creating and controlling voice, and easily handling various complex texts. Stable, controllable, and human-like speech synthesis is widely viewed as key capability on the path to AGI. Modern neural text-to-speech (TTS) models, trained on large-scale datasets, already deliver exceptional capability to generate high-quality speech from few seconds of reference audio (Wang et al., 2023; Shen et al., 2023; Ju et al., 2024; Yang et al., 2023; Eskimez et al., 2024; Chen et al., 2024; Du et al., 2024a; Wang et al., 2025; 2024; Ye et al., 2025b). Among them, discrete speech tokenization (Défossez et al., 2022; Zeghidour et al., 2022; Kumar et al., 2023) combined with autoregressive language modeling of discrete units has gained traction, offering improved stability while preserving high naturalness and human-likeness (Du et al., 2025; Liao et al., 2024; Défossez et al., 2024; Xu et al., 2025; Zhang et al., 2025a). Conditioning on vocal features or text instructions facilitates finer-grained control over prosody and style, 1 Table 1: Overview of the Qwen3-TTS model family. Model Name Qwen3-TTS-12Hz-1.7B-Base Qwen3-TTS-12Hz-1.7B-VoiceDesign Qwen3-TTS-12Hz-1.7B-CustomVoice Qwen3-TTS-12Hz-0.6B-Base Qwen3-TTS-12Hz-0.6B-CustomVoice Qwen3-TTS-25Hz-1.7B-Base Qwen3-TTS-25Hz-1.7B-VoiceEditing Qwen3-TTS-25Hz-1.7B-CustomVoice Qwen3-TTS-25Hz-0.6B-Base Qwen3-TTS-25Hz-0.6B-CustomVoice Streaming Multilinguality Voice Clone Instruction Following resulting in outputs of greater richness and diversity (Du et al., 2024b; Lyth & King, 2024; Zhou et al., 2024; Ji et al., 2025; Leng et al.). These breakthroughs are paving the way for diverse applications in fields such as virtual assistants and automated content creation. In this report, we take step toward stable, controllable, and human-like speech synthesis and introduce Qwen3-TTS, the first text-to-speech model in the Qwen series. Qwen3-TTS exhibits the following properties: 1) Controllability: Qwen3-TTS allows users to create new voices or manipulate fine-grained attributes of generated speech via natural language descriptions, while also supporting the stable generation of any content using the created voice. 2) Voice Cloning and Predefined Voice Profiles: Qwen3-TTS supports 3-second voice cloning and generation using set of curated, high-quality preset voices. 3) Naturalness: Beyond achieving robust synthesis, Qwen3-TTS excels in generating highly natural and expressive speech. Our 1.7B model, in particular, delivers state-of-the-art, human-like quality, demonstrating our approach successfully maximizes perceptual quality without overfitting to ASR-related metrics. 4) Multilinguality: The model is trained across more than 10 languages and supports speaker-consistent multilingual generation. 5) Streaming: Designed for streaming text input and streaming audio output, it achieves first-packet latency as low as 97 ms (0.6B variant) and 101 ms (1.7B variant). Beyond the aforementioned aspects, and from broader perspective of practical application, it is crucial for our model to integrate seamlessly with Large Language Models (LLMs) and achieve extremely low first-packet latency. To this end, we use discrete speech representations as the cornerstone of our architecture and introduce two tokenizers in the Qwen3-TTS family: 1) Qwen-TTS-Tokenizer-25Hz employs 25 Hz single-codebook representation with waveform reconstruction via block-wise flow matching to enable streaming synthesis (Xu et al., 2025). Empirically, we find that semantic tokenizers lack expressive power, whereas purely acoustic tokenizers inject excessive low-level detail that complicates LLM-based modeling and leads to long-horizon error accumulation. To balance these factors, Qwen-TTS-Tokenizer25Hz integrates semantic and acoustic cues, leveraging the Qwen2-Audio encoder for both expressivity and tractability. Although it supports streaming with block-wise diffusion decoder, we found that its single-codebook design limits suitability for ultra-low-latency applications and general speech synthesis. Therefore, we develop 2) Qwen-TTS-Tokenizer-12Hz, which adopts 12.5 Hz multi-codebook scheme inspired by Zhang et al. (2023b). Its first codebook layer encodes semantic content, while the subsequent layers capture acoustic details. The increased capacity permits waveform reconstruction using only lightweight causal ConvNet, eliminating the need for speaker vector extraction or complex diffusion models (Du et al., 2024b; Zhang et al., 2025a). To further support ultralow-latency streaming, we designed dual-track autoregressive architecture for streaming text input and audio output. This architecture incorporates Multi-Token Prediction (MTP) module to effectively model the multi-codebook sequence, which enables immediate speech decoding from the first codec frame. "
[23.01.2026 03:45] Mistral response. {"id": "9d8ffb2a47c74c07b6a0d474f3f3b279", "created": 1769139956, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1740, "total_tokens": 1750, "completion_tokens": 10}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Qwen Team\"]\n```"}}]}
[23.01.2026 03:45] Response: ```python
["Qwen Team"]
```
[23.01.2026 03:45] Deleting PDF ./assets/pdf/2601.15621.pdf.
[23.01.2026 03:45] Success.
[23.01.2026 03:45] Downloading and parsing paper https://huggingface.co/papers/2601.11868.
[23.01.2026 03:45] Downloading paper 2601.11868 from https://arxiv.org/pdf/2601.11868v1...
[23.01.2026 03:46] Extracting affiliations from text.
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 8 6 8 1 1 . 1 0 6 2 : r TERMINAL-BENCH: BENCHMARKING AGENTS ON HARD, REALISTIC TASKS IN COMMAND LINE INTERFACES Mike A. Merrill1,*, Alexander G. Shaw2,*, Nicholas Carlini3, Boxuan Li4, Harsh Raj5, Ivan Bercovich6, Lin Shi7, Jeong Yeon Shin1,8, Thomas Walshe9, E. Kelly Buchanan1 Junhong Shen10, Guanghao Ye11, Haowei Lin12, Jason Poulos4, Maoyu Wang4, Marianna Nezhurina13,14, Jenia Jitsev13,14, Di Lu15, Orfeas Menis Mastromichalakis16,17, Zhiwei Xu18, Zizhao Chen7, Yue Liu19,20, Robert Zhang21, Leon Liangyu Chen1, Anurag Kashyap22, Jan-Lucas Uslu1, Jeffrey Li23, Jianbo Wu4, Minghao Yan24, Song Bian24, Vedang Sharma4, Ke Sun4, Steven Dillmann1, Akshay Anand25, Andrew Lanpouthakoun1, Bardia Koopah25, Changran Hu26, Etash Guha1,23, Gabriel H. S. Dreiman4, Jiacheng Zhu4, Karl Krauth1, Li Zhong3, Niklas Muennighoff1, Robert Amanfu4, Shangyin Tan25, Shreyas Pimpalgaonkar27, Tushar Aggarwal1, Xiangning Lin10, Xin Lan28, Xuandong Zhao25, Yiqing Liang29, Yuanli Wang30, Zilong Wang31 Changzhi Zhou32, David Heineman33, Hange Liu4, Harsh Trivedi33, John Yang1, Junhong Lin11, Manish Shetty25, Michael Yang6, Nabil Omi23, Negin Raoof25, Shanda Li10, Terry Yue Zhuo34,35, Wuwei Lin4, Yiwei Dai7, Yuxin Wang36, Wenhao Chai37, Shang Zhou31, Dariush Wahdany38, Ziyu She39, Jiaming Hu30, Zhikang Dong40, Yuxuan Zhu41, Sasha Cui42, Ahson Saiyed43, Arinbjorn Kolbeinsson43, Jesse Hu 44, Christopher Michael Rytting2, Ryan Marten27, Yixin Wang18, Alex Dimakis25,27 Andy Konwinski2, Ludwig Schmidt1 1Stanford University, 2Laude Institute, 3Anthropic, 4Independent, 5Northeastern University, 6University of California, Santa Barbara, 7Cornell University, 8Snorkel AI, 9Reflection AI, 10Carnegie Mellon University, 11Massachusetts Institute of Technology, 12Peking University, 13LAION, 14JSC, FZJ, 15Tencent, 16National Technical University of Athens, 17Nerion, 18University of Michigan, 19National University of Singapore, 20Moonshot AI, 21University of Texas at Austin, 22Amazon, 23Univer"
[23.01.2026 03:46] Response: ```python
[
    "Stanford University",
    "Laude Institute",
    "Anthropic",
    "Independent",
    "Northeastern University",
    "University of California, Santa Barbara",
    "Cornell University",
    "Snorkel AI",
    "Reflection AI",
    "Carnegie Mellon University",
    "Massachusetts Institute of Technology",
    "Peking University",
    "LAION",
    "JSC, FZJ",
    "Tencent",
    "National Technical University of Athens",
    "Nerion",
    "University of Michigan",
    "National University of Singapore",
    "Moonshot AI",
    "University of Texas at Austin",
    "Amazon"
]
```
[23.01.2026 03:46] Deleting PDF ./assets/pdf/2601.11868.pdf.
[23.01.2026 03:46] Success.
[23.01.2026 03:46] Downloading and parsing paper https://huggingface.co/papers/2601.16004.
[23.01.2026 03:46] Downloading paper 2601.16004 from https://arxiv.org/pdf/2601.16004v1...
[23.01.2026 03:46] Extracting affiliations from text.
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Wigners Friend as Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware Christopher Altman * January 2026 Abstract We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris (2026) for estimating operational inter-branch communication witnesses, i.e., correlations in classical measurement records produced by compiled Wigners-friend-style circuits. Concretely, we realize five-qubit instance of the protocol as an inter-register message-transfer pattern in the circuit (not physical signaling), and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution in which an observer subsystem (Wigners friend) evolves differently depending on the state of control qubit Q, followed by controlled transfer operation that probes correlations between branch-conditioned histories (conditional measurement contexts within single compiled circuit). We evaluate coherence-sensitive witnesses on specified four-qubit subset. Executing on the ibm fez backend (N = 20,000 shots), we obtain population-based visibility = 0.8771 0.0034, coherence witnesses WX = 0.8398 0.0038 and WY = 0.8107 0.0041, + 2 and phase-sensitive magnitude Cmag = (W 2 )1/2 = 1.1673 0.0040. This work does not claim to test or discriminate among interpretations of quantum mechanics. Unitary interpretations that reproduce standard quantum predictions remain operationally equivalent for these circuits. Instead, we frame the experiment as publicly reproducible constraint pipeline: given parameterized family of non-ideal channels (e.g., dephasing or collapselike perturbations) inserted at specified circuit locations, the coherence-sensitive witnesses define detectability thresholds relative to calibrated device noise. The accompanying reproducibility bundle provides full provenance (job IDs, calibration snapshots, software versions) enabling independent verification and extension. The Wign"
[23.01.2026 03:46] Response: ```python
[]
```
[23.01.2026 03:46] Extracting affiliations from text.
[23.01.2026 03:46] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Wigners Friend as Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware Christopher Altman * January 2026 Abstract We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris (2026) for estimating operational inter-branch communication witnesses, i.e., correlations in classical measurement records produced by compiled Wigners-friend-style circuits. Concretely, we realize five-qubit instance of the protocol as an inter-register message-transfer pattern in the circuit (not physical signaling), and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution in which an observer subsystem (Wigners friend) evolves differently depending on the state of control qubit Q, followed by controlled transfer operation that probes correlations between branch-conditioned histories (conditional measurement contexts within single compiled circuit). We evaluate coherence-sensitive witnesses on specified four-qubit subset. Executing on the ibm fez backend (N = 20,000 shots), we obtain population-based visibility = 0.8771 0.0034, coherence witnesses WX = 0.8398 0.0038 and WY = 0.8107 0.0041, + 2 and phase-sensitive magnitude Cmag = (W 2 )1/2 = 1.1673 0.0040. This work does not claim to test or discriminate among interpretations of quantum mechanics. Unitary interpretations that reproduce standard quantum predictions remain operationally equivalent for these circuits. Instead, we frame the experiment as publicly reproducible constraint pipeline: given parameterized family of non-ideal channels (e.g., dephasing or collapselike perturbations) inserted at specified circuit locations, the coherence-sensitive witnesses define detectability thresholds relative to calibrated device noise. The accompanying reproducibility bundle provides full provenance (job IDs, calibration snapshots, software versions) enabling independent verification and extension.The Wigners friend thought experiment [1] and its extensions [2, 3] probe foundational questions about unitary quantum dynamics, measurement, and observer perspectives. The quantum information community has increasingly adopted circuits-as-foundations approach: encoding these thought experiments as explicit quantum circuits that can be analyzed, simulated, and executed on real quantum hardware [4, 5]. 6 2 0 2 2 2 ] - u [ 1 4 0 0 6 1 . 1 0 6 2 : r * Chief Scientist, Quantum Technology & Artificial Intelligence, Astradyne. Web: lab.christopheraltman.com Email: x@christopheraltman.com. 1 Violaris [6] proposes family of inter-branch communication protocols in which operations on superposed friend register enable conditional message transfer between branch-conditioned internal states. Branch interference effects, if present, should manifest in specific correlations that can be probed via appropriately chosen observables. The protocol provides concrete circuit template suitable for near-term quantum processors. Contributions of this work. We provide publicly reproducible IBM-hardware execution of minimal branch-transfer circuit benchmark with the following features: 1. Coherence-witness diagnostics: Beyond the standard population-based visibility , we measure multi-qubit Pauli-parity correlators (WX , WY ) that are sensitive to off-diagonal coherences potentially missed by . 2. Backend-matched noise modeling: We compare hardware results against Qiskit Aer simulations using noise models derived from contemporaneous ibm fez calibration data [7]. 3. Nonunitary channel constraint pipeline: We demonstrate how parameterized dephasing channels can be constrained relative to observed witness values and device-noise bands. 4. Full provenance: Job IDs, calibration snapshots, software versions, and SHA256 hashes for all artifacts enable independent verification. We emphasize what this work does not claim: it does not uniquely test or confirm any interpretation of quantum mechanics. The Many-Worlds interpretation, Copenhagen-style collapse models, and other unitary completions all predict identical measurement statistics for these circuits when the evolution is unitary. The experiments value lies in (a) demonstrating that branch-transfer primitives behave consistently with unitary predictions on real hardware, and (b) establishing methodology for constraining specific nonunitary perturbations should future experiments seek to test such models. Related work. Experimental implementations of Wigners-friend scenarios on quantum processors include the photonic realization by Proietti et al. [4] and the extended Wigners friend test by Bong et al. [5]. Those works focus on Bell-type inequalities and local friendliness assumptions. Our work differs in targeting the inter-branch communication primitive specifically and in emphasizing coherence-witness diagnostics as complement to population-based metrics. The IBM Quantum platform [8] and Qiskit software stack [9, 10] provide the infrastructure for our hardware execution. Related constraint results. Mukherjee and Hance [11] analyze timelike constraints on absoluteness of observed events assumptions in Wigners-friend settings. Our work is complementary: we focus on an implementable circuit primitive inspired by inter-branch communication protocols and report its empirical compilation and noise behavior on superconducting quantum hardware, without adopting observer-independent event absolutism.The circuit uses five qubits in total, with four qubits measured for the coherence-witness protocol: Q: Control qubit determining branch superposition (logical index 0) 2 Figure 1: Branch-transfer circuit schematic (five-qubit primitive) as transpiled and executed on hardware. Coherence-witness measurements act on the four-qubit subset (Q, R, F, ); the auxiliary qubit participates in the controlled transfer but is not measured in coherence-witness mode. Qubit labels correspond to logical register assignments; physical qubit indices after transpilation are recorded in the reproducibility bundle. R: Reference/ancilla qubit for message encoding (logical index 1) : Friend register encoding the observers internal state (logical index 2) : Probe qubit for readout correlations (logical index 3) Logical index 4: Auxiliary qubit used in the branch-transfer primitive (not measured in coherence-witness mode) Note on qubit labeling: The indices 04 above refer to logical register assignments in the abstract circuit. After transp"
[23.01.2026 03:46] Mistral response. {"id": "906144fa35ba40f286a54d265ac1ec6f", "created": 1769139986, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1366, "total_tokens": 1377, "completion_tokens": 11}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Astradyne\"]\n```"}}]}
[23.01.2026 03:46] Response: ```python
["Astradyne"]
```
[23.01.2026 03:46] Deleting PDF ./assets/pdf/2601.16004.pdf.
[23.01.2026 03:46] Success.
[23.01.2026 03:46] Enriching papers with extra data.
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 0. Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 1. BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation bu...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 2. LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 3. A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and ...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 4. Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? ...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 5. Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation a...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 6. An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper p...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 7. Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Repre...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 8. A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture comple...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 9. The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we p...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 10. Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmar...
[23.01.2026 03:46] ********************************************************************************
[23.01.2026 03:46] Abstract 11. Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM ...
[23.01.2026 03:46] Read previous papers.
[23.01.2026 03:46] Generating reviews via LLM API.
[23.01.2026 03:46] Querying the API.
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap
[23.01.2026 03:46] Response: ```json
{
  "desc": "Исследование показывает, что диффузионные большие языковые модели, которые генерируют токены в произвольном порядке, на самом деле ограничивают способность к рассуждениям, так как модель избегает неопределённых токенов и преждевременно сужает пространство решений. Авторы обнаружили, что стандартная оптимизация политики Group Relative Policy Optimization (GRPO) эффективнее, чем сложные методы усиленного обучения, которые пытаются сохранить гибкость порядка генерации. Предложенный подход JustGRPO демонстрирует, что отказ от произвольного порядка в пользу фиксированного порядка генерации достигает высокой точности на задачах математического рассуждения, сохраняя при этом возможность параллельного декодирования. Работа ставит под сомнение существующие предположения о преимуществах гибкого порядка токенизации в диффузионных моделях.",
  "emoji": "🔄",
  "title": "Гибкость, ставшая ловушкой: почему упорядоченная генерация лучше для рассуждений"
}
```
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap"

[23.01.2026 03:46] Response: ```python
["RL", "RLHF", "TRAINING", "MATH"]
```
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap"

[23.01.2026 03:46] Response: ```python
['DIFFUSION', 'REASONING', 'OPTIMIZATION']
```
[23.01.2026 03:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses how the ability of diffusion large language models (dLLMs) to generate tokens in any order can actually limit their reasoning capabilities. The authors argue that this flexibility leads to a premature collapse of the solution space, as dLLMs tend to avoid exploring uncertain tokens that are essential for effective reasoning. They propose a new method called JustGRPO, which uses standard Group Relative Policy Optimization instead of relying on arbitrary order generation. Their results show that JustGRPO can achieve high accuracy while maintaining the advantages of parallel decoding in dLLMs.","title":"Unlocking Reasoning Potential in Diffusion Models with JustGRPO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses how the ability of diffusion large language models (dLLMs) to generate tokens in any order can actually limit their reasoning capabilities. The authors argue that this flexibility leads to a premature collapse of the solution space, as dLLMs tend to avoid exploring uncertain tokens that are essential for effective reasoning. They propose a new method called JustGRPO, which uses standard Group Relative Policy Optimization instead of relying on arbitrary order generation. Their results show that JustGRPO can achieve high accuracy while maintaining the advantages of parallel decoding in dLLMs.', title='Unlocking Reasoning Potential in Diffusion Models with JustGRPO'))
[23.01.2026 03:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"在这篇论文中，我们探讨了扩散大型语言模型（dLLMs）在生成顺序上的灵活性如何影响推理能力。尽管dLLMs允许以任意顺序生成标记，但我们发现这种灵活性实际上会导致推理边界的收窄。具体来说，dLLMs倾向于跳过高不确定性的标记，这些标记对于探索至关重要，从而导致解决方案空间的提前崩溃。我们提出了一种新的方法JustGRPO，通过放弃任意顺序生成，采用标准的群体相对策略优化，显著提高了推理效果。","title":"放弃任意顺序，提升推理能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='在这篇论文中，我们探讨了扩散大型语言模型（dLLMs）在生成顺序上的灵活性如何影响推理能力。尽管dLLMs允许以任意顺序生成标记，但我们发现这种灵活性实际上会导致推理边界的收窄。具体来说，dLLMs倾向于跳过高不确定性的标记，这些标记对于探索至关重要，从而导致解决方案空间的提前崩溃。我们提出了一种新的方法JustGRPO，通过放弃任意顺序生成，采用标准的群体相对策略优化，显著提高了推理效果。', title='放弃任意顺序，提升推理能力'))
[23.01.2026 03:46] Querying the API.
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.
[23.01.2026 03:46] Response: ```json
{
  "desc": "В статье предложен BayesianVLA — фреймворк для обучения vision-language-action моделей роботов, который решает проблему коллапса информации, возникающую при целевом сборе данных. Авторы выявили, что инструкции становятся предсказуемы из визуальных наблюдений, что приводит к вырождению моделей в чистые vision-only политики, игнорирующие язык. Метод использует байесовскую декомпозицию с learnable latent action queries для построения двойной архитектуры, оценивающей vision-only prior и language-conditioned posterior. Подход максимизирует условное взаимное количество информации (PMI) между действиями и инструкциями, демонстрируя улучшение на 11.3% в условиях распределения.",
  "emoji": "🤖",
  "title": "Язык в действии: байесовское заземление инструкций для манипуляционных роботов"
}
```
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action."

[23.01.2026 03:46] Response: ```python
["ROBOTICS", "MULTIMODAL", "TRAINING"]
```
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action."

[23.01.2026 03:46] Response: ```python
['REASONING', 'ALIGNMENT']
```
[23.01.2026 03:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BayesianVLA is a framework designed to enhance the ability of robots to understand and execute language instructions during manipulation tasks. It addresses the problem of Information Collapse, where models rely too heavily on visual data and ignore language cues, leading to poor performance in new situations. By using Bayesian decomposition and introducing Latent Action Queries, the model learns to differentiate between vision-only actions and those that are conditioned on language. This approach not only improves generalization to out-of-distribution scenarios but also ensures that robots follow instructions more accurately, as demonstrated by significant performance gains in various benchmarks.","title":"Enhancing Robot Language Understanding with BayesianVLA"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BayesianVLA is a framework designed to enhance the ability of robots to understand and execute language instructions during manipulation tasks. It addresses the problem of Information Collapse, where models rely too heavily on visual data and ignore language cues, leading to poor performance in new situations. By using Bayesian decomposition and introducing Latent Action Queries, the model learns to differentiate between vision-only actions and those that are conditioned on language. This approach not only improves generalization to out-of-distribution scenarios but also ensures that robots follow instructions more accurately, as demonstrated by significant performance gains in various benchmarks.', title='Enhancing Robot Language Understanding with BayesianVLA'))
[23.01.2026 03:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BayesianVLA 是一种新颖的框架，旨在解决机器人操作中的语言与动作的基础问题。通过贝叶斯分解，该方法防止信息崩溃，并提高模型在未见数据上的泛化能力。我们引入可学习的潜在动作查询，构建双分支架构，以同时估计视觉优先和语言条件后验。实验结果表明，BayesianVLA 在多个基准测试中显著提高了模型的表现，尤其是在复杂的多任务场景中。","title":"贝叶斯分解提升机器人语言与动作的理解能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BayesianVLA 是一种新颖的框架，旨在解决机器人操作中的语言与动作的基础问题。通过贝叶斯分解，该方法防止信息崩溃，并提高模型在未见数据上的泛化能力。我们引入可学习的潜在动作查询，构建双分支架构，以同时估计视觉优先和语言条件后验。实验结果表明，BayesianVLA 在多个基准测试中显著提高了模型的表现，尤其是在复杂的多任务场景中。', title='贝叶斯分解提升机器人语言与动作的理解能力'))
[23.01.2026 03:46] Querying the API.
[23.01.2026 03:46] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.
[23.01.2026 03:47] Response: ```json
{
  "desc": "Работа представляет LLM-in-Sandbox — фреймворк, позволяющий большим языковым моделям использовать виртуальное окружение с кодом для решения задач в различных предметных областях без дополнительного обучения. Авторы показывают, что LLM спонтанно учатся получать доступ к внешним ресурсам, работать с файловой системой для обработки длинных контекстов и выполнять скрипты для форматирования результатов. Способности моделей к исследованию песочницы дополнительно улучшаются через обучение с подкреплением на основе неагентивных данных, позволяя моделям лучше адаптироваться к новым задачам. Экспериментально демонстрируется надёжная обобщаемость подхода в математике, физике, химии, биомедицине и задачах обработки протяжённых текстов.",
  "emoji": "🏜️",
  "title": "Виртуальная песочница как универсальный интерпретатор разума модели"
}
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."

[23.01.2026 03:47] Response: ```python
["AGENTS", "RL", "TRAINING"]
```

**Justification:**

1. **AGENTS**: The paper explicitly focuses on "agentic capabilities" and describes LLMs exploring a code sandbox environment to perform tasks autonomously, which is central to agent-based architectures.

2. **RL**: The paper introduces "LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL)" as a method to enhance the model's capabilities through reinforcement learning.

3. **TRAINING**: The paper discusses training methods, including both "training-free" and "post-trained settings," and describes using "non-agentic data to train models for sandbox exploration.
[23.01.2026 03:47] Error. Failed to parse JSON from LLM. ["AGENTS", "RL", "TRAINING"]


**Justification:**

1. **AGENTS**: The paper explicitly focuses on "agentic capabilities" and describes LLMs exploring a code sandbox environment to perform tasks autonomously, which is central to agent-based architectures.

2. **RL**: The paper introduces "LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL)" as a method to enhance the model"s capabilities through reinforcement learning.

3. **TRAINING**: The paper discusses training methods, including both "training-free" and "post-trained settings," and describes using "non-agentic data to train models for sandbox exploration.
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."

[23.01.2026 03:47] Response: ```python
['AGI', 'LONG_CONTEXT', 'SCIENCE', 'OPEN_SOURCE']
```

**Justification:**

- **AGI**: The paper explicitly discusses "general intelligence" capabilities and demonstrates LLMs performing diverse tasks across multiple domains without additional training.

- **LONG_CONTEXT**: The paper mentions "leverage the file system to handle long contexts" and explicitly lists "long-context understanding" as one of the experimental domains.

- **SCIENCE**: The paper demonstrates applications in scientific domains including "mathematics, physics, chemistry, biomedicine."

- **OPEN_SOURCE**: The paper states "open-source it as a Python package to facilitate real-world deployment.
[23.01.2026 03:47] Error. Failed to parse JSON from LLM. ["AGI", "LONG_CONTEXT", "SCIENCE", "OPEN_SOURCE"]


**Justification:**

- **AGI**: The paper explicitly discusses "general intelligence" capabilities and demonstrates LLMs performing diverse tasks across multiple domains without additional training.

- **LONG_CONTEXT**: The paper mentions "leverage the file system to handle long contexts" and explicitly lists "long-context understanding" as one of the experimental domains.

- **SCIENCE**: The paper demonstrates applications in scientific domains including "mathematics, physics, chemistry, biomedicine."

- **OPEN_SOURCE**: The paper states "open-source it as a Python package to facilitate real-world deployment.
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LLM-in-Sandbox is a framework that allows large language models (LLMs) to perform tasks across various domains by utilizing a code sandbox environment. This approach enables LLMs to generalize their capabilities without needing additional training, as they can access external resources and manage long contexts effectively. The framework also incorporates Reinforcement Learning to enhance the models\' ability to explore the sandbox, using only non-agentic data for training. Overall, LLM-in-Sandbox demonstrates strong performance in diverse fields such as mathematics, physics, and biomedicine, while being efficient and easy to deploy.","title":"Unlocking General Intelligence with LLM-in-Sandbox"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="LLM-in-Sandbox is a framework that allows large language models (LLMs) to perform tasks across various domains by utilizing a code sandbox environment. This approach enables LLMs to generalize their capabilities without needing additional training, as they can access external resources and manage long contexts effectively. The framework also incorporates Reinforcement Learning to enhance the models' ability to explore the sandbox, using only non-agentic data for training. Overall, LLM-in-Sandbox demonstrates strong performance in diverse fields such as mathematics, physics, and biomedicine, while being efficient and easy to deploy.", title='Unlocking General Intelligence with LLM-in-Sandbox'))
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LLM-in-Sandbox 是一种新方法，允许大型语言模型在代码沙箱环境中探索，从而在不同领域执行一般智能任务。该方法展示了强大的语言模型在没有额外训练的情况下，能够利用代码沙箱进行非代码任务的泛化能力。模型可以自发访问外部资源获取新知识，利用文件系统处理长上下文，并执行脚本满足格式要求。通过 LLM-in-Sandbox 强化学习，模型的探索能力得到了进一步增强，实验结果表明该方法在多个领域表现出色。","title":"LLM-in-Sandbox：让语言模型在沙箱中探索智能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LLM-in-Sandbox 是一种新方法，允许大型语言模型在代码沙箱环境中探索，从而在不同领域执行一般智能任务。该方法展示了强大的语言模型在没有额外训练的情况下，能够利用代码沙箱进行非代码任务的泛化能力。模型可以自发访问外部资源获取新知识，利用文件系统处理长上下文，并执行脚本满足格式要求。通过 LLM-in-Sandbox 强化学习，模型的探索能力得到了进一步增强，实验结果表明该方法在多个领域表现出色。', title='LLM-in-Sandbox：让语言模型在沙箱中探索智能'))
[23.01.2026 03:47] Querying the API.
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.
[23.01.2026 03:47] Response: ```json
{
  "desc": "В статье представлен новый эталонный набор данных EDIR для задачи поиска составных изображений (CIR) на основе техник редактирования изображений. Авторы используют контролируемое редактирование для синтеза разнообразных запросов, охватывающих пять основных категорий и пятнадцать подкатегорий. Комплексная оценка тринадцати мультимодальных моделей показала значительный разрыв в возможностях: даже передовые модели демонстрируют нестабильную производительность на различных подкатегориях. Исследование выявило фундаментальные ограничения существующих бенчмарков, включая модальные смещения и недостаточное охватывание категорий.",
  "emoji": "🔍",
  "title": "Тонкозернистый бенчмарк для поиска составных изображений раскрывает слабости современных мультимодальных моделей"
}
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures."

[23.01.2026 03:47] Response: ```python
["DATASET", "BENCHMARK", "MULTIMODAL", "CV"]
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures."

[23.01.2026 03:47] Response: ```python
["SYNTHETIC"]
```

The paper is classified as SYNTHETIC because it explicitly uses image editing techniques to synthesize queries for constructing a benchmark dataset (EDIR). The text states: "we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories." This directly relates to generating and leveraging artificial/synthetic data for training and evaluation purposes.
[23.01.2026 03:47] Error. Failed to parse JSON from LLM. ["SYNTHETIC"]


The paper is classified as SYNTHETIC because it explicitly uses image editing techniques to synthesize queries for constructing a benchmark dataset (EDIR). The text states: "we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories." This directly relates to generating and leveraging artificial/synthetic data for training and evaluation purposes.
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new benchmark for fine-grained composed image retrieval (CIR) using image editing techniques. The existing benchmarks are limited in their query categories and do not reflect the complexities of real-world scenarios. The new benchmark, called EDIR, includes 5,000 high-quality queries across various categories and subcategories, revealing significant performance gaps in current multimodal models. The study highlights the limitations of existing benchmarks and demonstrates the challenges faced by state-of-the-art models in accurately retrieving composed images.","title":"Bridging the Gap in Composed Image Retrieval with EDIR"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new benchmark for fine-grained composed image retrieval (CIR) using image editing techniques. The existing benchmarks are limited in their query categories and do not reflect the complexities of real-world scenarios. The new benchmark, called EDIR, includes 5,000 high-quality queries across various categories and subcategories, revealing significant performance gaps in current multimodal models. The study highlights the limitations of existing benchmarks and demonstrates the challenges faced by state-of-the-art models in accurately retrieving composed images.', title='Bridging the Gap in Composed Image Retrieval with EDIR'))
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种新颖的细粒度组合图像检索基准，利用图像编辑技术揭示了现有多模态模型的能力差距和当前基准的局限性。组合图像检索（CIR）是多模态理解中的一个重要且复杂的任务，现有基准通常查询类别有限，无法满足真实场景的多样化需求。为了解决这一评估差距，我们构建了EDIR基准，包含5000个高质量查询，涵盖五个主要类别和十五个子类别。通过对13种多模态嵌入模型的综合评估，我们发现即使是最先进的模型在所有子类别中的表现也不一致，突显了我们基准的严格性。","title":"细粒度组合图像检索的新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种新颖的细粒度组合图像检索基准，利用图像编辑技术揭示了现有多模态模型的能力差距和当前基准的局限性。组合图像检索（CIR）是多模态理解中的一个重要且复杂的任务，现有基准通常查询类别有限，无法满足真实场景的多样化需求。为了解决这一评估差距，我们构建了EDIR基准，包含5000个高质量查询，涵盖五个主要类别和十五个子类别。通过对13种多模态嵌入模型的综合评估，我们发现即使是最先进的模型在所有子类别中的表现也不一致，突显了我们基准的严格性。', title='细粒度组合图像检索的新基准'))
[23.01.2026 03:47] Querying the API.
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.
[23.01.2026 03:47] Response: ```json
{
  "desc": "Статья представляет метод Test-Time Training to Discover (TTT-Discover), который позволяет LLM продолжать обучение во время тестирования на конкретной задаче через reinforcement learning, вместо использования замороженной модели. В отличие от традиционного подхода, целью является найти одно оптимальное решение для конкретной научной проблемы, а не добиться хорошей обобщающей способности на множестве задач. Авторы демонстрируют эффективность метода на разнообразных задачах: математические проблемы, оптимизация GPU ядер, алгоритмические соревнования и биологические вычисления. TTT-Discover достигает state-of-the-art результатов на всех протестированных задачах, используя открытую модель и относительно небольшие вычислительные ресурсы.",
  "emoji": "🎯",
  "title": "Специализированное обучение LLM для поиска оптимальных решений конкретных научных задач"
}
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem."

[23.01.2026 03:47] Response: ```python
["RL", "TRAINING", "MATH"]
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem."

[23.01.2026 03:47] Response: ```python
['SCIENCE', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Test-Time Training to Discover (TTT-Discover), a method that allows AI systems to adaptively learn and optimize solutions for specific scientific problems during the testing phase. Unlike traditional approaches that focus on generalization, TTT-Discover emphasizes continual learning tailored to individual challenges, aiming to find the best solution rather than an average one. The method employs reinforcement learning to refine a frozen large language model (LLM) based on experiences specific to the test problem, leading to significant improvements in various domains such as mathematics and biology. The results demonstrate that TTT-Discover achieves state-of-the-art performance across multiple tasks, all while using an open model and maintaining reproducibility with publicly available code.","title":"Discovering Optimal Solutions with Test-Time Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Test-Time Training to Discover (TTT-Discover), a method that allows AI systems to adaptively learn and optimize solutions for specific scientific problems during the testing phase. Unlike traditional approaches that focus on generalization, TTT-Discover emphasizes continual learning tailored to individual challenges, aiming to find the best solution rather than an average one. The method employs reinforcement learning to refine a frozen large language model (LLM) based on experiences specific to the test problem, leading to significant improvements in various domains such as mathematics and biology. The results demonstrate that TTT-Discover achieves state-of-the-art performance across multiple tasks, all while using an open model and maintaining reproducibility with publicly available code.', title='Discovering Optimal Solutions with Test-Time Training'))
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一种名为测试时训练发现（TTT-Discover）的方法，旨在通过持续学习来解决特定科学问题。与传统的通用化方法不同，该方法专注于为每个具体问题找到最佳解决方案，而不是平均多个良好解决方案。通过在测试时进行强化学习，模型能够根据特定问题的经验进行训练，从而提高解决问题的效率。研究结果显示，TTT-Discover在多个领域（如数学、GPU内核工程、算法设计和生物学）中设定了新的最优标准。","title":"测试时训练：为科学问题找到最佳解决方案"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一种名为测试时训练发现（TTT-Discover）的方法，旨在通过持续学习来解决特定科学问题。与传统的通用化方法不同，该方法专注于为每个具体问题找到最佳解决方案，而不是平均多个良好解决方案。通过在测试时进行强化学习，模型能够根据特定问题的经验进行训练，从而提高解决问题的效率。研究结果显示，TTT-Discover在多个领域（如数学、GPU内核工程、算法设计和生物学）中设定了新的最优标准。', title='测试时训练：为科学问题找到最佳解决方案'))
[23.01.2026 03:47] Querying the API.
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.
[23.01.2026 03:47] Response: ```json
{
  "desc": "Stable-DiffCoder — это диффузионная модель для моделирования кода, которая использует блочную генерацию вместо последовательного авторегрессивного подхода. Модель применяет специализированную предварительную подготовку с адаптивным расписанием шума и прогревом для стабильного обучения и эффективного усвоения знаний. Экспериментальные результаты показывают, что Stable-DiffCoder превосходит автор­егрессивные базовые модели сравнимого размера на различных бенчмарках кодирования. Кроме того, диффузионный подход с генерацией в произвольном порядке улучшает моделирование структурированного кода для редактирования и рассуждений, а также помогает низкоресурсным языкам программирования благодаря аугментации данных.",
  "emoji": "🧩",
  "title": "Диффузионный подход превосходит автор­егрессию в моделировании кода"
}
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages."

[23.01.2026 03:47] Response: ```python
["PLP", "ARCHITECTURE", "TRAINING"]
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages."

[23.01.2026 03:47] Response: ```python
["DIFFUSION", "OPTIMIZATION", "LOW_RESOURCE"]
```
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Stable-DiffCoder is a new model for generating code that outperforms traditional autoregressive models by using a method called block diffusion continual pretraining. This approach allows the model to generate code in blocks rather than one piece at a time, which improves efficiency and data reuse. The model incorporates a special training process that includes a warmup phase and a noise schedule to ensure stable learning. Overall, Stable-DiffCoder shows better performance on various coding tasks and enhances the ability to work with structured code, making it beneficial for less common programming languages.","title":"Revolutionizing Code Generation with Block Diffusion!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Stable-DiffCoder is a new model for generating code that outperforms traditional autoregressive models by using a method called block diffusion continual pretraining. This approach allows the model to generate code in blocks rather than one piece at a time, which improves efficiency and data reuse. The model incorporates a special training process that includes a warmup phase and a noise schedule to ensure stable learning. Overall, Stable-DiffCoder shows better performance on various coding tasks and enhances the ability to work with structured code, making it beneficial for less common programming languages.', title='Revolutionizing Code Generation with Block Diffusion!'))
[23.01.2026 03:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Stable-DiffCoder是一种基于块扩散的代码模型，展示了比自回归基线更优越的代码建模性能。它通过块扩散持续预训练和高效的训练机制，重新利用了Seed-Coder架构和数据。该模型在相同的数据和架构下，在多种代码基准测试中整体超越了自回归模型。通过扩散训练，Stable-DiffCoder不仅提升了代码建模质量，还改善了结构化代码的编辑和推理能力，特别是对低资源编程语言的支持。","title":"稳定扩散编码器：超越自回归的代码建模新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Stable-DiffCoder是一种基于块扩散的代码模型，展示了比自回归基线更优越的代码建模性能。它通过块扩散持续预训练和高效的训练机制，重新利用了Seed-Coder架构和数据。该模型在相同的数据和架构下，在多种代码基准测试中整体超越了自回归模型。通过扩散训练，Stable-DiffCoder不仅提升了代码建模质量，还改善了结构化代码的编辑和推理能力，特别是对低资源编程语言的支持。', title='稳定扩散编码器：超越自回归的代码建模新方法'))
[23.01.2026 03:47] Querying the API.
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.
[23.01.2026 03:47] Response: ```json
{
  "desc": "Статья представляет OpenVision 3 — продвинутый визуальный энкодер, который обучается единому представлению изображений для решения задач как понимания, так и генерации. Архитектура комбинирует сжатые VAE-латенты с трансформером ViT, применяя совместную оптимизацию сигналов реконструкции и семантики. Энкодер одновременно обучается восстанавливать исходное изображение через ViT-VAE декодер и извлекать семантические признаки с помощью контрастивного обучения и описания изображений. Унифицированный подход позволяет модели достичь хороших результатов как в мультимодальном понимании, так и в генеративных задачах, превосходя стандартные решения на основе CLIP.",
  "emoji": "🎨",
  "title": "Единое визуальное представление для понимания и генерации изображений"
}
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling."

[23.01.2026 03:47] Response: ```python
['CV', 'ARCHITECTURE', 'MULTIMODAL']
```
[23.01.2026 03:47] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling."

[23.01.2026 03:47] Response: ```python
['OPEN_SOURCE']
```
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OpenVision 3 is an advanced vision encoder that creates a unified visual representation for both understanding and generating images. It combines Variational Autoencoder (VAE) compressed image latents with a Vision Transformer (ViT) architecture, optimizing for both reconstruction and semantic understanding. The model is trained to reconstruct images while also learning from contrastive and image-captioning tasks, enhancing its ability to capture meaningful features. Extensive evaluations show that OpenVision 3 performs competitively with existing models, demonstrating its effectiveness in multimodal tasks and image generation.","title":"Unified Visual Representation for Understanding and Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OpenVision 3 is an advanced vision encoder that creates a unified visual representation for both understanding and generating images. It combines Variational Autoencoder (VAE) compressed image latents with a Vision Transformer (ViT) architecture, optimizing for both reconstruction and semantic understanding. The model is trained to reconstruct images while also learning from contrastive and image-captioning tasks, enhancing its ability to capture meaningful features. Extensive evaluations show that OpenVision 3 performs competitively with existing models, demonstrating its effectiveness in multimodal tasks and image generation.', title='Unified Visual Representation for Understanding and Generation'))
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OpenVision 3是一种先进的视觉编码器，它通过结合变分自编码器（VAE）压缩的图像潜变量和视觉变换器（ViT）架构，学习统一的视觉表示，适用于图像理解和生成。该模型的核心架构简单，将VAE压缩的图像潜变量输入ViT编码器，并训练其输出以支持重建和语义信号的双重角色。通过在共享潜在空间中联合优化重建和语义驱动的信号，编码器学习到的表示能够在这两种任务中良好协同和泛化。实验结果表明，OpenVision 3在多模态理解和生成任务中均表现出色，推动了统一建模的研究。","title":"统一视觉表示的未来"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OpenVision 3是一种先进的视觉编码器，它通过结合变分自编码器（VAE）压缩的图像潜变量和视觉变换器（ViT）架构，学习统一的视觉表示，适用于图像理解和生成。该模型的核心架构简单，将VAE压缩的图像潜变量输入ViT编码器，并训练其输出以支持重建和语义信号的双重角色。通过在共享潜在空间中联合优化重建和语义驱动的信号，编码器学习到的表示能够在这两种任务中良好协同和泛化。实验结果表明，OpenVision 3在多模态理解和生成任务中均表现出色，推动了统一建模的研究。', title='统一视觉表示的未来'))
[23.01.2026 03:48] Querying the API.
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.
[23.01.2026 03:48] Response: ```json
{
  "desc": "В статье представлены Representation Autoencoders (RAE) — архитектура, которая превосходит VAE в задаче генерации изображений по текстовому описанию в больших масштабах. Авторы масштабировали RAE декодеры на замороженном энкодере SigLIP-2, обнаружив, что шумовое расписание, зависящее от размерности, остаётся критичным, а многие архитектурные сложности не дают заметных преимуществ при масштабировании. Сравнение RAE с FLUX VAE на диффузионных трансформерах от 0.5B до 9.8B параметров показало, что RAE-модели демонстрируют более быструю сходимость, лучшее качество генерации и не страдают от катастрофического переобучения при файнтюнинге. Ключевое преимущество — возможность унифицированного мультимодального рассуждения в общем пространстве представлений, где визуальное понимание и генерация работают согласованно.",
  "emoji": "🖼️",
  "title": "Представленческие автоэнкодеры превосходят VAE в генерации изображений из текста"
}
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models."

[23.01.2026 03:48] Response: ```python
['CV', 'MULTIMODAL', 'TRAINING', 'ARCHITECTURE']
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models."

[23.01.2026 03:48] Response: ```python
['DIFFUSION', 'SYNTHETIC', 'OPEN_SOURCE']
```
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Representation Autoencoders (RAEs) outperform Variational Autoencoders (VAEs) in large-scale text-to-image generation by providing better stability, faster convergence, and higher quality outputs. This study explores the scalability of RAEs in high-dimensional semantic latent spaces, particularly for freeform text-to-image tasks. The findings indicate that while increasing model scale enhances general fidelity, the composition of training data is crucial for specific applications like text. Overall, RAEs demonstrate superior performance and robustness compared to VAEs, making them a more effective choice for multimodal reasoning in shared representation spaces.","title":"RAEs: The Superior Choice for Text-to-Image Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Representation Autoencoders (RAEs) outperform Variational Autoencoders (VAEs) in large-scale text-to-image generation by providing better stability, faster convergence, and higher quality outputs. This study explores the scalability of RAEs in high-dimensional semantic latent spaces, particularly for freeform text-to-image tasks. The findings indicate that while increasing model scale enhances general fidelity, the composition of training data is crucial for specific applications like text. Overall, RAEs demonstrate superior performance and robustness compared to VAEs, making them a more effective choice for multimodal reasoning in shared representation spaces.', title='RAEs: The Superior Choice for Text-to-Image Generation'))
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"表示自编码器（RAEs）在大规模文本到图像生成中表现优于变分自编码器（VAEs），展现出更好的稳定性、更快的收敛速度和更高的生成质量。研究表明，RAEs在高维语义潜在空间中的扩展能力使其在图像生成任务中具有明显优势。通过对RAE设计选择的严格测试，发现简化的框架在大规模训练中更为有效，尽管噪声调度仍然重要，但复杂的架构设计带来的好处微乎其微。最终，RAE在各个模型规模的预训练和微调阶段均表现出更好的性能，确立了其作为大规模文本到图像生成的更强基础。","title":"表示自编码器：大规模生成的更强基础"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='表示自编码器（RAEs）在大规模文本到图像生成中表现优于变分自编码器（VAEs），展现出更好的稳定性、更快的收敛速度和更高的生成质量。研究表明，RAEs在高维语义潜在空间中的扩展能力使其在图像生成任务中具有明显优势。通过对RAE设计选择的严格测试，发现简化的框架在大规模训练中更为有效，尽管噪声调度仍然重要，但复杂的架构设计带来的好处微乎其微。最终，RAE在各个模型规模的预训练和微调阶段均表现出更好的性能，确立了其作为大规模文本到图像生成的更强基础。', title='表示自编码器：大规模生成的更强基础'))
[23.01.2026 03:48] Querying the API.
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/
[23.01.2026 03:48] Response: ```json
{
  "desc": "В работе предлагается Cosmos Policy — метод адаптации предобученной видеомодели в политику робота через единственный этап постобучения на демонстрационных данных без изменения архитектуры. Модель обучается непосредственно генерировать действия робота, закодированные как скрытые фреймы в процессе латентной диффузии видеомодели. Кроме того, Cosmos Policy генерирует будущие состояния и функции ценности, что позволяет использовать планирование траекторий действий во время тестирования. Подход показывает передовые результаты на бенчмарках LIBERO и RoboCasa, а также демонстрирует лучшую производительность на сложных задачах манипуляции с двумя руками в реальном мире по сравнению с диффузионными политиками и моделями видения-языка-действия.",
  "emoji": "🤖",
  "title": "От видеопредикции к робот-политике в один этап обучения"
}
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/"

[23.01.2026 03:48] Response: ```python
['ROBOTICS', 'VIDEO', 'TRAINING', 'BENCHMARK']
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/"

[23.01.2026 03:48] Response: ```python
['TRANSFER_LEARNING', 'DIFFUSION', 'OPEN_SOURCE']
```

**Justification:**

1. **TRANSFER_LEARNING**: The paper explicitly discusses adapting a pretrained video model (Cosmos-Predict2) for robot policy learning, which is a core transfer learning approach - leveraging knowledge from one domain (video generation) to another (robotics).

2. **DIFFUSION**: The paper describes using a latent diffusion process and mentions "diffusion policies" as baselines. The approach encodes actions as latent frames within the video model's latent diffusion process.

3. **OPEN_SOURCE**: The paper explicitly states "We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/", indicating the authors are contributing to open-source by releasing their models, code, and datasets.
[23.01.2026 03:48] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "DIFFUSION", "OPEN_SOURCE"]


**Justification:**

1. **TRANSFER_LEARNING**: The paper explicitly discusses adapting a pretrained video model (Cosmos-Predict2) for robot policy learning, which is a core transfer learning approach - leveraging knowledge from one domain (video generation) to another (robotics).

2. **DIFFUSION**: The paper describes using a latent diffusion process and mentions "diffusion policies" as baselines. The approach encodes actions as latent frames within the video model"s latent diffusion process.

3. **OPEN_SOURCE**: The paper explicitly states "We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/", indicating the authors are contributing to open-source by releasing their models, code, and datasets.
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Cosmos Policy, a method that adapts a pretrained video model for use in robotics without changing its architecture. By using a single-stage post-training approach, it allows the model to generate robot actions directly from video data. Cosmos Policy captures complex action distributions and predicts future states, enabling effective planning during robot operations. The method shows superior performance in various benchmarks, demonstrating its efficiency compared to other existing policies.","title":"Transforming Video Models into Robot Action Policies with Ease"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents Cosmos Policy, a method that adapts a pretrained video model for use in robotics without changing its architecture. By using a single-stage post-training approach, it allows the model to generate robot actions directly from video data. Cosmos Policy captures complex action distributions and predicts future states, enabling effective planning during robot operations. The method shows superior performance in various benchmarks, demonstrating its efficiency compared to other existing policies.', title='Transforming Video Models into Robot Action Policies with Ease'))
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文介绍了一种名为Cosmos Policy的方法，它通过单阶段后训练将预训练的视频模型（Cosmos-Predict2）适配为机器人策略。该方法无需对模型架构进行修改，直接利用机器人演示数据生成机器人动作。Cosmos Policy能够生成未来状态图像和价值（预期累积奖励），从而在测试时进行更高成功率的动作轨迹规划。评估结果表明，Cosmos Policy在LIBERO和RoboCasa模拟基准测试中表现优异，成功率达到98.5%和67.1%。","title":"简化机器人策略的预训练视频模型适配"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文介绍了一种名为Cosmos Policy的方法，它通过单阶段后训练将预训练的视频模型（Cosmos-Predict2）适配为机器人策略。该方法无需对模型架构进行修改，直接利用机器人演示数据生成机器人动作。Cosmos Policy能够生成未来状态图像和价值（预期累积奖励），从而在测试时进行更高成功率的动作轨迹规划。评估结果表明，Cosmos Policy在LIBERO和RoboCasa模拟基准测试中表现优异，成功率达到98.5%和67.1%。', title='简化机器人策略的预训练视频模型适配'))
[23.01.2026 03:48] Querying the API.
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.
[23.01.2026 03:48] Response: ```json
{
  "desc": "Qwen3-TTS — это семейство продвинутых многоязычных моделей синтеза речи с поддержкой клонирования голоса за 3 секунды и детального контроля над характеристиками выходной речи. Модели используют двухканальную архитектуру языковой модели и два специализированных речевых токенайзера: первый (25Hz) подчеркивает семантическое содержание для потоковой передачи, второй (12Hz) обеспечивает экстремальное сжатие битрейта и сверхнизкую задержку с первым пакетом за 97 мс. Система обучена на более чем 5 миллионах часов речевых данных на 10 языках и показывает лучшие результаты на различных объективных и субъективных бенчмарках. Авторы публикуют оба токенайзера и модели с открытой лицензией Apache 2.0 для развития сообщества.",
  "emoji": "🎤",
  "title": "Трёхсекундное клонирование голоса с ультранизкой задержкой"
}
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license."

[23.01.2026 03:48] Response: ```python
["AUDIO", "MULTILINGUAL", "MULTIMODAL", "ARCHITECTURE", "DATASET"]
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license."

[23.01.2026 03:48] Response: ```python
['OPEN_SOURCE']
```
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Qwen3-TTS series introduces cutting-edge multilingual text-to-speech models that excel in voice cloning and speech generation control. These models utilize a dual-track language model architecture and specialized speech tokenizers to enable efficient streaming synthesis. With training on over 5 million hours of speech data in 10 languages, they support rapid voice cloning and allow for detailed manipulation of the generated speech. The release of these models and tokenizers under the Apache 2.0 license aims to promote further research and development in the community.","title":"Revolutionizing Speech: Multilingual TTS with Voice Cloning and Control"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The Qwen3-TTS series introduces cutting-edge multilingual text-to-speech models that excel in voice cloning and speech generation control. These models utilize a dual-track language model architecture and specialized speech tokenizers to enable efficient streaming synthesis. With training on over 5 million hours of speech data in 10 languages, they support rapid voice cloning and allow for detailed manipulation of the generated speech. The release of these models and tokenizers under the Apache 2.0 license aims to promote further research and development in the community.', title='Revolutionizing Speech: Multilingual TTS with Voice Cloning and Control'))
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Qwen3-TTS系列是先进的多语言文本转语音模型，具备语音克隆和可控语音生成的能力。该模型采用双轨语言模型架构和专门的语音标记器，实现高效的流式合成。Qwen3-TTS支持3秒的语音克隆和基于描述的控制，能够创建全新的声音并精细调整输出语音。经过超过500万小时的语音数据训练，Qwen3-TTS在多种基准测试中表现出色，推动了社区的研究与开发。","title":"先进的多语言文本转语音技术"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Qwen3-TTS系列是先进的多语言文本转语音模型，具备语音克隆和可控语音生成的能力。该模型采用双轨语言模型架构和专门的语音标记器，实现高效的流式合成。Qwen3-TTS支持3秒的语音克隆和基于描述的控制，能够创建全新的声音并精细调整输出语音。经过超过500万小时的语音数据训练，Qwen3-TTS在多种基准测试中表现出色，推动了社区的研究与开发。', title='先进的多语言文本转语音技术'))
[23.01.2026 03:48] Querying the API.
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .
[23.01.2026 03:48] Response: ```json
{
  "desc": "Terminal-Bench 2.0 — это сложный бенчмарк, содержащий 89 задач в терминальной среде для оценки способностей AI агентов решать реальные проблемы. Авторы критикуют существующие бенчмарки за то, что они либо не измеряют практические задачи, либо недостаточно сложны для оценки передовых моделей. Каждая задача включает уникальное окружение, написанное человеком решение и комплексные тесты верификации. Исследование показывает, что лучшие модели и агенты достигают менее 65% на бенчмарке, и авторы проводят анализ ошибок для определения направлений улучшения.",
  "emoji": "🖥️",
  "title": "Реальные задачи для проверки способности AI агентов к автономному выполнению сложных работ"
}
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ ."

[23.01.2026 03:48] Response: ```python
["BENCHMARK", "AGENTS", "DATASET"]
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ ."

[23.01.2026 03:48] Response: ```python
["OPEN_SOURCE"]
```

The paper mentions publishing "the dataset and evaluation harness" publicly at a URL, which indicates a contribution to open-source resources for the research community.
[23.01.2026 03:48] Error. Failed to parse JSON from LLM. ["OPEN_SOURCE"]


The paper mentions publishing "the dataset and evaluation harness" publicly at a URL, which indicates a contribution to open-source resources for the research community.
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Terminal-Bench 2.0 is a new benchmark designed to test AI agents on 89 challenging tasks that mimic real-world terminal environments. These tasks are specifically created to evaluate the capabilities of advanced AI models in completing complex, long-term objectives. The benchmark reveals that even the best current models score below 65%, highlighting significant areas for improvement. By providing a dataset and evaluation tools, this work aims to support further research and development in AI task completion.","title":"Challenging AI Agents with Real-World Terminal Tasks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Terminal-Bench 2.0 is a new benchmark designed to test AI agents on 89 challenging tasks that mimic real-world terminal environments. These tasks are specifically created to evaluate the capabilities of advanced AI models in completing complex, long-term objectives. The benchmark reveals that even the best current models score below 65%, highlighting significant areas for improvement. By providing a dataset and evaluation tools, this work aims to support further research and development in AI task completion.', title='Challenging AI Agents with Real-World Terminal Tasks'))
[23.01.2026 03:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Terminal-Bench 2.0 是一个具有挑战性的基准测试，包含89个基于终端的任务，用于评估人工智能代理在现实场景中的能力。这些任务灵感来源于真实工作流程中的问题，旨在测量前沿模型在复杂任务上的表现。研究表明，当前的前沿模型和代理在该基准测试中的得分低于65%，并进行了错误分析以识别改进的方向。我们还发布了数据集和评估工具，以支持开发者和研究人员的后续工作。","title":"挑战极限，提升AI能力！"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Terminal-Bench 2.0 是一个具有挑战性的基准测试，包含89个基于终端的任务，用于评估人工智能代理在现实场景中的能力。这些任务灵感来源于真实工作流程中的问题，旨在测量前沿模型在复杂任务上的表现。研究表明，当前的前沿模型和代理在该基准测试中的得分低于65%，并进行了错误分析以识别改进的方向。我们还发布了数据集和评估工具，以支持开发者和研究人员的后续工作。', title='挑战极限，提升AI能力！'))
[23.01.2026 03:48] Querying the API.
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.   Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.   This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.
[23.01.2026 03:48] Response: ```json
{
  "desc": "В этой работе авторы реализуют и тестируют на квантовом процессоре IBM семейство квантовых схем для оценки свидетелей межветвевых корреляций в классических измерениях, полученных из экспериментов типа Вигнера-друга. Протокол кодирует эволюцию подсистемы наблюдателя, управляемую контрольным кубитом, и использует управляемую операцию передачи для зондирования корреляций между контекстами условных измерений. Авторы анализируют поведение схемы при реалистичном шуме устройства и ограничениях компиляции, наблюдая видимость 0.877 и свидетелей когерентности 0.840 и -0.811. Работа предоставляет воспроизводимый операциональный метод для оценки обнаруживаемости неидеальных квантовых каналов относительно калиброванного шума устройства.",
  "emoji": "⚛️",
  "title": "Квантовые свидетели межветвевых корреляций на реальном оборудовании"
}
```
[23.01.2026 03:48] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.   Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.   This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise."

[23.01.2026 03:49] Response: ```python
["BENCHMARK"]
```

The paper is primarily focused on implementing and benchmarking quantum circuits on IBM Quantum hardware, evaluating their performance under realistic device conditions. The core contribution is the benchmarking of a specific circuit family and measurement of various metrics (visibility, coherence witnesses) on quantum hardware, which aligns with the BENCHMARK topic of proposing or analyzing model evaluation frameworks and benchmarks.
[23.01.2026 03:49] Error. Failed to parse JSON from LLM. ["BENCHMARK"]


The paper is primarily focused on implementing and benchmarking quantum circuits on IBM Quantum hardware, evaluating their performance under realistic device conditions. The core contribution is the benchmarking of a specific circuit family and measurement of various metrics (visibility, coherence witnesses) on quantum hardware, which aligns with the BENCHMARK topic of proposing or analyzing model evaluation frameworks and benchmarks.
[23.01.2026 03:49] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.   Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.   This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise."

[23.01.2026 03:49] Response: ```python
[]
```

This paper is about quantum computing and quantum circuit implementation on IBM Quantum hardware. It does not relate to any of the provided topics, which are all focused on machine learning, artificial intelligence, and language models. The paper discusses quantum mechanics, quantum circuits, and witness measurements—areas outside the scope of the given topic list.
[23.01.2026 03:49] Error. Failed to parse JSON from LLM. []


This paper is about quantum computing and quantum circuit implementation on IBM Quantum hardware. It does not relate to any of the provided topics, which are all focused on machine learning, artificial intelligence, and language models. The paper discusses quantum mechanics, quantum circuits, and witness measurements—areas outside the scope of the given topic list.
[23.01.2026 03:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents the implementation and benchmarking of quantum circuits on IBM Quantum hardware to estimate operational inter-branch communication witnesses. The authors utilize a five-qubit circuit to analyze correlations in measurement records from Wigner\'s-friend-style circuits, focusing on the effects of realistic device noise. They report visibility and coherence witness measurements, highlighting the circuit\'s performance under various noise conditions. The study aims to establish a reproducible framework for assessing the detectability of non-ideal quantum channels without addressing interpretations of quantum mechanics.","title":"Benchmarking Quantum Circuits for Communication Witnesses"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents the implementation and benchmarking of quantum circuits on IBM Quantum hardware to estimate operational inter-branch communication witnesses. The authors utilize a five-qubit circuit to analyze correlations in measurement records from Wigner's-friend-style circuits, focusing on the effects of realistic device noise. They report visibility and coherence witness measurements, highlighting the circuit's performance under various noise conditions. The study aims to establish a reproducible framework for assessing the detectability of non-ideal quantum channels without addressing interpretations of quantum mechanics.", title='Benchmarking Quantum Circuits for Communication Witnesses'))
[23.01.2026 03:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文实现并基准测试了在IBM量子硬件上估计操作性跨分支通信见证的量子电路。我们采用了Violaris提出的电路系列，定义为由编译的Wigner朋友风格电路产生的经典测量记录中的相关性。通过在单个电路中实现五量子比特实例，我们评估了在现实设备噪声和编译约束下的行为。研究结果显示，能见度和相干性见证在不同条件下的表现，为评估非理想通道的可检测性提供了可重复的操作约束管道。","title":"量子电路实现与基准测试的创新"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文实现并基准测试了在IBM量子硬件上估计操作性跨分支通信见证的量子电路。我们采用了Violaris提出的电路系列，定义为由编译的Wigner朋友风格电路产生的经典测量记录中的相关性。通过在单个电路中实现五量子比特实例，我们评估了在现实设备噪声和编译约束下的行为。研究结果显示，能见度和相干性见证在不同条件下的表现，为评估非理想通道的可检测性提供了可重复的操作约束管道。', title='量子电路实现与基准测试的创新'))
[23.01.2026 03:49] Renaming data file.
[23.01.2026 03:49] Renaming previous data. hf_papers.json to ./d/2026-01-23.json
[23.01.2026 03:49] Saving new data file.
[23.01.2026 03:49] Generating page.
[23.01.2026 03:49] Renaming previous page.
[23.01.2026 03:49] Renaming previous data. index.html to ./d/2026-01-23.html
[23.01.2026 03:49] Writing result.
[23.01.2026 03:49] Renaming log file.
[23.01.2026 03:49] Renaming previous data. log.txt to ./logs/2026-01-23_last_log.txt
