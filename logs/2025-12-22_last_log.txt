[22.12.2025 21:20] Read previous papers.
[22.12.2025 21:20] Generating top page (month).
[22.12.2025 21:20] Writing top page (month).
[22.12.2025 22:22] Read previous papers.
[22.12.2025 22:22] Get feed.
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16969
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16793
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17901
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17260
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17909
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17012
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16041
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17897
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17495
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11362
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17351
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14870
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17796
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17008
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17419
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16848
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16483
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15586
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17532
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.17459
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16978
[22.12.2025 22:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13427
[22.12.2025 22:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.12.2025 22:22] No deleted papers detected.
[22.12.2025 22:22] Downloading and parsing papers (pdf, html). Total: 22.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.16969.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.16969.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.16969.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.16793.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.16793.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.16793.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17901.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17901.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17901.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17260.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17260.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17260.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17909.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17909.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17909.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17012.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17012.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17012.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.16041.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.16041.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.16041.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17897.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17897.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17897.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17495.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17495.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17495.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.11362.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.11362.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.11362.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17351.
[22.12.2025 22:22] Downloading paper 2512.17351 from https://arxiv.org/pdf/2512.17351v1...
[22.12.2025 22:22] Failed to download and parse paper https://huggingface.co/papers/2512.17351: 'LTChar' object is not iterable
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.14870.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.14870.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.14870.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17796.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17796.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17796.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17008.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17008.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17008.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17419.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17419.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17419.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.16848.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.16848.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.16848.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.16483.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.16483.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.16483.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.15586.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.15586.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.15586.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17532.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17532.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17532.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.17459.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.17459.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.17459.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.16978.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.16978.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.16978.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Downloading and parsing paper https://huggingface.co/papers/2512.13427.
[22.12.2025 22:22] Extra JSON file exists (./assets/json/2512.13427.json), skip PDF parsing.
[22.12.2025 22:22] Paper image links file exists (./assets/img_data/2512.13427.json), skip HTML parsing.
[22.12.2025 22:22] Success.
[22.12.2025 22:22] Enriching papers with extra data.
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 0. A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.  					AI-generated summary 				 Despite advances in scientific AI, a coherent framework fo...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 1. Proposed Egocentric2Embodiment pipeline translates human egocentric videos into structured training data for robots, enhancing their egocentric understanding and task performance.  					AI-generated summary 				 Robotic generalization relies on physical intelligence: the ability to reason about stat...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 2. A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.  					AI-generated summary 				 Despite the superior performan...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 3. Seed-Prover 1.5, a formal theorem-proving model using large-scale agentic reinforcement learning and an efficient test-time scaling workflow, demonstrates superior performance in solving mathematical problems across various levels with reduced computational resources.  					AI-generated summary 				...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 4. A systematic framework adapts understanding-oriented encoder features for generative tasks by introducing a semantic-pixel reconstruction objective, enabling state-of-the-art image reconstruction and generation.  					AI-generated summary 				 Modern Latent Diffusion Models (LDMs) typically operate ...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 5. 4D-RGPT, a specialized multimodal LLM, enhances 4D perception in video inputs through Perceptual 4D Distillation and is evaluated on R4D-Bench, a new benchmark for depth-aware dynamic scenes.  					AI-generated summary 				 Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 6. Sage is a human-free evaluation suite for LLM-as-a-Judge, using rational choice theory to assess local and global consistency, revealing significant reliability issues with current LLM judges.  					AI-generated summary 				 LLM-as-a-Judge has been widely adopted as an evaluation method and served a...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 7. RadarGen uses diffusion models to synthesize realistic radar point clouds from multi-view camera images, incorporating depth, semantic, and motion cues from pretrained models to enhance physical plausibility.  					AI-generated summary 				 We present RadarGen, a diffusion model for synthesizing rea...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 8. GroundingME benchmark evaluates multimodal large language models' visual grounding capabilities across complexity dimensions, revealing significant gaps and proposing strategies for improvement.  					AI-generated summary 				 Visual grounding, localizing objects from natural language descriptions, ...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 9. This survey provides a structured overview of Vision-Language-Action models, detailing key challenges and opportunities in areas such as representation, execution, generalization, safety, and datasets in the field of robotics.  					AI-generated summary 				 Vision-Language-Action (VLA) models are d...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 10. Canon layers, lightweight architectural components, enhance reasoning depth and breadth in language models, improving performance in synthetic and real-world pretraining tasks.  					AI-generated summary 				 Understanding architectural differences in language models is challenging, especially at ac...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 11. HERBench is a VideoQA benchmark that tests models' ability to integrate multiple, temporally separated visual cues, revealing significant challenges in current Video-LLMs.  					AI-generated summary 				 Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answe...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 12. AniX synthesizes temporally coherent videos by extending controllable-entity models to support diverse, user-defined character interactions in static 3D environments using conditional autoregressive video generation.  					AI-generated summary 				 Recent advances in world models have greatly enhanc...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 13. Turn-PPO, a variant of PPO tailored for multi-turn RL tasks, improves advantage estimation and performance over GRPO in environments requiring long-horizon reasoning.  					AI-generated summary 				 Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents ...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 14. SWE-Bench++ is an automated framework generating repository-level coding tasks from live GitHub pull requests, offering a scalable, multilingual benchmark for evaluating and improving code generation models.  					AI-generated summary 				 Benchmarks like SWE-bench have standardized the evaluation o...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 15. LaMer, a Meta-RL framework, enhances LLM agents' exploration and adaptation capabilities in RL tasks, leading to improved performance and generalization.  					AI-generated summary 				 Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the env...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 16. StageVAR accelerates visual autoregressive models by selectively pruning or approximating less critical later stages, achieving significant speedup with minimal quality loss.  					AI-generated summary 				 Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of tradi...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 17. Bolmo, a family of competitive byte-level language models, is trained by converting existing subword-level models, overcoming character understanding and efficiency limitations while achieving performance comparable to subword models.  					AI-generated summary 				 We introduce Bolmo, the first fam...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 18. A novel framework, Robust-R1, enhances multimodal large language models' robustness to visual degradations through explicit modeling, supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, achieving state-of-the-art performance on real-world degradation benchmarks.  		...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 19. 3D-RE-GEN reconstructs single images into modifiable 3D textured mesh scenes with comprehensive backgrounds, achieving top performance through compositional generation and scene optimization.  					AI-generated summary 				 Recent advances in 3D scene generation produce visually appealing output, bu...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 20. LongShOTBench is a diagnostic benchmark for long-form multimodal video understanding with open-ended questions, dialogues, and multimodal reasoning tasks, highlighting the challenges for state-of-the-art models.  					AI-generated summary 				 Long-form multimodal video understanding requires integr...
[22.12.2025 22:22] ********************************************************************************
[22.12.2025 22:22] Abstract 21. MineTheGap is a method using a genetic algorithm and a bias score to identify prompts that cause Text-to-Image models to generate biased outputs, aiming to reduce redundancy and improve diversity.  					AI-generated summary 				 Text-to-Image (TTI) models generate images based on text prompts, which...
[22.12.2025 22:22] Read previous papers.
[22.12.2025 22:22] Generating reviews via LLM API.
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#agi", "#science", "#dataset", "#reasoning", "#rag", "#benchmark", "#optimization", "#rl", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–û—Ç –æ–±—â–µ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –∫ –Ω–∞—É—á–Ω–æ–º—É –æ—Ç–∫—Ä—ã—Ç–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –ò–ò —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞—Ç—å –Ω–∞—É–∫—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–∞–º–∫–∞ –¥–ª—è –æ
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#video", "#dataset", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –≤–∑–≥–ª—è–¥–∞ —á–µ–ª–æ–≤–µ–∫–∞ –∫ –¥–µ–π—Å—Ç–≤–∏—è–º —Ä–æ–±–æ—Ç–∞: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ –≤ —Ä–æ–±–æ—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω pipeline Egocentric2Embodiment, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤–∏–¥–µ–æ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#benchmark", "#training", "#math"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ó–∞–∫–æ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ compute-–∑–∞–∫–æ–Ω—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ framework Laws of Reasoning (LoRe) –¥–ª—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∂–µ–ª–∞–µ–º–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#plp", "#agents", "#benchmark", "#math", "#optimization", "#rl"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–ø—ã—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π", "desc": "Seed-Prover 1.5 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#diffusion", "#architecture", "#training"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è 
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#video", "#open_source", "#training", "#dataset", "#transfer_learning", "#reasoning", "#3d", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–í–æ—Å–ø—Ä–∏—è—Ç–∏–µ 4D –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å 4D-RGPT, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#dataset", "#rlhf", "#benchmark"], "emoji": "‚öñÔ∏è", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ LLM-—Å—É–¥–µ–π –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —á–µ—Ä–µ–∑ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Sage ‚Äî –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ —Å—É–¥–µ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —É—á–∞—Å—Ç–∏—è 
[22.12.2025 22:22] Using data from previous issue: {"categories": [], "emoji": "üì°", "ru": {"title": "–û—Ç –∫–∞–º–µ—Ä –∫ —Ä–∞–¥–∞—Ä—É: –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫", "desc": "RadarGen ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫ —Ä–∞–¥–∞—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ–∫–∞–º–µ—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–∞–¥–∞—Ä–Ω—ã–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#dataset", "#hallucinations", "#multimodal", "#interpretability"], "emoji": "üéØ", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–æ–≤ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GroundingME –¥–ª—è –æ
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#cv", "#robotics", "#benchmark", "#dataset", "#survey", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–†–æ–±–æ—Ç—ã –ø–æ–Ω–∏–º–∞—é—Ç —è–∑—ã–∫: –ø—É—Ç—å –∫ –≤–æ–ø–ª–æ—â—ë–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –º–æ–¥–µ–ª—è–º Vision-Language-Action (VLA), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –∑—Ä–µ–Ω–∏–µ, —è–∑—ã–∫ –∏ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#reasoning", "#synthetic", "#architecture", "#benchmark", "#training"], "emoji": "üéº", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏—á–Ω—ã–π –ø–æ—Ç–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: Canon layers –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã Canon layers ‚Äî –ª—ë–≥–∫–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#survey", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞–∑–Ω–µ—Å—ë–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ ‚Äî –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "HERBench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ-–≤–æ–ø—Ä–æ—Å–Ω–æ
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#video", "#games", "#3d", "#agents", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏ –≤ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö 3D —Å—Ü–µ–Ω–∞—Ö —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "AniX ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä—ë—Ö–º–µ—Ä
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#agents", "#dataset", "#benchmark", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "Turn-PPO: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Turn-PPO, –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Proximal Policy Optimization –¥
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#plp", "#synthetic", "#benchmark", "#optimization", "#multilingual"], "emoji": "üîß", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è", "desc": "SWE-Bench++ ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π 
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#training", "#rl", "#agents"], "emoji": "üß†", "ru": {"title": "–ú–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è LLM –∞–≥–µ–Ω—Ç–æ–≤", "desc": "LaMer ‚Äî —ç—Ç–æ Meta-RL —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –∞–≥–µ–Ω—Ç–∞–º –∞–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ 
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#cv", "#architecture", "#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç—Ç–∞–ø–æ–≤", "desc": "StageVAR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—É—Ç—ë–º –ø—Ä–µ
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#training", "#optimization", "#transfer_learning", "#open_source", "#architecture", "#small_models"], "emoji": "üîÑ", "ru": {"title": "–û—Ç —Å–ª–æ–≤ –∫ –±–∞–π—Ç–∞–º: –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Bolmo ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã—Ö –±–∞–π—Ç-—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#training", "#security", "#dataset", "#alignment", "#reasoning", "#rlhf", "#benchmark", "#multimodal"], "emoji": "üõ°Ô∏è", "ru": {"title": "–Ø–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–∞–¥—ë–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM", "desc": "Robust-R1 ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#optimization", "#3d", "#multimodal", "#games"], "emoji": "üé¨", "ru": {"title": "–û—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ–π 3D —Å—Ü–µ–Ω–µ —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", "desc": "3D-RE-GEN ‚Äî —ç—Ç–æ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D —Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç —Ç–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
[22.12.2025 22:22] Using data from previous issue: {"categories": ["#open_source", "#agents", "#benchmark", "#video", "#long_context", "#reasoning", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LongShOTBench ‚Äî –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å
[22.12.2025 22:22] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–µ–π –≤ –º–æ–¥–µ–ª—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "MineTheGap ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—è–≤–ª—è—é—Ç –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ –º–æ–¥–µ–ª—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥–µ–Ω–µ—Ç–∏—á–µ—Å
[22.12.2025 22:22] Renaming data file.
[22.12.2025 22:22] Renaming previous data. hf_papers.json to ./d/2025-12-22.json
[22.12.2025 22:22] Saving new data file.
[22.12.2025 22:22] Generating page.
[22.12.2025 22:22] Renaming previous page.
[22.12.2025 22:22] Renaming previous data. index.html to ./d/2025-12-22.html
[22.12.2025 22:22] Writing result.
[22.12.2025 22:22] Renaming log file.
[22.12.2025 22:22] Renaming previous data. log.txt to ./logs/2025-12-22_last_log.txt
