[21.07.2025 22:12] Read previous papers.
[21.07.2025 22:12] Generating top page (month).
[21.07.2025 22:12] Writing top page (month).
[21.07.2025 23:12] Read previous papers.
[21.07.2025 23:12] Get feed.
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13563
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11097
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14137
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12566
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13984
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13158
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10605
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.12455
[21.07.2025 23:12] Extract page data from URL. URL: https://huggingface.co/papers/2507.14129
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13302
[21.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13391
[21.07.2025 23:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.07.2025 23:12] No deleted papers detected.
[21.07.2025 23:12] Downloading and parsing papers (pdf, html). Total: 11.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.13563.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.13563.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.13563.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.11097.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.11097.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.11097.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.14137.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.14137.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.14137.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.12566.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.12566.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.12566.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.13984.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.13984.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.13984.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.13158.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.13158.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.13158.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.10605.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.10605.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.10605.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.12455.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.12455.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.12455.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.14129.
[21.07.2025 23:12] Downloading paper 2507.14129 from http://arxiv.org/pdf/2507.14129v1...
[21.07.2025 23:12] Extracting affiliations from text.
[21.07.2025 23:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"OpenBEATs: Fully Open-Source General-Purpose Audio Encoder Shikhar Bharadwaj1, Samuele Cornell1, Kwanghee Choi1, Satoru Fukayama2, Hye-jin Shim1, Soham Deshmukh1, Shinji Watanabe1 1Carnegie Mellon University, USA 2National Institute of Advanced Industrial Science and Technology (AIST), Japan sbharad2@andrew.cmu.edu 5 2 0 2 8 ] . [ 1 9 2 1 4 1 . 7 0 5 2 : r AbstractMasked token prediction has emerged as powerful pretraining objective across language, vision, and speech, offering the potential to unify these diverse modalities through single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multidomain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs 1. 1. INTRODUCTION Self-supervised learning (SSL) has shown significant promise across wide range of audio processing tasks. It allows models to learn generalpurpose representations that transfer effectively to various downstream applications. Notable e"
[21.07.2025 23:12] Response: ```python
["Carnegie Mellon University, USA", "National Institute of Advanced Industrial Science and Technology (AIST), Japan"]
```
[21.07.2025 23:12] Deleting PDF ./assets/pdf/2507.14129.pdf.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.13302.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.13302.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.13302.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.13391.
[21.07.2025 23:12] Extra JSON file exists (./assets/json/2507.13391.json), skip PDF parsing.
[21.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.13391.json), skip HTML parsing.
[21.07.2025 23:12] Success.
[21.07.2025 23:12] Enriching papers with extra data.
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 0. Balalaika, a large Russian speech dataset with detailed annotations, improves performance in speech synthesis and enhancement tasks.  					AI-generated summary 				 Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, hom...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 1. DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerg...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 2. Franca, an open-source vision foundation model, achieves high performance using a transparent training pipeline and novel clustering and disentanglement techniques.  					AI-generated summary 				 We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vis...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 3. Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic ...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 4. CSD-VAR, a Visual Autoregressive Modeling approach, enhances content-style decomposition by introducing scale-aware optimization, SVD-based rectification, and augmented K-V memory, outperforming diffusion models in content preservation and stylization.  					AI-generated summary 				 Disentangling c...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 5. A review of advancements in aligning large language models using inverse reinforcement learning, emphasizing challenges and opportunities in neural reward modeling and sparse-reward reinforcement learning.  					AI-generated summary 				 In the era of Large Language Models (LLMs), alignment has emer...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 6. RedOne, a domain-specific LLM, enhances performance across multiple SNS tasks through a three-stage training strategy, improving generalization and reducing harmful content exposure.  					AI-generated summary 				 As a primary medium for modern information dissemination, social networking services ...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 7. SENTINEL reduces hallucinations in multimodal large language models by iteratively generating and validating sentence-level outputs using in-domain preference learning and context-aware preference loss.  					AI-generated summary 				 Multimodal large language models (MLLMs) have revolutionized cros...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 8. Masked token prediction has emerged as a powerful pre-training objective across language, vision, and speech, offering the potential to unify these diverse modalities through a single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being ...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 9. GEA, a public arena that includes energy consumption data, shows that users often prefer smaller, more energy-efficient language models over larger, more complex ones.  					AI-generated summary 				 The evaluation of large language models is a complex task, in which several approaches have been pro...
[21.07.2025 23:12] ********************************************************************************
[21.07.2025 23:12] Abstract 10. This research presents a framework for quantitative risk management in volatile markets, specifically focusing on expectile-based methodologies applied to the FTSE 100 index. Traditional risk measures such as Value-at-Risk (VaR) have demonstrated significant limitations during periods of market stre...
[21.07.2025 23:12] Read previous papers.
[21.07.2025 23:12] Generating reviews via LLM API.
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#data", "#low_resource", "#dataset", "#audio", "#synthetic"], "emoji": "üéª", "ru": {"title": "Balalaika: –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ä–µ—á–µ–≤–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Balalaika. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 2000 —á
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#training", "#architecture", "#diffusion", "#alignment", "#security"], "emoji": "üïµÔ∏è", "ru": {"title": "–£—è–∑–≤–∏–º–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –Ω–æ–≤—ã–π —Ñ—Ä–æ–Ω—Ç –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò", "desc": "DIJA - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –º–µ—Ö–∞–Ω–∏
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#training", "#cv", "#optimization", "#dataset", "#architecture", "#open_source"], "emoji": "üëÅÔ∏è", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "Franca - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#multimodal", "#training", "#architecture", "#agi", "#optimization", "#benchmark"], "emoji": "üß†", "ru": {"title": "Mono-InternVL: –ú–æ–Ω–æ–ª–∏—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º", "desc": "Mono-InternVL - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –º–æ–Ω–æ–ª–∏—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#benchmark", "#synthetic", "#cv"], "emoji": "üé®", "ru": {"title": "CSD-VAR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Å—Ç–∏–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "CSD-VAR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Å—Ç–∏–ª—è –∏–∑–æ–±—Ä–∞–∂–µ
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#rlhf", "#training", "#rl", "#survey", "#benchmark", "#alignment"], "emoji": "üß†", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π 
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#training", "#alignment", "#optimization", "#dataset", "#multilingual", "#science"], "emoji": "üöÄ", "ru": {"title": "RedOne: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö", "desc": "RedOne - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#hallucinations", "#multimodal", "#open_source", "#benchmark", "#data", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "SENTINEL - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –∏—Å–ø–æ
[21.07.2025 23:12] Querying the API.
[21.07.2025 23:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Masked token prediction has emerged as a powerful pre-training objective across language, vision, and speech, offering the potential to unify these diverse modalities through a single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multi-domain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding a billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs at https://shikhar-s.github.io/OpenBEATs
[21.07.2025 23:12] Response: {
  "desc": "OpenBEATs - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –û–Ω–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ BEATs –∑–∞ —Å—á–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–¥–æ–º–µ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö. OpenBEATs –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –±–∏–æ–∞–∫—É—Å—Ç–∏–∫—É, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–∏—Ö –∑–≤—É–∫–æ–≤ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ–± –∞—É–¥–∏–æ. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏.",
  "emoji": "üéß",
  "title": "OpenBEATs: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–¥–æ–º–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ"
}
[21.07.2025 23:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Masked token prediction has emerged as a powerful pre-training objective across language, vision, and speech, offering the potential to unify these diverse modalities through a single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multi-domain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding a billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs at https://shikhar-s.github.io/OpenBEATs"

[21.07.2025 23:12] Response: ```python
['DATASET', 'AUDIO', 'MULTIMODAL', 'TRAINING']
```
[21.07.2025 23:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Masked token prediction has emerged as a powerful pre-training objective across language, vision, and speech, offering the potential to unify these diverse modalities through a single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multi-domain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding a billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs at https://shikhar-s.github.io/OpenBEATs"

[21.07.2025 23:12] Response: ```python
['OPEN_SOURCE', 'REASONING']
```
[21.07.2025 23:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces OpenBEATs, an open-source framework that enhances the BEATs model for audio understanding by utilizing multi-domain audio pre-training. It addresses the limitations of BEATs, which was primarily trained on AudioSet and lacked open-source resources. OpenBEATs is evaluated across various tasks and datasets, demonstrating superior performance in audio reasoning and bioacoustics compared to larger models. The findings highlight the effectiveness of masked token prediction in learning versatile audio representations, promoting further research in the field.","title":"Unifying Audio Understanding with OpenBEATs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces OpenBEATs, an open-source framework that enhances the BEATs model for audio understanding by utilizing multi-domain audio pre-training. It addresses the limitations of BEATs, which was primarily trained on AudioSet and lacked open-source resources. OpenBEATs is evaluated across various tasks and datasets, demonstrating superior performance in audio reasoning and bioacoustics compared to larger models. The findings highlight the effectiveness of masked token prediction in learning versatile audio representations, promoting further research in the field.', title='Unifying Audio Understanding with OpenBEATs'))
[21.07.2025 23:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜOpenBEATsÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂ§öÈ¢ÜÂüüÈü≥È¢ëÈ¢ÑËÆ≠ÁªÉÊâ©Â±ïBEATsÊ®°Âûã„ÄÇÂ∞ΩÁÆ°BEATsÂú®Èü≥È¢ëÁêÜËß£ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂Â∫îÁî®‰ªçÁÑ∂ÊúâÈôêÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫Áº∫‰πèÂºÄÊ∫ê‰ª£Á†ÅÂíåËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄß„ÄÇOpenBEATsÂú®ÂÖ≠Áßç‰ªªÂä°„ÄÅ‰∫åÂçÅ‰∫î‰∏™Êï∞ÊçÆÈõÜÂíå‰∏â‰∏™Èü≥È¢ëÈ¢ÜÂüü‰∏≠ËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÁîüÁâ©Â£∞Â≠¶ÂíåÁéØÂ¢ÉÂ£∞Èü≥Êï∞ÊçÆÈõÜ‰∏äÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ§öÈ¢ÜÂüüÊï∞ÊçÆÈõÜÂíåÊé©Á†Å‰ª§ÁâåÈ¢ÑÊµã‰ªªÂä°Âú®Â≠¶‰π†ÈÄöÁî®Èü≥È¢ëË°®Á§∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ","title":"OpenBEATsÔºöÂ§öÈ¢ÜÂüüÈü≥È¢ëÈ¢ÑËÆ≠ÁªÉÁöÑÁ™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜOpenBEATsÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂ§öÈ¢ÜÂüüÈü≥È¢ëÈ¢ÑËÆ≠ÁªÉÊâ©Â±ïBEATsÊ®°Âûã„ÄÇÂ∞ΩÁÆ°BEATsÂú®Èü≥È¢ëÁêÜËß£ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÖ∂Â∫îÁî®‰ªçÁÑ∂ÊúâÈôêÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫Áº∫‰πèÂºÄÊ∫ê‰ª£Á†ÅÂíåËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄß„ÄÇOpenBEATsÂú®ÂÖ≠Áßç‰ªªÂä°„ÄÅ‰∫åÂçÅ‰∫î‰∏™Êï∞ÊçÆÈõÜÂíå‰∏â‰∏™Èü≥È¢ëÈ¢ÜÂüü‰∏≠ËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÁîüÁâ©Â£∞Â≠¶ÂíåÁéØÂ¢ÉÂ£∞Èü≥Êï∞ÊçÆÈõÜ‰∏äÁöÑÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ§öÈ¢ÜÂüüÊï∞ÊçÆÈõÜÂíåÊé©Á†Å‰ª§ÁâåÈ¢ÑÊµã‰ªªÂä°Âú®Â≠¶‰π†ÈÄöÁî®Èü≥È¢ëË°®Á§∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ', title='OpenBEATsÔºöÂ§öÈ¢ÜÂüüÈü≥È¢ëÈ¢ÑËÆ≠ÁªÉÁöÑÁ™ÅÁ†¥'))
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#benchmark", "#small_models", "#optimization"], "emoji": "üåø", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Ä–∞–∑–º–µ—Ä–∞: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤—ã–±–∏—Ä–∞—é—Ç —ç–Ω–µ—Ä–≥–æ—ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ GEA (Generative Energy Arena) - –ø—É–±–ª–∏—á–Ω–∞—è –∞—Ä–µ–Ω–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã
[21.07.2025 23:12] Using data from previous issue: {"categories": ["#dataset", "#math"], "emoji": "üìä", "ru": {"title": "–≠–∫—Å–ø–µ–∫—Ç–∏–ª—å-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö —Ä—ã–Ω–∫–æ–≤", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é —Ä–∏—Å–∫–∞–º–∏ –Ω–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö —Ä—ã–Ω–∫–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ —ç–∫—Å–ø–µ–∫—Ç–∏–ª–µ–π, –ø—Ä–∏–º–µ–Ω—è–µ–º–æ–π 
[21.07.2025 23:12] Renaming data file.
[21.07.2025 23:12] Renaming previous data. hf_papers.json to ./d/2025-07-21.json
[21.07.2025 23:12] Saving new data file.
[21.07.2025 23:12] Generating page.
[21.07.2025 23:12] Renaming previous page.
[21.07.2025 23:12] Renaming previous data. index.html to ./d/2025-07-21.html
[21.07.2025 23:12] Writing result.
[21.07.2025 23:12] Renaming log file.
[21.07.2025 23:12] Renaming previous data. log.txt to ./logs/2025-07-21_last_log.txt
