[21.07.2025 05:21] Read previous papers.
[21.07.2025 05:21] Generating top page (month).
[21.07.2025 05:21] Writing top page (month).
[21.07.2025 06:19] Read previous papers.
[21.07.2025 06:19] Get feed.
[21.07.2025 06:19] Extract page data from URL. URL: https://huggingface.co/papers/2507.12566
[21.07.2025 06:19] Extract page data from URL. URL: https://huggingface.co/papers/2507.11097
[21.07.2025 06:19] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10605
[21.07.2025 06:19] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.07.2025 06:19] No deleted papers detected.
[21.07.2025 06:19] Downloading and parsing papers (pdf, html). Total: 3.
[21.07.2025 06:19] Downloading and parsing paper https://huggingface.co/papers/2507.12566.
[21.07.2025 06:19] Downloading paper 2507.12566 from http://arxiv.org/pdf/2507.12566v1...
[21.07.2025 06:19] Extracting affiliations from text.
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MONO-INTERNVL-1.5 1 Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai 5 2 0 2 J 6 1 ] . [ 1 6 6 5 2 1 . 7 0 5 2 : r AbstractThis paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed new visual parameter space into pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates set of visual experts through multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pretraining (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and reorganizes the pre-training process in an efficient manner. During inference, Mono-InternVL-1.5 includes fused CUDA kernel to speed up its MoE operations. With these designs, MonoInternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with MonoInternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modula"
[21.07.2025 06:19] Response: ```python
[]
```
[21.07.2025 06:19] Extracting affiliations from text.
[21.07.2025 06:19] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MONO-INTERNVL-1.5 1 Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai 5 2 0 2 J 6 1 ] . [ 1 6 6 5 2 1 . 7 0 5 2 : r AbstractThis paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed new visual parameter space into pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates set of visual experts through multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pretraining (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and reorganizes the pre-training process in an efficient manner. During inference, Mono-InternVL-1.5 includes fused CUDA kernel to speed up its MoE operations. With these designs, MonoInternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with MonoInternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL. Index TermsMultimodal Large Language Model, Visual Pretraining, Monolithic Model I. INTRODUCTION Recent years have witnessed the significant achievement of Multimodal Large Language Models (MLLMs) [1][3] in various vision-language tasks. As illustrated in Fig. 1(a), most existing Multimodal Large Language Models (MLLMs) adopt modular architecture, where visual encoding and language decoding are handled separately. This approach is typically realized by combining pre-trained visual encoder [4] with an LLM [5][7]. In contrast, monolithic MLLMs [8][10] have become another popular research trend in the community, as shown in Fig. 1(b), which integrate visual perception and Gen Luo, Wenhao Li, Weiyun Wang and Yu Qiao are with Shanghai Artificial Intelligence Laboratory. Wenhan Dou, Xizhou Zhu and Jifeng Dai are with Tsinghua University. Changyao Tian, Hao Li and Wenhai Wang are with The Chinese University of Hong Kong. Zhaokai Wang and Xue Yang are with Shanghai Jiao Tong University. Corresponding author: Jifeng Dai (daijifeng@tsinghua.edu.cn). TABLE I: Overall comparison of Mono-InternVL and Mono-InternVL-1.5. Mono-InternVL-1.5 greatly improves the training and inference efficiency while maintaining competitive downstream performance. Method Training Data Inference Speed VQA Bench MLLM Bench Mono-InternVL [11] Mono-InternVL-1.5 1.1B 0.5B -58% 61 tokens/s 77 tokens/s +26% 70.1 70.4 53.7 55.6 +0.3% +1.9% multimodal understanding within unified LLM framework. Compared to modular MLLMs, monolithic MLLMs often exhibit better potential in terms of design simplicity and deployment efficiency [9], [10]. Despite these advancements, training monolithic MLLM that achieves competitive performance still remains significant challenge. Among them, native pre-training [12] pre-trains monolithic MLLM from scratch using combination of textonly and multimodal data. However, this method demands extremely high computational resources and is prone to optimization instability [12]. Another promising solution is to extend the pre-trained LLM to multimodality via additional visual pre-training, namely continuous pre-training [9]. Such approaches typically require much cheaper training costs but easily incurs the catastrophic forgetting issue [13], thereby undermining the pre-trained language knowledge. In this paper, we aim to address the forgetting issue of continuous pre-training from the perspective of delta tuning [14]. Specifically, delta tuning fine-tunes set of newly added parameters in the model while keeping the rest frozen, thereby preserving the original knowledge. However, existing methods adopt shared architecture for joint vision and language modeling, where optimizations for vision can negatively impact language capabilities. Therefore, it is natural thought to introduce an independent visual parameter set into the pre-trained LLM, thus retaining the language knowledge by freezing the entire LLM while facilitating visual learning. This principle is also aligned with previous endeavors in modular MLLMs, e.g., QwenVL [15] and InternVL [6], where the visual parameters are placed outside the LLM. Based on the above principle, we propose novel monolithic MLLM, namely Mono-InternVL. As shown in Fig. 2, the visual parameters in Mono-InternVL are instantiated as set of expert networks via the mixture-of-experts (MoEs) mechanism. Based on this architecture, we present an innovative Endogenous Visual Pre-training (EViP) method to optimize the visual MONO-INTERNVL-1.5 2 Fig. 1: Comparison of Mono-InternVL, Mono-InternVL-1.5 and existing MLLMs. Compared with modular MLLMs, Mono-InternVL and Mono-InternVL-1.5 embed visual experts into the pre-trained LLM and integrates visual encoding and language decoding into single LLM. Through endogenous visual pre-training (EViP), Mono-InternVL significantly pushes the performance boundaries of monolithic MLLMs. With EViP++, Mono-InternVL-1.5 not only significantly reduces data costs, but also maintains the competitive performance of downstream tasks. parameters. Specifically, EViP is formulated as progressive learning process of three stages: 1) concept learning to grasp basic visual concepts, 2) semantic learning to capture high-level semantics, e.g., world knowledge, and 3) alignment learn"
[21.07.2025 06:19] Mistral response. {"id": "5f3720f1f35e4c789e7c929a95964b98", "object": "chat.completion", "created": 1753078782, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Shanghai Artificial Intelligence Laboratory\",\n    \"Tsinghua University\",\n    \"The Chinese University of Hong Kong\",\n    \"Shanghai Jiao Tong University\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1752, "total_tokens": 1805, "completion_tokens": 53}}
[21.07.2025 06:19] Response: ```python
[
    "Shanghai Artificial Intelligence Laboratory",
    "Tsinghua University",
    "The Chinese University of Hong Kong",
    "Shanghai Jiao Tong University"
]
```
[21.07.2025 06:19] Deleting PDF ./assets/pdf/2507.12566.pdf.
[21.07.2025 06:19] Success.
[21.07.2025 06:19] Downloading and parsing paper https://huggingface.co/papers/2507.11097.
[21.07.2025 06:19] Downloading paper 2507.11097 from http://arxiv.org/pdf/2507.11097v1...
[21.07.2025 06:19] Extracting affiliations from text.
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. THE DEVIL BEHIND THE MASK: AN EMERGENT SAFETY VULNERABILITY OF DIFFUSION LLMS Jiashu Qu2 Dongrui Liu2 Zhiyuan Liu1,2 Ruixi Wu1,2 Zichen Wen1,2 Yicun Yang1 Xiangqi Jin1 Haoyun Xu1 Xuyang Liu1 Weijia Li3,2 Chaochao Lu2 1EPIC Lab, Shanghai Jiao Tong University zichen.wen@outlook.com, heconghui@pjlab.org.cn, zhanglinfeng@sjtu.edu.cn Jing Shao2 Conghui He2 Linfeng Zhang1 2Shanghai AI Laboratory 3Sun Yat-sen University 5 2 0 2 5 1 ] . [ 1 7 9 0 1 1 . 7 0 5 2 : r WARNING: The paper contains content that may be offensive and disturbing in nature. "
[21.07.2025 06:19] Response: ```python
[
    "EPIC Lab, Shanghai Jiao Tong University",
    "Shanghai AI Laboratory",
    "Sun Yat-sen University"
]
```
[21.07.2025 06:19] Deleting PDF ./assets/pdf/2507.11097.pdf.
[21.07.2025 06:19] Success.
[21.07.2025 06:19] Downloading and parsing paper https://huggingface.co/papers/2507.10605.
[21.07.2025 06:19] Extra JSON file exists (./assets/json/2507.10605.json), skip PDF parsing.
[21.07.2025 06:19] Paper image links file exists (./assets/img_data/2507.10605.json), skip HTML parsing.
[21.07.2025 06:19] Success.
[21.07.2025 06:19] Enriching papers with extra data.
[21.07.2025 06:19] ********************************************************************************
[21.07.2025 06:19] Abstract 0. Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic ...
[21.07.2025 06:19] ********************************************************************************
[21.07.2025 06:19] Abstract 1. DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerg...
[21.07.2025 06:19] ********************************************************************************
[21.07.2025 06:19] Abstract 2. RedOne, a domain-specific LLM, enhances performance across multiple SNS tasks through a three-stage training strategy, improving generalization and reducing harmful content exposure.  					AI-generated summary 				 As a primary medium for modern information dissemination, social networking services ...
[21.07.2025 06:19] Read previous papers.
[21.07.2025 06:19] Generating reviews via LLM API.
[21.07.2025 06:19] Querying the API.
[21.07.2025 06:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.
[21.07.2025 06:19] Response: {
  "desc": "Mono-InternVL - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Mono-InternVL Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ½Ğ´Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (EViP) Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ MLLM, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….",
  "emoji": "ğŸ§ ",
  "title": "Mono-InternVL: ĞœĞ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼"
}
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL."

[21.07.2025 06:19] Response: ```python
['MULTIMODAL', 'ARCHITECTURE', 'TRAINING', 'BENCHMARK']
```
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL."

[21.07.2025 06:19] Response: ```python
["OPTIMIZATION", "AGI"]
```
[21.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Mono-InternVL, a monolithic Multimodal Large Language Model (MLLM) that combines visual and language processing into one model. It addresses issues like unstable optimization and catastrophic forgetting by embedding a new visual parameter space into a pre-trained language model, allowing for stable learning from noisy data. The model incorporates a multimodal mixture-of-experts architecture and an innovative Endogenous Visual Pre-training (EViP) strategy to enhance visual capabilities. Additionally, Mono-InternVL-1.5 is presented as a more efficient version that reduces training costs while maintaining competitive performance across multiple benchmarks.","title":"Revolutionizing Visual Learning with Mono-InternVL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Mono-InternVL, a monolithic Multimodal Large Language Model (MLLM) that combines visual and language processing into one model. It addresses issues like unstable optimization and catastrophic forgetting by embedding a new visual parameter space into a pre-trained language model, allowing for stable learning from noisy data. The model incorporates a multimodal mixture-of-experts architecture and an innovative Endogenous Visual Pre-training (EViP) strategy to enhance visual capabilities. Additionally, Mono-InternVL-1.5 is presented as a more efficient version that reduces training costs while maintaining competitive performance across multiple benchmarks.', title='Revolutionizing Visual Learning with Mono-InternVL'))
[21.07.2025 06:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Mono-InternVLæ˜¯ä¸€ç§å…ˆè¿›çš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰ä¸“å®¶å’Œæ”¹è¿›çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºè§†è§‰å­¦ä¹ å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†æ–°çš„è§†è§‰å‚æ•°ç©ºé—´åµŒå…¥åˆ°é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œè§£å†³äº†ä¸ç¨³å®šä¼˜åŒ–å’Œç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚Mono-InternVL-1.5åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†æ”¹è¿›çš„å†…ç”Ÿè§†è§‰é¢„è®­ç»ƒï¼ˆEViP++ï¼‰ï¼Œå¼•å…¥äº†é¢å¤–çš„è§†è§‰æ³¨æ„åŠ›ä¸“å®¶ï¼Œå¹¶é«˜æ•ˆåœ°é‡æ–°ç»„ç»‡äº†é¢„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMono-InternVLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸”åœ¨æ¨ç†æ—¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚","title":"Mono-InternVLï¼šé«˜æ•ˆçš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Mono-InternVLæ˜¯ä¸€ç§å…ˆè¿›çš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰ä¸“å®¶å’Œæ”¹è¿›çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºè§†è§‰å­¦ä¹ å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†æ–°çš„è§†è§‰å‚æ•°ç©ºé—´åµŒå…¥åˆ°é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œè§£å†³äº†ä¸ç¨³å®šä¼˜åŒ–å’Œç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚Mono-InternVL-1.5åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œé‡‡ç”¨äº†æ”¹è¿›çš„å†…ç”Ÿè§†è§‰é¢„è®­ç»ƒï¼ˆEViP++ï¼‰ï¼Œå¼•å…¥äº†é¢å¤–çš„è§†è§‰æ³¨æ„åŠ›ä¸“å®¶ï¼Œå¹¶é«˜æ•ˆåœ°é‡æ–°ç»„ç»‡äº†é¢„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMono-InternVLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸”åœ¨æ¨ç†æ—¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚', title='Mono-InternVLï¼šé«˜æ•ˆçš„å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹'))
[21.07.2025 06:20] Querying the API.
[21.07.2025 06:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.
[21.07.2025 06:20] Response: {
  "desc": "DIJA - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ÑÑ‚ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ²Ğ¾Ğ´Ğ¾Ğ¼. DIJA ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DIJA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ³Ñ€Ğ¾Ğ·.",
  "emoji": "ğŸ•µï¸",
  "title": "Ğ£ÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€Ğ¾Ğ½Ñ‚ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜"
}
[21.07.2025 06:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA."

[21.07.2025 06:20] Response: ```python
["ARCHITECTURE", "TRAINING"]
```
[21.07.2025 06:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA."

[21.07.2025 06:20] Response: ```python
["SECURITY", "ALIGNMENT", "DIFFUSION"]
```
[21.07.2025 06:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DIJA is a novel framework that identifies and exploits safety vulnerabilities in diffusion-based large language models (dLLMs) through the use of adversarial prompts. It reveals that existing alignment mechanisms are inadequate in preventing harmful outputs when faced with context-aware, masked-input prompts. By leveraging the unique features of dLLMs, such as bidirectional modeling and parallel decoding, DIJA demonstrates how these models can produce unsafe content despite alignment efforts. The framework significantly outperforms previous methods, highlighting the critical need for improved safety measures in the design of dLLMs.","title":"Unmasking Vulnerabilities in Diffusion-Based Language Models with DIJA"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DIJA is a novel framework that identifies and exploits safety vulnerabilities in diffusion-based large language models (dLLMs) through the use of adversarial prompts. It reveals that existing alignment mechanisms are inadequate in preventing harmful outputs when faced with context-aware, masked-input prompts. By leveraging the unique features of dLLMs, such as bidirectional modeling and parallel decoding, DIJA demonstrates how these models can produce unsafe content despite alignment efforts. The framework significantly outperforms previous methods, highlighting the critical need for improved safety measures in the design of dLLMs.', title='Unmasking Vulnerabilities in Diffusion-Based Language Models with DIJA'))
[21.07.2025 06:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DIJAæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å®‰å…¨å¼±ç‚¹ï¼Œé€šè¿‡æ„é€ å¯¹æŠ—æ€§æç¤ºï¼Œå±•ç¤ºäº†å…¶å¯¹é½æœºåˆ¶çš„æ˜¾è‘—è„†å¼±æ€§ã€‚å°½ç®¡æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œæ–‡æœ¬å¡«å……æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¯¹é½æœºåˆ¶æ— æ³•æœ‰æ•ˆé˜²æŠ¤é’ˆå¯¹ä¸Šä¸‹æ–‡çš„å¯¹æŠ—æ€§æç¤ºã€‚DIJAé€šè¿‡æ„å»ºå¯¹æŠ—æ€§äº¤é”™æ©ç æ–‡æœ¬æç¤ºï¼Œåˆ©ç”¨äº†æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæœºåˆ¶ï¼Œå¯¼è‡´æ ‡å‡†å¯¹é½æœºåˆ¶å¤±æ•ˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDIJAåœ¨æ­ç¤ºæ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹æ¶æ„ä¸­çš„æ–°å¨èƒæ–¹é¢ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¶Šç‹±æ–¹æ³•ã€‚","title":"æ­ç¤ºæ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨è„†å¼±æ€§"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DIJAæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å®‰å…¨å¼±ç‚¹ï¼Œé€šè¿‡æ„é€ å¯¹æŠ—æ€§æç¤ºï¼Œå±•ç¤ºäº†å…¶å¯¹é½æœºåˆ¶çš„æ˜¾è‘—è„†å¼±æ€§ã€‚å°½ç®¡æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œæ–‡æœ¬å¡«å……æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¯¹é½æœºåˆ¶æ— æ³•æœ‰æ•ˆé˜²æŠ¤é’ˆå¯¹ä¸Šä¸‹æ–‡çš„å¯¹æŠ—æ€§æç¤ºã€‚DIJAé€šè¿‡æ„å»ºå¯¹æŠ—æ€§äº¤é”™æ©ç æ–‡æœ¬æç¤ºï¼Œåˆ©ç”¨äº†æ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæœºåˆ¶ï¼Œå¯¼è‡´æ ‡å‡†å¯¹é½æœºåˆ¶å¤±æ•ˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDIJAåœ¨æ­ç¤ºæ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹æ¶æ„ä¸­çš„æ–°å¨èƒæ–¹é¢ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¶Šç‹±æ–¹æ³•ã€‚', title='æ­ç¤ºæ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨è„†å¼±æ€§'))
[21.07.2025 06:20] Using data from previous issue: {"categories": ["#training", "#alignment", "#optimization", "#dataset", "#multilingual", "#science"], "emoji": "ğŸš€", "ru": {"title": "RedOne: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…", "desc": "RedOne - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ñ
[21.07.2025 06:20] Renaming data file.
[21.07.2025 06:20] Renaming previous data. hf_papers.json to ./d/2025-07-21.json
[21.07.2025 06:20] Saving new data file.
[21.07.2025 06:20] Generating page.
[21.07.2025 06:20] Renaming previous page.
[21.07.2025 06:20] Renaming previous data. index.html to ./d/2025-07-21.html
[21.07.2025 06:20] Writing result.
[21.07.2025 06:20] Renaming log file.
[21.07.2025 06:20] Renaming previous data. log.txt to ./logs/2025-07-21_last_log.txt
