[21.07.2025 05:21] Read previous papers.
[21.07.2025 05:21] Generating top page (month).
[21.07.2025 05:21] Writing top page (month).
[21.07.2025 06:19] Read previous papers.
[21.07.2025 06:19] Get feed.
[21.07.2025 06:19] Extract page data from URL. URL: https://huggingface.co/papers/2507.12566
[21.07.2025 06:19] Extract page data from URL. URL: https://huggingface.co/papers/2507.11097
[21.07.2025 06:19] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10605
[21.07.2025 06:19] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.07.2025 06:19] No deleted papers detected.
[21.07.2025 06:19] Downloading and parsing papers (pdf, html). Total: 3.
[21.07.2025 06:19] Downloading and parsing paper https://huggingface.co/papers/2507.12566.
[21.07.2025 06:19] Downloading paper 2507.12566 from http://arxiv.org/pdf/2507.12566v1...
[21.07.2025 06:19] Extracting affiliations from text.
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MONO-INTERNVL-1.5 1 Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai 5 2 0 2 J 6 1 ] . [ 1 6 6 5 2 1 . 7 0 5 2 : r AbstractThis paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed new visual parameter space into pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates set of visual experts through multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pretraining (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and reorganizes the pre-training process in an efficient manner. During inference, Mono-InternVL-1.5 includes fused CUDA kernel to speed up its MoE operations. With these designs, MonoInternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with MonoInternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modula"
[21.07.2025 06:19] Response: ```python
[]
```
[21.07.2025 06:19] Extracting affiliations from text.
[21.07.2025 06:19] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MONO-INTERNVL-1.5 1 Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai 5 2 0 2 J 6 1 ] . [ 1 6 6 5 2 1 . 7 0 5 2 : r AbstractThis paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed new visual parameter space into pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates set of visual experts through multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pretraining (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and reorganizes the pre-training process in an efficient manner. During inference, Mono-InternVL-1.5 includes fused CUDA kernel to speed up its MoE operations. With these designs, MonoInternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with MonoInternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL. Index TermsMultimodal Large Language Model, Visual Pretraining, Monolithic Model I. INTRODUCTION Recent years have witnessed the significant achievement of Multimodal Large Language Models (MLLMs) [1][3] in various vision-language tasks. As illustrated in Fig. 1(a), most existing Multimodal Large Language Models (MLLMs) adopt modular architecture, where visual encoding and language decoding are handled separately. This approach is typically realized by combining pre-trained visual encoder [4] with an LLM [5][7]. In contrast, monolithic MLLMs [8][10] have become another popular research trend in the community, as shown in Fig. 1(b), which integrate visual perception and Gen Luo, Wenhao Li, Weiyun Wang and Yu Qiao are with Shanghai Artificial Intelligence Laboratory. Wenhan Dou, Xizhou Zhu and Jifeng Dai are with Tsinghua University. Changyao Tian, Hao Li and Wenhai Wang are with The Chinese University of Hong Kong. Zhaokai Wang and Xue Yang are with Shanghai Jiao Tong University. Corresponding author: Jifeng Dai (daijifeng@tsinghua.edu.cn). TABLE I: Overall comparison of Mono-InternVL and Mono-InternVL-1.5. Mono-InternVL-1.5 greatly improves the training and inference efficiency while maintaining competitive downstream performance. Method Training Data Inference Speed VQA Bench MLLM Bench Mono-InternVL [11] Mono-InternVL-1.5 1.1B 0.5B -58% 61 tokens/s 77 tokens/s +26% 70.1 70.4 53.7 55.6 +0.3% +1.9% multimodal understanding within unified LLM framework. Compared to modular MLLMs, monolithic MLLMs often exhibit better potential in terms of design simplicity and deployment efficiency [9], [10]. Despite these advancements, training monolithic MLLM that achieves competitive performance still remains significant challenge. Among them, native pre-training [12] pre-trains monolithic MLLM from scratch using combination of textonly and multimodal data. However, this method demands extremely high computational resources and is prone to optimization instability [12]. Another promising solution is to extend the pre-trained LLM to multimodality via additional visual pre-training, namely continuous pre-training [9]. Such approaches typically require much cheaper training costs but easily incurs the catastrophic forgetting issue [13], thereby undermining the pre-trained language knowledge. In this paper, we aim to address the forgetting issue of continuous pre-training from the perspective of delta tuning [14]. Specifically, delta tuning fine-tunes set of newly added parameters in the model while keeping the rest frozen, thereby preserving the original knowledge. However, existing methods adopt shared architecture for joint vision and language modeling, where optimizations for vision can negatively impact language capabilities. Therefore, it is natural thought to introduce an independent visual parameter set into the pre-trained LLM, thus retaining the language knowledge by freezing the entire LLM while facilitating visual learning. This principle is also aligned with previous endeavors in modular MLLMs, e.g., QwenVL [15] and InternVL [6], where the visual parameters are placed outside the LLM. Based on the above principle, we propose novel monolithic MLLM, namely Mono-InternVL. As shown in Fig. 2, the visual parameters in Mono-InternVL are instantiated as set of expert networks via the mixture-of-experts (MoEs) mechanism. Based on this architecture, we present an innovative Endogenous Visual Pre-training (EViP) method to optimize the visual MONO-INTERNVL-1.5 2 Fig. 1: Comparison of Mono-InternVL, Mono-InternVL-1.5 and existing MLLMs. Compared with modular MLLMs, Mono-InternVL and Mono-InternVL-1.5 embed visual experts into the pre-trained LLM and integrates visual encoding and language decoding into single LLM. Through endogenous visual pre-training (EViP), Mono-InternVL significantly pushes the performance boundaries of monolithic MLLMs. With EViP++, Mono-InternVL-1.5 not only significantly reduces data costs, but also maintains the competitive performance of downstream tasks. parameters. Specifically, EViP is formulated as progressive learning process of three stages: 1) concept learning to grasp basic visual concepts, 2) semantic learning to capture high-level semantics, e.g., world knowledge, and 3) alignment learn"
[21.07.2025 06:19] Mistral response. {"id": "5f3720f1f35e4c789e7c929a95964b98", "object": "chat.completion", "created": 1753078782, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Shanghai Artificial Intelligence Laboratory\",\n    \"Tsinghua University\",\n    \"The Chinese University of Hong Kong\",\n    \"Shanghai Jiao Tong University\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1752, "total_tokens": 1805, "completion_tokens": 53}}
[21.07.2025 06:19] Response: ```python
[
    "Shanghai Artificial Intelligence Laboratory",
    "Tsinghua University",
    "The Chinese University of Hong Kong",
    "Shanghai Jiao Tong University"
]
```
[21.07.2025 06:19] Deleting PDF ./assets/pdf/2507.12566.pdf.
[21.07.2025 06:19] Success.
[21.07.2025 06:19] Downloading and parsing paper https://huggingface.co/papers/2507.11097.
[21.07.2025 06:19] Downloading paper 2507.11097 from http://arxiv.org/pdf/2507.11097v1...
[21.07.2025 06:19] Extracting affiliations from text.
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. THE DEVIL BEHIND THE MASK: AN EMERGENT SAFETY VULNERABILITY OF DIFFUSION LLMS Jiashu Qu2 Dongrui Liu2 Zhiyuan Liu1,2 Ruixi Wu1,2 Zichen Wen1,2 Yicun Yang1 Xiangqi Jin1 Haoyun Xu1 Xuyang Liu1 Weijia Li3,2 Chaochao Lu2 1EPIC Lab, Shanghai Jiao Tong University zichen.wen@outlook.com, heconghui@pjlab.org.cn, zhanglinfeng@sjtu.edu.cn Jing Shao2 Conghui He2 Linfeng Zhang1 2Shanghai AI Laboratory 3Sun Yat-sen University 5 2 0 2 5 1 ] . [ 1 7 9 0 1 1 . 7 0 5 2 : r WARNING: The paper contains content that may be offensive and disturbing in nature. "
[21.07.2025 06:19] Response: ```python
[
    "EPIC Lab, Shanghai Jiao Tong University",
    "Shanghai AI Laboratory",
    "Sun Yat-sen University"
]
```
[21.07.2025 06:19] Deleting PDF ./assets/pdf/2507.11097.pdf.
[21.07.2025 06:19] Success.
[21.07.2025 06:19] Downloading and parsing paper https://huggingface.co/papers/2507.10605.
[21.07.2025 06:19] Extra JSON file exists (./assets/json/2507.10605.json), skip PDF parsing.
[21.07.2025 06:19] Paper image links file exists (./assets/img_data/2507.10605.json), skip HTML parsing.
[21.07.2025 06:19] Success.
[21.07.2025 06:19] Enriching papers with extra data.
[21.07.2025 06:19] ********************************************************************************
[21.07.2025 06:19] Abstract 0. Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic ...
[21.07.2025 06:19] ********************************************************************************
[21.07.2025 06:19] Abstract 1. DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerg...
[21.07.2025 06:19] ********************************************************************************
[21.07.2025 06:19] Abstract 2. RedOne, a domain-specific LLM, enhances performance across multiple SNS tasks through a three-stage training strategy, improving generalization and reducing harmful content exposure.  					AI-generated summary 				 As a primary medium for modern information dissemination, social networking services ...
[21.07.2025 06:19] Read previous papers.
[21.07.2025 06:19] Generating reviews via LLM API.
[21.07.2025 06:19] Querying the API.
[21.07.2025 06:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.
[21.07.2025 06:19] Response: {
  "desc": "Mono-InternVL - это усовершенствованная монолитная мультимодальная большая языковая модель (MLLM), которая объединяет визуальное кодирование и языковое декодирование в единую модель. Она использует визуальных экспертов и улучшенные стратегии предобучения для повышения визуального обучения и снижения вычислительных затрат. Mono-InternVL применяет архитектуру смеси экспертов и инновационное эндогенное визуальное предобучение (EViP) для максимизации своих визуальных возможностей. Модель демонстрирует конкурентоспособную производительность по сравнению с существующими MLLM, значительно улучшая результаты на различных бенчмарках.",
  "emoji": "🧠",
  "title": "Mono-InternVL: Монолитная мультимодальная модель с улучшенным визуальным обучением"
}
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL."

[21.07.2025 06:19] Response: ```python
['MULTIMODAL', 'ARCHITECTURE', 'TRAINING', 'BENCHMARK']
```
[21.07.2025 06:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  					AI-generated summary 				 This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL."

[21.07.2025 06:19] Response: ```python
["OPTIMIZATION", "AGI"]
```
[21.07.2025 06:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Mono-InternVL, a monolithic Multimodal Large Language Model (MLLM) that combines visual and language processing into one model. It addresses issues like unstable optimization and catastrophic forgetting by embedding a new visual parameter space into a pre-trained language model, allowing for stable learning from noisy data. The model incorporates a multimodal mixture-of-experts architecture and an innovative Endogenous Visual Pre-training (EViP) strategy to enhance visual capabilities. Additionally, Mono-InternVL-1.5 is presented as a more efficient version that reduces training costs while maintaining competitive performance across multiple benchmarks.","title":"Revolutionizing Visual Learning with Mono-InternVL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Mono-InternVL, a monolithic Multimodal Large Language Model (MLLM) that combines visual and language processing into one model. It addresses issues like unstable optimization and catastrophic forgetting by embedding a new visual parameter space into a pre-trained language model, allowing for stable learning from noisy data. The model incorporates a multimodal mixture-of-experts architecture and an innovative Endogenous Visual Pre-training (EViP) strategy to enhance visual capabilities. Additionally, Mono-InternVL-1.5 is presented as a more efficient version that reduces training costs while maintaining competitive performance across multiple benchmarks.', title='Revolutionizing Visual Learning with Mono-InternVL'))
[21.07.2025 06:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Mono-InternVL是一种先进的单体多模态大语言模型，结合了视觉专家和改进的预训练策略，以增强视觉学习并降低计算成本，同时保持竞争力的性能。该模型通过将新的视觉参数空间嵌入到预训练的语言模型中，解决了不稳定优化和灾难性遗忘的问题。Mono-InternVL-1.5在此基础上进一步优化，采用了改进的内生视觉预训练（EViP++），引入了额外的视觉注意力专家，并高效地重新组织了预训练过程。实验结果表明，Mono-InternVL在多个基准测试中表现优于现有的单体多模态大语言模型，且在推理时显著降低了延迟。","title":"Mono-InternVL：高效的单体多模态大语言模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Mono-InternVL是一种先进的单体多模态大语言模型，结合了视觉专家和改进的预训练策略，以增强视觉学习并降低计算成本，同时保持竞争力的性能。该模型通过将新的视觉参数空间嵌入到预训练的语言模型中，解决了不稳定优化和灾难性遗忘的问题。Mono-InternVL-1.5在此基础上进一步优化，采用了改进的内生视觉预训练（EViP++），引入了额外的视觉注意力专家，并高效地重新组织了预训练过程。实验结果表明，Mono-InternVL在多个基准测试中表现优于现有的单体多模态大语言模型，且在推理时显著降低了延迟。', title='Mono-InternVL：高效的单体多模态大语言模型'))
[21.07.2025 06:20] Querying the API.
[21.07.2025 06:20] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.
[21.07.2025 06:20] Response: {
  "desc": "DIJA - это фреймворк для создания состязательных промптов, который использует уязвимости в механизмах безопасности диффузионных языковых моделей. Он демонстрирует, что существующие методы выравнивания не защищают эти модели от вредоносных запросов с маскированным вводом. DIJA конструирует промпты, чередуя маскированный и обычный текст, что позволяет обойти стандартные механизмы безопасности. Эксперименты показывают, что DIJA значительно превосходит существующие методы взлома языковых моделей, раскрывая новую поверхность угроз.",
  "emoji": "🕵️",
  "title": "Уязвимости диффузионных языковых моделей: новый фронт в безопасности ИИ"
}
[21.07.2025 06:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA."

[21.07.2025 06:20] Response: ```python
["ARCHITECTURE", "TRAINING"]
```
[21.07.2025 06:20] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  					AI-generated summary 				 Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA."

[21.07.2025 06:20] Response: ```python
["SECURITY", "ALIGNMENT", "DIFFUSION"]
```
[21.07.2025 06:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DIJA is a novel framework that identifies and exploits safety vulnerabilities in diffusion-based large language models (dLLMs) through the use of adversarial prompts. It reveals that existing alignment mechanisms are inadequate in preventing harmful outputs when faced with context-aware, masked-input prompts. By leveraging the unique features of dLLMs, such as bidirectional modeling and parallel decoding, DIJA demonstrates how these models can produce unsafe content despite alignment efforts. The framework significantly outperforms previous methods, highlighting the critical need for improved safety measures in the design of dLLMs.","title":"Unmasking Vulnerabilities in Diffusion-Based Language Models with DIJA"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DIJA is a novel framework that identifies and exploits safety vulnerabilities in diffusion-based large language models (dLLMs) through the use of adversarial prompts. It reveals that existing alignment mechanisms are inadequate in preventing harmful outputs when faced with context-aware, masked-input prompts. By leveraging the unique features of dLLMs, such as bidirectional modeling and parallel decoding, DIJA demonstrates how these models can produce unsafe content despite alignment efforts. The framework significantly outperforms previous methods, highlighting the critical need for improved safety measures in the design of dLLMs.', title='Unmasking Vulnerabilities in Diffusion-Based Language Models with DIJA'))
[21.07.2025 06:20] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DIJA是一个框架，利用扩散型大语言模型中的安全弱点，通过构造对抗性提示，展示了其对齐机制的显著脆弱性。尽管扩散型大语言模型在代码生成和文本填充方面表现出色，但我们发现现有的对齐机制无法有效防护针对上下文的对抗性提示。DIJA通过构建对抗性交错掩码文本提示，利用了扩散型大语言模型的文本生成机制，导致标准对齐机制失效。我们的实验表明，DIJA在揭示扩散型大语言模型架构中的新威胁方面，显著优于现有的越狱方法。","title":"揭示扩散型大语言模型的安全脆弱性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DIJA是一个框架，利用扩散型大语言模型中的安全弱点，通过构造对抗性提示，展示了其对齐机制的显著脆弱性。尽管扩散型大语言模型在代码生成和文本填充方面表现出色，但我们发现现有的对齐机制无法有效防护针对上下文的对抗性提示。DIJA通过构建对抗性交错掩码文本提示，利用了扩散型大语言模型的文本生成机制，导致标准对齐机制失效。我们的实验表明，DIJA在揭示扩散型大语言模型架构中的新威胁方面，显著优于现有的越狱方法。', title='揭示扩散型大语言模型的安全脆弱性'))
[21.07.2025 06:20] Using data from previous issue: {"categories": ["#training", "#alignment", "#optimization", "#dataset", "#multilingual", "#science"], "emoji": "🚀", "ru": {"title": "RedOne: универсальная языковая модель для революции в социальных сетях", "desc": "RedOne - это специализированная языковая модель для социальных сетей, разработанная с
[21.07.2025 06:20] Renaming data file.
[21.07.2025 06:20] Renaming previous data. hf_papers.json to ./d/2025-07-21.json
[21.07.2025 06:20] Saving new data file.
[21.07.2025 06:20] Generating page.
[21.07.2025 06:20] Renaming previous page.
[21.07.2025 06:20] Renaming previous data. index.html to ./d/2025-07-21.html
[21.07.2025 06:20] Writing result.
[21.07.2025 06:20] Renaming log file.
[21.07.2025 06:20] Renaming previous data. log.txt to ./logs/2025-07-21_last_log.txt
