[17.12.2025 08:33] Read previous papers.
[17.12.2025 08:33] Generating top page (month).
[17.12.2025 08:33] Writing top page (month).
[17.12.2025 09:30] Read previous papers.
[17.12.2025 09:30] Get feed.
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14691
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14614
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12675
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13660
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14051
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14336
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13303
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14503
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14699
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14531
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13678
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14442
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14284
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13961
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14666
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14550
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14273
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14008
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13607
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14391
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14697
[17.12.2025 09:30] Extract page data from URL. URL: https://huggingface.co/papers/2512.14620
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14067
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14014
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13655
[17.12.2025 09:30] Extract page data from URL. URL: https://huggingface.co/papers/2512.12980
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14440
[17.12.2025 09:30] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10945
[17.12.2025 09:30] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.12.2025 09:30] No deleted papers detected.
[17.12.2025 09:30] Downloading and parsing papers (pdf, html). Total: 28.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14691.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14691.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14691.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14614.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14614.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14614.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.12675.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.12675.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.12675.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.13660.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.13660.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.13660.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14051.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14051.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14051.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14336.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14336.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14336.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.13303.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.13303.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.13303.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14503.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14503.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14503.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14699.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14699.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14699.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14531.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14531.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14531.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.13678.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.13678.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.13678.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14442.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14442.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14442.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14284.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14284.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14284.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.13961.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.13961.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.13961.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14666.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14666.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14666.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14550.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14550.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14550.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14273.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14273.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14273.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14008.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14008.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14008.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.13607.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.13607.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.13607.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14391.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14391.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14391.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14697.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14697.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14697.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14620.
[17.12.2025 09:30] Downloading paper 2512.14620 from https://arxiv.org/pdf/2512.14620v1...
[17.12.2025 09:30] Extracting affiliations from text.
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 0 2 6 4 1 . 2 1 5 2 : r JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction Atsuyuki Miyai Shota Onohara Jeonghun Baek Kiyoharu Aizawa miyai@cvm.t.u-tokyo.ac.jp {onohara, baek, aizawa}@hal.t.u-tokyo.ac.jp The University of Tokyo (cid:153) https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/ Dataset (cid:135) Code 3 Leaderboard Figure 1: Building JMMMU-Pro via Vibe Benchmark Construction. JMMMU-Pro extends JMMMU by embedding each question image and text into single image. To construct JMMMU-Pro, we propose Vibe Benchmark Construction, where an image generation model creates questions, followed by human verification and prompt refinement to ensure quality. Experiments indicate that current open-source LMMs struggle with JMMMU-Pro. "
[17.12.2025 09:30] Response: ```python
["The University of Tokyo"]
```
[17.12.2025 09:30] Deleting PDF ./assets/pdf/2512.14620.pdf.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14067.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14067.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14067.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14014.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14014.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14014.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.13655.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.13655.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.13655.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.12980.
[17.12.2025 09:30] Downloading paper 2512.12980 from https://arxiv.org/pdf/2512.12980v1...
[17.12.2025 09:30] Extracting affiliations from text.
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 0 8 9 2 1 . 2 1 5 2 : r Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views Tingyang Chen Zhejiang University Hangzhou, China chenty@zju.edu.cn Haotian Wu Zhejiang University Hangzhou, China haotian.wu@zju.edu.cn Yunjun Gao Zhejiang University Hangzhou, China gaoyj@zju.edu.cn Cong Fu Shopee Pte. Ltd. Singapore, Singapore fc731097343@gmail.com Jiahua Wu Shopee Pte. Ltd. Singapore, Singapore gauvain.wujiahua@gmail.com Hua Fan Alibaba Cloud Computing Hangzhou, China guanming.fh@alibaba-inc.com Xiangyu Ke Zhejiang University Hangzhou, China xiangyu.ke@zju.edu.cn Yabo Ni Nanyang Technological University Singapore, Singapore yabo001@e.ntu.edu.sg Anxiang Zeng Nanyang Technological University Singapore, Singapore zeng0118@ntu.edu.sg Abstract Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recalllatency trade-off against ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice. We present Iceberg, holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face reco"
[17.12.2025 09:30] Response: ```python
[
    "Zhejiang University",
    "Shopee Pte. Ltd.",
    "Alibaba Cloud Computing",
    "Nanyang Technological University"
]
```
[17.12.2025 09:30] Deleting PDF ./assets/pdf/2512.12980.pdf.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.14440.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.14440.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.14440.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Downloading and parsing paper https://huggingface.co/papers/2512.10945.
[17.12.2025 09:30] Extra JSON file exists (./assets/json/2512.10945.json), skip PDF parsing.
[17.12.2025 09:30] Paper image links file exists (./assets/img_data/2512.10945.json), skip HTML parsing.
[17.12.2025 09:30] Success.
[17.12.2025 09:30] Enriching papers with extra data.
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 0. MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  					AI-generated summary 				 Video foundation models generate visually realistic and temporally coherent content...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 1. WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  					AI-generated summary 				 This paper presents WorldPlay, a streaming...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 2. Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  					AI-generated summary 				 Subject-driven image generation has advanced from single- to multi-s...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 3. RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  					AI-generated summary 				 Spatial traci...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 4. OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 5. A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to gr...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 6. ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  					AI-generated summary 				 While existing generation and un...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 7. RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 8. MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  					AI-generated summary 				 The core challenge for streaming video generation is maintaining the co...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 9. The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting t...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 10. Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  					AI-generated summary 				 Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and rob...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 11. A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  					AI-generated summary 				 Affordance prediction, which identifies interaction regions on objects based o...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 12. SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynami...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 13. Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  					AI-generated summary 				 We introduce Olmo 3, a family of state-of-the-art, fu...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 14. EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving tru...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 15. A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recov...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 16. Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize rel...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 17. Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance acr...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 18. Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning mo...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 19. RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to mode...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 20. Lattice coding provides a unified framework for non-parametric quantization, with Leech lattice-based quantization achieving superior performance in image tokenization, compression, and generation tasks.  					AI-generated summary 				 Non-parametric quantization has received much attention due to i...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 21. JMMMU-Pro, an image-based Japanese multi-discipline multimodal understanding benchmark, challenges open-source large multimodal models through integrated visual-textual understanding and is constructed using Vibe Benchmark Construction, a cost-effective method leveraging realistic image generation. ...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 22. AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  					AI-generated summary 				 Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, ...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 23. A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work la...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 24. Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  					AI-generated summary 				 Safety alignment mechan...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 25. Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.  					AI-generated summary 				 Vector Similarity Search (VSS) in high-dimensional sp...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 26. An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance ...
[17.12.2025 09:30] ********************************************************************************
[17.12.2025 09:30] Abstract 27. A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  					AI-generated summary 				 This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focus...
[17.12.2025 09:30] Read previous papers.
[17.12.2025 09:30] Generating reviews via LLM API.
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#survey", "#multimodal", "#video", "#reasoning"], "emoji": "üß†", "ru": {"title": "–õ–æ–≥–∏–∫–∞ –≤–∞–∂–Ω–µ–µ –∫—Ä–∞—Å–æ—Ç—ã: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MMGR ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–πÊ°ÜÊû∂–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#training", "#3d", "#diffusion", "#architecture", "#video", "#long_context"], "emoji": "üéÆ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –º–∏—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "WorldPlay ‚Äî —ç—Ç–æ –ø–æ—Ç–æ–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#multimodal", "#open_source"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ–º: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–Ω–æ–≥–æ–æ–±—ä–µ–∫—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Scone ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#dataset", "#rl", "#3d", "#robotics", "#multimodal", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboTracer ‚Äî —ç—Ç–æ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#training", "#data"], "emoji": "üîç", "ru": {"title": "ÈÄèËßÜ–¥–∞–Ω–Ω—ã–µ: –æ—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –Ω–∞—É–∫–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "OpenDataArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –∫–ª—é—á –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã SVG-–≥—Ä–∞—Ñ–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. 
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üìä", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å–∫—É—Å—Å—Ç–≤–æ: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π—Å—è pipeline –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü", "desc": "ShowTable ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#alignment", "#agents", "#rlhf", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ LLM –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —è–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –æ–± –∏–Ω—Ç–µ–Ω—Ç–∞—Ö", "desc": "RecGPT-V2 —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#long_context"], "emoji": "üé¨", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MemFlow ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ì–∏–±–∫–æ–µ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LLM", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç VersatileFFN ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∏—Ö –ø
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#rlhf", "#3d", "#training", "#multimodal"], "emoji": "üé®", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ControlNet", "desc": "Steer3D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏, —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥. –ê–≤—Ç–æ—Ä—ã –∞
[17.12.2025 09:30] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "A4-Agent ‚Äî —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è affordance (–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π) —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∑–∞–¥–∞—á—É –Ω–∞ —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#training", "#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π 4D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "SS4D ‚Äî —ç—Ç–æ –Ω–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ 3D –æ–±—ä–µ–∫—Ç—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–û–ª–º–æ 3: –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ–π Olmo 3 ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–ª—Ä–¥ –∏ 32 –º–ª—Ä–¥ –ø–∞
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#multimodal", "#robotics", "#rl", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "EVOLVE-VLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫
[17.12.2025 09:30] Using data from previous issue: {"categories": [], "emoji": "üè•", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∑–∞–¥–∞—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TAT (Task-Adaptive Transformer) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#video", "#multimodal", "#optimization", "#rlhf", "#long_context", "#hallucinations", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –æ–±—â–µ–≥–æ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É: —Ç–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã", "desc": "Zoom-Zero ‚Äî —ç—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è —Ç–∏–ø–∞ \"–≥—Ä—É–±—ã–π-
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É—Å–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Sparse-LaViDa –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É—Å–µ—á–µ–Ω–∏—è –º–∞—Å–∫–∏—Ä
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#alignment", "#rlhf", "#benchmark", "#training"], "emoji": "üèÜ", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–≥—Ä—É–∑–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç RePo ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üî∑", "ru": {"title": "–†–µ—à—ë—Ç–∫–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º—É –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ —Ç–µ–æ—Ä–∏—é —Ä–µ—à—ë—Ç–æ–∫ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–≤–∞–Ω—Ç
[17.12.2025 09:30] Querying the API.
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

JMMMU-Pro, an image-based Japanese multi-discipline multimodal understanding benchmark, challenges open-source large multimodal models through integrated visual-textual understanding and is constructed using Vibe Benchmark Construction, a cost-effective method leveraging realistic image generation.  					AI-generated summary 				 This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.
[17.12.2025 09:30] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω JMMMU-Pro ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —è–ø–æ–Ω—Å–∫–æ–º —è–∑—ã–∫–µ, —Ç—Ä–µ–±—É—é—â–∏–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é Vibe Benchmark Construction, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –Ω–∏–∑–∫–∏–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏, –∞ –∑–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –≤—Ä—É—á–Ω—É—é. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á —ç—Ç–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –µ–≥–æ –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —è–ø–æ–Ω—Å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LMM. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á.",
  "emoji": "üáØüáµ",
  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
}
```
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"JMMMU-Pro, an image-based Japanese multi-discipline multimodal understanding benchmark, challenges open-source large multimodal models through integrated visual-textual understanding and is constructed using Vibe Benchmark Construction, a cost-effective method leveraging realistic image generation.  					AI-generated summary 				 This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks."

[17.12.2025 09:30] Response: ```python
["DATASET", "BENCHMARK", "MULTIMODAL", "MULTILINGUAL"]
```
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"JMMMU-Pro, an image-based Japanese multi-discipline multimodal understanding benchmark, challenges open-source large multimodal models through integrated visual-textual understanding and is constructed using Vibe Benchmark Construction, a cost-effective method leveraging realistic image generation.  					AI-generated summary 				 This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks."

[17.12.2025 09:30] Response: ```python
["SYNTHETIC", "OPEN_SOURCE", "LOW_RESOURCE"]
```
[17.12.2025 09:30] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents JMMMU-Pro, a benchmark designed to evaluate large multimodal models on their ability to understand both images and text in Japanese. It introduces a novel construction method called Vibe Benchmark Construction, which uses an image generative model to create realistic visual questions that are then verified by humans. By combining visual and textual elements into a single image, JMMMU-Pro challenges models to demonstrate integrated visual-textual understanding. The results indicate that current open-source large multimodal models face significant difficulties with this benchmark, highlighting its importance for future research in the field.","title":"JMMMU-Pro: A New Benchmark for Multimodal Understanding in Japanese"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents JMMMU-Pro, a benchmark designed to evaluate large multimodal models on their ability to understand both images and text in Japanese. It introduces a novel construction method called Vibe Benchmark Construction, which uses an image generative model to create realistic visual questions that are then verified by humans. By combining visual and textual elements into a single image, JMMMU-Pro challenges models to demonstrate integrated visual-textual understanding. The results indicate that current open-source large multimodal models face significant difficulties with this benchmark, highlighting its importance for future research in the field.', title='JMMMU-Pro: A New Benchmark for Multimodal Understanding in Japanese'))
[17.12.2025 09:30] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜJMMMU-ProÔºåËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éÂõæÂÉèÁöÑÊó•Êú¨Â§öÂ≠¶ÁßëÂ§öÊ®°ÊÄÅÁêÜËß£Âü∫ÂáÜÔºåÊó®Âú®ÈÄöËøáÁªºÂêàËßÜËßâ-ÊñáÊú¨ÁêÜËß£Êù•ÊåëÊàòÂºÄÊ∫êÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã„ÄÇJMMMU-ProÈÄöËøáÂ∞ÜÈóÆÈ¢òÂõæÂÉèÂíåÈóÆÈ¢òÊñáÊú¨ÂêàÊàêÂà∞Âçï‰∏ÄÂõæÂÉè‰∏≠ÔºåÊâ©Â±ï‰∫Ü‰πãÂâçÁöÑJMMUÂü∫ÂáÜÔºåË¶ÅÊ±ÇÊ®°ÂûãÂÖ∑Â§áÊõ¥Âº∫ÁöÑËßÜËßâÊÑüÁü•ËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÊûÑÂª∫JMMMU-ProÔºåÊèêÂá∫‰∫ÜVibeÂü∫ÂáÜÊûÑÂª∫ÊñπÊ≥ïÔºåÂà©Áî®ÂõæÂÉèÁîüÊàêÊ®°ÂûãÁîüÊàêÂÄôÈÄâËßÜËßâÈóÆÈ¢òÔºåÂπ∂ÈÄöËøá‰∫∫Â∑•È™åËØÅÁ°Æ‰øùËæìÂá∫Ë¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊúâÂºÄÊ∫êÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®JMMMU-Pro‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂº∫Ë∞É‰∫ÜËØ•Âü∫ÂáÜÂú®Êú™Êù•ÂºÄÊ∫êÁ§æÂå∫‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ","title":"JMMMU-ProÔºöÊåëÊàòÂ§öÊ®°ÊÄÅÁêÜËß£ÁöÑÊñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜJMMMU-ProÔºåËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éÂõæÂÉèÁöÑÊó•Êú¨Â§öÂ≠¶ÁßëÂ§öÊ®°ÊÄÅÁêÜËß£Âü∫ÂáÜÔºåÊó®Âú®ÈÄöËøáÁªºÂêàËßÜËßâ-ÊñáÊú¨ÁêÜËß£Êù•ÊåëÊàòÂºÄÊ∫êÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã„ÄÇJMMMU-ProÈÄöËøáÂ∞ÜÈóÆÈ¢òÂõæÂÉèÂíåÈóÆÈ¢òÊñáÊú¨ÂêàÊàêÂà∞Âçï‰∏ÄÂõæÂÉè‰∏≠ÔºåÊâ©Â±ï‰∫Ü‰πãÂâçÁöÑJMMUÂü∫ÂáÜÔºåË¶ÅÊ±ÇÊ®°ÂûãÂÖ∑Â§áÊõ¥Âº∫ÁöÑËßÜËßâÊÑüÁü•ËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÊûÑÂª∫JMMMU-ProÔºåÊèêÂá∫‰∫ÜVibeÂü∫ÂáÜÊûÑÂª∫ÊñπÊ≥ïÔºåÂà©Áî®ÂõæÂÉèÁîüÊàêÊ®°ÂûãÁîüÊàêÂÄôÈÄâËßÜËßâÈóÆÈ¢òÔºåÂπ∂ÈÄöËøá‰∫∫Â∑•È™åËØÅÁ°Æ‰øùËæìÂá∫Ë¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊúâÂºÄÊ∫êÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®JMMMU-Pro‰∏äË°®Áé∞‰∏ç‰Ω≥ÔºåÂº∫Ë∞É‰∫ÜËØ•Âü∫ÂáÜÂú®Êú™Êù•ÂºÄÊ∫êÁ§æÂå∫‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ', title='JMMMU-ProÔºöÊåëÊàòÂ§öÊ®°ÊÄÅÁêÜËß£ÁöÑÊñ∞Âü∫ÂáÜ'))
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#multimodal", "#agents", "#dataset"], "emoji": "üåç", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —É–º–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∑–∞–º–µ–Ω–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#training", "#security", "#benchmark", "#alignment", "#interpretability"], "emoji": "üî™", "ru": {"title": "–•–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–æ–≤: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ç—ã—Ä—ë—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ—Ç–∫–∞–∑–∞ 
[17.12.2025 09:30] Querying the API.
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.  					AI-generated summary 				 Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.   We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.
[17.12.2025 09:30] Response: ```json
{
  "desc": "Iceberg ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ ¬´–í–æ—Ä–æ–Ω–∫—É –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏¬ª, –æ–ø—Ä–µ–¥–µ–ª—è—é—â—É—é —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ—Ç—Ä–∏–∫ –∑–∞–¥–∞—á–µ –∏ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –≤–æ—Å–µ–º—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å 1-100 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç 13 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ –ø–æ–ª–Ω–æ–π —Ü–µ–ø–æ—á–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –∞ –Ω–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö insights –∞–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–µ –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ø–æ–º–æ—â–∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º –≤ –≤—ã–±–æ—Ä–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏.",
  "emoji": "üßä",
  "title": "–û—Ç –º–µ—Ç—Ä–∏–∫ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º: –æ—Ü–µ–Ω–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö"
}
```
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.  					AI-generated summary 				 Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.   We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads."

[17.12.2025 09:30] Response: ```python
["BENCHMARK", "DATASET", "MULTIMODAL"]
```
[17.12.2025 09:30] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.  					AI-generated summary 				 Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.   We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads."

[17.12.2025 09:30] Response: ```python
["OPTIMIZATION"]
```

The paper focuses on benchmarking and evaluating vector similarity search methods, identifying performance degradation sources, and providing guidance for selecting and tuning these methods. This relates to optimization of search and retrieval methods in practical applications. While the paper mentions LLMs in context, it is not primarily about language models themselves but rather about the vector search infrastructure that supports them.
[17.12.2025 09:30] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on benchmarking and evaluating vector similarity search methods, identifying performance degradation sources, and providing guidance for selecting and tuning these methods. This relates to optimization of search and retrieval methods in practical applications. While the paper mentions LLMs in context, it is not primarily about language models themselves but rather about the vector search infrastructure that supports them.
[17.12.2025 09:30] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Iceberg is a benchmark suite designed to evaluate vector similarity search (VSS) methods in real-world applications, focusing on how retrieval quality affects downstream tasks. It identifies three main sources of performance degradation: embedding loss, metric misuse, and data distribution sensitivity. By using diverse datasets and task-specific metrics, Iceberg provides a more comprehensive assessment of VSS methods compared to traditional recall-latency benchmarks. The suite also offers guidance for practitioners through a decision tree that helps in selecting and tuning VSS methods based on specific workloads.","title":"Iceberg: Elevating Vector Similarity Search Evaluation for Real-World Applications"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Iceberg is a benchmark suite designed to evaluate vector similarity search (VSS) methods in real-world applications, focusing on how retrieval quality affects downstream tasks. It identifies three main sources of performance degradation: embedding loss, metric misuse, and data distribution sensitivity. By using diverse datasets and task-specific metrics, Iceberg provides a more comprehensive assessment of VSS methods compared to traditional recall-latency benchmarks. The suite also offers guidance for practitioners through a decision tree that helps in selecting and tuning VSS methods based on specific workloads.', title='Iceberg: Elevating Vector Similarity Search Evaluation for Real-World Applications'))
[17.12.2025 09:30] Response: ParsedChatCompletionMessage[Article](content='{"desc":"IcebergÊòØ‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÂ•ó‰ª∂ÔºåÁî®‰∫éËØÑ‰º∞Âú®ÁúüÂÆûÂ∫îÁî®Âú∫ÊôØ‰∏≠ÂêëÈáèÁõ∏‰ººÊÄßÊêúÁ¥¢ÔºàVSSÔºâÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇÂÆÉÊè≠Á§∫‰∫Ü‰ø°ÊÅØÊçüÂ§±ÊºèÊñóÔºåËØÜÂà´Âá∫ÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÁöÑ‰∏â‰∏™‰∏ªË¶ÅÊù•Ê∫êÔºöÁâπÂæÅÊèêÂèñ‰∏≠ÁöÑÂµåÂÖ•ÊçüÂ§±„ÄÅÂ∫¶ÈáèËØØÁî®‰ª•ÂèäÊï∞ÊçÆÂàÜÂ∏ÉÊïèÊÑüÊÄß„ÄÇIcebergÊ∂µÁõñ‰∫ÜÂÖ´‰∏™‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÔºåÊîØÊåÅÂõæÂÉèÂàÜÁ±ª„ÄÅ‰∫∫ËÑ∏ËØÜÂà´„ÄÅÊñáÊú¨Ê£ÄÁ¥¢ÂíåÊé®ËçêÁ≥ªÁªüÁ≠âÂÖ≥ÈîÆÈ¢ÜÂüüÁöÑËØÑ‰º∞„ÄÇÈÄöËøáÂØπ13ÁßçÊúÄÂÖàËøõÁöÑVSSÊñπÊ≥ïËøõË°åÂü∫ÂáÜÊµãËØïÔºåIcebergÊèê‰æõ‰∫ÜÂü∫‰∫éÂ∫îÁî®Á∫ßÊåáÊ†áÁöÑÈáçÊñ∞ÊéíÂêçÔºåÂ∏ÆÂä©ÂÆûË∑µËÄÖÈÄâÊã©ÂíåË∞ÉÊï¥ÈÄÇÂêàÂÖ∂ÁâπÂÆöÂ∑•‰ΩúË¥üËΩΩÁöÑVSSÊñπÊ≥ï„ÄÇ","title":"IcebergÔºöÁúüÂÆûÂú∫ÊôØ‰∏ãÁöÑÂêëÈáèÁõ∏‰ººÊÄßÊêúÁ¥¢ËØÑ‰º∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='IcebergÊòØ‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïÂ•ó‰ª∂ÔºåÁî®‰∫éËØÑ‰º∞Âú®ÁúüÂÆûÂ∫îÁî®Âú∫ÊôØ‰∏≠ÂêëÈáèÁõ∏‰ººÊÄßÊêúÁ¥¢ÔºàVSSÔºâÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇÂÆÉÊè≠Á§∫‰∫Ü‰ø°ÊÅØÊçüÂ§±ÊºèÊñóÔºåËØÜÂà´Âá∫ÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÁöÑ‰∏â‰∏™‰∏ªË¶ÅÊù•Ê∫êÔºöÁâπÂæÅÊèêÂèñ‰∏≠ÁöÑÂµåÂÖ•ÊçüÂ§±„ÄÅÂ∫¶ÈáèËØØÁî®‰ª•ÂèäÊï∞ÊçÆÂàÜÂ∏ÉÊïèÊÑüÊÄß„ÄÇIcebergÊ∂µÁõñ‰∫ÜÂÖ´‰∏™‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÔºåÊîØÊåÅÂõæÂÉèÂàÜÁ±ª„ÄÅ‰∫∫ËÑ∏ËØÜÂà´„ÄÅÊñáÊú¨Ê£ÄÁ¥¢ÂíåÊé®ËçêÁ≥ªÁªüÁ≠âÂÖ≥ÈîÆÈ¢ÜÂüüÁöÑËØÑ‰º∞„ÄÇÈÄöËøáÂØπ13ÁßçÊúÄÂÖàËøõÁöÑVSSÊñπÊ≥ïËøõË°åÂü∫ÂáÜÊµãËØïÔºåIcebergÊèê‰æõ‰∫ÜÂü∫‰∫éÂ∫îÁî®Á∫ßÊåáÊ†áÁöÑÈáçÊñ∞ÊéíÂêçÔºåÂ∏ÆÂä©ÂÆûË∑µËÄÖÈÄâÊã©ÂíåË∞ÉÊï¥ÈÄÇÂêàÂÖ∂ÁâπÂÆöÂ∑•‰ΩúË¥üËΩΩÁöÑVSSÊñπÊ≥ï„ÄÇ', title='IcebergÔºöÁúüÂÆûÂú∫ÊôØ‰∏ãÁöÑÂêëÈáèÁõ∏‰ººÊÄßÊêúÁ¥¢ËØÑ‰º∞'))
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#video", "#dataset", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å
[17.12.2025 09:30] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "–î–≤–∏–∂–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ: –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç MeViS –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π, –æ–ø–∏—Å—ã–≤–∞—é—â–∏—Ö –¥–≤–∏
[17.12.2025 09:30] Renaming data file.
[17.12.2025 09:30] Renaming previous data. hf_papers.json to ./d/2025-12-17.json
[17.12.2025 09:30] Saving new data file.
[17.12.2025 09:30] Generating page.
[17.12.2025 09:30] Renaming previous page.
[17.12.2025 09:30] Renaming previous data. index.html to ./d/2025-12-17.html
[17.12.2025 09:30] Writing result.
[17.12.2025 09:30] Renaming log file.
[17.12.2025 09:30] Renaming previous data. log.txt to ./logs/2025-12-17_last_log.txt
