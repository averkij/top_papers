[17.12.2025 05:27] Read previous papers.
[17.12.2025 05:27] Generating top page (month).
[17.12.2025 05:27] Writing top page (month).
[17.12.2025 06:36] Read previous papers.
[17.12.2025 06:36] Get feed.
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14691
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12675
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13660
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14614
[17.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.14051
[17.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.14336
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14503
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13303
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14699
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14531
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14442
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14284
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13678
[17.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.14550
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14008
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13961
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14666
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14391
[17.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.14273
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14067
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13655
[17.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.13607
[17.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.14440
[17.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14014
[17.12.2025 06:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.12.2025 06:36] No deleted papers detected.
[17.12.2025 06:36] Downloading and parsing papers (pdf, html). Total: 24.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14691.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14691.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14691.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.12675.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.12675.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.12675.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13660.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13660.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13660.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14614.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14614.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14614.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14051.
[17.12.2025 06:36] Downloading paper 2512.14051 from https://arxiv.org/pdf/2512.14051v1...
[17.12.2025 06:36] Extracting affiliations from text.
[17.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 1 5 0 4 1 . 2 1 5 2 : r OpenDataArena: Fair and Open Arena for Benchmarking Post-Training Dataset Value OpenDataArena Team1 1Shanghai Artificial Intelligence Laboratory, OpenDataLab The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains black boxcharacterized by opaque composition, uncertain provenance, and lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes comprehensive ecosystem comprising four key pillars: (i) unified trainingevaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODAcovering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data pointsreveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding leaderboard, ODA envisions shift from trial-and-error data curation to principled science of Data-Centric AI, paving the way for rigorous "
[17.12.2025 06:36] Response: ```python
[
    "Shanghai Artificial Intelligence Laboratory, OpenDataLab"
]
```
[17.12.2025 06:36] Deleting PDF ./assets/pdf/2512.14051.pdf.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14336.
[17.12.2025 06:36] Downloading paper 2512.14336 from https://arxiv.org/pdf/2512.14336v1...
[17.12.2025 06:36] Extracting affiliations from text.
[17.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure Jooyeol Yun, KAIST AI {blizzard072, jchoo}@kaist.ac.kr 5 2 0 2 6 1 ] . [ 1 6 3 3 4 1 . 2 1 5 2 : r I want the emoji to look to the left and right. want the elements smoothly pop up in lively manner. want the compass needle to quickly spin around once. want the buttons to bounce in one by one. Figure 1. Animations generated by Vector Prism. Please view them in Adobe Acrobat or the Firefox browser for the best experience. An HTML version is available in the project page. "
[17.12.2025 06:36] Response: ```python
["KAIST"]
```
[17.12.2025 06:36] Deleting PDF ./assets/pdf/2512.14336.pdf.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14503.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14503.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14503.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13303.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13303.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13303.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14699.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14699.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14699.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14531.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14531.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14531.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14442.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14442.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14442.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14284.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14284.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14284.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13678.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13678.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13678.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14550.
[17.12.2025 06:36] Downloading paper 2512.14550 from https://arxiv.org/pdf/2512.14550v1...
[17.12.2025 06:36] Extracting affiliations from text.
[17.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 0 5 5 4 1 . 2 1 5 2 : r TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration Zhiwen Yang1, Jiaju Zhang1, Yang Yi1, Jian Liang2, Bingzheng Wei3, and Yan Xu1((cid:0)) 1 School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China xuyan04@gmail.com 2 Sinovision Technologies (Beijing) Co., Ltd., Beijing 101102, China 3 ByteDance Inc., Beijing 100098, China Abstract. Medical image restoration (MedIR) aims to recover highquality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose task-adaptive Transformer (TAT), novel framework that dynamically adapts to different tasks through two key innovations. First, task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedI"
[17.12.2025 06:36] Response: ```python
[
    "School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China",
    "Sinovision Technologies (Beijing) Co., Ltd., Beijing 101102, China",
    "ByteDance Inc., Beijing 100098, China"
]
```
[17.12.2025 06:36] Deleting PDF ./assets/pdf/2512.14550.pdf.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14008.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14008.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14008.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13961.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13961.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13961.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14666.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14666.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14666.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14391.
[17.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14391.json), skip PDF parsing.
[17.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14391.json), skip HTML parsing.
[17.12.2025 06:36] Success.
[17.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14273.
[17.12.2025 06:36] Downloading paper 2512.14273 from https://arxiv.org/pdf/2512.14273v1...
[17.12.2025 06:37] Extracting affiliations from text.
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-12-17 Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in Xiaoqian Shen1,2* Min-Hung Chen1 Yu-Chiang Frank Wang1 Mohamed Elhoseiny2 Ryo Hachiuma 1 NVIDIA 2 KAUST 5 2 0 2 6 1 ] . [ 1 3 7 2 4 1 . 2 1 5 2 : r a "
[17.12.2025 06:37] Response: ```python
['NVIDIA', 'KAUST']
```
[17.12.2025 06:37] Deleting PDF ./assets/pdf/2512.14273.pdf.
[17.12.2025 06:37] Success.
[17.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.14067.
[17.12.2025 06:37] Extra JSON file exists (./assets/json/2512.14067.json), skip PDF parsing.
[17.12.2025 06:37] Paper image links file exists (./assets/img_data/2512.14067.json), skip HTML parsing.
[17.12.2025 06:37] Success.
[17.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.13655.
[17.12.2025 06:37] Extra JSON file exists (./assets/json/2512.13655.json), skip PDF parsing.
[17.12.2025 06:37] Paper image links file exists (./assets/img_data/2512.13655.json), skip HTML parsing.
[17.12.2025 06:37] Success.
[17.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.13607.
[17.12.2025 06:37] Downloading paper 2512.13607 from https://arxiv.org/pdf/2512.13607v1...
[17.12.2025 06:37] Extracting affiliations from text.
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-12-8 Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models Boxin Wang, Chankyu Lee*, Nayeon Lee*, Sheng-Chieh Lin*, Wenliang Dai*, Yang Chen*, Yangyi Chen*, Zhuolin Yang*, Zihan Liu*, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping* "
[17.12.2025 06:37] Response: ```python
["NVIDIA"]
```
[17.12.2025 06:37] Deleting PDF ./assets/pdf/2512.13607.pdf.
[17.12.2025 06:37] Success.
[17.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.14440.
[17.12.2025 06:37] Downloading paper 2512.14440 from https://arxiv.org/pdf/2512.14440v1...
[17.12.2025 06:37] Extracting affiliations from text.
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation 5 2 0 2 6 1 ] . [ 1 0 4 4 4 1 . 2 1 5 2 : r Figure 1. Learning Unsupervised Video Instance Segmentation from Real Videos. Given single-frame unsupervised instance masks, we first discover temporally-coherent, high-quality keymasks. This sparse labelset is then propagated using Sparse-To-Dense Distillation, aided by Temporal DropLoss. Finally, we train our model on the resulting dense labelset and demonstrate state-of-the-art results. "
[17.12.2025 06:37] Response: ```python
[]
```
[17.12.2025 06:37] Extracting affiliations from text.
[17.12.2025 06:37] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation5 2 0 2 6 1 ] . [ 1 0 4 4 4 1 . 2 1 5 2 : r Figure 1. Learning Unsupervised Video Instance Segmentation from Real Videos. Given single-frame unsupervised instance masks, we first discover temporally-coherent, high-quality keymasks. This sparse labelset is then propagated using Sparse-To-Dense Distillation, aided by Temporal DropLoss. Finally, we train our model on the resulting dense labelset and demonstrate state-of-the-art results.In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train segmentation model for implicit mask propagation, for which we propose Sparse-To-Dense Distillation approach aided by Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks. Project Page: leonsick.github.io/s2d 1. Introduction Video instance segmentation is key task in perceptive computer vision, empowering range of applications, such as autonomous driving, AR/VR applications, and video editing. Compared to image instance segmentation, video instance segmentation though is fundamentally more challenging. To segment videos, models must learn temporal dynamics and maintain identity-preserving masks, while instances may appear, disappear and undergo occlusions. In recent years, range of supervised Video Instance Segmentation models have advanced rapidly [4, 34, 36, 41, 43], steadily increasing segmentation accuracy. However, many of these models rely on per-frame human annotations, which require immense annotation efforts and thus result 1 in high costs. For example, the largest video segmentation dataset yet, SA-V [23], contains 190K manual annotations, for which the authors employed large group of crowdworkers. Such costly efforts have sparked the field of Unsupervised Video Instance Segmentation. This research area follows the same core motivation as unsupervised image segmentation: Enabling segmentation without task-specific or any other human annotations, therefore removing entirely the need for human labeling anywhere in the pipeline [1, 10, 11, 25, 26, 32, 33]. Wang et al. [33] have proposed VideoCutLER, video segmenter trained on synthesized sequences of shifting instances through object-centric images from ImageNet [7]. Critically, their training data models only single-object translational movement examples, that lack the complexity of multiple dynamic objects in real-world videos. To overcome this limitation, we enable our model to learn complex temporal dynamics, involving multi-instance motion, from real-world videos without requiring human annotations. Specifically, we propose S2D, an approach for training video instance segmentation model to make dense predictions by training on sparse set of temporally-coherent, high-quality keymasks, predicted by an unsupervised image instance segmenter. We achieve this relying on three key insights. First, we can extract high-confidence masks for certain frames with existing image unsupervised instance segmentation models. Second, we can identify which frames contain reliable masks by inspecting their temporal coherence. Lastly, we can train video instance segmentation model on sparse image annotations via Sparse-To-Dense distillation to achieve mask propagation. Our proposed pipeline is illustrated in Figure 1. In the first step, our Keymask Discovery algorithm identifies the quality of frame-wise image masks without human supervision by leveraging point tracks as deep motion prior. The resulting masks are identified as keymasks of the video, i.e., temporally-coherent instance masks. We predict single-frame masks using an off-the-shelf unsupervised image instance segmentation model. By tracking each single-frame mask over the entire video sequence, we identify when appearances and disappearances occur. Performing this analysis for all single-frame masks yields visibility groupings, which contain instance masks whose proxypropagations, i.e., point tracks, appear and disappear together. Following, we determine which of the simultaneously visible masks correspond to each other by applying our temporal correspondence matching. Using the overlap of proxy-propagations and per-frame instance masks, we subdivide the visibility groups into clusters that share temporal and spatial correspondence. This process yields sparse video annotations, i.e., the video instances have annotations for some frames where high-quality masks could be identified. However, our method aims to obtain mask annotations for video instances in every frame where they are visible, i.e., dense annotation set. To achieve this, we demonstrate that VideoMask2Former [4] can be trained as an implicit mask propagation & object discovery model. We propose novel approach for training this model on sparse annotations to complete the missing video instance masks. First, we waive the loss penalty for missing instance annotations by proposing Temporal DropLoss. Second, we propose Sparse-To-Dense Distillation, where we train student model to learn dense predictions, guided by teacher model. Sparse-To-Dense Distillation results in dense video annotation set, which we further refine in another round of training. Since our process to generate pseudolabelsets is applicable to any video dataset, we experiment with scaling the training video data beyond the in-domain data and find that this yields strong zero-shot performance. In summary, we propose the following contributions: 1. We propose Keymask Discovery algorithm to construct sparse pseudo-labelset of hig"
[17.12.2025 06:37] Mistral response. {"id": "00dd596a30fe47ad9740069a8901ce1f", "created": 1765953447, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1337, "total_tokens": 1343, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[17.12.2025 06:37] Response: ```python
[]
```
[17.12.2025 06:37] Deleting PDF ./assets/pdf/2512.14440.pdf.
[17.12.2025 06:37] Success.
[17.12.2025 06:37] Downloading and parsing paper https://huggingface.co/papers/2512.14014.
[17.12.2025 06:37] Extra JSON file exists (./assets/json/2512.14014.json), skip PDF parsing.
[17.12.2025 06:37] Paper image links file exists (./assets/img_data/2512.14014.json), skip HTML parsing.
[17.12.2025 06:37] Success.
[17.12.2025 06:37] Enriching papers with extra data.
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 0. MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  					AI-generated summary 				 Video foundation models generate visually realistic and temporally coherent content...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 1. Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  					AI-generated summary 				 Subject-driven image generation has advanced from single- to multi-s...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 2. RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  					AI-generated summary 				 Spatial traci...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 3. WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  					AI-generated summary 				 This paper presents WorldPlay, a streaming...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 4. OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 5. A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to gr...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 6. RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 7. ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  					AI-generated summary 				 While existing generation and un...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 8. MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  					AI-generated summary 				 The core challenge for streaming video generation is maintaining the co...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 9. The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting t...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 10. A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  					AI-generated summary 				 Affordance prediction, which identifies interaction regions on objects based o...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 11. SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynami...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 12. Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  					AI-generated summary 				 Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and rob...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 13. A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recov...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 14. Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance acr...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 15. Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  					AI-generated summary 				 We introduce Olmo 3, a family of state-of-the-art, fu...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 16. EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving tru...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 17. RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to mode...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 18. Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize rel...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 19. AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  					AI-generated summary 				 Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, ...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 20. Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  					AI-generated summary 				 Safety alignment mechan...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 21. Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning mo...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 22. An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance ...
[17.12.2025 06:37] ********************************************************************************
[17.12.2025 06:37] Abstract 23. A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work la...
[17.12.2025 06:37] Read previous papers.
[17.12.2025 06:37] Generating reviews via LLM API.
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#survey", "#multimodal", "#video", "#reasoning"], "emoji": "üß†", "ru": {"title": "–õ–æ–≥–∏–∫–∞ –≤–∞–∂–Ω–µ–µ –∫—Ä–∞—Å–æ—Ç—ã: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MMGR ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–πÊ°ÜÊû∂–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#multimodal", "#open_source"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ–º: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–Ω–æ–≥–æ–æ–±—ä–µ–∫—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Scone ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#dataset", "#rl", "#3d", "#robotics", "#multimodal", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboTracer ‚Äî —ç—Ç–æ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#training", "#3d", "#diffusion", "#architecture", "#video", "#long_context"], "emoji": "üéÆ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –º–∏—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "WorldPlay ‚Äî —ç—Ç–æ –ø–æ—Ç–æ–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç
[17.12.2025 06:37] Querying the API.
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary 				 The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.
[17.12.2025 06:37] Response: ```json
{
  "desc": "OpenDataArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –≤–∫–ª—é—á–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π pipeline –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏, –º–Ω–æ–≥–æ–º–µ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –±–æ–ª–µ–µ —á–µ–º 120 –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏ 40 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∞–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –æ –≤–ª–∏—è–Ω–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π. –ü—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –æ—Ç —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∫ –Ω–∞—É—á–Ω–æ–º—É –ø–æ–¥—Ö–æ–¥—É –≤ Data-Centric AI.",
  "emoji": "üîç",
  "title": "ÈÄèËßÜ–¥–∞–Ω–Ω—ã–µ: –æ—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –Ω–∞—É–∫–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤"
}
```
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary 				 The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models."

[17.12.2025 06:37] Response: ```python
["DATASET", "BENCHMARK", "DATA", "TRAINING"]
```
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary 				 The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models."

[17.12.2025 06:37] Response: ```python
["OPEN_SOURCE", "SURVEY"]
```

**Justification:**

1. **OPEN_SOURCE**: The paper explicitly states "a fully open-source toolkit for training, evaluation, and scoring" and "We release all results, tools, and configurations to democratize access to high-quality data evaluation." This demonstrates a clear contribution of open-source resources to the community.

2. **SURVEY**: The paper presents OpenDataArena as a comprehensive benchmarking platform that systematically evaluates post-training datasets across "over 120 training datasets across multiple domains on 22 benchmarks" with "more than 600 training runs." This extensive empirical analysis and systematic evaluation of data characteristics across multiple dimensions constitutes a survey-like contribution to understanding post-training data quality.
[17.12.2025 06:37] Error. Failed to parse JSON from LLM. ["OPEN_SOURCE", "SURVEY"]


**Justification:**

1. **OPEN_SOURCE**: The paper explicitly states "a fully open-source toolkit for training, evaluation, and scoring" and "We release all results, tools, and configurations to democratize access to high-quality data evaluation." This demonstrates a clear contribution of open-source resources to the community.

2. **SURVEY**: The paper presents OpenDataArena as a comprehensive benchmarking platform that systematically evaluates post-training datasets across "over 120 training datasets across multiple domains on 22 benchmarks" with "more than 600 training runs." This extensive empirical analysis and systematic evaluation of data characteristics across multiple dimensions constitutes a survey-like contribution to understanding post-training data quality.
[17.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OpenDataArena (ODA) is a platform that benchmarks post-training datasets for Large Language Models (LLMs) to improve understanding of how data affects model performance. It addresses the issue of data opacity by providing a unified pipeline for training and evaluation, allowing for fair comparisons across different models. ODA features a multi-dimensional scoring system to assess data quality and an interactive tool to explore the lineage of datasets, revealing their sources and relationships. By making its tools and findings publicly available, ODA aims to promote a more scientific approach to data-centric AI, moving beyond simple data collection to informed data curation.","title":"Unlocking Data Insights for Better AI Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OpenDataArena (ODA) is a platform that benchmarks post-training datasets for Large Language Models (LLMs) to improve understanding of how data affects model performance. It addresses the issue of data opacity by providing a unified pipeline for training and evaluation, allowing for fair comparisons across different models. ODA features a multi-dimensional scoring system to assess data quality and an interactive tool to explore the lineage of datasets, revealing their sources and relationships. By making its tools and findings publicly available, ODA aims to promote a more scientific approach to data-centric AI, moving beyond simple data collection to informed data curation.', title='Unlocking Data Insights for Better AI Models'))
[17.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OpenDataArenaÔºàODAÔºâÊòØ‰∏Ä‰∏™ÂºÄÊîæÂπ≥Âè∞ÔºåÊó®Âú®ÈÄöËøáÁªü‰∏ÄÁöÑÊµÅÁ®ã„ÄÅÂ§öÁª¥ËØÑÂàÜÂíåÊï∞ÊçÆÊù•Ê∫êÊé¢Á¥¢Êù•ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂêéËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑ‰ª∑ÂÄº„ÄÇËØ•Âπ≥Âè∞Ëß£ÂÜ≥‰∫ÜÊï∞ÊçÆÈÄèÊòéÂ∫¶‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂ∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÁêÜËß£Êï∞ÊçÆÁâπÊÄß‰∏éÊ®°ÂûãË°å‰∏∫‰πãÈó¥ÁöÑÂõ†ÊûúÂÖ≥Á≥ª„ÄÇODAÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁîüÊÄÅÁ≥ªÁªüÔºåÂåÖÊã¨Áªü‰∏ÄÁöÑËÆ≠ÁªÉËØÑ‰º∞ÊµÅÁ®ã„ÄÅÂ§öÁª¥ËØÑÂàÜÊ°ÜÊû∂„ÄÅ‰∫§‰∫íÂºèÊï∞ÊçÆÊù•Ê∫êÊé¢Á¥¢Â∑•ÂÖ∑ÂíåÂºÄÊ∫êÂ∑•ÂÖ∑ÂåÖÔºå‰ª•‰øÉËøõÊï∞ÊçÆÁ†îÁ©∂„ÄÇÈÄöËøáÂØπ120Â§ö‰∏™ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂπøÊ≥õÂÆûÈ™åÔºåODAÊè≠Á§∫‰∫ÜÊï∞ÊçÆÂ§çÊùÇÊÄß‰∏é‰ªªÂä°ÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°ÔºåÊé®Âä®‰∫ÜÊï∞ÊçÆ‰∏≠ÂøÉ‰∫∫Â∑•Êô∫ËÉΩÁöÑÁßëÂ≠¶Á†îÁ©∂„ÄÇ","title":"ÂºÄÊîæÊï∞ÊçÆÂπ≥Âè∞ÔºåÊèêÂçáÊ®°ÂûãÊï∞ÊçÆÈÄèÊòéÂ∫¶"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OpenDataArenaÔºàODAÔºâÊòØ‰∏Ä‰∏™ÂºÄÊîæÂπ≥Âè∞ÔºåÊó®Âú®ÈÄöËøáÁªü‰∏ÄÁöÑÊµÅÁ®ã„ÄÅÂ§öÁª¥ËØÑÂàÜÂíåÊï∞ÊçÆÊù•Ê∫êÊé¢Á¥¢Êù•ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂêéËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑ‰ª∑ÂÄº„ÄÇËØ•Âπ≥Âè∞Ëß£ÂÜ≥‰∫ÜÊï∞ÊçÆÈÄèÊòéÂ∫¶‰∏çË∂≥ÁöÑÈóÆÈ¢òÔºåÂ∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÁêÜËß£Êï∞ÊçÆÁâπÊÄß‰∏éÊ®°ÂûãË°å‰∏∫‰πãÈó¥ÁöÑÂõ†ÊûúÂÖ≥Á≥ª„ÄÇODAÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁîüÊÄÅÁ≥ªÁªüÔºåÂåÖÊã¨Áªü‰∏ÄÁöÑËÆ≠ÁªÉËØÑ‰º∞ÊµÅÁ®ã„ÄÅÂ§öÁª¥ËØÑÂàÜÊ°ÜÊû∂„ÄÅ‰∫§‰∫íÂºèÊï∞ÊçÆÊù•Ê∫êÊé¢Á¥¢Â∑•ÂÖ∑ÂíåÂºÄÊ∫êÂ∑•ÂÖ∑ÂåÖÔºå‰ª•‰øÉËøõÊï∞ÊçÆÁ†îÁ©∂„ÄÇÈÄöËøáÂØπ120Â§ö‰∏™ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÂπøÊ≥õÂÆûÈ™åÔºåODAÊè≠Á§∫‰∫ÜÊï∞ÊçÆÂ§çÊùÇÊÄß‰∏é‰ªªÂä°ÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°ÔºåÊé®Âä®‰∫ÜÊï∞ÊçÆ‰∏≠ÂøÉ‰∫∫Â∑•Êô∫ËÉΩÁöÑÁßëÂ≠¶Á†îÁ©∂„ÄÇ', title='ÂºÄÊîæÊï∞ÊçÆÂπ≥Âè∞ÔºåÊèêÂçáÊ®°ÂûãÊï∞ÊçÆÈÄèÊòéÂ∫¶'))
[17.12.2025 06:37] Querying the API.
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.
[17.12.2025 06:37] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã SVG-–≥—Ä–∞—Ñ–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∞–≥—Ä–µ–≥–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–∞–±—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∏–∑ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏. –ü—É—Ç–µ–º —Ä–µ–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ SVG –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç vision-language –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –∞–Ω–∏–º–∞—Ü–∏–∏. –ü–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ - —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —ç—Ç–∞–ø –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏.",
  "emoji": "üé¨",
  "title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –∫–ª—é—á –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏"
}
```
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics."

[17.12.2025 06:37] Response: ```python
['CV', 'MULTIMODAL']
```
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics."

[17.12.2025 06:37] Response: ```python
['INTERPRETABILITY']
```

The paper discusses making VLM interactions with vector graphics more interpretable by recovering semantic structure from SVGs. The focus on understanding and explaining how VLMs handle vector graphics, and making their behavior more transparent and coherent, aligns with the INTERPRETABILITY topic.
[17.12.2025 06:37] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY"]


The paper discusses making VLM interactions with vector graphics more interpretable by recovering semantic structure from SVGs. The focus on understanding and explaining how VLMs handle vector graphics, and making their behavior more transparent and coherent, aligns with the INTERPRETABILITY topic.
[17.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework that enhances the animation of Scalable Vector Graphics (SVG) by recovering their semantic structure. Current vision-language models (VLMs) struggle with SVGs because they often break down coherent visual elements into smaller, less meaningful parts. The proposed method aggregates weak predictions to identify and group these parts semantically, allowing for more coherent animations. Experimental results show that this semantic recovery significantly improves the performance of VLMs in generating SVG animations and facilitates better interactions with vector graphics.","title":"Unlocking Coherent SVG Animation through Semantic Recovery"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework that enhances the animation of Scalable Vector Graphics (SVG) by recovering their semantic structure. Current vision-language models (VLMs) struggle with SVGs because they often break down coherent visual elements into smaller, less meaningful parts. The proposed method aggregates weak predictions to identify and group these parts semantically, allowing for more coherent animations. Experimental results show that this semantic recovery significantly improves the performance of VLMs in generating SVG animations and facilitates better interactions with vector graphics.', title='Unlocking Coherent SVG Animation through Semantic Recovery'))
[17.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ°ÜÊû∂ÔºåÈÄöËøáËÅöÂêàÂº±È¢ÑÊµãÊù•ÊÅ¢Â§çËØ≠‰πâÁªìÊûÑÔºå‰ªéËÄåÂÆûÁé∞ËøûË¥ØÁöÑSVGÂä®ÁîªÔºåÂπ∂ÊîπÂñÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ‰∏éÁü¢ÈáèÂõæÂΩ¢ÁöÑ‰∫§‰∫í„ÄÇÂ∞ΩÁÆ°Âú®‰ª£Á†ÅÁîüÊàêÂíåËøêÂä®ËßÑÂàíÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜËá™Âä®ÂåñÁü¢ÈáèÂõæÂΩ¢ÁöÑÂä®Áîª‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÁé∞ÊúâÁöÑVLMÁ≥ªÁªüÂ∏∏Â∏∏Â∞ÜËßÜËßâ‰∏äËøûË¥ØÁöÑÈÉ®ÂàÜÂàÜÂâ≤Êàê‰ΩéÁ∫ßÂΩ¢Áä∂ÔºåÂØºËá¥Êó†Ê≥ïÊúâÊïàÊåáÂØºÂÖÉÁ¥†ÁöÑÁßªÂä®„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÈÄöËøáÁªüËÆ°ËÅöÂêàÂ§ö‰∏™Âº±È¢ÑÊµãÔºåÁ®≥ÂÆöÂú∞Êé®Êñ≠Âá∫ËØ≠‰πâÔºå‰ªéËÄå‰ΩøVLMËÉΩÂ§üÁîüÊàêÊõ¥ÂÖ∑ËøûË¥ØÊÄßÁöÑÂä®Áîª„ÄÇ","title":"ÊÅ¢Â§çËØ≠‰πâÁªìÊûÑÔºåÊèêÂçáSVGÂä®ÁîªËøûË¥ØÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ°ÜÊû∂ÔºåÈÄöËøáËÅöÂêàÂº±È¢ÑÊµãÊù•ÊÅ¢Â§çËØ≠‰πâÁªìÊûÑÔºå‰ªéËÄåÂÆûÁé∞ËøûË¥ØÁöÑSVGÂä®ÁîªÔºåÂπ∂ÊîπÂñÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ‰∏éÁü¢ÈáèÂõæÂΩ¢ÁöÑ‰∫§‰∫í„ÄÇÂ∞ΩÁÆ°Âú®‰ª£Á†ÅÁîüÊàêÂíåËøêÂä®ËßÑÂàíÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜËá™Âä®ÂåñÁü¢ÈáèÂõæÂΩ¢ÁöÑÂä®Áîª‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÁé∞ÊúâÁöÑVLMÁ≥ªÁªüÂ∏∏Â∏∏Â∞ÜËßÜËßâ‰∏äËøûË¥ØÁöÑÈÉ®ÂàÜÂàÜÂâ≤Êàê‰ΩéÁ∫ßÂΩ¢Áä∂ÔºåÂØºËá¥Êó†Ê≥ïÊúâÊïàÊåáÂØºÂÖÉÁ¥†ÁöÑÁßªÂä®„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÈÄöËøáÁªüËÆ°ËÅöÂêàÂ§ö‰∏™Âº±È¢ÑÊµãÔºåÁ®≥ÂÆöÂú∞Êé®Êñ≠Âá∫ËØ≠‰πâÔºå‰ªéËÄå‰ΩøVLMËÉΩÂ§üÁîüÊàêÊõ¥ÂÖ∑ËøûË¥ØÊÄßÁöÑÂä®Áîª„ÄÇ', title='ÊÅ¢Â§çËØ≠‰πâÁªìÊûÑÔºåÊèêÂçáSVGÂä®ÁîªËøûË¥ØÊÄß'))
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#alignment", "#agents", "#rlhf", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ LLM –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —è–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –æ–± –∏–Ω—Ç–µ–Ω—Ç–∞—Ö", "desc": "RecGPT-V2 —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üìä", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å–∫—É—Å—Å—Ç–≤–æ: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π—Å—è pipeline –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü", "desc": "ShowTable ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#long_context"], "emoji": "üé¨", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MemFlow ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ì–∏–±–∫–æ–µ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LLM", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç VersatileFFN ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∏—Ö –ø
[17.12.2025 06:37] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "A4-Agent ‚Äî —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è affordance (–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π) —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∑–∞–¥–∞—á—É –Ω–∞ —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#training", "#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π 4D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "SS4D ‚Äî —ç—Ç–æ –Ω–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ 3D –æ–±—ä–µ–∫—Ç—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏
[17.12.2025 06:37] Using data from previous issue: {"categories": ["#rlhf", "#3d", "#training", "#multimodal"], "emoji": "üé®", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ControlNet", "desc": "Steer3D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏, —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥. –ê–≤—Ç–æ—Ä—ã –∞
[17.12.2025 06:37] Querying the API.
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.
[17.12.2025 06:37] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TAT (Task-Adaptive Transformer) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º. –û—Å–Ω–æ–≤–Ω–æ–π –≤–∫–ª–∞–¥ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –¥–≤—É—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –≤–µ—Å–æ–≤ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏. –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∑–∞–¥–∞—á –∏ –∏—Ö –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –µ–¥–∏–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞ –≤ —Ç—Ä—ë—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö: —Å–∏–Ω—Ç–µ–∑ PET, —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ CT –∏ —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ MRI.",
  "emoji": "üè•",
  "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∑–∞–¥–∞—á–∏"
}
```
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT."

[17.12.2025 06:37] Response: ```python
["HEALTHCARE", "ARCHITECTURE", "TRAINING"]
```

**Justification:**

1. **HEALTHCARE**: The paper explicitly focuses on medical image restoration (MedIR), addressing medical imaging tasks including PET synthesis, CT denoising, and MRI super-resolution - all direct applications of ML to medical/healthcare domains.

2. **ARCHITECTURE**: The paper proposes a novel neural architecture - the task-adaptive Transformer (TAT) framework - which includes novel components like task-adaptive weight generation strategy and task-adaptive loss balancing strategy.

3. **TRAINING**: The paper addresses training challenges including task interference and task imbalance, proposing methods to dynamically adjust task-specific weights and loss balances during model training/optimization.
[17.12.2025 06:37] Error. Failed to parse JSON from LLM. ["HEALTHCARE", "ARCHITECTURE", "TRAINING"]


**Justification:**

1. **HEALTHCARE**: The paper explicitly focuses on medical image restoration (MedIR), addressing medical imaging tasks including PET synthesis, CT denoising, and MRI super-resolution - all direct applications of ML to medical/healthcare domains.

2. **ARCHITECTURE**: The paper proposes a novel neural architecture - the task-adaptive Transformer (TAT) framework - which includes novel components like task-adaptive weight generation strategy and task-adaptive loss balancing strategy.

3. **TRAINING**: The paper addresses training challenges including task interference and task imbalance, proposing methods to dynamically adjust task-specific weights and loss balances during model training/optimization.
[17.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT."

[17.12.2025 06:38] Response: ```python
["OPTIMIZATION"]
```

**Reasoning:** The paper focuses on addressing optimization challenges in multi-task medical image restoration through dynamic weight adjustment and loss balancing strategies. The core contributions involve optimizing task-specific parameters and loss weights to improve training efficiency and performance, which directly relates to training optimization methods.
[17.12.2025 06:38] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


**Reasoning:** The paper focuses on addressing optimization challenges in multi-task medical image restoration through dynamic weight adjustment and loss balancing strategies. The core contributions involve optimizing task-specific parameters and loss weights to improve training efficiency and performance, which directly relates to training optimization methods.
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents a Task-Adaptive Transformer (TAT) framework designed to improve medical image restoration (MedIR) by addressing task interference and task imbalance. It introduces a task-adaptive weight generation strategy that creates specific weights for each task, reducing conflicts during gradient updates. Additionally, a task-adaptive loss balancing strategy is implemented to adjust loss weights according to the learning difficulties of each task, ensuring balanced optimization. The TAT framework demonstrates superior performance in multiple MedIR tasks, including PET synthesis, CT denoising, and MRI super-resolution, outperforming existing models.","title":"Dynamic Adaptation for Superior Medical Image Restoration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents a Task-Adaptive Transformer (TAT) framework designed to improve medical image restoration (MedIR) by addressing task interference and task imbalance. It introduces a task-adaptive weight generation strategy that creates specific weights for each task, reducing conflicts during gradient updates. Additionally, a task-adaptive loss balancing strategy is implemented to adjust loss weights according to the learning difficulties of each task, ensuring balanced optimization. The TAT framework demonstrates superior performance in multiple MedIR tasks, including PET synthesis, CT denoising, and MRI super-resolution, outperforming existing models.', title='Dynamic Adaptation for Superior Medical Image Restoration'))
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªªÂä°Ëá™ÈÄÇÂ∫îÂèòÊç¢Âô®ÔºàTATÔºâÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÂåªÂ≠¶ÂõæÂÉèÊÅ¢Â§ç‰∏≠ÁöÑÊåëÊàò„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥‰ªªÂä°ÁâπÂÆöÁöÑÊùÉÈáçÂíåÊçüÂ§±Âπ≥Ë°°ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂú®‰∏çÂêåÂåªÂ≠¶ÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇTATÂºïÂÖ•‰∫Ü‰ªªÂä°Ëá™ÈÄÇÂ∫îÊùÉÈáçÁîüÊàêÁ≠ñÁï•Ôºå‰ª•ÂáèÂ∞ë‰ªªÂä°Âπ≤Êâ∞ÔºåÂπ∂ÈÄöËøá‰ªªÂä°Ëá™ÈÄÇÂ∫îÊçüÂ§±Âπ≥Ë°°Á≠ñÁï•Êù•Â∫îÂØπ‰ªªÂä°‰∏çÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTATÂú®PETÂêàÊàê„ÄÅCTÂéªÂô™ÂíåMRIË∂ÖÂàÜËæ®ÁéáÁ≠â‰ªªÂä°‰∏≠ÂùáËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ","title":"Âä®ÊÄÅÈÄÇÂ∫îÔºåÊèêÂçáÂåªÂ≠¶ÂõæÂÉèÊÅ¢Â§çÊïàÊûú"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªªÂä°Ëá™ÈÄÇÂ∫îÂèòÊç¢Âô®ÔºàTATÔºâÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÂåªÂ≠¶ÂõæÂÉèÊÅ¢Â§ç‰∏≠ÁöÑÊåëÊàò„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥‰ªªÂä°ÁâπÂÆöÁöÑÊùÉÈáçÂíåÊçüÂ§±Âπ≥Ë°°ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂú®‰∏çÂêåÂåªÂ≠¶ÂõæÂÉèÊÅ¢Â§ç‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇTATÂºïÂÖ•‰∫Ü‰ªªÂä°Ëá™ÈÄÇÂ∫îÊùÉÈáçÁîüÊàêÁ≠ñÁï•Ôºå‰ª•ÂáèÂ∞ë‰ªªÂä°Âπ≤Êâ∞ÔºåÂπ∂ÈÄöËøá‰ªªÂä°Ëá™ÈÄÇÂ∫îÊçüÂ§±Âπ≥Ë°°Á≠ñÁï•Êù•Â∫îÂØπ‰ªªÂä°‰∏çÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTATÂú®PETÂêàÊàê„ÄÅCTÂéªÂô™ÂíåMRIË∂ÖÂàÜËæ®ÁéáÁ≠â‰ªªÂä°‰∏≠ÂùáËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ', title='Âä®ÊÄÅÈÄÇÂ∫îÔºåÊèêÂçáÂåªÂ≠¶ÂõæÂÉèÊÅ¢Â§çÊïàÊûú'))
[17.12.2025 06:38] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É—Å–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Sparse-LaViDa –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É—Å–µ—á–µ–Ω–∏—è –º–∞—Å–∫–∏—Ä
[17.12.2025 06:38] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–û–ª–º–æ 3: –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ–π Olmo 3 ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–ª—Ä–¥ –∏ 32 –º–ª—Ä–¥ –ø–∞
[17.12.2025 06:38] Using data from previous issue: {"categories": ["#multimodal", "#robotics", "#rl", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "EVOLVE-VLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫
[17.12.2025 06:38] Using data from previous issue: {"categories": ["#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–≥—Ä—É–∑–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç RePo ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å
[17.12.2025 06:38] Querying the API.
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.
[17.12.2025 06:38] Response: ```json
{
  "desc": "Zoom-Zero ‚Äî —ç—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è —Ç–∏–ø–∞ \"–≥—Ä—É–±—ã–π-–∫-—Ç–æ—á–Ω–æ–º—É\", –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –≤–∏–¥–µ–æ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–æ–π. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –≥—Ä—É–ø–ø, –¥–æ–±–∞–≤–ª—è—è –Ω–∞–≥—Ä–∞–¥—É –∑–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –º–∞—Å—à—Ç–∞–±–∞ –∏ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ —Ç–æ–∫–µ–Ω–∞–º. –≠—Ç–∞ –Ω–∞–≥—Ä–∞–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ–Ω–∫—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.",
  "emoji": "üé¨",
  "title": "–û—Ç –æ–±—â–µ–≥–æ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É: —Ç–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã"
}
```
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks."

[17.12.2025 06:38] Response: ```python
["VIDEO", "RLHF", "MULTIMODAL", "BENCHMARK"]
```
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks."

[17.12.2025 06:38] Response: ```python
['HALLUCINATIONS', 'OPTIMIZATION', 'LONG_CONTEXT']
```
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Zoom-Zero, a new framework designed to improve grounded video question answering (GVQA) by enhancing both temporal grounding and answer accuracy. It addresses the limitations of existing large video-language models (LVLMs) that struggle with temporal awareness and often mislocalize answers. Zoom-Zero employs a coarse-to-fine approach, first identifying relevant video segments and then zooming in on key frames for detailed visual verification. Key innovations include a zoom-in accuracy reward for validating temporal predictions and token-selective credit assignment to better manage reward signals, resulting in significant improvements in both grounding and answer accuracy across various benchmarks.","title":"Zoom-Zero: Enhancing Video Question Answering with Precision and Context"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Zoom-Zero, a new framework designed to improve grounded video question answering (GVQA) by enhancing both temporal grounding and answer accuracy. It addresses the limitations of existing large video-language models (LVLMs) that struggle with temporal awareness and often mislocalize answers. Zoom-Zero employs a coarse-to-fine approach, first identifying relevant video segments and then zooming in on key frames for detailed visual verification. Key innovations include a zoom-in accuracy reward for validating temporal predictions and token-selective credit assignment to better manage reward signals, resulting in significant improvements in both grounding and answer accuracy across various benchmarks.', title='Zoom-Zero: Enhancing Video Question Answering with Precision and Context'))
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Zoom-Zero ÊòØ‰∏Ä‰∏™Á≤óÂà∞ÁªÜÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáËßÜÈ¢ëÈóÆÁ≠îÁöÑÂáÜÁ°ÆÊÄßÂíåÊó∂Èó¥ÂÆö‰ΩçËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÂºïÂÖ•ÊîæÂ§ßÂáÜÁ°ÆÊÄßÂ•ñÂä±ÂíåÈÄâÊã©ÊÄß‰ø°Áî®ÂàÜÈÖçÊù•Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïÂú®Êó∂Èó¥ÂÆö‰Ωç‰∏äÁöÑ‰∏çË∂≥„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖàÂÆö‰Ωç‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊó∂Èó¥ÊÆµÔºåÁÑ∂ÂêéËÅöÁÑ¶‰∫éÊúÄÈáçË¶ÅÁöÑÂ∏ßËøõË°åÊõ¥Á≤æÁªÜÁöÑËßÜËßâÈ™åËØÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåZoom-Zero Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊó∂Èó¥ÂÆö‰ΩçÂíåÁ≠îÊ°àÂáÜÁ°ÆÊÄß„ÄÇ","title":"Zoom-ZeroÔºöÊèêÂçáËßÜÈ¢ëÈóÆÁ≠îÁöÑÊó∂Èó¥ÂÆö‰Ωç‰∏éÂáÜÁ°ÆÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Zoom-Zero ÊòØ‰∏Ä‰∏™Á≤óÂà∞ÁªÜÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáËßÜÈ¢ëÈóÆÁ≠îÁöÑÂáÜÁ°ÆÊÄßÂíåÊó∂Èó¥ÂÆö‰ΩçËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÂºïÂÖ•ÊîæÂ§ßÂáÜÁ°ÆÊÄßÂ•ñÂä±ÂíåÈÄâÊã©ÊÄß‰ø°Áî®ÂàÜÈÖçÊù•Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïÂú®Êó∂Èó¥ÂÆö‰Ωç‰∏äÁöÑ‰∏çË∂≥„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖàÂÆö‰Ωç‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑÊó∂Èó¥ÊÆµÔºåÁÑ∂ÂêéËÅöÁÑ¶‰∫éÊúÄÈáçË¶ÅÁöÑÂ∏ßËøõË°åÊõ¥Á≤æÁªÜÁöÑËßÜËßâÈ™åËØÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåZoom-Zero Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊó∂Èó¥ÂÆö‰ΩçÂíåÁ≠îÊ°àÂáÜÁ°ÆÊÄß„ÄÇ', title='Zoom-ZeroÔºöÊèêÂçáËßÜÈ¢ëÈóÆÁ≠îÁöÑÊó∂Èó¥ÂÆö‰Ωç‰∏éÂáÜÁ°ÆÊÄß'))
[17.12.2025 06:38] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º
[17.12.2025 06:38] Using data from previous issue: {"categories": ["#training", "#security", "#benchmark", "#alignment", "#interpretability"], "emoji": "üî™", "ru": {"title": "–•–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–æ–≤: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ç—ã—Ä—ë—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ—Ç–∫–∞–∑–∞ 
[17.12.2025 06:38] Querying the API.
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.
[17.12.2025 06:38] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Cascade RL) –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –∏–Ω–∂–µ–Ω–µ—Ä–∏—é –∏ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å Nemotron-Cascade –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —É—á–∏—Ç–µ–ª—å—Å–∫—É—é –º–æ–¥–µ–ª—å DeepSeek-R1 –Ω–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ RLHF –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞–¥–∏–∏ –¥–æ–º–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–µ –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö.",
  "emoji": "üèÜ",
  "title": "–ö–∞—Å–∫–∞–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è"
}
```
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes."

[17.12.2025 06:38] Response: ```python
["RL", "RLHF", "BENCHMARK", "TRAINING"]
```
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes."

[17.12.2025 06:38] Response: ```python
["REASONING", "OPTIMIZATION", "ALIGNMENT"]
```
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Cascaded Domain-Wise Reinforcement Learning (Cascade RL) to improve general-purpose reasoning models. It addresses challenges in reinforcement learning caused by variability in response lengths and verification times across different domains. By using a sequential approach to domain-specific reinforcement learning, Cascade RL simplifies the training process and enhances model performance. The proposed model, Nemotron-Cascade, demonstrates superior reasoning capabilities and outperforms its teacher model in competitive coding benchmarks.","title":"Cascade RL: Simplifying Training for Superior Reasoning Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Cascaded Domain-Wise Reinforcement Learning (Cascade RL) to improve general-purpose reasoning models. It addresses challenges in reinforcement learning caused by variability in response lengths and verification times across different domains. By using a sequential approach to domain-specific reinforcement learning, Cascade RL simplifies the training process and enhances model performance. The proposed model, Nemotron-Cascade, demonstrates superior reasoning capabilities and outperforms its teacher model in competitive coding benchmarks.', title='Cascade RL: Simplifying Training for Superior Reasoning Models'))
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ∫ßËÅîÈ¢ÜÂüüÂº∫ÂåñÂ≠¶‰π†ÔºàCascade RLÔºâÁöÑÊñπÊ≥ïÔºå‰ª•Â¢ûÂº∫ÈÄöÁî®Êé®ÁêÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑË°®Áé∞ÔºåÂπ∂Âú®ÁºñÁ†ÅÁ´ûËµõ‰∏≠Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇCascade RLÈÄöËøáÈ°∫Â∫èÁöÑÈ¢ÜÂüüÂº∫ÂåñÂ≠¶‰π†ÔºåÁÆÄÂåñ‰∫ÜÂ∑•Á®ãÂ§çÊùÇÊÄßÔºåÂπ∂Âú®‰∏çÂêåÈ¢ÜÂüü‰∏≠‰øùÊåÅ‰∫ÜÈ´òÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑ14BÊ®°ÂûãÂú®ÁªèËøáÂº∫ÂåñÂ≠¶‰π†ÂêéÔºåÂú®LiveCodeBench v5/v6/Pro‰∏äË∂ÖË∂ä‰∫ÜÂÖ∂SFTÊïôÂ∏àÊ®°ÂûãÔºåÂπ∂Âú®2025Âπ¥ÂõΩÈôÖ‰ø°ÊÅØÂ≠¶Â••ÊûóÂåπÂÖãÁ´ûËµõ‰∏≠Ëé∑ÂæóÈì∂Áâå„ÄÇ","title":"Á∫ßËÅîÈ¢ÜÂüüÂº∫ÂåñÂ≠¶‰π†ÔºöÊèêÂçáÈÄöÁî®Êé®ÁêÜÊ®°ÂûãÁöÑÂà©Âô®"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ∫ßËÅîÈ¢ÜÂüüÂº∫ÂåñÂ≠¶‰π†ÔºàCascade RLÔºâÁöÑÊñπÊ≥ïÔºå‰ª•Â¢ûÂº∫ÈÄöÁî®Êé®ÁêÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑË°®Áé∞ÔºåÂπ∂Âú®ÁºñÁ†ÅÁ´ûËµõ‰∏≠Ë∂ÖË∂ä‰∫ÜÊïôÂ∏àÊ®°Âûã„ÄÇCascade RLÈÄöËøáÈ°∫Â∫èÁöÑÈ¢ÜÂüüÂº∫ÂåñÂ≠¶‰π†ÔºåÁÆÄÂåñ‰∫ÜÂ∑•Á®ãÂ§çÊùÇÊÄßÔºåÂπ∂Âú®‰∏çÂêåÈ¢ÜÂüü‰∏≠‰øùÊåÅ‰∫ÜÈ´òÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑ14BÊ®°ÂûãÂú®ÁªèËøáÂº∫ÂåñÂ≠¶‰π†ÂêéÔºåÂú®LiveCodeBench v5/v6/Pro‰∏äË∂ÖË∂ä‰∫ÜÂÖ∂SFTÊïôÂ∏àÊ®°ÂûãÔºåÂπ∂Âú®2025Âπ¥ÂõΩÈôÖ‰ø°ÊÅØÂ≠¶Â••ÊûóÂåπÂÖãÁ´ûËµõ‰∏≠Ëé∑ÂæóÈì∂Áâå„ÄÇ', title='Á∫ßËÅîÈ¢ÜÂüüÂº∫ÂåñÂ≠¶‰π†ÔºöÊèêÂçáÈÄöÁî®Êé®ÁêÜÊ®°ÂûãÁöÑÂà©Âô®'))
[17.12.2025 06:38] Querying the API.
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.
[17.12.2025 06:38] Response: ```json
{
  "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É —à—É–º–∞ –≤ –º–∞—Å–∫–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—É—Ç—ë–º –≤—ã—è–≤–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–ø–æ—Ä–Ω—ã—Ö –º–∞—Å–æ–∫ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–∏—Ö –ø—Ä–∏–æ—Ä–∏—Ç–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∞ Sparse-To-Dense Distillation —Å Temporal DropLoss –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –º–∞—Å–æ–∫ –Ω–∞ –≤–µ—Å—å –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.",
  "emoji": "üé¨",
  "title": "–í–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å"
}
```
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks."

[17.12.2025 06:38] Response: ```python
["VIDEO", "CV", "DATASET"]
```
[17.12.2025 06:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks."

[17.12.2025 06:38] Response: ```python
["SYNTHETIC"]
```

The paper is classified as SYNTHETIC because it explicitly discusses the limitations of synthetic video data (generated from object-centric image datasets like ImageNet) and proposes an alternative approach using real video data instead. The paper contrasts their method with existing approaches that rely on synthetic video synthesis, making synthetic data generation a central topic of discussion.
[17.12.2025 06:38] Error. Failed to parse JSON from LLM. ["SYNTHETIC"]


The paper is classified as SYNTHETIC because it explicitly discusses the limitations of synthetic video data (generated from object-centric image datasets like ImageNet) and proposes an alternative approach using real video data instead. The paper contrasts their method with existing approaches that rely on synthetic video synthesis, making synthetic data generation a central topic of discussion.
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel unsupervised video instance segmentation model that utilizes real video data instead of synthetic data, which often fails to capture realistic motion dynamics. The model improves temporal coherence by identifying high-quality keymasks using deep motion priors, addressing the noise and variability in single-frame segmentations. It employs a Sparse-To-Dense Distillation technique to enhance the training process, allowing for better mask propagation across frames. As a result, the proposed method achieves superior performance compared to existing techniques on multiple benchmarks.","title":"Real Data, Real Motion: Advancing Video Instance Segmentation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel unsupervised video instance segmentation model that utilizes real video data instead of synthetic data, which often fails to capture realistic motion dynamics. The model improves temporal coherence by identifying high-quality keymasks using deep motion priors, addressing the noise and variability in single-frame segmentations. It employs a Sparse-To-Dense Distillation technique to enhance the training process, allowing for better mask propagation across frames. As a result, the proposed method achieves superior performance compared to existing techniques on multiple benchmarks.', title='Real Data, Real Motion: Advancing Video Instance Segmentation'))
[17.12.2025 06:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÁõëÁù£ËßÜÈ¢ëÂÆû‰æãÂàÜÂâ≤Ê®°ÂûãÔºåËØ•Ê®°Âûã‰ªÖ‰ΩøÁî®ÁúüÂÆûËßÜÈ¢ëÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂÖãÊúç‰∫Ü‰ª•ÂæÄ‰æùËµñÂêàÊàêËßÜÈ¢ëÊï∞ÊçÆÁöÑÂ±ÄÈôê„ÄÇÈÄöËøáÂà©Áî®Ê∑±Â∫¶ËøêÂä®ÂÖàÈ™åÔºåÊàë‰ª¨Âú®ËßÜÈ¢ë‰∏≠ËØÜÂà´È´òË¥®ÈáèÁöÑÂÖ≥ÈîÆÊé©Á†ÅÔºå‰ªéËÄåÂª∫Á´ãÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÊé•ÁùÄÔºåÈááÁî®Á®ÄÁñèÂà∞ÂØÜÈõÜÁöÑËí∏È¶èÊñπÊ≥ïÔºåÂà©Áî®Á®ÄÁñèÁöÑÂÖ≥ÈîÆÊé©Á†Å‰º™Ê†áÊ≥®Êù•ËÆ≠ÁªÉÂàÜÂâ≤Ê®°ÂûãÔºåÂÆûÁé∞ÈöêÂºèÊé©Á†Å‰º†Êí≠„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØ„ÄÇ","title":"ÁúüÂÆûËßÜÈ¢ëÊï∞ÊçÆÈ©±Âä®ÁöÑÊó†ÁõëÁù£ÂÆû‰æãÂàÜÂâ≤Êñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÁõëÁù£ËßÜÈ¢ëÂÆû‰æãÂàÜÂâ≤Ê®°ÂûãÔºåËØ•Ê®°Âûã‰ªÖ‰ΩøÁî®ÁúüÂÆûËßÜÈ¢ëÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂÖãÊúç‰∫Ü‰ª•ÂæÄ‰æùËµñÂêàÊàêËßÜÈ¢ëÊï∞ÊçÆÁöÑÂ±ÄÈôê„ÄÇÈÄöËøáÂà©Áî®Ê∑±Â∫¶ËøêÂä®ÂÖàÈ™åÔºåÊàë‰ª¨Âú®ËßÜÈ¢ë‰∏≠ËØÜÂà´È´òË¥®ÈáèÁöÑÂÖ≥ÈîÆÊé©Á†ÅÔºå‰ªéËÄåÂª∫Á´ãÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇÊé•ÁùÄÔºåÈááÁî®Á®ÄÁñèÂà∞ÂØÜÈõÜÁöÑËí∏È¶èÊñπÊ≥ïÔºåÂà©Áî®Á®ÄÁñèÁöÑÂÖ≥ÈîÆÊé©Á†Å‰º™Ê†áÊ≥®Êù•ËÆ≠ÁªÉÂàÜÂâ≤Ê®°ÂûãÔºåÂÆûÁé∞ÈöêÂºèÊé©Á†Å‰º†Êí≠„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØ„ÄÇ', title='ÁúüÂÆûËßÜÈ¢ëÊï∞ÊçÆÈ©±Âä®ÁöÑÊó†ÁõëÁù£ÂÆû‰æãÂàÜÂâ≤Êñ∞Á™ÅÁ†¥'))
[17.12.2025 06:38] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#multimodal", "#agents", "#dataset"], "emoji": "üåç", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —É–º–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∑–∞–º–µ–Ω–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω
[17.12.2025 06:38] Renaming data file.
[17.12.2025 06:38] Renaming previous data. hf_papers.json to ./d/2025-12-17.json
[17.12.2025 06:38] Saving new data file.
[17.12.2025 06:38] Generating page.
[17.12.2025 06:38] Renaming previous page.
[17.12.2025 06:38] Renaming previous data. index.html to ./d/2025-12-17.html
[17.12.2025 06:38] Writing result.
[17.12.2025 06:38] Renaming log file.
[17.12.2025 06:38] Renaming previous data. log.txt to ./logs/2025-12-17_last_log.txt
