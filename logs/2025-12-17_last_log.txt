[17.12.2025 13:35] Read previous papers.
[17.12.2025 13:35] Generating top page (month).
[17.12.2025 13:35] Writing top page (month).
[17.12.2025 14:25] Read previous papers.
[17.12.2025 14:25] Get feed.
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14691
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14614
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13281
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12675
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13660
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14051
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14336
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12980
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14503
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13303
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14699
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14531
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14284
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13678
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14442
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13607
[17.12.2025 14:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.13399
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13961
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14666
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14273
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14067
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14008
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13525
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14550
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14697
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14391
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14620
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14014
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13655
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14440
[17.12.2025 14:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10945
[17.12.2025 14:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.07328
[17.12.2025 14:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.12.2025 14:25] No deleted papers detected.
[17.12.2025 14:25] Downloading and parsing papers (pdf, html). Total: 32.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14691.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14691.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14691.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14614.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14614.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14614.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13281.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13281.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13281.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.12675.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.12675.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.12675.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13660.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13660.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13660.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14051.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14051.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14051.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14336.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14336.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14336.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.12980.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.12980.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.12980.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14503.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14503.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14503.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13303.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13303.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13303.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14699.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14699.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14699.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14531.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14531.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14531.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14284.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14284.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14284.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13678.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13678.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13678.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14442.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14442.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14442.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13607.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13607.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13607.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13399.
[17.12.2025 14:25] Downloading paper 2512.13399 from https://arxiv.org/pdf/2512.13399v1...
[17.12.2025 14:25] Extracting affiliations from text.
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 9 9 3 3 1 . 2 1 5 2 : r a Sitao Cheng1, Tianle Li2, Xuhan Huang3, Xunjian Yin4, Difan Zou2 1University of Waterloo 3The Chinese University of Hong Kong, Shenzhen 4Duke University 2The University of Hong Kong sitao.cheng@uwaterloo.ca tianleli@connect.hku.hk xuhanhuang@link.cuhk.edu.cn "
[17.12.2025 14:25] Response: ```python
[
    "University of Waterloo",
    "The University of Hong Kong",
    "The Chinese University of Hong Kong, Shenzhen",
    "Duke University"
]
```
[17.12.2025 14:25] Deleting PDF ./assets/pdf/2512.13399.pdf.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13961.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13961.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13961.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14666.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14666.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14666.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14273.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14273.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14273.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14067.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14067.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14067.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14008.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14008.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14008.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13525.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13525.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13525.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14550.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14550.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14550.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14697.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14697.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14697.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14391.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14391.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14391.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14620.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14620.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14620.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14014.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14014.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14014.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.13655.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.13655.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.13655.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.14440.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.14440.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.14440.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.10945.
[17.12.2025 14:25] Extra JSON file exists (./assets/json/2512.10945.json), skip PDF parsing.
[17.12.2025 14:25] Paper image links file exists (./assets/img_data/2512.10945.json), skip HTML parsing.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Downloading and parsing paper https://huggingface.co/papers/2512.07328.
[17.12.2025 14:25] Downloading paper 2512.07328 from https://arxiv.org/pdf/2512.07328v1...
[17.12.2025 14:25] Extracting affiliations from text.
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 8 2 3 7 0 . 2 1 5 2 : r ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation Yu-Wing Tai Figure 1. Overview of ContextAnyone. Given reference image and text prompt, our model generates character-consistent videos that preserve visual details across diverse scenes, while prior methods struggle to retain all elements from the reference. Pink boxes highlight key details such as the chef hat, collar shape, and pant. Green boxes indicate regions where these details are faithfully preserved, whereas red boxes mark inconsistencies, such as the collar mismatch in the lower left. Many other inconsistencies are omitted for simplicity. "
[17.12.2025 14:25] Response: ```python
[]
```
[17.12.2025 14:25] Extracting affiliations from text.
[17.12.2025 14:25] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 8 2 3 7 0 . 2 1 5 2 : r ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video GenerationYu-Wing TaiFigure 1. Overview of ContextAnyone. Given reference image and text prompt, our model generates character-consistent videos that preserve visual details across diverse scenes, while prior methods struggle to retain all elements from the reference. Pink boxes highlight key details such as the chef hat, collar shape, and pant. Green boxes indicate regions where these details are faithfully preserved, whereas red boxes mark inconsistencies, such as the collar mismatch in the lower left. Many other inconsistencies are omitted for simplicity.fail identity but Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains major challenge. Existing personalization methods often focus on facial to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose ContextAnyone, context-aware diffusion framework that achieves character-consistent video generation from text and single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into DiT-based diffusion backbone through novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing referenceto-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: https://github.com/ziyang1106/ContextAnyone. 1. Introduction Recent advances in text-to-video (T2V) generation[5, 6, 15, 23, 3335, 41] have enabled realistic and temporally coherent videos from natural language descriptions. However, generating characters that remain visually consistent across scenes and motions remains challenging. In real-world storytelling and film production, character is defined not only by facial identity but also by visual context such as hairstyle, outfit, and body shape. Preserving this context is essential for narrative continuity and visual coherence. As shown in the second and third rows of Figure 1, failing to preserve context leads to inconsistent clothing, causing perceptual discontinuity and reduced realism. Although recent diffusion-based models have improved fidelity and motion coherence, many personalization methods [24, 42, 43, 45] focus on identity transfer by injecting face features from face encoders. These approaches often capture only partial identity cues while neglecting contextual appearance. Recent works [10, 11, 20, 25] move beyond traditional identity-injection frameworks by introducing pixel-level [10] or channel-level [11] fusion to enhance contextual and subject consistency. However, such fusion does not ensure full utilization of reference information, and the model may still suffer from identity drift with complex visual structures, as shown in Figure 1. Furthermore, conventional cross-attention in video diffusion models offers limited control over how reference information influences temporal tokens, often causing unstable identity transfer during denoising. We present ContextAnyone, context-aware diffusion framework designed to generate character-consistent videos from text and single reference image. Our approach simultaneously reconstructs the reference image and generates new video frames. Reconstructing the reference image enables the model to fully perceive and understand all visual cues of the reference identity. Meanwhile, the reconstructed result can also serve as an anchor that provides detailed appearance inforamtion for subsequent video generation. To efficiently inject this anchor information into newly generated frames, we propose an attention modulation strategy, including an Emphasize-Attention module that selectively reinforces reference-aware information during denoising. To avoid the temporal collapse caused by reconstructing the reference image and generating new video frames simultaneously, we introduce Gap-RoPE to explicitly separate the positional embeddings of reference and video tokens for stable temporal modeling. In addition, we introduce dual encoders that extract complementary semantic and fine-grained features. Together, these components enable our model to produce coherent, identity-consistent, and context-preserving character videos under complex motions and scene variations. Comprehensive experiments demonstrate that ContextAnyone achieves superior performance compared with state-of-the-art reference-to-video methods under the same parameter scale. As shown in Figure 1, our model generates realistic and temporally stable videos that preserve visual context and identity fidelity across diverse scenes. Quantitative results confirm higher identity consistency and visual quality, while ablation studies highlight the effectiveness of the various designs. Our main contributions are summarized as follows: We propose ContextAnyone, context-aware framework that jointly reconstructs the reference image and generates new frames, enabling comprehensive identity transferring. We introduce an attention modulation strategy with Emphasize-Attention to efficiently inject reference information and reinforce reference-aware cues. We design Gap-RoPE to explicitly separate positional embeddings of reference and video tokens for improved temporal stability, and employ dual encoders to capture complementary semantic and fine-grained visual features. Our method achieve SOTA performance among others under same parameter scale. 2. Related Works Diffusion Models for Video Generation Diffusion models have recently driven major advances in text-to-video (T2V) synthesis. Early methods [1, 12, 21, 31] extend pretrained image diffusion frameworks such as Stable Diffusion [30] by inserting lightweight temporal attention or motion adapter layers [16]. These extensions reuse strong spatial priors learned from large image datasets, enabling short video g"
[17.12.2025 14:25] Mistral response. {"id": "94c3226f4a4a4f0d94b787738551b5ec", "created": 1765981516, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1251, "total_tokens": 1257, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[17.12.2025 14:25] Response: ```python
[]
```
[17.12.2025 14:25] Deleting PDF ./assets/pdf/2512.07328.pdf.
[17.12.2025 14:25] Success.
[17.12.2025 14:25] Enriching papers with extra data.
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 0. MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  					AI-generated summary 				 Video foundation models generate visually realistic and temporally coherent content...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 1. WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  					AI-generated summary 				 This paper presents WorldPlay, a streaming...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 2. The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  					AI-generated summary 				 Recent advances ...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 3. Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  					AI-generated summary 				 Subject-driven image generation has advanced from single- to multi-s...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 4. RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  					AI-generated summary 				 Spatial traci...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 5. OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 6. A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to gr...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 7. Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.  					AI-generated summary 				 Vector Similarity Search (VSS) in high-dimensional sp...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 8. RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 9. ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  					AI-generated summary 				 While existing generation and un...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 10. MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  					AI-generated summary 				 The core challenge for streaming video generation is maintaining the co...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 11. The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting t...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 12. SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynami...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 13. Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  					AI-generated summary 				 Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and rob...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 14. A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  					AI-generated summary 				 Affordance prediction, which identifies interaction regions on objects based o...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 15. Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning mo...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 16. A bilevel differentiable approach for evolving reward functions in reinforcement learning enhances agent performance across various domains by capturing task structure through reinforcement learning.  					AI-generated summary 				 The design of effective reward functions presents a central and ofte...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 17. Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  					AI-generated summary 				 We introduce Olmo 3, a family of state-of-the-art, fu...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 18. EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving tru...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 19. Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize rel...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 20. AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  					AI-generated summary 				 Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, ...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 21. Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance acr...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 22. Janus is a scalable Mixture-of-Experts (MoE) inference system that disaggregates attention and expert modules for independent scaling, improving throughput and latency.  					AI-generated summary 				 Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dyna...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 23. A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recov...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 24. Lattice coding provides a unified framework for non-parametric quantization, with Leech lattice-based quantization achieving superior performance in image tokenization, compression, and generation tasks.  					AI-generated summary 				 Non-parametric quantization has received much attention due to i...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 25. RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to mode...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 26. JMMMU-Pro, an image-based Japanese multi-discipline multimodal understanding benchmark, challenges open-source large multimodal models through integrated visual-textual understanding and is constructed using Vibe Benchmark Construction, a cost-effective method leveraging realistic image generation. ...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 27. A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work la...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 28. Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  					AI-generated summary 				 Safety alignment mechan...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 29. An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance ...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 30. A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  					AI-generated summary 				 This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focus...
[17.12.2025 14:25] ********************************************************************************
[17.12.2025 14:25] Abstract 31. ContextAnyone uses a diffusion framework with emphasis attention and gap-rope positional embeddings to generate consistent character videos from text and a single reference image.  					AI-generated summary 				 Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent characte...
[17.12.2025 14:25] Read previous papers.
[17.12.2025 14:25] Generating reviews via LLM API.
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#survey", "#multimodal", "#video", "#reasoning"], "emoji": "üß†", "ru": {"title": "–õ–æ–≥–∏–∫–∞ –≤–∞–∂–Ω–µ–µ –∫—Ä–∞—Å–æ—Ç—ã: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MMGR ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–πÊ°ÜÊû∂–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#training", "#3d", "#diffusion", "#architecture", "#video", "#long_context"], "emoji": "üéÆ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –º–∏—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "WorldPlay ‚Äî —ç—Ç–æ –ø–æ—Ç–æ–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#multimodal", "#audio", "#video", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∞–ª–∏–∑–º–∞: –∫–æ–≥–¥–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –≤–∏–¥–µ–æ –æ–±–º–∞–Ω—ã–≤–∞–µ—Ç –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ Video Reality Test –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ ASMR —Å –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–æ–π
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#multimodal", "#open_source"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ–º: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–Ω–æ–≥–æ–æ–±—ä–µ–∫—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Scone ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#dataset", "#rl", "#3d", "#robotics", "#multimodal", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboTracer ‚Äî —ç—Ç–æ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#training", "#data"], "emoji": "üîç", "ru": {"title": "ÈÄèËßÜ–¥–∞–Ω–Ω—ã–µ: –æ—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –Ω–∞—É–∫–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "OpenDataArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –∫–ª—é—á –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã SVG-–≥—Ä–∞—Ñ–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. 
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "üßä", "ru": {"title": "–û—Ç –º–µ—Ç—Ä–∏–∫ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º: –æ—Ü–µ–Ω–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö", "desc": "Iceberg ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –±
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#alignment", "#agents", "#rlhf", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ LLM –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —è–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –æ–± –∏–Ω—Ç–µ–Ω—Ç–∞—Ö", "desc": "RecGPT-V2 —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üìä", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å–∫—É—Å—Å—Ç–≤–æ: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π—Å—è pipeline –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü", "desc": "ShowTable ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#long_context"], "emoji": "üé¨", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MemFlow ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ì–∏–±–∫–æ–µ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LLM", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç VersatileFFN ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∏—Ö –ø
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#training", "#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π 4D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "SS4D ‚Äî —ç—Ç–æ –Ω–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ 3D –æ–±—ä–µ–∫—Ç—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#rlhf", "#3d", "#training", "#multimodal"], "emoji": "üé®", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ControlNet", "desc": "Steer3D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏, —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥. –ê–≤—Ç–æ—Ä—ã –∞
[17.12.2025 14:25] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "A4-Agent ‚Äî —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è affordance (–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π) —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∑–∞–¥–∞—á—É –Ω–∞ —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#alignment", "#rlhf", "#benchmark", "#training"], "emoji": "üèÜ", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 
[17.12.2025 14:25] Querying the API.
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A bilevel differentiable approach for evolving reward functions in reinforcement learning enhances agent performance across various domains by capturing task structure through reinforcement learning.  					AI-generated summary 				 The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.
[17.12.2025 14:25] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DERL ‚Äî –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, DERL –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–µ—Ç–∞-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç —É—Å–ø–µ—Ö–∞ –∑–∞–¥–∞—á–∏. –ú–µ—Ç–∞-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç –∞—Ç–æ–º–∞—Ä–Ω—ã–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º —Ü–∏–∫–ª–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ç—Ä—ë—Ö –¥–æ–º–µ–Ω–∞—Ö (—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞, –Ω–∞—É—á–Ω—ã–µ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ DERL –Ω–∞–¥ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –≤–Ω–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞.",
  "emoji": "üéØ",
  "title": "–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è —ç–≤–æ–ª—é—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –º–µ—Ç–∞–æ–±—É—á–µ–Ω–∏–µ"
}
```
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A bilevel differentiable approach for evolving reward functions in reinforcement learning enhances agent performance across various domains by capturing task structure through reinforcement learning.  					AI-generated summary 				 The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention."

[17.12.2025 14:25] Response: ```python
["RL", "AGENTS", "TRAINING", "MATH"]
```
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A bilevel differentiable approach for evolving reward functions in reinforcement learning enhances agent performance across various domains by capturing task structure through reinforcement learning.  					AI-generated summary 				 The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention."

[17.12.2025 14:25] Response: ```python
['REASONING', 'OPTIMIZATION', 'SCIENCE']
```
[17.12.2025 14:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Differentiable Evolutionary Reinforcement Learning (DERL), a novel approach for optimizing reward functions in reinforcement learning. DERL employs a bilevel framework where a Meta-Optimizer evolves a reward function by using structured atomic primitives, enhancing the training of an inner-loop policy. Unlike traditional methods that treat reward functions as black boxes, DERL is differentiable, allowing it to learn from the performance of the policy and improve the reward signals iteratively. The results demonstrate that DERL outperforms existing heuristic methods across various domains, effectively capturing task structures and enabling autonomous agent improvement.","title":"Evolving Reward Functions for Smarter Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Differentiable Evolutionary Reinforcement Learning (DERL), a novel approach for optimizing reward functions in reinforcement learning. DERL employs a bilevel framework where a Meta-Optimizer evolves a reward function by using structured atomic primitives, enhancing the training of an inner-loop policy. Unlike traditional methods that treat reward functions as black boxes, DERL is differentiable, allowing it to learn from the performance of the policy and improve the reward signals iteratively. The results demonstrate that DERL outperforms existing heuristic methods across various domains, effectively capturing task structures and enabling autonomous agent improvement.', title='Evolving Reward Functions for Smarter Agents'))
[17.12.2025 14:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÂ±ÇÂèØÂæÆÂàÜÁöÑËøõÂåñÂ•ñÂä±ÂáΩÊï∞ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçáÂº∫ÂåñÂ≠¶‰π†‰∏≠Êô∫ËÉΩ‰ΩìÁöÑË°®Áé∞„ÄÇÈÄöËøáÊûÑÂª∫ÁªìÊûÑÂåñÁöÑÂéüÂ≠êÂü∫ÂÖÉÔºåMeta-‰ºòÂåñÂô®ËÉΩÂ§üËá™‰∏ªÂèëÁé∞ÊúÄ‰Ω≥ÁöÑÂ•ñÂä±‰ø°Âè∑Ôºå‰ªéËÄåÊåáÂØºÂÜÖÂæ™ÁéØÁ≠ñÁï•ÁöÑËÆ≠ÁªÉ„ÄÇ‰∏é‰º†ÁªüÁöÑËøõÂåñÊñπÊ≥ï‰∏çÂêåÔºåDERLÂú®ÂÖÉ‰ºòÂåñËøáÁ®ã‰∏≠ÊòØÂèØÂæÆÂàÜÁöÑÔºåËÉΩÂ§üÂà©Áî®ÂÜÖÂæ™ÁéØÈ™åËØÅÊÄßËÉΩ‰Ωú‰∏∫‰ø°Âè∑Êù•Êõ¥Êñ∞Meta-‰ºòÂåñÂô®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDERLÂú®Â§ö‰∏™È¢ÜÂüü‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÊòæËëóË∂ÖË∂ä‰∫Ü‰æùËµñÂêØÂèëÂºèÂ•ñÂä±ÁöÑÊñπÊ≥ï„ÄÇ","title":"Êô∫ËÉΩ‰ΩìËá™Êàë‰ºòÂåñÁöÑËøõÂåñÂ•ñÂä±ÂáΩÊï∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèåÂ±ÇÂèØÂæÆÂàÜÁöÑËøõÂåñÂ•ñÂä±ÂáΩÊï∞ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçáÂº∫ÂåñÂ≠¶‰π†‰∏≠Êô∫ËÉΩ‰ΩìÁöÑË°®Áé∞„ÄÇÈÄöËøáÊûÑÂª∫ÁªìÊûÑÂåñÁöÑÂéüÂ≠êÂü∫ÂÖÉÔºåMeta-‰ºòÂåñÂô®ËÉΩÂ§üËá™‰∏ªÂèëÁé∞ÊúÄ‰Ω≥ÁöÑÂ•ñÂä±‰ø°Âè∑Ôºå‰ªéËÄåÊåáÂØºÂÜÖÂæ™ÁéØÁ≠ñÁï•ÁöÑËÆ≠ÁªÉ„ÄÇ‰∏é‰º†ÁªüÁöÑËøõÂåñÊñπÊ≥ï‰∏çÂêåÔºåDERLÂú®ÂÖÉ‰ºòÂåñËøáÁ®ã‰∏≠ÊòØÂèØÂæÆÂàÜÁöÑÔºåËÉΩÂ§üÂà©Áî®ÂÜÖÂæ™ÁéØÈ™åËØÅÊÄßËÉΩ‰Ωú‰∏∫‰ø°Âè∑Êù•Êõ¥Êñ∞Meta-‰ºòÂåñÂô®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDERLÂú®Â§ö‰∏™È¢ÜÂüü‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÊòæËëóË∂ÖË∂ä‰∫Ü‰æùËµñÂêØÂèëÂºèÂ•ñÂä±ÁöÑÊñπÊ≥ï„ÄÇ', title='Êô∫ËÉΩ‰ΩìËá™Êàë‰ºòÂåñÁöÑËøõÂåñÂ•ñÂä±ÂáΩÊï∞'))
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–û–ª–º–æ 3: –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ–π Olmo 3 ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–ª—Ä–¥ –∏ 32 –º–ª—Ä–¥ –ø–∞
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#multimodal", "#robotics", "#rl", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "EVOLVE-VLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#video", "#multimodal", "#optimization", "#rlhf", "#long_context", "#hallucinations", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –æ–±—â–µ–≥–æ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É: —Ç–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã", "desc": "Zoom-Zero ‚Äî —ç—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è —Ç–∏–ø–∞ \"–≥—Ä—É–±—ã–π-
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É—Å–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Sparse-LaViDa –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É—Å–µ—á–µ–Ω–∏—è –º–∞—Å–∫–∏—Ä
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ù–µ–∑–∞–≤–∏—Å–∏–º–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ MoE-–º–æ–¥–µ–ª—è—Ö", "desc": "Janus ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–µ–π Mixture-of-Experts, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –±–ª–æ–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –º–æ–¥—É–ª–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ GPU-–∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –Ω–µ
[17.12.2025 14:25] Using data from previous issue: {"categories": [], "emoji": "üè•", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∑–∞–¥–∞—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TAT (Task-Adaptive Transformer) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üî∑", "ru": {"title": "–†–µ—à—ë—Ç–∫–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º—É –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ —Ç–µ–æ—Ä–∏—é —Ä–µ—à—ë—Ç–æ–∫ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–≤–∞–Ω—Ç
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–≥—Ä—É–∑–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç RePo ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#open_source", "#synthetic", "#low_resource", "#dataset", "#benchmark", "#multilingual", "#multimodal"], "emoji": "üáØüáµ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω JMMMU-Pro ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#multimodal", "#agents", "#dataset"], "emoji": "üåç", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —É–º–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∑–∞–º–µ–Ω–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#training", "#security", "#benchmark", "#alignment", "#interpretability"], "emoji": "üî™", "ru": {"title": "–•–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–æ–≤: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ç—ã—Ä—ë—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ—Ç–∫–∞–∑–∞ 
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#video", "#dataset", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å
[17.12.2025 14:25] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "–î–≤–∏–∂–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ: –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç MeViS –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π, –æ–ø–∏—Å—ã–≤–∞—é—â–∏—Ö –¥–≤–∏
[17.12.2025 14:25] Querying the API.
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ContextAnyone uses a diffusion framework with emphasis attention and gap-rope positional embeddings to generate consistent character videos from text and a single reference image.  					AI-generated summary 				 Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose ContextAnyone, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: https://github.com/ziyang1106/ContextAnyone{https://github.com/ziyang1106/ContextAnyone}.
[17.12.2025 14:25] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ ContextAnyone ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–º–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–π —Å—Ü–µ–Ω—ã. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Emphasize-Attention –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥—Ä–µ–π—Ñ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –¥–≤–æ–π–Ω—É—é —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é –∑–∞–¥–∞—á–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç Gap-RoPE –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ.",
  "emoji": "üé¨",
  "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å —É—Å—Ç–æ–π—á–∏–≤–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å—é –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é"
}
```
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ContextAnyone uses a diffusion framework with emphasis attention and gap-rope positional embeddings to generate consistent character videos from text and a single reference image.  					AI-generated summary 				 Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose ContextAnyone, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: https://github.com/ziyang1106/ContextAnyone{https://github.com/ziyang1106/ContextAnyone}."

[17.12.2025 14:25] Response: ```python
["VIDEO", "MULTIMODAL", "ARCHITECTURE"]
```
[17.12.2025 14:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ContextAnyone uses a diffusion framework with emphasis attention and gap-rope positional embeddings to generate consistent character videos from text and a single reference image.  					AI-generated summary 				 Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose ContextAnyone, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: https://github.com/ziyang1106/ContextAnyone{https://github.com/ziyang1106/ContextAnyone}."

[17.12.2025 14:25] Response: ```python
["DIFFUSION", "OPEN_SOURCE"]
```
[17.12.2025 14:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces ContextAnyone, a novel diffusion framework designed for generating character-consistent videos from text and a single reference image. It addresses the challenge of maintaining character identity by integrating broader contextual cues like hairstyle and outfit, rather than just facial features. The framework employs an Emphasize-Attention module to enhance reference-aware features and prevent identity drift, while a dual-guidance loss improves visual fidelity. Additionally, Gap-RoPE positional embeddings are used to stabilize the temporal modeling of video frames, resulting in high-quality, coherent character videos across various scenes.","title":"Context-Aware Video Generation for Consistent Character Identity"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces ContextAnyone, a novel diffusion framework designed for generating character-consistent videos from text and a single reference image. It addresses the challenge of maintaining character identity by integrating broader contextual cues like hairstyle and outfit, rather than just facial features. The framework employs an Emphasize-Attention module to enhance reference-aware features and prevent identity drift, while a dual-guidance loss improves visual fidelity. Additionally, Gap-RoPE positional embeddings are used to stabilize the temporal modeling of video frames, resulting in high-quality, coherent character videos across various scenes.', title='Context-Aware Video Generation for Consistent Character Identity'))
[17.12.2025 14:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ContextAnyoneÁöÑÊâ©Êï£Ê°ÜÊû∂ÔºåÊó®Âú®‰ªéÊñáÊú¨ÂíåÂçï‰∏ÄÂèÇËÄÉÂõæÂÉèÁîüÊàê‰∏ÄËá¥ÁöÑËßíËâ≤ËßÜÈ¢ë„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËÅîÂêàÈáçÂª∫ÂèÇËÄÉÂõæÂÉèÂíåÁîüÊàêÊñ∞ÁöÑËßÜÈ¢ëÂ∏ßÔºåÂÖÖÂàÜÂà©Áî®ÂèÇËÄÉ‰ø°ÊÅØÔºå‰ªéËÄå‰øùÊåÅËßíËâ≤Âú®‰∏çÂêåÂú∫ÊôØ‰∏≠ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜEmphasize-AttentionÊ®°ÂùóÔºåÈÄâÊã©ÊÄßÂú∞Â¢ûÂº∫‰∏éÂèÇËÄÉÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÔºåÈò≤Ê≠¢Âú®Â∏ß‰πãÈó¥Âá∫Áé∞Ë∫´‰ªΩÊºÇÁßª„ÄÇÊ≠§Â§ñÔºåGap-RoPE‰ΩçÁΩÆÂµåÂÖ•ÊúâÊïàÂú∞ÂàÜÁ¶ª‰∫ÜÂèÇËÄÉÂíåËßÜÈ¢ëÊ†áËÆ∞ÔºåÁ®≥ÂÆö‰∫ÜÊó∂Èó¥Âª∫Ê®°„ÄÇ","title":"ÁîüÊàê‰∏ÄËá¥ËßíËâ≤ËßÜÈ¢ëÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ContextAnyoneÁöÑÊâ©Êï£Ê°ÜÊû∂ÔºåÊó®Âú®‰ªéÊñáÊú¨ÂíåÂçï‰∏ÄÂèÇËÄÉÂõæÂÉèÁîüÊàê‰∏ÄËá¥ÁöÑËßíËâ≤ËßÜÈ¢ë„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËÅîÂêàÈáçÂª∫ÂèÇËÄÉÂõæÂÉèÂíåÁîüÊàêÊñ∞ÁöÑËßÜÈ¢ëÂ∏ßÔºåÂÖÖÂàÜÂà©Áî®ÂèÇËÄÉ‰ø°ÊÅØÔºå‰ªéËÄå‰øùÊåÅËßíËâ≤Âú®‰∏çÂêåÂú∫ÊôØ‰∏≠ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜEmphasize-AttentionÊ®°ÂùóÔºåÈÄâÊã©ÊÄßÂú∞Â¢ûÂº∫‰∏éÂèÇËÄÉÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÔºåÈò≤Ê≠¢Âú®Â∏ß‰πãÈó¥Âá∫Áé∞Ë∫´‰ªΩÊºÇÁßª„ÄÇÊ≠§Â§ñÔºåGap-RoPE‰ΩçÁΩÆÂµåÂÖ•ÊúâÊïàÂú∞ÂàÜÁ¶ª‰∫ÜÂèÇËÄÉÂíåËßÜÈ¢ëÊ†áËÆ∞ÔºåÁ®≥ÂÆö‰∫ÜÊó∂Èó¥Âª∫Ê®°„ÄÇ', title='ÁîüÊàê‰∏ÄËá¥ËßíËâ≤ËßÜÈ¢ëÁöÑÊñ∞ÊñπÊ≥ï'))
[17.12.2025 14:25] Renaming data file.
[17.12.2025 14:25] Renaming previous data. hf_papers.json to ./d/2025-12-17.json
[17.12.2025 14:25] Saving new data file.
[17.12.2025 14:25] Generating page.
[17.12.2025 14:25] Renaming previous page.
[17.12.2025 14:25] Renaming previous data. index.html to ./d/2025-12-17.html
[17.12.2025 14:25] Writing result.
[17.12.2025 14:25] Renaming log file.
[17.12.2025 14:25] Renaming previous data. log.txt to ./logs/2025-12-17_last_log.txt
