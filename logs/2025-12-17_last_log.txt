[17.12.2025 04:36] Read previous papers.
[17.12.2025 04:36] Generating top page (month).
[17.12.2025 04:36] Writing top page (month).
[17.12.2025 05:25] Read previous papers.
[17.12.2025 05:25] Get feed.
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14691
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13660
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12675
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14614
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14699
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14503
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13303
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14531
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14442
[17.12.2025 05:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.14284
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13678
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13961
[17.12.2025 05:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.14666
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14067
[17.12.2025 05:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.14008
[17.12.2025 05:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13655
[17.12.2025 05:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.14391
[17.12.2025 05:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.14014
[17.12.2025 05:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.12.2025 05:25] No deleted papers detected.
[17.12.2025 05:25] Downloading and parsing papers (pdf, html). Total: 18.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14691.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.14691.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.14691.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.13660.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.13660.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.13660.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.12675.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.12675.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.12675.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14614.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.14614.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.14614.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14699.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.14699.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.14699.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14503.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.14503.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.14503.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.13303.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.13303.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.13303.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14531.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.14531.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.14531.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14442.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.14442.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.14442.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14284.
[17.12.2025 05:25] Downloading paper 2512.14284 from https://arxiv.org/pdf/2512.14284v1...
[17.12.2025 05:25] Extracting affiliations from text.
[17.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SS4D: Native 4D Generative Model via Structured Spacetime Latents ZHIBING LI, The Chinese University of Hong Kong, China and Shanghai AI Laboratory, China MENGCHEN ZHANG, Shanghai AI Laboratory, China and Zhejiang University, China TONG WU, Stanford University, USA JING TAN, The Chinese University of Hong Kong, China JIAQI WANG, Shanghai AI Laboratory, China DAHUA LIN, The Chinese University of Hong Kong, China 5 2 0 2 6 1 ] . [ 1 4 8 2 4 1 . 2 1 5 2 : r Fig. 1. SS4D generates high-quality 4D content in 2 minutes. The monocular input videos, corresponding voxelized structure, and the final 4D content from an alternative viewpoint are presented. For additional dynamic results, please refer to our supplementary demo video. https://lizb6626.github.io/SS4D/ We present SS4D, native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that Equal contribution. Corresponding Authors. Authors Contact Information: Zhibing Li, The Chinese University of Hong Kong, China and Shanghai AI Laboratory, China, lz022@ie.cuhk.edu.hk; Mengchen Zhang, Shanghai AI Laboratory, China and Zhejiang University, China, zhangmengchen@zju.edu.cn; Tong Wu, Stanford University, USA, wutong16@stanford.edu; Jing Tan, The Chinese University of Hong Kong, China, tj023@ie.cuhk.edu.hk; Jiaqi Wang, wjqdev@gmail. com, Shanghai AI Laboratory, China; Dahua Lin, dhlin@ie.cuhk.edu.hk, The Chinese University of Hong Kong, China. construct 4D representations by optimizing over 3D or video generative models, we train generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To su"
[17.12.2025 05:25] Response: ```python
[
    "The Chinese University of Hong Kong, China",
    "Shanghai AI Laboratory, China",
    "Zhejiang University, China",
    "Stanford University, USA"
]
```
[17.12.2025 05:25] Deleting PDF ./assets/pdf/2512.14284.pdf.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.13678.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.13678.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.13678.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.13961.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.13961.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.13961.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14666.
[17.12.2025 05:25] Downloading paper 2512.14666 from https://arxiv.org/pdf/2512.14666v1...
[17.12.2025 05:25] Extracting affiliations from text.
[17.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 6 6 6 4 1 . 2 1 5 2 : r EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models Zechen Bai, Chen Gao, Mike Zheng Shou* Show Lab, National University of Singapore https://showlab.github.io/EVOLVE-VLA "
[17.12.2025 05:25] Response: ```python
["National University of Singapore"]
```
[17.12.2025 05:25] Deleting PDF ./assets/pdf/2512.14666.pdf.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14067.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.14067.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.14067.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14008.
[17.12.2025 05:25] Downloading paper 2512.14008 from https://arxiv.org/pdf/2512.14008v1...
[17.12.2025 05:25] Extracting affiliations from text.
[17.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models Shufan Li1,2,, Jiuxiang Gu1, Kangning Liu1, Zhe Lin1 Zijun Wei1, Aditya Grover2, Jason Kuen1 1Adobe 2UCLA * Work done primarily during internship at Adobe Research 5 2 0 2 6 1 ] . [ 1 8 0 0 4 1 . 2 1 5 2 : r a "
[17.12.2025 05:25] Response: ```python
["Adobe", "UCLA"]
```
[17.12.2025 05:25] Deleting PDF ./assets/pdf/2512.14008.pdf.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.13655.
[17.12.2025 05:25] Extra JSON file exists (./assets/json/2512.13655.json), skip PDF parsing.
[17.12.2025 05:25] Paper image links file exists (./assets/img_data/2512.13655.json), skip HTML parsing.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14391.
[17.12.2025 05:25] Downloading paper 2512.14391 from https://arxiv.org/pdf/2512.14391v1...
[17.12.2025 05:25] Extracting affiliations from text.
[17.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"REPO: Language Models with Context Re-Positioning Huayang Li 1 2 Tianyu Zhao 1 Richard Sproat 1 5 2 0 2 6 1 ] . [ 1 1 9 3 4 1 . 2 1 5 2 : r a "
[17.12.2025 05:25] Response: ```python
[]
```
[17.12.2025 05:25] Extracting affiliations from text.
[17.12.2025 05:25] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"REPO: Language Models with Context Re-Positioning Huayang Li 1 2 Tianyu Zhao 1 Richard Sproat 1 5 2 0 2 6 1 ] . [ 1 1 9 3 4 1 . 2 1 5 2 : r aIn-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose REPO, novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, REPO utilizes differentiable module, fœï, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pretraining on the OLMo-2 1B backbone, we demonstrate that REPO significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general shortcontext tasks. Detailed analysis reveals that REPO successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo. 1. Introduction The emergence of large language models (LLMs) (Brown et al., 2020) has enabled wide range of applications, including few-shot learning (Brown et al., 2020), retrievalaugmented generation (Lewis et al., 2020; Yao et al., 2023), and agentic systems (Schick et al., 2023; Park et al., 2023). At the core of these applications is in-context learning (Shen et al., 2024), form analogous to human working memory, 1Sakana AI, Japan 2Nara Institute of Science and Technology to: Huayang Li <li.huayang.lh6@is.naist.jp>, Tianyu Zhao <tianyu@sakana.ai>, Richard Sproat <rws@sakana.ai>. Correspondence (NAIST), Japan. Preprint. December 17, 2025. 1 Figure 1. Overall performance on four evaluation dimensions. where information within limited context window is temporarily stored and processed to solve task. Consequently, exploring how to effectively utilize context information has become fundamental research line in the LLM era (Wei et al., 2022; Weston & Sukhbaatar, 2023; Chen et al., 2023). Recent studies show that an LLMs ability to leverage contextual information is strongly influenced by its position encoding scheme (Vaswani et al., 2017; Press et al., 2021; Su et al., 2024). Most LLMs impose fixed contextual structure by assigning tokens consecutive integer indices from 0 to 1 (Vaswani et al., 2017) or constant index for all tokens (Kazemnejad et al., 2023). These indices are then integrated into model through position encoding functions, enforcing rigid organization of context. Although fixed position assignments have become the de facto standard, they deviate from how human working memory processes information. According to Cognitive Load Theory (CLT), the capacity of working memory during problem solving can be consumed by costs arising from how information is organized and presented, referred to as extraneous load (Sweller, 1994; Paas et al., 2003). CLT studies suggest that humans can actively reduce this extraneous load by reorganizing context, e.g., grouping related information into meaningful chunks (Miller, 1956) or removing irrelevant details from instructions (Sweller, 1994). Since working memory capacity is fixed, the capacity saved through such reorganization can be reallocated to deeper reasoning REPO : Language Models with Context Re-Positioning processes associated with the the germane load, thereby improving problem-solving performance (Sweller, 1994). However, the critical ability to reorganize and restructure contextual information (Vaswani et al., 2017; Yang et al., 2025a; Dubey et al., 2024) is absent from the architectural design of modern LLMs. From the perspective of CLT (Sweller, 1994; Paas et al., 2003), rigid linear or constant position structures can be interpreted as introducing additional extraneous load, which in turn impairs attention allocation and deeper contextual reasoning (i.e., germane processing). As consequence, tasks that require strong long-range or fine-grained contextual dependencies, e.g., needle-in-a-haystack (NIAH) problems (Kamradt, 2023) or question answering under highly diluted contexts (Hsieh et al.), exhibit notable performance degradation, mirroring the effects predicted by CLT under high extraneous load. Moreover, from probabilistic standpoint, these position assignment strategies, essentially uniform distributions over fixed integer ranges, are the least informative organizations of context and therefore limit representational efficiency. We propose an internal mechanism for LLMs to reduce extraneous load by re-organizing the positions of tokens. We formalize this process, termed context Re-Positioning (REPO), as learning non-constrained position values based on information relevance of tokens, instead of using the fixed linear positions in prior work. To this end, we introduce differentiable module fœï, which assigns position value in continuous space for each token based on its hidden state. The fœï can be independently learned for each attention head of an LLM. Trained on general data, fœï learns to re-position tokens free from conventional constraints like monotonicity or integer values. The continuity of modern position encoding functions, e.g., RoPE (Su et al., 2024) and ALiBi (Press et al., 2021) methods, is key to the end-to-end optimization of fœï, as it allows these assigned positions to be integrated in differentiable manner. We find that LLMs using the REPO method achieve consistent performance gains on tasks involving noisy context, structured data, and longer context. In our experiments, we continually pre-trained LLMs with the REPO method and several baselines based on the OLMo-2 1B model. Within the training context length, our REPO method outperforms other baselines by at least 6.24 and 1.16 points on noisy context and structured data tasks, respectively. In addition, when extending the testing context length to 16K tokens using the YaRN (Peng et al., 2024) method, our REPO method outperforms other baselines by at least 13.25 EM points on the QA and Needle-in-the-haystack (NIAH) tasks and 5.48 points on LongBench (Bai et al., 2"
[17.12.2025 05:25] Mistral response. {"id": "d87affa6946c4718a6c0bc1a5b01946b", "created": 1765949151, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1584, "total_tokens": 1605, "completion_tokens": 21}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Sakana AI, Japan\", \"Nara Institute of Science and Technology\"]\n```"}}]}
[17.12.2025 05:25] Response: ```python
["Sakana AI, Japan", "Nara Institute of Science and Technology"]
```
[17.12.2025 05:25] Deleting PDF ./assets/pdf/2512.14391.pdf.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Downloading and parsing paper https://huggingface.co/papers/2512.14014.
[17.12.2025 05:25] Downloading paper 2512.14014 from https://arxiv.org/pdf/2512.14014v1...
[17.12.2025 05:25] Extracting affiliations from text.
[17.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MobileWorldBench: Towards Semantic World Modeling For Mobile Agents Shufan Li1, Konstantinos Kallidromitis2, Akash Gokul3 Yusuke Kato2, Kazuki Kozuka 2, Aditya Grover1 1 UCLA 2Panasonic AI Research 3Salesforce AI Research *Equal Contribution Correspondence to jacklishufan@cs.ucla.edu 5 2 0 2 6 1 ] . [ 1 4 1 0 4 1 . 2 1 5 2 : r a "
[17.12.2025 05:25] Response: ```python
['UCLA', 'Panasonic AI Research', 'Salesforce AI Research']
```
[17.12.2025 05:25] Deleting PDF ./assets/pdf/2512.14014.pdf.
[17.12.2025 05:25] Success.
[17.12.2025 05:25] Enriching papers with extra data.
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 0. MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  					AI-generated summary 				 Video foundation models generate visually realistic and temporally coherent content...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 1. RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  					AI-generated summary 				 Spatial traci...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 2. Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  					AI-generated summary 				 Subject-driven image generation has advanced from single- to multi-s...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 3. WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  					AI-generated summary 				 This paper presents WorldPlay, a streaming...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 4. MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  					AI-generated summary 				 The core challenge for streaming video generation is maintaining the co...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 5. RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 6. ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  					AI-generated summary 				 While existing generation and un...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 7. The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting t...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 8. A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  					AI-generated summary 				 Affordance prediction, which identifies interaction regions on objects based o...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 9. SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynami...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 10. Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  					AI-generated summary 				 Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and rob...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 11. Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  					AI-generated summary 				 We introduce Olmo 3, a family of state-of-the-art, fu...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 12. EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving tru...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 13. AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  					AI-generated summary 				 Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, ...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 14. Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance acr...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 15. Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  					AI-generated summary 				 Safety alignment mechan...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 16. RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to mode...
[17.12.2025 05:25] ********************************************************************************
[17.12.2025 05:25] Abstract 17. A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work la...
[17.12.2025 05:25] Read previous papers.
[17.12.2025 05:25] Generating reviews via LLM API.
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#survey", "#multimodal", "#video", "#reasoning"], "emoji": "üß†", "ru": {"title": "–õ–æ–≥–∏–∫–∞ –≤–∞–∂–Ω–µ–µ –∫—Ä–∞—Å–æ—Ç—ã: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MMGR ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–πÊ°ÜÊû∂–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#dataset", "#rl", "#3d", "#robotics", "#multimodal", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboTracer ‚Äî —ç—Ç–æ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#multimodal", "#open_source"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ–º: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–Ω–æ–≥–æ–æ–±—ä–µ–∫—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Scone ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#training", "#3d", "#diffusion", "#architecture", "#video", "#long_context"], "emoji": "üéÆ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –º–∏—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "WorldPlay ‚Äî —ç—Ç–æ –ø–æ—Ç–æ–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#long_context"], "emoji": "üé¨", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MemFlow ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#alignment", "#agents", "#rlhf", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ LLM –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —è–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –æ–± –∏–Ω—Ç–µ–Ω—Ç–∞—Ö", "desc": "RecGPT-V2 —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üìä", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å–∫—É—Å—Å—Ç–≤–æ: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π—Å—è pipeline –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü", "desc": "ShowTable ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏
[17.12.2025 05:25] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ì–∏–±–∫–æ–µ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LLM", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç VersatileFFN ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∏—Ö –ø
[17.12.2025 05:25] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "A4-Agent ‚Äî —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è affordance (–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π) —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∑–∞–¥–∞—á—É –Ω–∞ —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä
[17.12.2025 05:25] Querying the API.
[17.12.2025 05:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion
[17.12.2025 05:26] Response: ```json
{
  "desc": "SS4D ‚Äî —ç—Ç–æ –Ω–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ 3D –æ–±—ä–µ–∫—Ç—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–≥–æ –≤–∏–¥–µ–æ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–æ–∏–ª–∏ 4D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—É—Ç—ë–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–¥ 3D –∏–ª–∏ –≤–∏–¥–µ–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∞–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–∞–ø—Ä—è–º—É—é –Ω–∞ 4D –¥–∞–Ω–Ω—ã—Ö. –ö–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –º–µ—Ç–æ–¥–∞ ‚Äî —Å–∂–∞—Ç—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–µ –≤—ã—Å–æ–∫—É—é –≤–µ—Ä–Ω–æ—Å—Ç—å –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –∏ —Ñ–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ 4D —Å–≤—ë—Ä—Ç–∫–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ.",
  "emoji": "üé¨",
  "title": "–°–∏–Ω—Ç–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π 4D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"
}
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion"

[17.12.2025 05:26] Response: ```python
['3D', 'VIDEO', 'ARCHITECTURE', 'TRAINING']
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion"

[17.12.2025 05:26] Response: ```python
['TRANSFER_LEARNING', 'SYNTHETIC']
```

**Justification:**

- **TRANSFER_LEARNING**: The paper explicitly mentions building on "a pre-trained single-image-to-3D model" to address scarcity of 4D training data, which is a direct application of knowledge transfer from a pre-trained model to a new task.

- **SYNTHETIC**: The paper addresses the scarcity of 4D training data by leveraging a pre-trained model and synthetic approaches to generate training data for the 4D generative model, which relates to using artificial/synthetic data for training.
[17.12.2025 05:26] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "SYNTHETIC"]


**Justification:**

- **TRANSFER_LEARNING**: The paper explicitly mentions building on "a pre-trained single-image-to-3D model" to address scarcity of 4D training data, which is a direct application of knowledge transfer from a pre-trained model to a new task.

- **SYNTHETIC**: The paper addresses the scarcity of 4D training data by leveraging a pre-trained model and synthetic approaches to generate training data for the 4D generative model, which relates to using artificial/synthetic data for training.
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces SS4D, a novel 4D generative model that creates dynamic 3D objects from single-camera video input. It improves upon previous methods by directly training on 4D data, which enhances the quality and consistency of the generated objects. The model utilizes structured spacetime latents to maintain spatial and temporal coherence, addressing challenges like data scarcity and occlusion. Key innovations include the use of pre-trained models for spatial consistency and specialized temporal layers for frame reasoning, along with efficient training techniques for long video sequences.","title":"Revolutionizing 3D Object Synthesis from Video with SS4D"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces SS4D, a novel 4D generative model that creates dynamic 3D objects from single-camera video input. It improves upon previous methods by directly training on 4D data, which enhances the quality and consistency of the generated objects. The model utilizes structured spacetime latents to maintain spatial and temporal coherence, addressing challenges like data scarcity and occlusion. Key innovations include the use of pre-trained models for spatial consistency and specialized temporal layers for frame reasoning, along with efficient training techniques for long video sequences.', title='Revolutionizing 3D Object Synthesis from Video with SS4D'))
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SS4DÊòØ‰∏ÄÁßçÂéüÁîüÁöÑ4DÁîüÊàêÊ®°ÂûãÔºåÂèØ‰ª•‰ªéÂçïÁõÆËßÜÈ¢ë‰∏≠ÂêàÊàêÂä®ÊÄÅ3DÁâ©‰Ωì„ÄÇ‰∏é‰πãÂâçÁöÑÊñπÊ≥ï‰∏çÂêåÔºåSS4DÁõ¥Êé•Âú®4DÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁîüÊàêÂô®ÔºåÁ°Æ‰øù‰∫ÜÈ´ò‰øùÁúüÂ∫¶„ÄÅÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÁªìÊûÑ‰∏ÄËá¥ÊÄß„ÄÇËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™ÂéãÁº©ÁöÑÁªìÊûÑÂåñÊó∂Á©∫ÊΩúÂèòÈáèÈõÜÔºåÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÂçïÂõæÂÉèÂà∞3DÊ®°ÂûãÊù•Ëß£ÂÜ≥4DËÆ≠ÁªÉÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÊîØÊåÅÈïøËßÜÈ¢ëÂ∫èÂàóÁöÑÈ´òÊïàËÆ≠ÁªÉÂíåÊé®ÁêÜÔºåSS4D‰ΩøÁî®‰∫ÜÂàÜËß£ÁöÑ4DÂç∑ÁßØÂíåÊó∂Èó¥‰∏ãÈááÊ†∑ÂùóÊù•ÂéãÁº©ÊΩúÂèòÈáèÂ∫èÂàó„ÄÇ","title":"‰ªéÂçïÁõÆËßÜÈ¢ëÂêàÊàêÂä®ÊÄÅ3DÁâ©‰ΩìÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SS4DÊòØ‰∏ÄÁßçÂéüÁîüÁöÑ4DÁîüÊàêÊ®°ÂûãÔºåÂèØ‰ª•‰ªéÂçïÁõÆËßÜÈ¢ë‰∏≠ÂêàÊàêÂä®ÊÄÅ3DÁâ©‰Ωì„ÄÇ‰∏é‰πãÂâçÁöÑÊñπÊ≥ï‰∏çÂêåÔºåSS4DÁõ¥Êé•Âú®4DÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁîüÊàêÂô®ÔºåÁ°Æ‰øù‰∫ÜÈ´ò‰øùÁúüÂ∫¶„ÄÅÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÁªìÊûÑ‰∏ÄËá¥ÊÄß„ÄÇËØ•ÊñπÊ≥ïÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™ÂéãÁº©ÁöÑÁªìÊûÑÂåñÊó∂Á©∫ÊΩúÂèòÈáèÈõÜÔºåÈÄöËøáÈ¢ÑËÆ≠ÁªÉÁöÑÂçïÂõæÂÉèÂà∞3DÊ®°ÂûãÊù•Ëß£ÂÜ≥4DËÆ≠ÁªÉÊï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜÊîØÊåÅÈïøËßÜÈ¢ëÂ∫èÂàóÁöÑÈ´òÊïàËÆ≠ÁªÉÂíåÊé®ÁêÜÔºåSS4D‰ΩøÁî®‰∫ÜÂàÜËß£ÁöÑ4DÂç∑ÁßØÂíåÊó∂Èó¥‰∏ãÈááÊ†∑ÂùóÊù•ÂéãÁº©ÊΩúÂèòÈáèÂ∫èÂàó„ÄÇ', title='‰ªéÂçïÁõÆËßÜÈ¢ëÂêàÊàêÂä®ÊÄÅ3DÁâ©‰ΩìÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[17.12.2025 05:26] Using data from previous issue: {"categories": ["#rlhf", "#3d", "#training", "#multimodal"], "emoji": "üé®", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ControlNet", "desc": "Steer3D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏, —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥. –ê–≤—Ç–æ—Ä—ã –∞
[17.12.2025 05:26] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–û–ª–º–æ 3: –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ–π Olmo 3 ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–ª—Ä–¥ –∏ 32 –º–ª—Ä–¥ –ø–∞
[17.12.2025 05:26] Querying the API.
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.
[17.12.2025 05:26] Response: ```json
{
  "desc": "EVOLVE-VLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Å–∏—Å—Ç–µ–º–∞–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π. –í–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–æ –≤—Ä–µ–º—è —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞–≥—Ä–∞–¥—ã, –∞–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–π –æ—Ü–µ–Ω—â–∏–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–ª–æ—Ç–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —à—É–º–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏: –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è: +8.6% –Ω–∞ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, +22.0% –ø—Ä–∏ –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏ 20.8% —É—Å–ø–µ—Ö–∞ –Ω–∞ –Ω–µ–≤–∏–¥–∏–º—ã—Ö –∑–∞–¥–∞—á–∞—Ö –±–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π.",
  "emoji": "ü§ñ",
  "title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ"
}
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements."

[17.12.2025 05:26] Response: ```python
['ROBOTICS', 'MULTIMODAL', 'TRAINING', 'RL']
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements."

[17.12.2025 05:26] Response: ```python
["TRANSFER_LEARNING", "OPTIMIZATION"]
```

**Justification:**

- **TRANSFER_LEARNING**: The paper discusses knowledge transfer and generalization across tasks. Specifically, it demonstrates "cross-task generalization" and the ability to achieve success on "unseen tasks without task-specific demonstrations," which represents transfer of learned knowledge to new domains/tasks.

- **OPTIMIZATION**: The paper presents EVOLVE-VLA as a test-time training framework that continuously optimizes model performance through environmental interaction. The technical contributions include mechanisms for optimization during deployment (progress estimator, accumulative progress estimation, progressive horizon extension strategy), which are optimization methods.
[17.12.2025 05:26] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "OPTIMIZATION"]


**Justification:**

- **TRANSFER_LEARNING**: The paper discusses knowledge transfer and generalization across tasks. Specifically, it demonstrates "cross-task generalization" and the ability to achieve success on "unseen tasks without task-specific demonstrations," which represents transfer of learned knowledge to new domains/tasks.

- **OPTIMIZATION**: The paper presents EVOLVE-VLA as a test-time training framework that continuously optimizes model performance through environmental interaction. The technical contributions include mechanisms for optimization during deployment (progress estimator, accumulative progress estimation, progressive horizon extension strategy), which are optimization methods.
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"EVOLVE-VLA is a framework designed for Vision-Language-Action (VLA) models that allows them to adapt continuously during test time by interacting with their environment. Unlike traditional methods that rely heavily on numerous task-specific demonstrations, EVOLVE-VLA requires minimal or no such demonstrations, enabling more flexible learning. The framework addresses the challenge of lacking reward signals during testing by using a learned progress estimator for feedback, which is refined through two innovative strategies. This approach leads to significant performance improvements, including better long-horizon task execution and the ability to generalize across different tasks without prior specific training.","title":"Continuous Learning for Adaptive Intelligence in VLA Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='EVOLVE-VLA is a framework designed for Vision-Language-Action (VLA) models that allows them to adapt continuously during test time by interacting with their environment. Unlike traditional methods that rely heavily on numerous task-specific demonstrations, EVOLVE-VLA requires minimal or no such demonstrations, enabling more flexible learning. The framework addresses the challenge of lacking reward signals during testing by using a learned progress estimator for feedback, which is refined through two innovative strategies. This approach leads to significant performance improvements, including better long-horizon task execution and the ability to generalize across different tasks without prior specific training.', title='Continuous Learning for Adaptive Intelligence in VLA Models'))
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"EVOLVE-VLAÊòØ‰∏ÄÁßçÁî®‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÊµãËØïÊó∂ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËÉΩÂ§üÈÄöËøá‰∏éÁéØÂ¢ÉÁöÑ‰∫íÂä®ÂÆûÁé∞ÊåÅÁª≠ÈÄÇÂ∫îÔºå‰∏îÂè™ÈúÄÊûÅÂ∞ëÁöÑÁâπÂÆö‰ªªÂä°Á§∫ËåÉ„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫Ü‰º†ÁªüÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®Ê≤°ÊúâÂ§ßÈáèÁ§∫ËåÉÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËá™ÊàëÊîπËøõ„ÄÇÈÄöËøáÂºïÂÖ•Â≠¶‰π†ÁöÑËøõÂ∫¶‰º∞ËÆ°Âô®Âíå‰∏§ÁßçÊú∫Âà∂Êù•Âπ≥ÊªëÂô™Â£∞‰ø°Âè∑ÔºåEVOLVE-VLAÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®ÈïøÊó∂Èó¥‰ªªÂä°ÂíåÂçïÊ¨°Â≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇÊúÄÁªàÔºåËØ•ÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÊ®°ÂûãÂú®Êú™ËßÅ‰ªªÂä°‰∏äÁöÑË∑®‰ªªÂä°Ê≥õÂåñËÉΩÂäõÔºåÊ†áÂøóÁùÄÂêëÁúüÊ≠£Ëá™ÊàëÂ≠¶‰π†ÂíåÈÄÇÂ∫îÁöÑÊô∫ËÉΩ‰ΩìËøàÂá∫‰∫ÜÈáçË¶Å‰∏ÄÊ≠•„ÄÇ","title":"ÊåÅÁª≠ÈÄÇÂ∫îÁöÑÊô∫ËÉΩ‰ΩìÔºöEVOLVE-VLAÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='EVOLVE-VLAÊòØ‰∏ÄÁßçÁî®‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÁöÑÊµãËØïÊó∂ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåËÉΩÂ§üÈÄöËøá‰∏éÁéØÂ¢ÉÁöÑ‰∫íÂä®ÂÆûÁé∞ÊåÅÁª≠ÈÄÇÂ∫îÔºå‰∏îÂè™ÈúÄÊûÅÂ∞ëÁöÑÁâπÂÆö‰ªªÂä°Á§∫ËåÉ„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫Ü‰º†ÁªüÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®Ê≤°ÊúâÂ§ßÈáèÁ§∫ËåÉÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËá™ÊàëÊîπËøõ„ÄÇÈÄöËøáÂºïÂÖ•Â≠¶‰π†ÁöÑËøõÂ∫¶‰º∞ËÆ°Âô®Âíå‰∏§ÁßçÊú∫Âà∂Êù•Âπ≥ÊªëÂô™Â£∞‰ø°Âè∑ÔºåEVOLVE-VLAÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®ÈïøÊó∂Èó¥‰ªªÂä°ÂíåÂçïÊ¨°Â≠¶‰π†‰∏≠ÁöÑË°®Áé∞„ÄÇÊúÄÁªàÔºåËØ•ÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÊ®°ÂûãÂú®Êú™ËßÅ‰ªªÂä°‰∏äÁöÑË∑®‰ªªÂä°Ê≥õÂåñËÉΩÂäõÔºåÊ†áÂøóÁùÄÂêëÁúüÊ≠£Ëá™ÊàëÂ≠¶‰π†ÂíåÈÄÇÂ∫îÁöÑÊô∫ËÉΩ‰ΩìËøàÂá∫‰∫ÜÈáçË¶Å‰∏ÄÊ≠•„ÄÇ', title='ÊåÅÁª≠ÈÄÇÂ∫îÁöÑÊô∫ËÉΩ‰ΩìÔºöEVOLVE-VLAÊ°ÜÊû∂'))
[17.12.2025 05:26] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º
[17.12.2025 05:26] Querying the API.
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.
[17.12.2025 05:26] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Sparse-LaViDa –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É—Å–µ—á–µ–Ω–∏—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª—É–∂–∞—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –î–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–µ —É—Å–µ—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –¥–≤—É–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –±–µ–∑ —Å–Ω–∏–∂–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.",
  "emoji": "‚ö°",
  "title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É—Å–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality."

[17.12.2025 05:26] Response: ```python
["INFERENCE", "MULTIMODAL", "ARCHITECTURE"]
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality."

[17.12.2025 05:26] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```

**Justification:**

1. **DIFFUSION**: The paper explicitly focuses on Masked Discrete Diffusion Models (MDMs) and proposes Sparse-LaViDa as an acceleration method for diffusion-based generative models.

2. **OPTIMIZATION**: The paper addresses inference speed optimization by dynamically truncating masked tokens, achieving up to 2x speedup, which is a training/inference optimization technique.
[17.12.2025 05:26] Error. Failed to parse JSON from LLM. ["DIFFUSION", "OPTIMIZATION"]


**Justification:**

1. **DIFFUSION**: The paper explicitly focuses on Masked Discrete Diffusion Models (MDMs) and proposes Sparse-LaViDa as an acceleration method for diffusion-based generative models.

2. **OPTIMIZATION**: The paper addresses inference speed optimization by dynamically truncating masked tokens, achieving up to 2x speedup, which is a training/inference optimization technique.
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Sparse-LaViDa is a new framework that speeds up Masked Discrete Diffusion Models (MDMs) by removing unnecessary masked tokens during the inference process. This approach helps to reduce the time taken for sampling without compromising the quality of the generated outputs. To achieve this, the framework uses specialized register tokens that represent the truncated tokens and a tailored attention mask to ensure consistency between training and inference. As a result, Sparse-LaViDa can deliver up to a 2x increase in speed for various tasks like text-to-image generation and image editing while maintaining high-quality results.","title":"Accelerating MDMs with Sparse Token Truncation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Sparse-LaViDa is a new framework that speeds up Masked Discrete Diffusion Models (MDMs) by removing unnecessary masked tokens during the inference process. This approach helps to reduce the time taken for sampling without compromising the quality of the generated outputs. To achieve this, the framework uses specialized register tokens that represent the truncated tokens and a tailored attention mask to ensure consistency between training and inference. As a result, Sparse-LaViDa can deliver up to a 2x increase in speed for various tasks like text-to-image generation and image editing while maintaining high-quality results.', title='Accelerating MDMs with Sparse Token Truncation'))
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Sparse-LaViDaÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂª∫Ê®°Ê°ÜÊû∂ÔºåÊó®Âú®Âä†ÈÄüÊé©Á†ÅÁ¶ªÊï£Êâ©Êï£Ê®°ÂûãÔºàMDMsÔºâÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÈÄöËøáÂú®ÊØè‰∏™Êé®ÁêÜÊ≠•È™§Âä®ÊÄÅÊà™Êñ≠‰∏çÂøÖË¶ÅÁöÑÊé©Á†Å‰ª§ÁâåÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÈ´òÈááÊ†∑ÈÄüÂ∫¶ÔºåÊúÄÈ´òÂèØÂÆûÁé∞2ÂÄçÁöÑÂä†ÈÄü„ÄÇ‰∏∫‰∫Ü‰øùÊåÅÁîüÊàêË¥®ÈáèÔºåSparse-LaViDaÂºïÂÖ•‰∫Ü‰∏ìÈó®ÁöÑÂØÑÂ≠òÂô®‰ª§ÁâåÔºå‰Ωú‰∏∫Ë¢´Êà™Êñ≠‰ª§ÁâåÁöÑÁ¥ßÂáëË°®Á§∫„ÄÇÊ≠§Â§ñÔºåËÆæËÆ°‰∫Ü‰∏ìÈó®ÁöÑÊ≥®ÊÑèÂäõÊé©Á†ÅÔºå‰ª•Á°Æ‰øùËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ","title":"Âä®ÊÄÅÊà™Êñ≠ÔºåÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶ÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Sparse-LaViDaÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂª∫Ê®°Ê°ÜÊû∂ÔºåÊó®Âú®Âä†ÈÄüÊé©Á†ÅÁ¶ªÊï£Êâ©Êï£Ê®°ÂûãÔºàMDMsÔºâÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÈÄöËøáÂú®ÊØè‰∏™Êé®ÁêÜÊ≠•È™§Âä®ÊÄÅÊà™Êñ≠‰∏çÂøÖË¶ÅÁöÑÊé©Á†Å‰ª§ÁâåÔºåËØ•ÊñπÊ≥ïËÉΩÂ§üÊèêÈ´òÈááÊ†∑ÈÄüÂ∫¶ÔºåÊúÄÈ´òÂèØÂÆûÁé∞2ÂÄçÁöÑÂä†ÈÄü„ÄÇ‰∏∫‰∫Ü‰øùÊåÅÁîüÊàêË¥®ÈáèÔºåSparse-LaViDaÂºïÂÖ•‰∫Ü‰∏ìÈó®ÁöÑÂØÑÂ≠òÂô®‰ª§ÁâåÔºå‰Ωú‰∏∫Ë¢´Êà™Êñ≠‰ª§ÁâåÁöÑÁ¥ßÂáëË°®Á§∫„ÄÇÊ≠§Â§ñÔºåËÆæËÆ°‰∫Ü‰∏ìÈó®ÁöÑÊ≥®ÊÑèÂäõÊé©Á†ÅÔºå‰ª•Á°Æ‰øùËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ', title='Âä®ÊÄÅÊà™Êñ≠ÔºåÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶ÔºÅ'))
[17.12.2025 05:26] Using data from previous issue: {"categories": ["#training", "#security", "#benchmark", "#alignment", "#interpretability"], "emoji": "üî™", "ru": {"title": "–•–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–æ–≤: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ç—ã—Ä—ë—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ—Ç–∫–∞–∑–∞ 
[17.12.2025 05:26] Querying the API.
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, f_œÜ, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.
[17.12.2025 05:26] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç RePo ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤. –ü–æ–¥—Ö–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç–µ–æ—Ä–∏–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏, –ø–æ–∑–≤–æ–ª—è—è –µ–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ –≥–ª—É–±–æ–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ RePo —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å —à—É–º–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö. –ú–µ—Ö–∞–Ω–∏–∑–º —É—Å–ø–µ—à–Ω–æ –≤—ã—É—á–∏–≤–∞–µ—Ç –Ω–∞–∑–Ω–∞—á–∞—Ç—å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –µ—ë –ø–æ–∑–∏—Ü–∏–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—è–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.",
  "emoji": "üß†",
  "title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–≥—Ä—É–∑–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è LLM"
}
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, f_œÜ, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo."

[17.12.2025 05:26] Response: ```python
["ARCHITECTURE", "TRAINING", "SMALL_MODELS"]
```

**Justification:**

- **ARCHITECTURE**: The paper proposes RePo, a novel mechanism/component for LLMs that modifies how positional indices are assigned to tokens using a differentiable module. This is a novel architectural contribution.

- **TRAINING**: The paper involves continual pre-training on the OLMo-2 1B backbone to implement and evaluate the proposed mechanism, which relates to training methodologies.

- **SMALL_MODELS**: The backbone model used is OLMo-2 1B, which is explicitly a 1 billion parameter model, fitting the criterion of "below 1 billion parameters or similar.
[17.12.2025 05:26] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING", "SMALL_MODELS"]


**Justification:**

- **ARCHITECTURE**: The paper proposes RePo, a novel mechanism/component for LLMs that modifies how positional indices are assigned to tokens using a differentiable module. This is a novel architectural contribution.

- **TRAINING**: The paper involves continual pre-training on the OLMo-2 1B backbone to implement and evaluate the proposed mechanism, which relates to training methodologies.

- **SMALL_MODELS**: The backbone model used is OLMo-2 1B, which is explicitly a 1 billion parameter model, fitting the criterion of "below 1 billion parameters or similar.
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, f_œÜ, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo."

[17.12.2025 05:26] Response: ```python
["LONG_CONTEXT", "OPEN_SOURCE"]
```
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces RePo, a new mechanism for Large Language Models (LLMs) that improves how token positions are assigned in context. By using a differentiable approach, RePo reduces unnecessary cognitive load, allowing the model to focus more on important information rather than being constrained by fixed positional indices. The authors show that RePo enhances performance on tasks with noisy or lengthy contexts while still performing well on shorter contexts. This method allows the model to better capture contextual dependencies and allocate attention effectively, leading to improved reasoning capabilities.","title":"RePo: Redefining Context for Enhanced LLM Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces RePo, a new mechanism for Large Language Models (LLMs) that improves how token positions are assigned in context. By using a differentiable approach, RePo reduces unnecessary cognitive load, allowing the model to focus more on important information rather than being constrained by fixed positional indices. The authors show that RePo enhances performance on tasks with noisy or lengthy contexts while still performing well on shorter contexts. This method allows the model to better capture contextual dependencies and allocate attention effectively, leading to improved reasoning capabilities.', title='RePo: Redefining Context for Enhanced LLM Performance'))
[17.12.2025 05:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RePoÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏ä‰∏ãÊñáÈáçÊñ∞ÂÆö‰ΩçÊú∫Âà∂ÔºåÊó®Âú®ÂáèÂ∞ëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÈ¢ùÂ§ñËÆ§Áü•Ë¥üÊãÖ„ÄÇÈÄöËøáÂèØÂæÆÂàÜÂú∞ÂàÜÈÖç‰ª§Áâå‰ΩçÁΩÆÔºåRePoËÉΩÂ§üÂú®Â§ÑÁêÜÂòàÊùÇÂíåÈïø‰∏ä‰∏ãÊñáÊó∂ÊèêÈ´òÊÄßËÉΩÔºåÂêåÊó∂‰∏çÂΩ±ÂìçÁü≠‰∏ä‰∏ãÊñá‰ªªÂä°ÁöÑË°®Áé∞„ÄÇËØ•Êú∫Âà∂Âü∫‰∫éËÆ§Áü•Ë¥üËç∑ÁêÜËÆ∫Ôºå‰ºòÂåñ‰∫Ü‰∏ä‰∏ãÊñáÁªìÊûÑÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Â∑•‰ΩúËÆ∞ÂøÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRePoÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂÖ≥Ê≥®Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ","title":"RePoÔºö‰ºòÂåñ‰∏ä‰∏ãÊñáÂ§ÑÁêÜÁöÑÂàõÊñ∞Êú∫Âà∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RePoÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰∏ä‰∏ãÊñáÈáçÊñ∞ÂÆö‰ΩçÊú∫Âà∂ÔºåÊó®Âú®ÂáèÂ∞ëÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÈ¢ùÂ§ñËÆ§Áü•Ë¥üÊãÖ„ÄÇÈÄöËøáÂèØÂæÆÂàÜÂú∞ÂàÜÈÖç‰ª§Áâå‰ΩçÁΩÆÔºåRePoËÉΩÂ§üÂú®Â§ÑÁêÜÂòàÊùÇÂíåÈïø‰∏ä‰∏ãÊñáÊó∂ÊèêÈ´òÊÄßËÉΩÔºåÂêåÊó∂‰∏çÂΩ±ÂìçÁü≠‰∏ä‰∏ãÊñá‰ªªÂä°ÁöÑË°®Áé∞„ÄÇËØ•Êú∫Âà∂Âü∫‰∫éËÆ§Áü•Ë¥üËç∑ÁêÜËÆ∫Ôºå‰ºòÂåñ‰∫Ü‰∏ä‰∏ãÊñáÁªìÊûÑÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Â∑•‰ΩúËÆ∞ÂøÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRePoÂú®Â§çÊùÇ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂÖ≥Ê≥®Áõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ', title='RePoÔºö‰ºòÂåñ‰∏ä‰∏ãÊñáÂ§ÑÁêÜÁöÑÂàõÊñ∞Êú∫Âà∂'))
[17.12.2025 05:26] Querying the API.
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld
[17.12.2025 05:26] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∑–∞–º–µ–Ω–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ MobileWorldBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –≤—ã—Å—Ç—É–ø–∞—Ç—å –≤ —Ä–æ–ª–∏ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞ —Ç–∞–∫–∂–µ –≤—ã–ø—É—Å—Ç–∏–ª–∏ –∫—Ä—É–ø–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç MobileWorld —Å 1.4 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø—Ä–∏–º–µ—Ä–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç VLM –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –ø–æ–∑–≤–æ–ª—è—è –∏–º –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –º–æ–±–∏–ª—å–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –ø–∏–∫—Å–µ–ª—å–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.",
  "emoji": "üåç",
  "title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —É–º–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"
}
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld"

[17.12.2025 05:26] Response: ```python
['DATASET', 'BENCHMARK', 'AGENTS', 'CV', 'MULTIMODAL']
```
[17.12.2025 05:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld"

[17.12.2025 05:27] Response: ```python
['OPEN_SOURCE']
```
[17.12.2025 05:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework for vision-language models (VLMs) that enhances the performance of mobile GUI agents by utilizing semantic world models instead of traditional pixel-based predictions. The authors introduce MobileWorldBench, a benchmark designed to assess how well VLMs can serve as world models in mobile GUI contexts. They also provide MobileWorld, a comprehensive dataset with 1.4 million samples that boosts the world modeling capabilities of VLMs. The proposed framework shows that using semantic descriptions for state transitions can significantly improve the task success rates of mobile agents.","title":"Empowering Mobile Agents with Semantic World Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework for vision-language models (VLMs) that enhances the performance of mobile GUI agents by utilizing semantic world models instead of traditional pixel-based predictions. The authors introduce MobileWorldBench, a benchmark designed to assess how well VLMs can serve as world models in mobile GUI contexts. They also provide MobileWorld, a comprehensive dataset with 1.4 million samples that boosts the world modeling capabilities of VLMs. The proposed framework shows that using semantic descriptions for state transitions can significantly improve the task success rates of mobile agents.', title='Empowering Mobile Agents with Semantic World Models'))
[17.12.2025 05:27] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÈÄöËøá‰ΩøÁî®ËØ≠‰πâ‰∏ñÁïåÊ®°ÂûãÊù•ÊèêÈ´òÁßªÂä®GUI‰ª£ÁêÜÁöÑ‰ªªÂä°ÊàêÂäüÁéáÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫éÂü∫‰∫éÂÉèÁ¥†ÁöÑÈ¢ÑÊµã„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰º†ÁªüÁöÑÂÉèÁ¥†Á©∫Èó¥‰∏ñÁïåÊ®°ÂûãÂú®GUIÁéØÂ¢É‰∏≠Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂõ†Ê≠§Êàë‰ª¨Êé¢Á¥¢‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∏ñÁïåÂª∫Ê®°ÊñπÊ≥ïÔºå‰ΩøÁî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Áä∂ÊÄÅËΩ¨Áßª„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜMobileWorldBenchÂü∫ÂáÜÔºåËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®ÁßªÂä®GUI‰ª£ÁêÜ‰∏≠ÁöÑË°®Áé∞ÔºåÂπ∂ÂèëÂ∏É‰∫ÜÂåÖÂê´140‰∏áÊ†∑Êú¨ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜMobileWorldÔºå‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑ‰∏ñÁïåÂª∫Ê®°ËÉΩÂäõ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ∞ÜËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰∏ñÁïåÊ®°ÂûãÊï¥ÂêàÂà∞ÁßªÂä®‰ª£ÁêÜËßÑÂàíÊ°ÜÊû∂‰∏≠ÁöÑÊñ∞ÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜËØ≠‰πâ‰∏ñÁïåÊ®°ÂûãÂèØ‰ª•Áõ¥Êé•ÊèêÈ´òÁßªÂä®‰ª£ÁêÜÁöÑ‰ªªÂä°ÊàêÂäüÁéá„ÄÇ","title":"ËØ≠‰πâ‰∏ñÁïåÊ®°ÂûãÊèêÂçáÁßªÂä®‰ª£ÁêÜ‰ªªÂä°ÊàêÂäüÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÈÄöËøá‰ΩøÁî®ËØ≠‰πâ‰∏ñÁïåÊ®°ÂûãÊù•ÊèêÈ´òÁßªÂä®GUI‰ª£ÁêÜÁöÑ‰ªªÂä°ÊàêÂäüÁéáÔºåËÄå‰∏çÊòØ‰æùËµñ‰∫éÂü∫‰∫éÂÉèÁ¥†ÁöÑÈ¢ÑÊµã„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰º†ÁªüÁöÑÂÉèÁ¥†Á©∫Èó¥‰∏ñÁïåÊ®°ÂûãÂú®GUIÁéØÂ¢É‰∏≠Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂõ†Ê≠§Êàë‰ª¨Êé¢Á¥¢‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∏ñÁïåÂª∫Ê®°ÊñπÊ≥ïÔºå‰ΩøÁî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞Áä∂ÊÄÅËΩ¨Áßª„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜMobileWorldBenchÂü∫ÂáÜÔºåËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂú®ÁßªÂä®GUI‰ª£ÁêÜ‰∏≠ÁöÑË°®Áé∞ÔºåÂπ∂ÂèëÂ∏É‰∫ÜÂåÖÂê´140‰∏áÊ†∑Êú¨ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜMobileWorldÔºå‰ª•Â¢ûÂº∫Ê®°ÂûãÁöÑ‰∏ñÁïåÂª∫Ê®°ËÉΩÂäõ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ∞ÜËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰∏ñÁïåÊ®°ÂûãÊï¥ÂêàÂà∞ÁßªÂä®‰ª£ÁêÜËßÑÂàíÊ°ÜÊû∂‰∏≠ÁöÑÊñ∞ÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜËØ≠‰πâ‰∏ñÁïåÊ®°ÂûãÂèØ‰ª•Áõ¥Êé•ÊèêÈ´òÁßªÂä®‰ª£ÁêÜÁöÑ‰ªªÂä°ÊàêÂäüÁéá„ÄÇ', title='ËØ≠‰πâ‰∏ñÁïåÊ®°ÂûãÊèêÂçáÁßªÂä®‰ª£ÁêÜ‰ªªÂä°ÊàêÂäüÁéá'))
[17.12.2025 05:27] Renaming data file.
[17.12.2025 05:27] Renaming previous data. hf_papers.json to ./d/2025-12-17.json
[17.12.2025 05:27] Saving new data file.
[17.12.2025 05:27] Generating page.
[17.12.2025 05:27] Renaming previous page.
[17.12.2025 05:27] Renaming previous data. index.html to ./d/2025-12-17.html
[17.12.2025 05:27] Writing result.
[17.12.2025 05:27] Renaming log file.
[17.12.2025 05:27] Renaming previous data. log.txt to ./logs/2025-12-17_last_log.txt
