[17.12.2025 11:23] Read previous papers.
[17.12.2025 11:23] Generating top page (month).
[17.12.2025 11:23] Writing top page (month).
[17.12.2025 12:48] Read previous papers.
[17.12.2025 12:48] Get feed.
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14691
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14614
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12675
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13281
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13660
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14051
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14336
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12980
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14503
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13303
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14699
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14531
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13678
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14442
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14284
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13607
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13961
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14666
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14273
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14067
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14008
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13525
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14550
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14697
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14391
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14620
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14014
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13655
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14440
[17.12.2025 12:48] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10945
[17.12.2025 12:48] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.12.2025 12:48] No deleted papers detected.
[17.12.2025 12:48] Downloading and parsing papers (pdf, html). Total: 30.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14691.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14691.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14691.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14614.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14614.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14614.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.12675.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.12675.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.12675.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13281.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13281.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13281.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13660.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13660.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13660.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14051.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14051.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14051.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14336.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14336.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14336.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.12980.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.12980.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.12980.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14503.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14503.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14503.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13303.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13303.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13303.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14699.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14699.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14699.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14531.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14531.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14531.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13678.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13678.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13678.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14442.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14442.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14442.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14284.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14284.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14284.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13607.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13607.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13607.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13961.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13961.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13961.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14666.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14666.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14666.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14273.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14273.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14273.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14067.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14067.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14067.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14008.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14008.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14008.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13525.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13525.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13525.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14550.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14550.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14550.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14697.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14697.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14697.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14391.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14391.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14391.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14620.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14620.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14620.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14014.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14014.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14014.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.13655.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.13655.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.13655.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.14440.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.14440.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.14440.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Downloading and parsing paper https://huggingface.co/papers/2512.10945.
[17.12.2025 12:48] Extra JSON file exists (./assets/json/2512.10945.json), skip PDF parsing.
[17.12.2025 12:48] Paper image links file exists (./assets/img_data/2512.10945.json), skip HTML parsing.
[17.12.2025 12:48] Success.
[17.12.2025 12:48] Enriching papers with extra data.
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 0. MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  					AI-generated summary 				 Video foundation models generate visually realistic and temporally coherent content...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 1. WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  					AI-generated summary 				 This paper presents WorldPlay, a streaming...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 2. Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  					AI-generated summary 				 Subject-driven image generation has advanced from single- to multi-s...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 3. The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  					AI-generated summary 				 Recent advances ...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 4. RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  					AI-generated summary 				 Spatial traci...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 5. OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 6. A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to gr...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 7. Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.  					AI-generated summary 				 Vector Similarity Search (VSS) in high-dimensional sp...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 8. RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 9. ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  					AI-generated summary 				 While existing generation and un...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 10. MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  					AI-generated summary 				 The core challenge for streaming video generation is maintaining the co...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 11. The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting t...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 12. Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  					AI-generated summary 				 Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and rob...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 13. A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  					AI-generated summary 				 Affordance prediction, which identifies interaction regions on objects based o...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 14. SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynami...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 15. Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning mo...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 16. Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  					AI-generated summary 				 We introduce Olmo 3, a family of state-of-the-art, fu...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 17. EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving tru...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 18. Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize rel...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 19. AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  					AI-generated summary 				 Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, ...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 20. Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance acr...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 21. Janus is a scalable Mixture-of-Experts (MoE) inference system that disaggregates attention and expert modules for independent scaling, improving throughput and latency.  					AI-generated summary 				 Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dyna...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 22. A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recov...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 23. Lattice coding provides a unified framework for non-parametric quantization, with Leech lattice-based quantization achieving superior performance in image tokenization, compression, and generation tasks.  					AI-generated summary 				 Non-parametric quantization has received much attention due to i...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 24. RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to mode...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 25. JMMMU-Pro, an image-based Japanese multi-discipline multimodal understanding benchmark, challenges open-source large multimodal models through integrated visual-textual understanding and is constructed using Vibe Benchmark Construction, a cost-effective method leveraging realistic image generation. ...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 26. A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work la...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 27. Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  					AI-generated summary 				 Safety alignment mechan...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 28. An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance ...
[17.12.2025 12:48] ********************************************************************************
[17.12.2025 12:48] Abstract 29. A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  					AI-generated summary 				 This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focus...
[17.12.2025 12:48] Read previous papers.
[17.12.2025 12:48] Generating reviews via LLM API.
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#survey", "#multimodal", "#video", "#reasoning"], "emoji": "üß†", "ru": {"title": "–õ–æ–≥–∏–∫–∞ –≤–∞–∂–Ω–µ–µ –∫—Ä–∞—Å–æ—Ç—ã: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MMGR ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–πÊ°ÜÊû∂–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#training", "#3d", "#diffusion", "#architecture", "#video", "#long_context"], "emoji": "üéÆ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –º–∏—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "WorldPlay ‚Äî —ç—Ç–æ –ø–æ—Ç–æ–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#multimodal", "#open_source"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ–º: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–Ω–æ–≥–æ–æ–±—ä–µ–∫—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Scone ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#audio", "#video", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∞–ª–∏–∑–º–∞: –∫–æ–≥–¥–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –≤–∏–¥–µ–æ –æ–±–º–∞–Ω—ã–≤–∞–µ—Ç –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ Video Reality Test –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ ASMR —Å –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–æ–π
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#dataset", "#rl", "#3d", "#robotics", "#multimodal", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboTracer ‚Äî —ç—Ç–æ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#training", "#data"], "emoji": "üîç", "ru": {"title": "ÈÄèËßÜ–¥–∞–Ω–Ω—ã–µ: –æ—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –Ω–∞—É–∫–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "OpenDataArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –∫–ª—é—á –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã SVG-–≥—Ä–∞—Ñ–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. 
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "üßä", "ru": {"title": "–û—Ç –º–µ—Ç—Ä–∏–∫ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º: –æ—Ü–µ–Ω–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö", "desc": "Iceberg ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –±
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#alignment", "#agents", "#rlhf", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ LLM –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —è–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –æ–± –∏–Ω—Ç–µ–Ω—Ç–∞—Ö", "desc": "RecGPT-V2 —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üìä", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å–∫—É—Å—Å—Ç–≤–æ: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π—Å—è pipeline –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü", "desc": "ShowTable ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#long_context"], "emoji": "üé¨", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MemFlow ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ì–∏–±–∫–æ–µ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LLM", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç VersatileFFN ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∏—Ö –ø
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#rlhf", "#3d", "#training", "#multimodal"], "emoji": "üé®", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ControlNet", "desc": "Steer3D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏, —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥. –ê–≤—Ç–æ—Ä—ã –∞
[17.12.2025 12:48] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "A4-Agent ‚Äî —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è affordance (–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π) —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∑–∞–¥–∞—á—É –Ω–∞ —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#training", "#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π 4D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "SS4D ‚Äî —ç—Ç–æ –Ω–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ 3D –æ–±—ä–µ–∫—Ç—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#alignment", "#rlhf", "#benchmark", "#training"], "emoji": "üèÜ", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–û–ª–º–æ 3: –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ–π Olmo 3 ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–ª—Ä–¥ –∏ 32 –º–ª—Ä–¥ –ø–∞
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#multimodal", "#robotics", "#rl", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "EVOLVE-VLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#video", "#multimodal", "#optimization", "#rlhf", "#long_context", "#hallucinations", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –æ–±—â–µ–≥–æ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É: —Ç–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã", "desc": "Zoom-Zero ‚Äî —ç—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è —Ç–∏–ø–∞ \"–≥—Ä—É–±—ã–π-
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É—Å–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Sparse-LaViDa –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É—Å–µ—á–µ–Ω–∏—è –º–∞—Å–∫–∏—Ä
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ù–µ–∑–∞–≤–∏—Å–∏–º–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ MoE-–º–æ–¥–µ–ª—è—Ö", "desc": "Janus ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–µ–π Mixture-of-Experts, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –±–ª–æ–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –º–æ–¥—É–ª–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ GPU-–∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –Ω–µ
[17.12.2025 12:48] Using data from previous issue: {"categories": [], "emoji": "üè•", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∑–∞–¥–∞—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TAT (Task-Adaptive Transformer) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üî∑", "ru": {"title": "–†–µ—à—ë—Ç–∫–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º—É –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ —Ç–µ–æ—Ä–∏—é —Ä–µ—à—ë—Ç–æ–∫ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–≤–∞–Ω—Ç
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–≥—Ä—É–∑–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç RePo ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#open_source", "#synthetic", "#low_resource", "#dataset", "#benchmark", "#multilingual", "#multimodal"], "emoji": "üáØüáµ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω JMMMU-Pro ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#multimodal", "#agents", "#dataset"], "emoji": "üåç", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —É–º–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∑–∞–º–µ–Ω–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#training", "#security", "#benchmark", "#alignment", "#interpretability"], "emoji": "üî™", "ru": {"title": "–•–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–æ–≤: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ç—ã—Ä—ë—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ—Ç–∫–∞–∑–∞ 
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#video", "#dataset", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å
[17.12.2025 12:48] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "–î–≤–∏–∂–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ: –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –¥–≤–∏–∂–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç MeViS –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π, –æ–ø–∏—Å—ã–≤–∞—é—â–∏—Ö –¥–≤–∏
[17.12.2025 12:48] Renaming data file.
[17.12.2025 12:48] Renaming previous data. hf_papers.json to ./d/2025-12-17.json
[17.12.2025 12:48] Saving new data file.
[17.12.2025 12:48] Generating page.
[17.12.2025 12:48] Renaming previous page.
[17.12.2025 12:48] Renaming previous data. index.html to ./d/2025-12-17.html
[17.12.2025 12:48] Writing result.
[17.12.2025 12:48] Renaming log file.
[17.12.2025 12:48] Renaming previous data. log.txt to ./logs/2025-12-17_last_log.txt
