[17.12.2025 06:38] Read previous papers.
[17.12.2025 06:38] Generating top page (month).
[17.12.2025 06:38] Writing top page (month).
[17.12.2025 07:24] Read previous papers.
[17.12.2025 07:24] Get feed.
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14691
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14614
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12675
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13660
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14051
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14336
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14503
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13303
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14699
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14531
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14442
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14284
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13678
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13961
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14666
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14550
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14008
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14391
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14273
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13607
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14067
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14014
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13655
[17.12.2025 07:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14440
[17.12.2025 07:24] Extract page data from URL. URL: https://huggingface.co/papers/2512.10945
[17.12.2025 07:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.12.2025 07:24] No deleted papers detected.
[17.12.2025 07:24] Downloading and parsing papers (pdf, html). Total: 25.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14691.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14691.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14691.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14614.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14614.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14614.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.12675.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.12675.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.12675.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.13660.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.13660.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.13660.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14051.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14051.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14051.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14336.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14336.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14336.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14503.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14503.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14503.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.13303.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.13303.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.13303.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14699.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14699.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14699.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14531.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14531.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14531.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14442.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14442.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14442.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14284.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14284.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14284.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.13678.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.13678.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.13678.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.13961.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.13961.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.13961.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14666.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14666.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14666.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14550.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14550.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14550.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14008.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14008.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14008.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14391.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14391.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14391.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14273.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14273.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14273.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.13607.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.13607.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.13607.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14067.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14067.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14067.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14014.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14014.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14014.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.13655.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.13655.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.13655.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.14440.
[17.12.2025 07:24] Extra JSON file exists (./assets/json/2512.14440.json), skip PDF parsing.
[17.12.2025 07:24] Paper image links file exists (./assets/img_data/2512.14440.json), skip HTML parsing.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Downloading and parsing paper https://huggingface.co/papers/2512.10945.
[17.12.2025 07:24] Downloading paper 2512.10945 from https://arxiv.org/pdf/2512.10945v1...
[17.12.2025 07:24] Extracting affiliations from text.
[17.12.2025 07:24] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 MeViS: Multi-Modal Dataset for Referring Motion Expression Video Segmentation Henghui Ding, Chang Liu, Shuting He, Kaining Ying, Xudong Jiang, Fellow, IEEE, Chen Change Loy, Senior Member, IEEE, Yu-Gang Jiang, Fellow, IEEE AbstractThis paper proposes large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the methods source code are publicly available at https://henghuiding.com/MeViS/. Index TermsMotion Expression Video Segmentation, MeVi"
[17.12.2025 07:24] Response: ```python
[]
```
[17.12.2025 07:24] Extracting affiliations from text.
[17.12.2025 07:24] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 MeViS: Multi-Modal Dataset for Referring Motion Expression Video Segmentation Henghui Ding, Chang Liu, Shuting He, Kaining Ying, Xudong Jiang, Fellow, IEEE, Chen Change Loy, Senior Member, IEEE, Yu-Gang Jiang, Fellow, IEEE AbstractThis paper proposes large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the methods source code are publicly available at https://henghuiding.com/MeViS/. Index TermsMotion Expression Video Segmentation, MeViS Dataset, Referring Video Object Segmentation, Audio-guided Video Object Segmentation, Referring Multi-object Tracking, Referring Motion Expression Generation, LMPM++.at segmenting and tracking the specific target object referred by given natural language expression [1], [2], [3], [4]. This task has traditionally been subset of semi-supervised video object segmentation, where the clue of target object is provided through means such as mask, scribble, or sentence in the first frame. Existing datasets in this context, such as DAVIS16-RVOS [3] and Refer-YouTube-VOS [4], typically encompass videos featuring isolated and salient objects with evident static characteristics. The corresponding expressions frequently contain static attributes like the objects color and shape, which can be identified from single frame. Consequently, motion properties of videos are less pronounced in these expressions, and methods designed for referring image segmentation can effectively be applied to referring video segmentation, yielding favorable performance [3], [5], [6], [7]. The motivation of this work is to emphasize the importance of temporal motion characteristics in videos and explore the feasibility of employing motion-related expressions to identify and segment objects within video content. To this end, we propose new large-scale dataset named Motion expressions Video Segmentation (MeViS). Some samples of MeViS are shown in Fig. 1. The MeViS dataset contains 2,006 videos with total of 8,171 distinct objects. In the conference version, MeViSv1 [1], 28,570 motion-related expressions are provided for referring and delineating these objects, focusing on direct motion descriptions Henghui Ding, Kaining Ying, and Yu-Gang Jiang are with Fudan University, China 200433. (e-mail: henghui.ding@gmail.com) Chang Liu and Shuting He are with Shanghai University of Finance and Economics, Shanghai, China, 200433. Xudong Jiang and Chen Change Loy are with Nanyang Technological University, Singapore 639798. of single or multiple targets. Compared to MeViSv1 [1], the updated MeViSv2 in this work significantly expands the dataset with more challenging motion expressions, adding audio format expressions, providing tracking annotations, and supporting more tasks. First, the updated dataset includes 4,502 new challenging expressions, bringing the total to 33,072 expressionsthe largest in the field of referring video. These additions include motion reasoning expressions, which involve implicit queries requiring complex reasoning, and no-target expressions, which are deceptive motion descriptions that relate to the video but do not refer to any actual object, as shown in Fig. 3. In addition to text expressions, the updated MeViSv2 further provides above 150,000 seconds audio expressions, facilitating the study of audio-guided video object segmentation (AVOS) and multi-modal referring expressions. Furthermore, we provide tracking annotations in MeViSv2, establishing it as the largest referring multi-object tracking (RMOT) dataset. Beyond perception tasks, we introduce new task based on MeViS: Referring Motion Expression Generation (RMEG). This task aims to generate an unambiguous and concise motion expression for the selected objects in given video. In the construction of the MeViS dataset, several steps are undertaken to highlight the temporal motions inherent to videos. First, an assortment of videos is selected with the criterion that they showcase multiple interacting objects in motion, deliberately excluding low-quality videos where isolated objects could be easily described through static attributes alone. Second, the dataset prioritizes language expressions that focus on motion clues (e.g., walking, moving) rather than static clues (e.g., color, shape). These rules distinguish our MeViS from earlier datasets like [3], [4], [8], which contain salient targets in their videos or include obvious static clues in their sentence annotations. MeViS also sets itself 5 2 0 2 1 ] . [ 1 5 4 9 0 1 . 2 1 5 2 : r IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 Fig. 1: Examples from Motion expressions Video Segmentation (MeViS) showing the datasets nature and complexity. The expressions in MeViS primarily focus on motion attributes, making it impossible to identify the target object from single frame. For example, the first example has three parrots with similar appearances, and the target object is identified as The bird flying away. This object can"
[17.12.2025 07:24] Mistral response. {"id": "20f90878774a4645a6c09f1518747b81", "created": 1765956296, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1430, "total_tokens": 1460, "completion_tokens": 30}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Fudan University\",\n    \"Shanghai University of Finance and Economics\",\n    \"Nanyang Technological University\"\n]\n```"}}]}
[17.12.2025 07:24] Response: ```python
[
    "Fudan University",
    "Shanghai University of Finance and Economics",
    "Nanyang Technological University"
]
```
[17.12.2025 07:24] Deleting PDF ./assets/pdf/2512.10945.pdf.
[17.12.2025 07:24] Success.
[17.12.2025 07:24] Enriching papers with extra data.
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 0. MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.  					AI-generated summary 				 Video foundation models generate visually realistic and temporally coherent content...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 1. WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.  					AI-generated summary 				 This paper presents WorldPlay, a streaming...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 2. Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.  					AI-generated summary 				 Subject-driven image generation has advanced from single- to multi-s...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 3. RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.  					AI-generated summary 				 Spatial traci...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 4. OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.  					AI-generated summary...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 5. A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.  					AI-generated summary 				 Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to gr...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 6. RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 7. ShowTable, a pipeline combining MLLMs and diffusion models, excels in creative table visualization by generating high-fidelity infographics from data tables, outperforming existing methods in multi-modal reasoning and error correction.  					AI-generated summary 				 While existing generation and un...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 8. MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.  					AI-generated summary 				 The core challenge for streaming video generation is maintaining the co...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 9. The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting t...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 10. A4-Agent, a training-free framework, decouples affordance prediction into three stages using specialized pre-trained models to enhance generalization and performance in real-world settings.  					AI-generated summary 				 Affordance prediction, which identifies interaction regions on objects based o...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 11. SS4D synthesizes dynamic 3D objects from monocular video using a native 4D generative model with structured spacetime latents, ensuring high fidelity, temporal coherence, and structural consistency.  					AI-generated summary 				 We present SS4D, a native 4D generative model that synthesizes dynami...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 12. Steer3D enables text-based editing of AI-generated 3D assets by adapting ControlNet for image-to-3D generation with flow-matching training and Direct Preference Optimization.  					AI-generated summary 				 Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and rob...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 13. Olmo 3, a family of state-of-the-art fully-open language models at 7B and 32B parameter scales, excels in long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall.  					AI-generated summary 				 We introduce Olmo 3, a family of state-of-the-art, fu...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 14. EVOLVE-VLA, a test-time training framework for Vision-Language-Action models, enables continuous adaptation through environmental interaction with minimal task-specific demonstrations, achieving significant improvements in performance and generalization.  					AI-generated summary 				 Achieving tru...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 15. A task-adaptive Transformer (TAT) framework addresses challenges in medical image restoration by dynamically adjusting task-specific weights and loss balances, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 Medical image restoration (MedIR) aims to recov...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 16. Sparse-LaViDa accelerates Masked Discrete Diffusion Models by dynamically truncating masked tokens during inference, maintaining quality and achieving up to a 2x speedup across various tasks.  					AI-generated summary 				 Masked Discrete Diffusion Models (MDMs) have achieved strong performance acr...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 17. RePo, a novel context re-positioning mechanism in LLMs, reduces extraneous cognitive load by differentiably assigning token positions, enhancing performance on noisy and long contexts without compromising short-context tasks.  					AI-generated summary 				 In-context learning is fundamental to mode...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 18. Zoom-Zero, a coarse-to-fine framework, enhances grounded video question answering by improving temporal grounding and answer accuracy through a zoom-in accuracy reward and token-selective credit assignment.  					AI-generated summary 				 Grounded video question answering (GVQA) aims to localize rel...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 19. Cascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.  					AI-generated summary 				 Building general-purpose reasoning mo...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 20. AR-to-dLM conversion enhances diffusion language models' efficiency and speed while maintaining task accuracy through refined attention patterns and token masking strategies.  					AI-generated summary 				 Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, ...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 21. A novel vision-language model framework improves task success rates for mobile GUI agents by using semantic world models instead of pixel-based predictions.  					AI-generated summary 				 World models have shown great utility in improving the task performance of embodied agents. While prior work la...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 22. Four abliteration tools are evaluated for their effectiveness in removing refusal representations from large language models, with findings showing variability in capability preservation and distribution shift across different models and tools.  					AI-generated summary 				 Safety alignment mechan...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 23. An unsupervised video instance segmentation model using real video data and deep motion priors outperforms existing methods by establishing temporal coherence and using sparse-to-dense distillation.  					AI-generated summary 				 In recent years, the state-of-the-art in unsupervised video instance ...
[17.12.2025 07:24] ********************************************************************************
[17.12.2025 07:24] Abstract 24. A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  					AI-generated summary 				 This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focus...
[17.12.2025 07:24] Read previous papers.
[17.12.2025 07:24] Generating reviews via LLM API.
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#survey", "#multimodal", "#video", "#reasoning"], "emoji": "üß†", "ru": {"title": "–õ–æ–≥–∏–∫–∞ –≤–∞–∂–Ω–µ–µ –∫—Ä–∞—Å–æ—Ç—ã: –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MMGR ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–πÊ°ÜÊû∂–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#training", "#3d", "#diffusion", "#architecture", "#video", "#long_context"], "emoji": "üéÆ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –º–∏—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "WorldPlay ‚Äî —ç—Ç–æ –ø–æ—Ç–æ–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#benchmark", "#multimodal", "#open_source"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è —Å —Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ–º: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–Ω–æ–≥–æ–æ–±—ä–µ–∫—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Scone ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#dataset", "#rl", "#3d", "#robotics", "#multimodal", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "RoboTracer ‚Äî —ç—Ç–æ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#training", "#data"], "emoji": "üîç", "ru": {"title": "ÈÄèËßÜ–¥–∞–Ω–Ω—ã–µ: –æ—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –Ω–∞—É–∫–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤", "desc": "OpenDataArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ—Ç–∫—Ä—ã—Ç—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –∫–ª—é—á –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã SVG-–≥—Ä–∞—Ñ–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. 
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#alignment", "#agents", "#rlhf", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ LLM –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —è–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –æ–± –∏–Ω—Ç–µ–Ω—Ç–∞—Ö", "desc": "RecGPT-V2 —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#dataset", "#reasoning", "#benchmark", "#multimodal", "#diffusion"], "emoji": "üìä", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∏—Å–∫—É—Å—Å—Ç–≤–æ: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π—Å—è pipeline –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü", "desc": "ShowTable ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π pipeline, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#long_context"], "emoji": "üé¨", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MemFlow ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#inference", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ì–∏–±–∫–æ–µ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è LLM", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç VersatileFFN ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∏—Ö –ø
[17.12.2025 07:24] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "A4-Agent ‚Äî —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è affordance (–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π) —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∑–∞–¥–∞—á—É –Ω–∞ —Ç—Ä–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#training", "#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π 4D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "SS4D ‚Äî —ç—Ç–æ –Ω–∞—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ 3D –æ–±—ä–µ–∫—Ç—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#rlhf", "#3d", "#training", "#multimodal"], "emoji": "üé®", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ControlNet", "desc": "Steer3D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏, —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥. –ê–≤—Ç–æ—Ä—ã –∞
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#long_context"], "emoji": "üß†", "ru": {"title": "–û–ª–º–æ 3: –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ–π Olmo 3 ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–ª—Ä–¥ –∏ 32 –º–ª—Ä–¥ –ø–∞
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#multimodal", "#robotics", "#rl", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ä–∞–∂–∞–Ω–∏—è –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "EVOLVE-VLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫
[17.12.2025 07:24] Using data from previous issue: {"categories": [], "emoji": "üè•", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∑–∞–¥–∞—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TAT (Task-Adaptive Transformer) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É—Å–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Sparse-LaViDa –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É—Å–µ—á–µ–Ω–∏—è –º–∞—Å–∫–∏—Ä
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–≥—Ä—É–∑–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç RePo ‚Äî –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –º–æ–¥—É–ª—å
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#video", "#multimodal", "#optimization", "#rlhf", "#long_context", "#hallucinations", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –æ–±—â–µ–≥–æ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É: —Ç–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã", "desc": "Zoom-Zero ‚Äî —ç—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è —Ç–∏–ø–∞ \"–≥—Ä—É–±—ã–π-
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#alignment", "#rlhf", "#benchmark", "#training"], "emoji": "üèÜ", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è 
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#multimodal", "#agents", "#dataset"], "emoji": "üåç", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —É–º–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –º–∏—Ä–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∑–∞–º–µ–Ω–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#training", "#security", "#benchmark", "#alignment", "#interpretability"], "emoji": "üî™", "ru": {"title": "–•–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–æ–≤: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —á–µ—Ç—ã—Ä—ë—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–±–ª—è—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –æ—Ç–∫–∞–∑–∞ 
[17.12.2025 07:24] Using data from previous issue: {"categories": ["#video", "#dataset", "#cv"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å
[17.12.2025 07:24] Querying the API.
[17.12.2025 07:24] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  					AI-generated summary 				 This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/
[17.12.2025 07:25] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç MeViS –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π, –æ–ø–∏—Å—ã–≤–∞—é—â–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 33,072 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –æ –¥–≤–∏–∂–µ–Ω–∏–∏ –Ω–∞ —Ç–µ–∫—Å—Ç –∏ –∞—É–¥–∏–æ, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 8,171 –æ–±—ä–µ–∫—Ç–æ–≤ –≤ 2,006 –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥ 15 —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ 4 –∑–∞–¥–∞—á–∞–º: —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –∞—É–¥–∏–æ, –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –¥–≤–∏–∂–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LMPM++, –¥–æ—Å—Ç–∏–≥—à–∏–π –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–≤–∏–∂–µ–Ω–∏—è.",
  "emoji": "üé¨",
  "title": "–î–≤–∏–∂–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–¥–µ–æ: –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –¥–≤–∏–∂–µ–Ω–∏—è"
}
```
[17.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  					AI-generated summary 				 This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/"

[17.12.2025 07:25] Response: ```python
["DATASET", "VIDEO", "MULTIMODAL", "BENCHMARK"]
```
[17.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A dataset for referring motion expression video segmentation, MeViS, is introduced to explore motion expression-guided video understanding and benchmarking.  					AI-generated summary 				 This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/"

[17.12.2025 07:25] Response: ```python
["REASONING", "OPEN_SOURCE"]
```

**Justification:**

1. **REASONING**: The paper emphasizes "motion reasoning clues" and the need for models to reason about motion expressions to understand videos. The task requires logical reasoning about how objects move based on language descriptions of their motions.

2. **OPEN_SOURCE**: The paper explicitly states "The proposed MeViS dataset and the method's source code are publicly available," indicating the authors are releasing both the dataset and code to the public.
[17.12.2025 07:25] Error. Failed to parse JSON from LLM. ["REASONING", "OPEN_SOURCE"]


**Justification:**

1. **REASONING**: The paper emphasizes "motion reasoning clues" and the need for models to reason about motion expressions to understand videos. The task requires logical reasoning about how objects move based on language descriptions of their motions.

2. **OPEN_SOURCE**: The paper explicitly states "The proposed MeViS dataset and the method"s source code are publicly available," indicating the authors are releasing both the dataset and code to the public.
[17.12.2025 07:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the MeViS dataset, which focuses on video segmentation guided by motion expressions. Unlike existing datasets that emphasize static attributes, MeViS highlights the importance of motion in understanding and tracking objects in videos. It includes over 33,000 annotated motion expressions across various scenarios, enabling researchers to benchmark multiple video segmentation methods. The study reveals limitations in current techniques and proposes a new approach, LMPM++, which sets new performance standards in motion expression-guided video understanding.","title":"Unlocking Video Understanding with Motion Expressions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the MeViS dataset, which focuses on video segmentation guided by motion expressions. Unlike existing datasets that emphasize static attributes, MeViS highlights the importance of motion in understanding and tracking objects in videos. It includes over 33,000 annotated motion expressions across various scenarios, enabling researchers to benchmark multiple video segmentation methods. The study reveals limitations in current techniques and proposes a new approach, LMPM++, which sets new performance standards in motion expression-guided video understanding.', title='Unlocking Video Understanding with Motion Expressions'))
[17.12.2025 07:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫MeViSÁöÑÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåÊó®Âú®Êé¢Á¥¢Âü∫‰∫éËøêÂä®Ë°®ËææÁöÑÂèÇËÄÉËßÜÈ¢ëÂàÜÂâ≤„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´33,072‰∏™ÁªèËøá‰∫∫Â∑•Ê†áÊ≥®ÁöÑËøêÂä®Ë°®ËææÔºåÊ∂µÁõñ8,171‰∏™ÂØπË±°Âíå2,006‰∏™Â§çÊùÇÂú∫ÊôØÁöÑËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÁöÑÊï∞ÊçÆÈõÜ‰∏çÂêåÔºåMeViSÂº∫Ë∞É‰∫ÜËøêÂä®Âú®ËßÜÈ¢ëÂíåËØ≠Ë®Ä‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÊîØÊåÅÂ§öÁßçËßÜÈ¢ëÁêÜËß£‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑLMPM++ÊñπÊ≥ïÂú®Â§ö‰∏™‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûúÔºåÊé®Âä®‰∫ÜËøêÂä®Ë°®ËææÂºïÂØºÁöÑËßÜÈ¢ëÁêÜËß£ÁÆóÊ≥ïÁöÑÂèëÂ±ï„ÄÇ","title":"ËøêÂä®Ë°®ËææÂºïÂØºÁöÑËßÜÈ¢ëÁêÜËß£Êñ∞Âπ≥Âè∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫MeViSÁöÑÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåÊó®Âú®Êé¢Á¥¢Âü∫‰∫éËøêÂä®Ë°®ËææÁöÑÂèÇËÄÉËßÜÈ¢ëÂàÜÂâ≤„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´33,072‰∏™ÁªèËøá‰∫∫Â∑•Ê†áÊ≥®ÁöÑËøêÂä®Ë°®ËææÔºåÊ∂µÁõñ8,171‰∏™ÂØπË±°Âíå2,006‰∏™Â§çÊùÇÂú∫ÊôØÁöÑËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÁöÑÊï∞ÊçÆÈõÜ‰∏çÂêåÔºåMeViSÂº∫Ë∞É‰∫ÜËøêÂä®Âú®ËßÜÈ¢ëÂíåËØ≠Ë®Ä‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÊîØÊåÅÂ§öÁßçËßÜÈ¢ëÁêÜËß£‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑLMPM++ÊñπÊ≥ïÂú®Â§ö‰∏™‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûúÔºåÊé®Âä®‰∫ÜËøêÂä®Ë°®ËææÂºïÂØºÁöÑËßÜÈ¢ëÁêÜËß£ÁÆóÊ≥ïÁöÑÂèëÂ±ï„ÄÇ', title='ËøêÂä®Ë°®ËææÂºïÂØºÁöÑËßÜÈ¢ëÁêÜËß£Êñ∞Âπ≥Âè∞'))
[17.12.2025 07:25] Renaming data file.
[17.12.2025 07:25] Renaming previous data. hf_papers.json to ./d/2025-12-17.json
[17.12.2025 07:25] Saving new data file.
[17.12.2025 07:25] Generating page.
[17.12.2025 07:25] Renaming previous page.
[17.12.2025 07:25] Renaming previous data. index.html to ./d/2025-12-17.html
[17.12.2025 07:25] Writing result.
[17.12.2025 07:25] Renaming log file.
[17.12.2025 07:25] Renaming previous data. log.txt to ./logs/2025-12-17_last_log.txt
