[30.09.2025 06:19] Read previous papers.
[30.09.2025 06:19] Generating top page (month).
[30.09.2025 06:19] Writing top page (month).
[30.09.2025 07:12] Read previous papers.
[30.09.2025 07:12] Get feed.
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24006
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23102
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24897
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25190
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24900
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23426
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25175
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25160
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24695
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23909
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24014
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24007
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25123
[30.09.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.25106
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22799
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24663
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24473
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22824
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25161
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23808
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22572
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25176
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25191
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23951
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23285
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23196
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24981
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25077
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24193
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23866
[30.09.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.25084
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21953
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25185
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25131
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24786
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24335
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23371
[30.09.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.24285
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22570
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25149
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25052
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24910
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24269
[30.09.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.23924
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23143
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22830
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22518
[30.09.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.24908
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24709
[30.09.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.23338
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23115
[30.09.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16538
[30.09.2025 07:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.09.2025 07:12] No deleted papers detected.
[30.09.2025 07:12] Downloading and parsing papers (pdf, html). Total: 52.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24006.
[30.09.2025 07:12] Downloading paper 2509.24006 from http://arxiv.org/pdf/2509.24006v1...
[30.09.2025 07:12] Failed to download and parse paper https://huggingface.co/papers/2509.24006: 'LTChar' object is not iterable
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23102.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23102.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23102.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24897.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24897.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24897.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25190.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25190.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25190.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24900.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24900.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24900.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23426.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23426.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23426.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25175.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25175.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25175.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25160.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25160.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25160.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24695.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24695.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24695.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23909.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23909.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23909.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24014.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24014.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24014.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24007.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24007.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24007.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25123.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25123.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25123.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25106.
[30.09.2025 07:12] Downloading paper 2509.25106 from http://arxiv.org/pdf/2509.25106v1...
[30.09.2025 07:12] Extracting affiliations from text.
[30.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 6 0 1 5 2 . 9 0 5 2 : r a TOWARDS PERSONALIZED DEEP RESEARCH: BENCHMARKS AND EVALUATIONS Yuan Liang, Jiaxian Li, Yuqing Wang, Piaohong Wang, Motong Tian, Pai Liu, Shuofei Qiao, Runnan Fang, He Zhu, Ge Zhang, Minghao Liu, Yuchen Eleanor Jiang, Ningyu Zhang, Wangchunshu Zhou OPPO Zhejiang University M-A-P 2077.AI {liang_yuan,zhangningyu}@zju.edu.cn,zhouwangchunshu@oppo.com "
[30.09.2025 07:12] Response: ```python
["OPPO", "Zhejiang University"]
```
[30.09.2025 07:12] Deleting PDF ./assets/pdf/2509.25106.pdf.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.22799.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.22799.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.22799.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24663.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24663.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24663.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24473.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24473.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24473.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.22824.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.22824.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.22824.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25161.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25161.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25161.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23808.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23808.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23808.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.22572.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.22572.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.22572.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25176.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25176.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25176.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25191.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25191.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25191.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23951.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23951.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23951.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23285.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23285.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23285.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23196.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23196.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23196.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24981.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24981.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24981.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25077.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25077.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25077.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24193.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24193.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24193.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23866.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23866.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23866.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25084.
[30.09.2025 07:12] Downloading paper 2509.25084 from http://arxiv.org/pdf/2509.25084v1...
[30.09.2025 07:12] Extracting affiliations from text.
[30.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 4 8 0 5 2 . 9 0 5 2 : r a SCALING GENERALIST DATA-ANALYTIC AGENTS Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen Zhejiang University Alibaba Group {shuofei,zhangningyu,huajunsir}@zju.edu.cn "
[30.09.2025 07:12] Response: ```python
["Zhejiang University", "Alibaba Group"]
```
[30.09.2025 07:12] Deleting PDF ./assets/pdf/2509.25084.pdf.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.21953.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.21953.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.21953.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25185.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25185.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25185.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25131.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25131.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25131.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24786.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24786.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24786.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24335.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24335.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24335.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23371.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.23371.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.23371.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24285.
[30.09.2025 07:12] Downloading paper 2509.24285 from http://arxiv.org/pdf/2509.24285v1...
[30.09.2025 07:12] Extracting affiliations from text.
[30.09.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SCI-VERIFIER: SCIENTIFIC VERIFIER WITH THINKING Shenghe Zheng1,2, Chenyu Huang3, Fangchen Yu1,6, Junchi Yao1,7, Jingqi Ye1,8, Tao Chen3, Yun Luo1, Ning Ding1,5, Lei Bai1, Ganqu Cui1, Peng Ye1,4 1 Shanghai AI Laboratory 4 CUHK 5 Tsinghua University 2 Harbin Institute of Technology 6 CUHK-Shenzhen 7 UESTC 8 USTC 3 Fudan University 5 2 0 S 9 2 ] . [ 1 5 8 2 4 2 . 9 0 5 2 : r a "
[30.09.2025 07:12] Response: ```python
["Shanghai AI Laboratory", "CUHK", "Tsinghua University", "Harbin Institute of Technology", "CUHK-Shenzhen", "UESTC", "USTC", "Fudan University"]
```
[30.09.2025 07:12] Deleting PDF ./assets/pdf/2509.24285.pdf.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.22570.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.22570.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.22570.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25149.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25149.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25149.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.25052.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.25052.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.25052.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24910.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24910.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24910.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.24269.
[30.09.2025 07:12] Extra JSON file exists (./assets/json/2509.24269.json), skip PDF parsing.
[30.09.2025 07:12] Paper image links file exists (./assets/img_data/2509.24269.json), skip HTML parsing.
[30.09.2025 07:12] Success.
[30.09.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2509.23924.
[30.09.2025 07:12] Downloading paper 2509.23924 from http://arxiv.org/pdf/2509.23924v1...
[30.09.2025 07:13] Extracting affiliations from text.
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TAMING MASKED DIFFUSION LANGUAGE MODELS VIA CONSISTENCY TRAJECTORY REINFORCEMENT LEARNING WITH FEWER DECODING STEP Jingyi Yang1, 2 Guanxu Chen2, 3 Xuhao Hu1, 2 1Fudan University 3Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory Jing Shao2 5 2 0 2 8 ] . [ 1 4 2 9 3 2 . 9 0 5 2 : r yangjingyi946@gmail.com, shaojing@pjlab.org.cn Code: https://github.com/yjyddq/EOSER-ASS-RL "
[30.09.2025 07:13] Response: ```python
["Fudan University", "Shanghai Jiao Tong University", "Shanghai Artificial Intelligence Laboratory"]
```
[30.09.2025 07:13] Deleting PDF ./assets/pdf/2509.23924.pdf.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.23143.
[30.09.2025 07:13] Extra JSON file exists (./assets/json/2509.23143.json), skip PDF parsing.
[30.09.2025 07:13] Paper image links file exists (./assets/img_data/2509.23143.json), skip HTML parsing.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.22830.
[30.09.2025 07:13] Extra JSON file exists (./assets/json/2509.22830.json), skip PDF parsing.
[30.09.2025 07:13] Paper image links file exists (./assets/img_data/2509.22830.json), skip HTML parsing.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.22518.
[30.09.2025 07:13] Extra JSON file exists (./assets/json/2509.22518.json), skip PDF parsing.
[30.09.2025 07:13] Paper image links file exists (./assets/img_data/2509.22518.json), skip HTML parsing.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.24908.
[30.09.2025 07:13] Downloading paper 2509.24908 from http://arxiv.org/pdf/2509.24908v1...
[30.09.2025 07:13] Extracting affiliations from text.
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 8 0 9 4 2 . 9 0 5 2 : r BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications BOE-XSUM: Resumenes Extremos en Lenguaje Claro de Decretos Notificaciones Legales en Espanol Andres Fernandez Garcıa1,, Javier de la Rosa2,, Julio Gonzalo1, Roser Morante1, Enrique Amigo1, Alejandro Benito-Santos1, Jorge Carrillo-de-Albornoz1 ,Vıctor Fresno1, Adrian Ghajari1, Guillermo Marco1, Laura Plaza1, Eva Sanchez Salido1 . 1Universidad Nacional de Educacion Distancia, Spain 2The National Library of Norway, Norway Correspondence: nandezgarcia@gmail.com Abstract: The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spains Boletın Oficial del Estado (BOE), the State Official Gazette. Each entry in the dataset includes short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOEXSUM, comparing them to general-purpose generative models in zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing modelBERTIN GPT-J 6B (32-bit precision)achieves 24% performance gain over the top zero-shot model, DeepSeekR1 (accuracies of 41.6% vs. 33.5%). Keywords: Extreme-summarization, legal texts, generative models, evaluation resources. Resumen: La capacidad de resumir documentos largos de forma concisa es cada vez mas importante en la vida cotidiana debido la sobrecarga de informacion, pero existe una notable escasez de este tipo de resumenes para documentos en espanol en general, en el ambito jurıdico en particular. En este trabajo, presentamos BOEXSUM, un conju"
[30.09.2025 07:13] Response: ```python
["Universidad Nacional de Educacion Distancia, Spain", "The National Library of Norway, Norway"]
```
[30.09.2025 07:13] Deleting PDF ./assets/pdf/2509.24908.pdf.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.24709.
[30.09.2025 07:13] Extra JSON file exists (./assets/json/2509.24709.json), skip PDF parsing.
[30.09.2025 07:13] Paper image links file exists (./assets/img_data/2509.24709.json), skip HTML parsing.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.23338.
[30.09.2025 07:13] Downloading paper 2509.23338 from http://arxiv.org/pdf/2509.23338v1...
[30.09.2025 07:13] Extracting affiliations from text.
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 8 3 3 3 2 . 9 0 5 2 : r PARROT: Benchmark for Evaluating LLMs in Cross-System SQL Translation Wei Zhou1, Guoliang Li2, Haoyu Wang3, Yuxing Han3, Xufei Wu1, Fan Wu1, Xuanhe Zhou (cid:66)1 1 Shanghai Jiao Tong University 2 Tsinghua University 3 ByteDance weizhoudb@sjtu.edu.cn "
[30.09.2025 07:13] Response: ```python
["Shanghai Jiao Tong University", "Tsinghua University", "ByteDance"]
```
[30.09.2025 07:13] Deleting PDF ./assets/pdf/2509.23338.pdf.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.23115.
[30.09.2025 07:13] Extra JSON file exists (./assets/json/2509.23115.json), skip PDF parsing.
[30.09.2025 07:13] Paper image links file exists (./assets/img_data/2509.23115.json), skip HTML parsing.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Downloading and parsing paper https://huggingface.co/papers/2509.16538.
[30.09.2025 07:13] Extra JSON file exists (./assets/json/2509.16538.json), skip PDF parsing.
[30.09.2025 07:13] Paper image links file exists (./assets/img_data/2509.16538.json), skip HTML parsing.
[30.09.2025 07:13] Success.
[30.09.2025 07:13] Enriching papers with extra data.
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 0. SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.  					AI-generated summary 				 In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bot...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 1. Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.  					AI-generated summary 				 Reinforcement learning from human feedback (RLHF) has emerged as the standard pa...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 2. RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.  					AI-generated summary 				 The integration of visual understanding and generation into uni...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 3. Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.  					AI-generated summary 				 Reinforcement learning based post-training has recent...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 4. OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.  					AI-generated summary 				 The performance of unified multimodal models for image generation and editing is fundamentally c...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 5. ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.  					AI-generated summary 				 AI scientists are emerging computational systems that serve as collaborative partners in dis...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 6. EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.  					AI-generated summary 				 Large language model (LLM) steering has emerged as a promising paradigm for controlling mod...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 7. GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.  					AI-generated summary 				 Vision language models (VLMs) achieve unified modeling of images and text, enabling them to...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 8. SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.  					AI-generated summary 				 We introduce SAN...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 9. A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.  					AI-generated summary 				 Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 10. SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.  					AI-generated summary 				 While diffusion language models (DLMs) offer a...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 11. Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.  					AI-generated summary 				 Diffusion language models (DLMs) have strong th...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 12. Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  					AI-generated summary 				 Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? T...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 13. A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.  					AI-generated summary 				 Deep Research Agents (DRAs) can autonomously conduct complex investigations...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 14. VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.  					AI-generated summary 				 Recent advances in text-to-video generation have produced increasingly realistic and div...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 15. A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.  					AI-generated summary 				 Long-sequence processing is a critical capability for modern large language mode...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 16. Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.  					AI-generated summary 				 Spatial intelligence spans a rich suite of abilities, including visualising and transforming sha...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 17. Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.  					AI-generated summary 				 Reinforcement Learning (RL) has emerged as a popular training paradigm, pa...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 18. Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.  					AI-generated summary 				 Streaming video generation, as one fundamental componen...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 19. Re-examining the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards through hidden-state analysis reveals opportunities for simultaneous enhancement using Effective Rank and its derivatives, leading to improved performance in diverse benchmarks.  					AI-generated su...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 20. Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.  					AI-generated summary 				 Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocatin...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 21. SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.  					AI-generated summary 				 We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleav...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 22. VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  					AI-generated summary 				 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 23. HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.  					AI-generated summary 				 We present HunyuanImage 3.0, a native mul...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 24. Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.  					AI-generated summary 				 Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 25. Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.  					AI-generated summary 				 Recent reasoning LLMs (RLMs), especially ...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 26. ROVER, a minimalist RL method, achieves superior performance and diversity in LLM math reasoning by leveraging Q-values from a fixed random policy, bypassing complex policy iteration.  					AI-generated summary 				 RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving ...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 27. BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  					AI-generated summary 				 Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 28. AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.  					AI-generated summary 				 Search-augmented LLMs often struggle with comp...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 29. DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.  					AI-generated summary 				 Vision-language model (VLM) based GUI...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 30. DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.  					AI-generated summary 				 Data-analytic agents...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 31. MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.  					AI-generated summary 				 Multi-subject im...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 32. PixelCraft, a multi-agent system, enhances visual reasoning in multimodal large language models by integrating high-fidelity image processing and flexible reasoning through a dynamic workflow and image memory.  					AI-generated summary 				 Structured images (e.g., charts and geometric diagrams) re...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 33. MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.  					AI-generated summary 				 We present MGM-Omni, a unified Omni LLM for omni-modal understanding and exp...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 34. LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.  					AI-generated summary 				 Long video understanding is still challenging for recent Large Video-Language ...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 35. SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.  					AI-generated summary 				 Autoregressive (AR) models are promising for image generation, ...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 36. Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.  					AI-generated summary 				 Preference optimization is crucial for aligning l...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 37. A framework combining SCI-VerifyBench and SCI-Verifier addresses challenges in verifying LLM-generated scientific answers through cross-disciplinary benchmarks and reasoning-augmented verification.  					AI-generated summary 				 As large language models (LLMs) are increasingly applied to scientific...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 38. UniMIC, a unified token-based framework, enhances multimodal communication by using compact tokenized representations and lightweight Transformer-based entropy models, achieving significant bitrate savings without compromising performance.  					AI-generated summary 				 The rapid progress of Large ...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 39. A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.  					AI-generated summary 				 Large Language Models...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 40. CEL, a novel agent architecture using a Large Language Model, learns to master complex environments through explicit reasoning and planning, achieving success in diverse grid-world tasks with sparse rewards.  					AI-generated summary 				 The pursuit of artificial agents that can learn to master co...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 41. SID, a self-improving demonstration approach, enhances exploration and generalization in goal-oriented language-guided navigation tasks, achieving state-of-the-art performance.  					AI-generated summary 				 Goal-oriented language-guided navigation requires robust exploration capabilities for agent...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 42. AdvChain enhances the safety and reliability of large reasoning models by teaching them dynamic self-correction through adversarial chain-of-thought tuning.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chai...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 43. Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.  					AI-generated summary 				 Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive ...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 44. MathBode provides a diagnostic for mathematical reasoning in LLMs by analyzing frequency-resolved metrics of model outputs compared to exact solutions, revealing systematic low-pass behavior and phase lag.  					AI-generated summary 				 This paper presents MathBode, a dynamic diagnostic for mathema...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 45. ChatInject, a novel attack exploiting structured chat templates and persuasive multi-turn dialogues, significantly enhances attack success rates on large language model-based agents compared to traditional methods.  					AI-generated summary 				 The growing deployment of large language model (LLM) ...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 46. The Reasoning Manifold framework quantifies and localizes reasoning failures in Large Language Models by analyzing geometric deviations in internal representations.  					AI-generated summary 				 Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms i...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 47. BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.  					AI-generated summary 				 The ability to summarize long documents succinctly is increasingly important in daily life due t...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 48. IWR-Bench evaluates Large Vision-Language Models in reconstructing interactive webpages from video, highlighting challenges in multi-modal reasoning and code generation.  					AI-generated summary 				 The webpage-to-code task requires models to understand visual representations of webpages and gene...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 49. PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.  					AI-generated summary 				 Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely relat...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 50. RHYTHM uses hierarchical temporal tokenization and large language models to predict human mobility, capturing long-range dependencies and multi-scale periodic behaviors efficiently.  					AI-generated summary 				 Predicting human mobility is inherently challenging due to complex long-range dependen...
[30.09.2025 07:13] ********************************************************************************
[30.09.2025 07:13] Abstract 51. VC-Inspector, a reference-free and factually grounded caption quality evaluator, uses large language models to generate pseudo captions and train a multimodal model, demonstrating superior performance in evaluating video captions across diverse domains.  					AI-generated summary 				 Video captions...
[30.09.2025 07:13] Read previous papers.
[30.09.2025 07:13] Generating reviews via LLM API.
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#video", "#training", "#optimization", "#architecture", "#diffusion"], "emoji": "⚡", "ru": {"title": "Ускорение видео-генерации через умное разделение внимания", "desc": "В статье представлен метод SLA (Sparse-Linear Attention), который ускоряет Diffusion Transformer модели для гене
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#training", "#alignment", "#optimization", "#rlhf"], "emoji": "🎮", "ru": {"title": "Многопользовательская игра для лучшего понимания предпочтений человека", "desc": "Исследователи предложили новый метод обучения языковых моделей на основе предпочтений людей, расширив подход N
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agi", "#multimodal", "#benchmark", "#survey", "#architecture"], "emoji": "🔄", "ru": {"title": "Унификация без синергии: почему объединение понимания и генерации пока не работает", "desc": "Исследователи представляют RealUnify - новый бенчмарк для оценки с
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#alignment", "#multimodal", "#3d", "#cv"], "emoji": "🧩", "ru": {"title": "Собираем пазл из визуальных данных для лучшего понимания", "desc": "Visual Jigsaw - это фреймворк самообучающегося reinforcement learning для улучшения визуального понимания у
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#synthetic", "#multimodal", "#dataset"], "emoji": "🎨", "ru": {"title": "Систематический подход к созданию данных — ключ к прорыву в мультимодальном AI", "desc": "Исследователи создали OpenGPT-4o-Image — крупномасштабный датасет для обучения му
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#data", "#dataset", "#science", "#open_source", "#multimodal", "#agents"], "emoji": "🔬", "ru": {"title": "Универсальная экосистема для создания AI-ученых", "desc": "ToolUniverse - это экосистема, которая стандартизирует и интегрирует инструменты, модели и данные для AI-ученых, обесп
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#training", "#alignment", "#inference", "#optimization", "#hallucinations", "#architecture"], "emoji": "🎛️", "ru": {"title": "Высокоскоростное управление LLM без переобучения", "desc": "EasySteer — это унифицированная система для эффективного управления поведением больших языковых м
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#survey", "#benchmark", "#reasoning", "#cv", "#dataset"], "emoji": "🧮", "ru": {"title": "Когда картинки ставят AI в тупик: визуальная математика как новый вызов для умных систем", "desc": "Исследователи создали новый бенчмарк GSM8K-V для оценки математического мышления vision-langua
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#video", "#training", "#inference", "#small_models", "#diffusion"], "emoji": "🎬", "ru": {"title": "Быстрая и экономичная генерация длинных видео высокого качества", "desc": "SANA-Video - это компактная диффузионная модель для генерации видео высокого разрешения до 720x1280 пикселей 
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#data", "#training", "#optimization", "#benchmark"], "emoji": "🎨", "ru": {"title": "Высококачественная reward модель - ключ к RL в редактировании изображений", "desc": "Исследователи создали специализированную reward модель EditScore для обучения с подкреплением в задаче реда
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#architecture", "#diffusion"], "emoji": "⚡", "ru": {"title": "Ускорение диффузионных моделей через умное разреженное внимание", "desc": "SparseD - это новый метод разреженного внимания для диффузионных языковых моделей, который решает 
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#training", "#optimization"], "emoji": "🔀", "ru": {"title": "Адаптивная генерация с переменной длиной блоков", "desc": "В работе представлена Sequential Diffusion Language Model (SDLM) - новая архитектура, которая улучшает предобученные авторегрессионн
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#reasoning", "#rl", "#synthetic", "#rlhf"], "emoji": "🧩", "ru": {"title": "RL учит LLM комбинировать навыки как конструктор", "desc": "Исследование показывает, что reinforcement learning позволяет большим языковым моделям приобретать новые навыки п
[30.09.2025 07:13] Querying the API.
[30.09.2025 07:13] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.  					AI-generated summary 				 Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.
[30.09.2025 07:13] Response: ```json
{
  "desc": "В статье представлен новый бенчмарк Personalized Deep Research Bench для оценки способностей агентов глубокого исследования (Deep Research Agents) к персонализации. Бенчмарк включает 50 исследовательских задач из 10 различных доменов и 25 аутентичных профилей пользователей, что дает 250 реалистичных запросов. Для оценки производительности предложен фреймворк PQR, который измеряет соответствие персонализации, качество контента и фактическую надежность. Работа закладывает основу для разработки и оценки следующего поколения персонализированных AI-ассистентов для исследований.",
  "emoji": "🔍",
  "title": "Персонализированная оценка AI-агентов для глубоких исследований"
}
```
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.  					AI-generated summary 				 Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants."

[30.09.2025 07:13] Response: ```python
['BENCHMARK', 'AGENTS']
```
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.  					AI-generated summary 				 Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants."

[30.09.2025 07:13] Response: ```python
["PERSONALIZATION"]
```
[30.09.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces the Personalized Deep Research Bench, a new benchmark designed to evaluate how well Deep Research Agents (DRAs) can personalize their responses based on different user profiles and tasks. It addresses the limitations of existing evaluations that often focus on closed-ended tasks and overlook personalized scenarios. By combining 50 diverse research tasks with 25 authentic user profiles, the benchmark creates 250 realistic queries to test DRAs. The proposed PQR Evaluation Framework measures Personalization Alignment, Content Quality, and Factual Reliability, providing a comprehensive assessment of DRA performance in personalized research contexts.","title":"Revolutionizing AI Research with Personalized Deep Research Bench"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces the Personalized Deep Research Bench, a new benchmark designed to evaluate how well Deep Research Agents (DRAs) can personalize their responses based on different user profiles and tasks. It addresses the limitations of existing evaluations that often focus on closed-ended tasks and overlook personalized scenarios. By combining 50 diverse research tasks with 25 authentic user profiles, the benchmark creates 250 realistic queries to test DRAs. The proposed PQR Evaluation Framework measures Personalization Alignment, Content Quality, and Factual Reliability, providing a comprehensive assessment of DRA performance in personalized research contexts.', title='Revolutionizing AI Research with Personalized Deep Research Bench'))
[30.09.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了一个新的基准测试，名为个性化深度研究基准，旨在评估深度研究代理在不同任务和用户档案下的个性化能力。该基准结合了50个多样化的研究任务和25个真实用户档案，生成了250个现实的用户-任务查询。为了评估系统性能，提出了PQR评估框架，分别测量个性化对齐、内容质量和事实可靠性。通过实验，本文揭示了当前系统在处理个性化深度研究方面的能力和局限性，为下一代个性化AI研究助手的开发和评估奠定了基础。","title":"个性化深度研究的未来基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了一个新的基准测试，名为个性化深度研究基准，旨在评估深度研究代理在不同任务和用户档案下的个性化能力。该基准结合了50个多样化的研究任务和25个真实用户档案，生成了250个现实的用户-任务查询。为了评估系统性能，提出了PQR评估框架，分别测量个性化对齐、内容质量和事实可靠性。通过实验，本文揭示了当前系统在处理个性化深度研究方面的能力和局限性，为下一代个性化AI研究助手的开发和评估奠定了基础。', title='个性化深度研究的未来基准'))
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#benchmark", "#video", "#rlhf", "#interpretability", "#alignment"], "emoji": "🎬", "ru": {"title": "Умная оценка AI-видео с объяснениями", "desc": "В статье представлена VideoScore2 — многомерная и интерпретируемая система для оценки качества видео, сгенерированных из текста. Модель 
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#reasoning", "#training", "#long_context"], "emoji": "⚡", "ru": {"title": "Умное переключение внимания для эффективной работы с длинными текстами", "desc": "Исследователи представили InfLLM-V2 — новый подход к обработке длинных последовательностей в 
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#multimodal", "#benchmark", "#transfer_learning"], "emoji": "📐", "ru": {"title": "Геометрия как ключ к пространственному интеллекту AI", "desc": "Исследователи создали датасет Euclid30K с 30 тысячами задач по планиметрии и стереометрии для обуч
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#benchmark", "#rl", "#rlhf", "#reasoning", "#training"], "emoji": "🔍", "ru": {"title": "Критическое мышление делает AI умнее", "desc": "В статье представлен метод Critique Reinforcement Learning (CRL), который обучает LLM генерировать критическ
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#games", "#video"], "emoji": "🎬", "ru": {"title": "Стриминг длинных видео без накопления ошибок", "desc": "Rolling Forcing - это новая техника генерации видео, которая решает проблему накопления ошибок при создании длинных видеопотоков. Метод исполь
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#benchmark", "#optimization"], "emoji": "⚖️", "ru": {"title": "Разделяй и властвуй: одновременное усиление исследования и эксплуатации в RL", "desc": "Исследователи переосмыслили компромисс между исследованием и эксплуатацией в обучении с подкреплен
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization"], "emoji": "🔄", "ru": {"title": "Динамический поиск экспертов: новое измерение для улучшения рассуждений LLM", "desc": "Исследование предлагает метод Dynamic Experts Search (DES), который улучшает рассуждения больших языковы
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#open_source", "#optimization"], "emoji": "🎯", "ru": {"title": "Умные рассуждения через сжатие и расширение контекста", "desc": "В статье представлен метод SIRI для обучения больших языковых моделей рассуждения с использованием reinforcement learnin
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#games", "#3d", "#optimization"], "emoji": "🎥", "ru": {"title": "Плотный синтез видов без COLMAP с помощью 3D Foundation Models", "desc": "Исследователи изучают применение 3D Foundation Models для плотного синтеза новых видов (Novel View Synthesis). Традиционные методы зависят от ме
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#architecture", "#data", "#open_source", "#diffusion", "#multimodal", "#training"], "emoji": "🎨", "ru": {"title": "Гигантская мультимодальная модель для генерации изображений с 80 миллиардами параметров", "desc": "HunyuanImage 3.0 - это мультимодальная модель с авторегрессивной архи
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#reasoning", "#training", "#rlhf"], "emoji": "🔧", "ru": {"title": "Умное использование инструментов через энтропию рассуждений", "desc": "Исследователи предлагают фреймворк Tool-Light для улучшения интеграции внешних инструментов в рассуждения больших яз
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#benchmark"], "emoji": "🔍", "ru": {"title": "От примеров к инсайтам: новый подход к few-shot рассуждениям", "desc": "Исследователи обнаружили, что современные reasoning LLM часто показывают худшие результаты при использовании few-sh
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#rlhf", "#optimization"], "emoji": "🎲", "ru": {"title": "Случайная политика превосходит сложные алгоритмы в математических рассуждениях", "desc": "В статье представлен ROVER - минималистичный метод обучения с подкреплением для улучшения математическ
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#training", "#data", "#optimization", "#rl", "#dataset", "#synthetic", "#cv"], "emoji": "🌉", "ru": {"title": "Мостик к точной оценке глубины через генерацию данных", "desc": "В статье представлен метод BRIDGE для улучшения моnocular depth estimation - задачи определения глубины сцен
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#small_models"], "emoji": "🔍", "ru": {"title": "Кооперативная самоигра для эффективного поиска и рассуждений", "desc": "В статье представлена AceSearcher — framework для кооперативной самоигры, который улучшает способности LLM к рас
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#rl", "#open_source", "#training", "#games", "#agents"], "emoji": "🤖", "ru": {"title": "Асинхронное обучение GUI агентов через децентрализованную архитектуру", "desc": "DART представляет собой децентрализованную архитектуру для обучения с подк
[30.09.2025 07:13] Querying the API.
[30.09.2025 07:13] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.  					AI-generated summary 				 Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.
[30.09.2025 07:13] Response: ```json
{
  "desc": "DataMind представляет новый подход к созданию open-source агентов для анализа данных, которые могут автоматизировать научные исследования. Исследователи разработали систему синтеза данных и обучения, которая решает проблемы недостатка качественных данных, неправильных стратегий обучения и нестабильной многошаговой работы с кодом. Метод включает детальную таксономию задач, стратегию сэмплирования траекторий с фильтрацией и динамические цели обучения, сочетающие supervised fine-tuning и reinforcement learning. Модель DataMind-14B достигла state-of-the-art результатов с 71.16% на бенчмарках анализа данных, превзойдя даже проприетарные модели вроде GPT-5.",
  "emoji": "📊",
  "title": "Open-source революция в автоматическом анализе данных"
}
```
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.  					AI-generated summary 				 Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research."

[30.09.2025 07:13] Response: ```python
["AGENTS", "DATASET", "DATA", "BENCHMARK", "TRAINING"]
```
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.  					AI-generated summary 				 Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research."

[30.09.2025 07:13] Response: ```python
["OPEN_SOURCE", "SCIENCE"]
```
[30.09.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DataMind is a framework designed to enhance the capabilities of open-source data-analytic agents by addressing key challenges in their development. It introduces a structured approach to task taxonomy and dynamic training objectives, allowing agents to handle diverse data formats and complex reasoning tasks more effectively. The framework utilizes advanced techniques like knowledge-augmented trajectory sampling and stable multi-turn rollouts to improve training efficiency and performance. As a result, DataMind achieves state-of-the-art results on data analysis benchmarks, outperforming existing proprietary models and providing valuable resources for future research.","title":"Empowering Open-Source Data-Analytic Agents with DataMind"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DataMind is a framework designed to enhance the capabilities of open-source data-analytic agents by addressing key challenges in their development. It introduces a structured approach to task taxonomy and dynamic training objectives, allowing agents to handle diverse data formats and complex reasoning tasks more effectively. The framework utilizes advanced techniques like knowledge-augmented trajectory sampling and stable multi-turn rollouts to improve training efficiency and performance. As a result, DataMind achieves state-of-the-art results on data analysis benchmarks, outperforming existing proprietary models and providing valuable resources for future research.', title='Empowering Open-Source Data-Analytic Agents with DataMind'))
[30.09.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DataMind 是一个旨在构建开源数据分析代理的框架，解决了数据资源不足、训练策略不当和多轮回合不稳定等关键挑战。它通过细化的任务分类和递归的简单到困难的任务组合机制，增加了合成查询的多样性和难度。DataMind 还采用了知识增强的轨迹采样策略和动态可调的训练目标，结合了监督学习和强化学习的损失。经过训练的 DataMind-14B 在多个数据分析基准测试中达到了最先进的性能，显示了其在自动化科学发现中的潜力。","title":"DataMind：开源数据分析的未来"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DataMind 是一个旨在构建开源数据分析代理的框架，解决了数据资源不足、训练策略不当和多轮回合不稳定等关键挑战。它通过细化的任务分类和递归的简单到困难的任务组合机制，增加了合成查询的多样性和难度。DataMind 还采用了知识增强的轨迹采样策略和动态可调的训练目标，结合了监督学习和强化学习的损失。经过训练的 DataMind-14B 在多个数据分析基准测试中达到了最先进的性能，显示了其在自动化科学发现中的潜力。', title='DataMind：开源数据分析的未来'))
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#leakage", "#alignment", "#multimodal", "#training", "#rl", "#architecture", "#synthetic"], "emoji": "🎨", "ru": {"title": "Точная генерация изображений с множественными объектами через разделение внимания", "desc": "Исследователи представили MultiCrafter - фреймворк для генерации из
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#interpretability", "#multimodal", "#agents"], "emoji": "🔍", "ru": {"title": "Мультиагентное визуальное рассуждение высокой точности", "desc": "PixelCraft — это мультиагентная система, которая улучшает визуальное рассуждение в мультимодальных LLM п
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#training", "#open_source", "#games", "#agi", "#long_context", "#multimodal", "#interpretability", "#audio", "#architecture"], "emoji": "🧠", "ru": {"title": "Единая модель для понимания и генерации речи с архитектурой мозг-рот", "desc": "MGM-Omni представляет собой унифицированную м
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#video", "#rl", "#training", "#reasoning", "#long_context", "#optimization", "#benchmark"], "emoji": "🔍", "ru": {"title": "Адаптивное масштабирование видео для лучшего понимания длинных роликов", "desc": "Исследователи представили LOVE-R1 - модель для понимания длинных видео, котора
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#architecture", "#diffusion"], "emoji": "🌐", "ru": {"title": "Гиперсферические ограничения для стабильной авторегрессионной генерации изображений", "desc": "В статье представлена SphereAR - авторегрессионная модель для генерации изображений, кото
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#alignment"], "emoji": "⚖️", "ru": {"title": "Умная балансировка данных для выравнивания LLM с человеческими предпочтениями", "desc": "В статье представлен MetaAPO - новый подход для выравнивания больших языковых моделей с человеческими предпоч
[30.09.2025 07:13] Querying the API.
[30.09.2025 07:13] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework combining SCI-VerifyBench and SCI-Verifier addresses challenges in verifying LLM-generated scientific answers through cross-disciplinary benchmarks and reasoning-augmented verification.  					AI-generated summary 				 As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains.
[30.09.2025 07:13] Response: ```json
{
  "desc": "В статье представлена система для проверки научных ответов, генерируемых большими языковыми моделями (LLM). Исследователи создали бенчмарк SCI-VerifyBench, который охватывает математику, физику, биологию, химию и общие научные вопросы. Также была разработана модель SCI-Verifier, которая использует усиленное рассуждение для верификации ответов в научных областях. Система решает проблемы существующих методов проверки, которые страдают от недостатка систематических стандартов и сложности кроссдисциплинарного применения.",
  "emoji": "🔬",
  "title": "Умная проверка научных ответов AI через междисциплинарные рассуждения"
}
```
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework combining SCI-VerifyBench and SCI-Verifier addresses challenges in verifying LLM-generated scientific answers through cross-disciplinary benchmarks and reasoning-augmented verification.  					AI-generated summary 				 As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains."

[30.09.2025 07:13] Response: ```python
['DATASET', 'BENCHMARK', 'MULTIMODAL']
```
[30.09.2025 07:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework combining SCI-VerifyBench and SCI-Verifier addresses challenges in verifying LLM-generated scientific answers through cross-disciplinary benchmarks and reasoning-augmented verification.  					AI-generated summary 				 As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains."

[30.09.2025 07:13] Response: ```python
['REASONING', 'SCIENCE']
```
[30.09.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a framework that combines SCI-VerifyBench and SCI-Verifier to improve the verification of scientific answers generated by large language models (LLMs). It identifies key challenges in current verification methods, such as lack of standardized evaluation and reliance on complex rule design. To tackle these issues, the authors introduce SCI-VerifyBench, a benchmark that spans multiple scientific disciplines and includes real LLM responses with domain-specific transformations. Additionally, they propose SCI-Verifier, a reasoning-augmented model that enhances verification accuracy through logical reasoning and equivalence judgment, ultimately aiming to increase the reliability of LLMs in scientific contexts.","title":"Enhancing Scientific Verification with LLMs through Reasoning and Benchmarks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a framework that combines SCI-VerifyBench and SCI-Verifier to improve the verification of scientific answers generated by large language models (LLMs). It identifies key challenges in current verification methods, such as lack of standardized evaluation and reliance on complex rule design. To tackle these issues, the authors introduce SCI-VerifyBench, a benchmark that spans multiple scientific disciplines and includes real LLM responses with domain-specific transformations. Additionally, they propose SCI-Verifier, a reasoning-augmented model that enhances verification accuracy through logical reasoning and equivalence judgment, ultimately aiming to increase the reliability of LLMs in scientific contexts.', title='Enhancing Scientific Verification with LLMs through Reasoning and Benchmarks'))
[30.09.2025 07:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一个结合SCI-VerifyBench和SCI-Verifier的框架，以解决验证大型语言模型（LLM）生成的科学答案的挑战。该框架通过跨学科基准和增强推理的验证方法，提升了答案验证的有效性。我们构建了SCI-VerifyBench，这是一个涵盖数学、物理、生物、化学等领域的跨学科基准，提供了真实的LLM响应和领域特定的等价转换。同时，SCI-Verifier作为一个统一的推理增强验证器，展示了强大的逻辑推理和等价判断能力，确保了科学领域验证的系统性和实用性。","title":"科学验证的新框架：SCI-VerifyBench与SCI-Verifier的结合"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一个结合SCI-VerifyBench和SCI-Verifier的框架，以解决验证大型语言模型（LLM）生成的科学答案的挑战。该框架通过跨学科基准和增强推理的验证方法，提升了答案验证的有效性。我们构建了SCI-VerifyBench，这是一个涵盖数学、物理、生物、化学等领域的跨学科基准，提供了真实的LLM响应和领域特定的等价转换。同时，SCI-Verifier作为一个统一的推理增强验证器，展示了强大的逻辑推理和等价判断能力，确保了科学领域验证的系统性和实用性。', title='科学验证的新框架：SCI-VerifyBench与SCI-Verifier的结合'))
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#data", "#games", "#optimization", "#multimodal", "#cv"], "emoji": "📡", "ru": {"title": "Токенизированное сжатие для эффективной мультимодальной коммуникации", "desc": "UniMIC представляет новый подход к сжатию мультимодальных данных для взаимодействия между устройствами и облачными
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training"], "emoji": "⚡", "ru": {"title": "Революция в обучении LLM: стабильная 4-битная точность", "desc": "Исследователи разработали новый метод обучения больших языковых моделей с использованием 4-битной точности NVFP4 вместо традиционной 8-битной 
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#interpretability", "#reasoning", "#games", "#agents"], "emoji": "🧠", "ru": {"title": "Агент, который учится мыслить: явное рассуждение вместо скрытых весов", "desc": "В статье представлена архитектура агента CEL (Cogito, ergo ludo), которая использует большую языковую модель
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#rl", "#agi", "#optimization", "#transfer_learning", "#agents"], "emoji": "🗺️", "ru": {"title": "Агенты учатся навигации, улучшая собственные демонстрации", "desc": "Статья представляет SID - подход для обучения навигационных агентов в задачах целевой навигации с языковым управление
[30.09.2025 07:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#security", "#alignment", "#rlhf"], "emoji": "⚖️", "ru": {"title": "Обучение AI самокоррекции через adversarial цепочки рассуждений", "desc": "Исследователи выявили проблему «эффекта снежного кома» в больших рассуждающих моделях (LRM), где небо
[30.09.2025 07:13] Querying the API.
[30.09.2025 07:13] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.  					AI-generated summary 				 Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.
[30.09.2025 07:14] Response: ```json
{
  "desc": "В работе исследуются masked diffusion language models (MDLM) как альтернатива autoregressive моделям, которые позволяют параллельное декодирование и гибкий порядок генерации. Авторы выявили проблемы с прямым переносом техник от AR моделей к MDLM, включая несоответствие между обучением и inference. Предложены новые стратегии декодирования EOS Early Rejection (EOSER) и Ascending Step-Size (ASS), а также алгоритм reinforcement learning CJ-GRPO для устранения несоответствий в траекториях оптимизации. Эксперименты на задачах reasoning показали эффективность предложенных методов для улучшения производительности MDLM.",
  "emoji": "🎯",
  "title": "Новые стратегии декодирования и RL для masked diffusion языковых моделей"
}
```
[30.09.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.  					AI-generated summary 				 Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL."

[30.09.2025 07:14] Response: ```python
['RL', 'INFERENCE', 'TRAINING', 'MATH', 'BENCHMARK']
```
[30.09.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.  					AI-generated summary 				 Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL."

[30.09.2025 07:14] Response: ```python
['DIFFUSION', 'OPTIMIZATION', 'REASONING']
```
[30.09.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper focuses on improving masked diffusion language models (MDLMs) by introducing new decoding strategies and reinforcement learning (RL) algorithms. The authors highlight that traditional methods used for autoregressive models are not optimal for MDLMs due to their unique non-causal decoding process. They propose two novel techniques, EOS Early Rejection (EOSER) and Ascending Step-Size (ASS), which enhance the efficiency of MDLMs during inference. Additionally, they introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) to align the training and inference processes, leading to better performance on reasoning tasks with fewer decoding steps.","title":"Unlocking the Power of Masked Diffusion Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper focuses on improving masked diffusion language models (MDLMs) by introducing new decoding strategies and reinforcement learning (RL) algorithms. The authors highlight that traditional methods used for autoregressive models are not optimal for MDLMs due to their unique non-causal decoding process. They propose two novel techniques, EOS Early Rejection (EOSER) and Ascending Step-Size (ASS), which enhance the efficiency of MDLMs during inference. Additionally, they introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) to align the training and inference processes, leading to better performance on reasoning tasks with fewer decoding steps.', title='Unlocking the Power of Masked Diffusion Language Models'))
[30.09.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出的解码策略和强化学习算法显著提升了掩蔽扩散语言模型（MDLMs）在推理过程中的性能和效率。MDLMs作为自回归语言模型（AR）的替代方案，具备并行解码和灵活生成顺序的优点，但现有的解码策略和强化学习算法尚未充分探索。我们提出的EOS早期拒绝（EOSER）和上升步长（ASS）解码调度器，能够充分发挥MDLMs的潜力，实现更少的解码步骤而保持竞争力的性能。此外，CJ-GRPO算法强调了回滚轨迹与优化轨迹之间的一致性，减少了由于跳步优化造成的优化误差。","title":"提升掩蔽扩散语言模型的解码效率与性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出的解码策略和强化学习算法显著提升了掩蔽扩散语言模型（MDLMs）在推理过程中的性能和效率。MDLMs作为自回归语言模型（AR）的替代方案，具备并行解码和灵活生成顺序的优点，但现有的解码策略和强化学习算法尚未充分探索。我们提出的EOS早期拒绝（EOSER）和上升步长（ASS）解码调度器，能够充分发挥MDLMs的潜力，实现更少的解码步骤而保持竞争力的性能。此外，CJ-GRPO算法强调了回滚轨迹与优化轨迹之间的一致性，减少了由于跳步优化造成的优化误差。', title='提升掩蔽扩散语言模型的解码效率与性能'))
[30.09.2025 07:14] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#reasoning", "#math", "#interpretability", "#dataset"], "emoji": "📊", "ru": {"title": "Частотный анализ математического мышления в LLM", "desc": "В статье представлен MathBode - новый диагностический инструмент для анализа математических способностей бо
[30.09.2025 07:14] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#security"], "emoji": "💬", "ru": {"title": "Обман через чат-шаблоны: новая угроза для AI-агентов", "desc": "Исследователи представили ChatInject - новый тип атаки на LLM-агентов, который использует структурированные chat-шаблоны для внедрения вредоносных инструкц
[30.09.2025 07:14] Using data from previous issue: {"categories": ["#architecture", "#data", "#interpretability", "#reasoning", "#multimodal"], "emoji": "🧠", "ru": {"title": "Геометрия мышления: как найти сбои в рассуждениях ИИ", "desc": "Исследователи предложили фреймворк Reasoning Manifold для анализа ошибок рассуждений в больших языковых моделях 
[30.09.2025 07:14] Querying the API.
[30.09.2025 07:14] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.  					AI-generated summary 				 The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is a notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, a curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spain's ``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each entry in the dataset includes a short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose generative models in a zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\% performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of 41.6\% vs.\ 33.5\%).
[30.09.2025 07:14] Response: ```json
{
  "desc": "Исследователи создали BOE-XSUM - датасет из 3,648 кратких резюме испанских юридических документов из официального бюллетеня Испании. Они сравнили производительность средних по размеру LLM, дообученных на этом датасете, с универсальными моделями в задаче суммаризации без предварительного обучения. Результаты показали, что специализированные модели значительно превосходят универсальные аналоги. Лучшая модель BERTIN GPT-J 6B показала на 24% лучшую точность по сравнению с лучшей zero-shot моделью.",
  "emoji": "⚖️",
  "title": "Специализация побеждает универсальность в суммаризации юридических текстов"
}
```
[30.09.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.  					AI-generated summary 				 The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is a notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, a curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spain's ``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each entry in the dataset includes a short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose generative models in a zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\% performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of 41.6\% vs.\ 33.5\%)."

[30.09.2025 07:14] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'MULTILINGUAL']
```
[30.09.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.  					AI-generated summary 				 The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is a notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, a curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spain's ``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each entry in the dataset includes a short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose generative models in a zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\% performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of 41.6\% vs.\ 33.5\%)."

[30.09.2025 07:14] Response: ```python
['TRANSLATION', 'LOW_RESOURCE']
```
[30.09.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces BOE-XSUM, a dataset specifically designed for summarizing Spanish legal documents. It contains 3,648 summaries that help address the lack of concise legal summaries in Spanish. The study shows that medium-sized large language models (LLMs) that are fine-tuned on this dataset perform better in zero-shot summarization tasks compared to general-purpose models. The results indicate a significant performance improvement, with the best model achieving a 24% higher accuracy than the leading zero-shot model.","title":"Fine-Tuned LLMs Excel in Spanish Legal Summarization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces BOE-XSUM, a dataset specifically designed for summarizing Spanish legal documents. It contains 3,648 summaries that help address the lack of concise legal summaries in Spanish. The study shows that medium-sized large language models (LLMs) that are fine-tuned on this dataset perform better in zero-shot summarization tasks compared to general-purpose models. The results indicate a significant performance improvement, with the best model achieving a 24% higher accuracy than the leading zero-shot model.', title='Fine-Tuned LLMs Excel in Spanish Legal Summarization'))
[30.09.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BOE-XSUM是一个西班牙法律文件摘要的数据集，展示了经过微调的中型大型语言模型在零样本摘要任务中的优越性。随着信息过载的加剧，简洁地总结长文档的能力变得越来越重要，但西班牙文档，尤其是法律领域的摘要仍然稀缺。该数据集包含3,648个来自西班牙《官方公报》的简明摘要，每个条目包括短摘要、原始文本及其文档类型标签。实验结果表明，经过BOE-XSUM微调的模型在性能上显著优于通用生成模型，最佳模型BERTIN GPT-J 6B的表现比顶级零样本模型DeepSeek-R1高出24%。","title":"微调模型在法律摘要中的优势"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BOE-XSUM是一个西班牙法律文件摘要的数据集，展示了经过微调的中型大型语言模型在零样本摘要任务中的优越性。随着信息过载的加剧，简洁地总结长文档的能力变得越来越重要，但西班牙文档，尤其是法律领域的摘要仍然稀缺。该数据集包含3,648个来自西班牙《官方公报》的简明摘要，每个条目包括短摘要、原始文本及其文档类型标签。实验结果表明，经过BOE-XSUM微调的模型在性能上显著优于通用生成模型，最佳模型BERTIN GPT-J 6B的表现比顶级零样本模型DeepSeek-R1高出24%。', title='微调模型在法律摘要中的优势'))
[30.09.2025 07:14] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#benchmark", "#reasoning", "#cv", "#multimodal"], "emoji": "🎬", "ru": {"title": "От видео к коду: новый вызов для AI в создании интерактивных веб-страниц", "desc": "В статье представлен IWR-Bench — новый бенчмарк для оценки способности больших vision
[30.09.2025 07:14] Querying the API.
[30.09.2025 07:14] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.  					AI-generated summary 				 Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.
[30.09.2025 07:14] Response: ```json
{
  "desc": "В статье представлен бенчмарк PARROT для оценки Cross-System SQL Translation - задачи перевода SQL-запросов между различными системами баз данных. Существующие SQL бенчмарки не подходят для этой задачи, так как фокусируются на ограниченном наборе систем и не учитывают специфичные диалекты SQL. PARROT включает 598 пар переводов из 38 открытых бенчмарков и реальных бизнес-сервисов, охватывая 22 промышленные системы баз данных. LLM показывают низкую точность менее 38.53% на этой задаче, что подчеркивает сложность межсистемного перевода SQL.",
  "emoji": "🦜",
  "title": "PARROT: реалистичный бенчмарк для перевода SQL между системами баз данных"
}
```
[30.09.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.  					AI-generated summary 				 Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/."

[30.09.2025 07:14] Response: ```python
['BENCHMARK']
```
[30.09.2025 07:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.  					AI-generated summary 				 Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/."

[30.09.2025 07:14] Response: ```python
['SURVEY', 'OPEN_SOURCE']
```
[30.09.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PARROT is a new benchmark designed to evaluate Cross-System SQL Translation, which is the process of converting SQL queries from one database system to another. It addresses the shortcomings of existing benchmarks that often focus on a narrow range of systems and fail to account for unique SQL dialects. The benchmark includes 598 translation pairs from various sources and offers different variants for testing, such as PARROT-Diverse and PARROT-Simple, to assess system-specific SQL understanding. By providing a public leaderboard and source code, PARROT aims to foster further research in this important area of machine learning and database management.","title":"PARROT: Advancing Cross-System SQL Translation Evaluation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PARROT is a new benchmark designed to evaluate Cross-System SQL Translation, which is the process of converting SQL queries from one database system to another. It addresses the shortcomings of existing benchmarks that often focus on a narrow range of systems and fail to account for unique SQL dialects. The benchmark includes 598 translation pairs from various sources and offers different variants for testing, such as PARROT-Diverse and PARROT-Simple, to assess system-specific SQL understanding. By providing a public leaderboard and source code, PARROT aims to foster further research in this important area of machine learning and database management.', title='PARROT: Advancing Cross-System SQL Translation Evaluation'))
[30.09.2025 07:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PARROT是一个用于评估跨系统SQL翻译的基准，旨在解决现有SQL基准的局限性。它包含来自38个开源基准和真实业务服务的598个翻译对，专门设计用于挑战系统特定的SQL理解。PARROT提供了多个基准变体，包括PARROT-Diverse和PARROT-Simple，覆盖22个生产级数据库系统。通过这个基准，研究人员可以更好地评估和改进跨系统SQL翻译的效果。","title":"PARROT：跨系统SQL翻译的全新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PARROT是一个用于评估跨系统SQL翻译的基准，旨在解决现有SQL基准的局限性。它包含来自38个开源基准和真实业务服务的598个翻译对，专门设计用于挑战系统特定的SQL理解。PARROT提供了多个基准变体，包括PARROT-Diverse和PARROT-Simple，覆盖22个生产级数据库系统。通过这个基准，研究人员可以更好地评估和改进跨系统SQL翻译的效果。', title='PARROT：跨系统SQL翻译的全新基准'))
[30.09.2025 07:14] Using data from previous issue: {"categories": ["#architecture", "#data", "#optimization", "#benchmark", "#open_source", "#reasoning", "#training", "#long_context", "#dataset"], "emoji": "🏃", "ru": {"title": "Ритмы движения: предсказание мобильности через иерархическую токенизацию", "desc": "В статье представлена модель RHYTHM для
[30.09.2025 07:14] Using data from previous issue: {"categories": ["#interpretability", "#video", "#multimodal", "#benchmark", "#optimization"], "emoji": "🎬", "ru": {"title": "Оценка видео субтитров без эталонов через фактическую обоснованность", "desc": "Исследователи представили VC-Inspector - новую систему оценки качества видео субтитров, которая
[30.09.2025 07:14] Renaming data file.
[30.09.2025 07:14] Renaming previous data. hf_papers.json to ./d/2025-09-30.json
[30.09.2025 07:14] Saving new data file.
[30.09.2025 07:14] Generating page.
[30.09.2025 07:14] Renaming previous page.
[30.09.2025 07:14] Renaming previous data. index.html to ./d/2025-09-30.html
[30.09.2025 07:14] Writing result.
[30.09.2025 07:14] Renaming log file.
[30.09.2025 07:14] Renaming previous data. log.txt to ./logs/2025-09-30_last_log.txt
