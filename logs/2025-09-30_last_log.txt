[30.09.2025 14:13] Read previous papers.
[30.09.2025 14:13] Generating top page (month).
[30.09.2025 14:13] Writing top page (month).
[30.09.2025 15:10] Read previous papers.
[30.09.2025 15:10] Get feed.
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24006
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22220
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23102
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24897
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24900
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23808
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25190
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24695
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23426
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22193
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25160
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23909
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24014
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25175
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25106
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24981
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24007
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22799
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22824
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25123
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24473
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22820
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25191
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25077
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25161
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24663
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25176
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23285
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22572
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25131
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25084
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23951
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23219
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25137
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22921
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23196
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23924
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23866
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24335
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24193
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25149
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24786
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24285
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21953
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25185
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23371
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23061
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24269
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24200
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23338
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23143
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22830
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22570
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25052
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24910
[30.09.2025 15:10] Extract page data from URL. URL: https://huggingface.co/papers/2509.24494
[30.09.2025 15:10] Extract page data from URL. URL: https://huggingface.co/papers/2509.24372
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22518
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24908
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24709
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24592
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23233
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23115
[30.09.2025 15:10] Extract page data from URL. URL: https://huggingface.co/papers/2509.24988
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23564
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21875
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19033
[30.09.2025 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16538
[30.09.2025 15:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.09.2025 15:10] No deleted papers detected.
[30.09.2025 15:10] Downloading and parsing papers (pdf, html). Total: 68.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24006.
[30.09.2025 15:10] Downloading paper 2509.24006 from http://arxiv.org/pdf/2509.24006v1...
[30.09.2025 15:10] Failed to download and parse paper https://huggingface.co/papers/2509.24006: 'LTChar' object is not iterable
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22220.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22220.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22220.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23102.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23102.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23102.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24897.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24897.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24897.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24900.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24900.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24900.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23808.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23808.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23808.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25190.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25190.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25190.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24695.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24695.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24695.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23426.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23426.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23426.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22193.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22193.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22193.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25160.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25160.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25160.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23909.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23909.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23909.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24014.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24014.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24014.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25175.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25175.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25175.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25106.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25106.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25106.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24981.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24981.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24981.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24007.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24007.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24007.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22799.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22799.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22799.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22824.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22824.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22824.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25123.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25123.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25123.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24473.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24473.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24473.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22820.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22820.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22820.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25191.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25191.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25191.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25077.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25077.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25077.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25161.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25161.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25161.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24663.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24663.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24663.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25176.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25176.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25176.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23285.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23285.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23285.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22572.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22572.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22572.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25131.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25131.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25131.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25084.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25084.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25084.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23951.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23951.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23951.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23219.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23219.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23219.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25137.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25137.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25137.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22921.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22921.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22921.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23196.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23196.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23196.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23924.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23924.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23924.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23866.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23866.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23866.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24335.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24335.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24335.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24193.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24193.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24193.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25149.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25149.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25149.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24786.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24786.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24786.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24285.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24285.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24285.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.21953.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.21953.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.21953.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25185.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25185.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25185.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23371.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23371.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23371.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23061.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23061.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23061.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24269.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24269.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24269.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24200.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24200.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24200.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23338.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23338.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23338.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.23143.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.23143.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.23143.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22830.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22830.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22830.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.22570.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.22570.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.22570.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.25052.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.25052.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.25052.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24910.
[30.09.2025 15:10] Extra JSON file exists (./assets/json/2509.24910.json), skip PDF parsing.
[30.09.2025 15:10] Paper image links file exists (./assets/img_data/2509.24910.json), skip HTML parsing.
[30.09.2025 15:10] Success.
[30.09.2025 15:10] Downloading and parsing paper https://huggingface.co/papers/2509.24494.
[30.09.2025 15:10] Downloading paper 2509.24494 from http://arxiv.org/pdf/2509.24494v1...
[30.09.2025 15:11] Extracting affiliations from text.
[30.09.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 4 9 4 4 2 . 9 0 5 2 : r Preprint. GRPO-MA: MULTI-ANSWER GENERATION IN GRPO FOR STABLE AND EFFICIENT CHAIN-OF-THOUGHT TRAINING Hongcheng Wang*1,2, Yinuo Huang*4,2, Sukai Wang3, Guanghui Ren(cid:134)3, and Hao Dong(cid:66)1,2 1CFCS, School of Computer Science, Peking University 2PKU-Agibot Joint Lab 3Agibot 4University of Electronic Science and Technology of China "
[30.09.2025 15:11] Response: ```python
[
    "CFCS, School of Computer Science, Peking University",
    "PKU-Agibot Joint Lab",
    "Agibot",
    "University of Electronic Science and Technology of China"
]
```
[30.09.2025 15:11] Deleting PDF ./assets/pdf/2509.24494.pdf.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.24372.
[30.09.2025 15:11] Downloading paper 2509.24372 from http://arxiv.org/pdf/2509.24372v1...
[30.09.2025 15:11] Extracting affiliations from text.
[30.09.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 2 7 3 4 2 . 9 0 5 2 : r EVOLUTION STRATEGIES AT SCALE: LLM FINETUNING BEYOND REINFORCEMENT LEARNING Xin Qiu Cognizant AI Lab Yulu Gan MIT Conor F. Hayes Cognizant AI Lab Qiyao Liang MIT Elliot Meyerson Cognizant AI Lab Babak Hodjat Cognizant AI Lab Risto Miikkulainen Cognizant AI Lab, UT Austin "
[30.09.2025 15:11] Response: ```python
["Cognizant AI Lab", "MIT", "UT Austin"]
```
[30.09.2025 15:11] Deleting PDF ./assets/pdf/2509.24372.pdf.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.22518.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.22518.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.22518.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.24908.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.24908.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.24908.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.24709.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.24709.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.24709.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.24592.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.24592.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.24592.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.23233.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.23233.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.23233.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.23115.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.23115.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.23115.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.24988.
[30.09.2025 15:11] Downloading paper 2509.24988 from http://arxiv.org/pdf/2509.24988v1...
[30.09.2025 15:11] Extracting affiliations from text.
[30.09.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 8 8 9 4 2 . 9 0 5 2 : r GENERALIZED CORRECTNESS MODELS: LEARNING CALIBRATED AND MODEL-AGNOSTIC CORRECTNESS PREDICTORS FROM HISTORICAL PATTERNS Hanqi Xiao1 Vaidehi Patil1 Hyunji Lee1 Elias Stengel-Eskin2 Mohit Bansal 1UNC Chapel Hill 2The University of Texas at Austin "
[30.09.2025 15:11] Response: ```python
["UNC Chapel Hill", "The University of Texas at Austin"]
```
[30.09.2025 15:11] Deleting PDF ./assets/pdf/2509.24988.pdf.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.23564.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.23564.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.23564.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.21875.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.21875.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.21875.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.19033.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.19033.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.19033.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.16538.
[30.09.2025 15:11] Extra JSON file exists (./assets/json/2509.16538.json), skip PDF parsing.
[30.09.2025 15:11] Paper image links file exists (./assets/img_data/2509.16538.json), skip HTML parsing.
[30.09.2025 15:11] Success.
[30.09.2025 15:11] Enriching papers with extra data.
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 0. SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.  					AI-generated summary 				 In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bot...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 1. StableToken, a multi-branch consensus-driven tokenizer, enhances token stability and robustness in speech processing, improving SpeechLLMs' performance under noisy conditions.  					AI-generated summary 				 Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprising...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 2. Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.  					AI-generated summary 				 Reinforcement learning from human feedback (RLHF) has emerged as the standard pa...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 3. RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.  					AI-generated summary 				 The integration of visual understanding and generation into uni...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 4. OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.  					AI-generated summary 				 The performance of unified multimodal models for image generation and editing is fundamentally c...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 5. Re-examining the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards through hidden-state analysis reveals opportunities for simultaneous enhancement using Effective Rank and its derivatives, leading to improved performance in diverse benchmarks.  					AI-generated su...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 6. Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.  					AI-generated summary 				 Reinforcement learning based post-training has recent...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 7. SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.  					AI-generated summary 				 We introduce SAN...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 8. ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.  					AI-generated summary 				 AI scientists are emerging computational systems that serve as collaborative partners in dis...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 9. Reasoning models enhance performance across various tasks, surpassing instruction fine-tuned models in reasoning-intensive and open-ended tasks, despite higher computational costs.  					AI-generated summary 				 Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 10. GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.  					AI-generated summary 				 Vision language models (VLMs) achieve unified modeling of images and text, enabling them to...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 11. A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.  					AI-generated summary 				 Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 12. SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.  					AI-generated summary 				 While diffusion language models (DLMs) offer a...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 13. EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.  					AI-generated summary 				 Large language model (LLM) steering has emerged as a promising paradigm for controlling mod...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 14. A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.  					AI-generated summary 				 Deep Research Agents (DRAs) can autonomously conduct complex investigations...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 15. ROVER, a minimalist RL method, achieves superior performance and diversity in LLM math reasoning by leveraging Q-values from a fixed random policy, bypassing complex policy iteration.  					AI-generated summary 				 RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 16. Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.  					AI-generated summary 				 Diffusion language models (DLMs) have strong th...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 17. VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.  					AI-generated summary 				 Recent advances in text-to-video generation have produced increasingly realistic and div...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 18. Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.  					AI-generated summary 				 Reinforcement Learning (RL) has emerged as a popular training paradigm, pa...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 19. Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  					AI-generated summary 				 Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? T...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 20. Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.  					AI-generated summary 				 Spatial intelligence spans a rich suite of abilities, including visualising and transforming sha...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 21. MMPB is a benchmark for evaluating the personalization capabilities of Vision-Language Models across various tasks and concepts, revealing significant challenges in maintaining consistency and adapting to user preferences.  					AI-generated summary 				 Visual personalization is essential in user-f...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 22. VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  					AI-generated summary 				 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 23. BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  					AI-generated summary 				 Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 24. Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.  					AI-generated summary 				 Streaming video generation, as one fundamental componen...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 25. A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.  					AI-generated summary 				 Long-sequence processing is a critical capability for modern large language mode...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 26. SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.  					AI-generated summary 				 We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleav...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 27. Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.  					AI-generated summary 				 Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 28. Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.  					AI-generated summary 				 Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocatin...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 29. MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.  					AI-generated summary 				 We present MGM-Omni, a unified Omni LLM for omni-modal understanding and exp...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 30. DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.  					AI-generated summary 				 Data-analytic agents...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 31. HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.  					AI-generated summary 				 We present HunyuanImage 3.0, a native mul...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 32. WirelessMathLM, a compact model trained with domain-specific reinforcement learning, achieves high accuracy on wireless mathematics problems and transfers well to general mathematics benchmarks.  					AI-generated summary 				 Large language models (LLMs) excel at general mathematical reasoning but ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 33. Reinforcement Learning from Human Interaction (RLHI) uses in-the-wild user conversations to improve conversational models, enhancing personalization and instruction-following through user-guided rewrites and persona-conditioned rewards.  					AI-generated summary 				 We posit that to achieve contin...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 34. A novel constrained reinforcement learning framework for LLM distillation maximizes task-specific rewards while maintaining constraint satisfaction without state augmentation or dual Lagrangian methods.  					AI-generated summary 				 We introduce a novel approach to large language model (LLM) disti...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 35. Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.  					AI-generated summary 				 Recent reasoning LLMs (RLMs), especially ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 36. Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.  					AI-generated summary 				 Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 37. DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.  					AI-generated summary 				 Vision-language model (VLM) based GUI...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 38. SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.  					AI-generated summary 				 Autoregressive (AR) models are promising for image generation, ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 39. AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.  					AI-generated summary 				 Search-augmented LLMs often struggle with comp...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 40. A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.  					AI-generated summary 				 Large Language Models...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 41. LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.  					AI-generated summary 				 Long video understanding is still challenging for recent Large Video-Language ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 42. A framework combining SCI-VerifyBench and SCI-Verifier addresses challenges in verifying LLM-generated scientific answers through cross-disciplinary benchmarks and reasoning-augmented verification.  					AI-generated summary 				 As large language models (LLMs) are increasingly applied to scientific...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 43. MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.  					AI-generated summary 				 Multi-subject im...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 44. PixelCraft, a multi-agent system, enhances visual reasoning in multimodal large language models by integrating high-fidelity image processing and flexible reasoning through a dynamic workflow and image memory.  					AI-generated summary 				 Structured images (e.g., charts and geometric diagrams) re...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 45. Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.  					AI-generated summary 				 Preference optimization is crucial for aligning l...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 46. DafnyCOMP evaluates large language models on generating compositional specifications in Dafny, highlighting their weaknesses in cross-functional reasoning.  					AI-generated summary 				 We introduce DafnyCOMP, a benchmark for evaluating large language models (LLMs) on compositional specification g...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 47. AdvChain enhances the safety and reliability of large reasoning models by teaching them dynamic self-correction through adversarial chain-of-thought tuning.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chai...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 48. UniVid combines video understanding and generation using an MLLM with a diffusion decoder, achieving state-of-the-art performance through Temperature Modality Alignment and Pyramid Reflection.  					AI-generated summary 				 Unified video modeling that combines generation and understanding capabilit...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 49. PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.  					AI-generated summary 				 Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely relat...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 50. MathBode provides a diagnostic for mathematical reasoning in LLMs by analyzing frequency-resolved metrics of model outputs compared to exact solutions, revealing systematic low-pass behavior and phase lag.  					AI-generated summary 				 This paper presents MathBode, a dynamic diagnostic for mathema...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 51. ChatInject, a novel attack exploiting structured chat templates and persuasive multi-turn dialogues, significantly enhances attack success rates on large language model-based agents compared to traditional methods.  					AI-generated summary 				 The growing deployment of large language model (LLM) ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 52. UniMIC, a unified token-based framework, enhances multimodal communication by using compact tokenized representations and lightweight Transformer-based entropy models, achieving significant bitrate savings without compromising performance.  					AI-generated summary 				 The rapid progress of Large ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 53. CEL, a novel agent architecture using a Large Language Model, learns to master complex environments through explicit reasoning and planning, achieving success in diverse grid-world tasks with sparse rewards.  					AI-generated summary 				 The pursuit of artificial agents that can learn to master co...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 54. SID, a self-improving demonstration approach, enhances exploration and generalization in goal-oriented language-guided navigation tasks, achieving state-of-the-art performance.  					AI-generated summary 				 Goal-oriented language-guided navigation requires robust exploration capabilities for agent...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 55. GRPO-MA improves the training of Chain-of-Thought reasoning in LLMs and VLMs by addressing gradient coupling, sparse rewards, and unstable advantage estimation through multi-answer generation.  					AI-generated summary 				 Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a ...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 56. Evolution strategies successfully scale to fine-tune large language models, outperforming reinforcement learning in sample efficiency and robustness.  					AI-generated summary 				 Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pip...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 57. The Reasoning Manifold framework quantifies and localizes reasoning failures in Large Language Models by analyzing geometric deviations in internal representations.  					AI-generated summary 				 Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms i...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 58. BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.  					AI-generated summary 				 The ability to summarize long documents succinctly is increasingly important in daily life due t...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 59. IWR-Bench evaluates Large Vision-Language Models in reconstructing interactive webpages from video, highlighting challenges in multi-modal reasoning and code generation.  					AI-generated summary 				 The webpage-to-code task requires models to understand visual representations of webpages and gene...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 60. BPMN Assistant uses Large Language Models to create and edit BPMN diagrams, evaluating process generation and editing performance using JSON and XML representations.  					AI-generated summary 				 This paper presents BPMN Assistant, a tool that leverages Large Language Models (LLMs) for natural lan...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 61. CLAIRE, an agentic system combining LLM reasoning and retrieval, improves Wikipedia accuracy by detecting inconsistencies, with human editors reporting higher confidence and identifying more issues.  					AI-generated summary 				 Wikipedia is the largest open knowledge corpus, widely used worldwide...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 62. RHYTHM uses hierarchical temporal tokenization and large language models to predict human mobility, capturing long-range dependencies and multi-scale periodic behaviors efficiently.  					AI-generated summary 				 Predicting human mobility is inherently challenging due to complex long-range dependen...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 63. Generalized Correctness Models (GCMs) improve LLM confidence estimation by leveraging historical correctness data, outperforming self-knowledge approaches and demonstrating generalizability across models and datasets.  					AI-generated summary 				 Generating accurate and calibrated confidence esti...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 64. PrefCleanBench evaluates 13 preference data cleaning methods for aligning large language models with human preferences, providing a standardized protocol to assess their effectiveness and generalizability.  					AI-generated summary 				 Human feedback plays a pivotal role in aligning large language...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 65. LUMINA detects hallucinations in RAG systems by quantifying external context utilization and internal knowledge utilization, outperforming existing methods on benchmarks.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LL...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 66. The study analyzes research trends in Italian Computational Linguistics and Natural Language Processing by compiling and examining the proceedings of the CLiC-it conference from 2014 to 2024.  					AI-generated summary 				 Over the past decade, Computational Linguistics (CL) and Natural Language Pr...
[30.09.2025 15:11] ********************************************************************************
[30.09.2025 15:11] Abstract 67. VC-Inspector, a reference-free and factually grounded caption quality evaluator, uses large language models to generate pseudo captions and train a multimodal model, demonstrating superior performance in evaluating video captions across diverse domains.  					AI-generated summary 				 Video captions...
[30.09.2025 15:11] Read previous papers.
[30.09.2025 15:11] Generating reviews via LLM API.
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#video", "#training", "#optimization", "#architecture", "#diffusion"], "emoji": "⚡", "ru": {"title": "Ускорение видео-генерации через умное разделение внимания", "desc": "В статье представлен метод SLA (Sparse-Linear Attention), который ускоряет Diffusion Transformer модели для гене
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#architecture", "#optimization"], "emoji": "🗳️", "ru": {"title": "Стабильные токены через голосование веток", "desc": "Существующие семантические токенизаторы для речи оказываются хрупкими и сильно меняют свой вывод даже при небольших акустических возмущения
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#alignment", "#optimization", "#rlhf"], "emoji": "🎮", "ru": {"title": "Многопользовательская игра для лучшего понимания предпочтений человека", "desc": "Исследователи предложили новый метод обучения языковых моделей на основе предпочтений людей, расширив подход N
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agi", "#multimodal", "#benchmark", "#survey", "#architecture"], "emoji": "🔄", "ru": {"title": "Унификация без синергии: почему объединение понимания и генерации пока не работает", "desc": "Исследователи представляют RealUnify - новый бенчмарк для оценки с
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#synthetic", "#multimodal", "#dataset"], "emoji": "🎨", "ru": {"title": "Систематический подход к созданию данных — ключ к прорыву в мультимодальном AI", "desc": "Исследователи создали OpenGPT-4o-Image — крупномасштабный датасет для обучения му
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#benchmark", "#optimization"], "emoji": "⚖️", "ru": {"title": "Разделяй и властвуй: одновременное усиление исследования и эксплуатации в RL", "desc": "Исследователи переосмыслили компромисс между исследованием и эксплуатацией в обучении с подкреплен
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#alignment", "#multimodal", "#3d", "#cv"], "emoji": "🧩", "ru": {"title": "Собираем пазл из визуальных данных для лучшего понимания", "desc": "Visual Jigsaw - это фреймворк самообучающегося reinforcement learning для улучшения визуального понимания у
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#video", "#training", "#inference", "#small_models", "#diffusion"], "emoji": "🎬", "ru": {"title": "Быстрая и экономичная генерация длинных видео высокого качества", "desc": "SANA-Video - это компактная диффузионная модель для генерации видео высокого разрешения до 720x1280 пикселей 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#data", "#dataset", "#science", "#open_source", "#multimodal", "#agents"], "emoji": "🔬", "ru": {"title": "Универсальная экосистема для создания AI-ученых", "desc": "ToolUniverse - это экосистема, которая стандартизирует и интегрирует инструменты, модели и данные для AI-ученых, обесп
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#math", "#reasoning", "#synthetic", "#data", "#training"], "emoji": "🧠", "ru": {"title": "Рассуждение побеждает размер: как reasoning модели превосходят крупные IFT системы", "desc": "Исследование сравнивает модели с возможностями рассуждения и модели, обученные на инструкциях (Inst
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#survey", "#benchmark", "#reasoning", "#cv", "#dataset"], "emoji": "🧮", "ru": {"title": "Когда картинки ставят AI в тупик: визуальная математика как новый вызов для умных систем", "desc": "Исследователи создали новый бенчмарк GSM8K-V для оценки математического мышления vision-langua
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#data", "#training", "#optimization", "#benchmark"], "emoji": "🎨", "ru": {"title": "Высококачественная reward модель - ключ к RL в редактировании изображений", "desc": "Исследователи создали специализированную reward модель EditScore для обучения с подкреплением в задаче реда
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#architecture", "#diffusion"], "emoji": "⚡", "ru": {"title": "Ускорение диффузионных моделей через умное разреженное внимание", "desc": "SparseD - это новый метод разреженного внимания для диффузионных языковых моделей, который решает 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#training", "#alignment", "#inference", "#optimization", "#hallucinations", "#architecture"], "emoji": "🎛️", "ru": {"title": "Высокоскоростное управление LLM без переобучения", "desc": "EasySteer — это унифицированная система для эффективного управления поведением больших языковых м
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#personalization"], "emoji": "🔍", "ru": {"title": "Персонализированная оценка AI-агентов для глубоких исследований", "desc": "В статье представлен новый бенчмарк Personalized Deep Research Bench для оценки способностей агентов глубокого исследования (Deep Re
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#rlhf", "#optimization"], "emoji": "🎲", "ru": {"title": "Случайная политика превосходит сложные алгоритмы в математических рассуждениях", "desc": "В статье представлен ROVER - минималистичный метод обучения с подкреплением для улучшения математическ
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#training", "#optimization"], "emoji": "🔀", "ru": {"title": "Адаптивная генерация с переменной длиной блоков", "desc": "В работе представлена Sequential Diffusion Language Model (SDLM) - новая архитектура, которая улучшает предобученные авторегрессионн
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#video", "#rlhf", "#interpretability", "#alignment"], "emoji": "🎬", "ru": {"title": "Умная оценка AI-видео с объяснениями", "desc": "В статье представлена VideoScore2 — многомерная и интерпретируемая система для оценки качества видео, сгенерированных из текста. Модель 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#benchmark", "#rl", "#rlhf", "#reasoning", "#training"], "emoji": "🔍", "ru": {"title": "Критическое мышление делает AI умнее", "desc": "В статье представлен метод Critique Reinforcement Learning (CRL), который обучает LLM генерировать критическ
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#reasoning", "#rl", "#synthetic", "#rlhf"], "emoji": "🧩", "ru": {"title": "RL учит LLM комбинировать навыки как конструктор", "desc": "Исследование показывает, что reinforcement learning позволяет большим языковым моделям приобретать новые навыки п
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#multimodal", "#benchmark", "#transfer_learning"], "emoji": "📐", "ru": {"title": "Геометрия как ключ к пространственному интеллекту AI", "desc": "Исследователи создали датасет Euclid30K с 30 тысячами задач по планиметрии и стереометрии для обуч
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#cv", "#benchmark", "#multimodal"], "emoji": "👤", "ru": {"title": "Персонализация Vision-Language Models: новый benchmark выявил серьезные ограничения", "desc": "Исследователи создали MMPB - первый крупный benchmark для оценки способностей Vision-L
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#games", "#3d", "#optimization"], "emoji": "🎥", "ru": {"title": "Плотный синтез видов без COLMAP с помощью 3D Foundation Models", "desc": "Исследователи изучают применение 3D Foundation Models для плотного синтеза новых видов (Novel View Synthesis). Традиционные методы зависят от ме
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#training", "#data", "#optimization", "#rl", "#dataset", "#synthetic", "#cv"], "emoji": "🌉", "ru": {"title": "Мостик к точной оценке глубины через генерацию данных", "desc": "В статье представлен метод BRIDGE для улучшения моnocular depth estimation - задачи определения глубины сцен
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#games", "#video"], "emoji": "🎬", "ru": {"title": "Стриминг длинных видео без накопления ошибок", "desc": "Rolling Forcing - это новая техника генерации видео, которая решает проблему накопления ошибок при создании длинных видеопотоков. Метод исполь
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#reasoning", "#training", "#long_context"], "emoji": "⚡", "ru": {"title": "Умное переключение внимания для эффективной работы с длинными текстами", "desc": "Исследователи представили InfLLM-V2 — новый подход к обработке длинных последовательностей в 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#open_source", "#optimization"], "emoji": "🎯", "ru": {"title": "Умные рассуждения через сжатие и расширение контекста", "desc": "В статье представлен метод SIRI для обучения больших языковых моделей рассуждения с использованием reinforcement learnin
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#reasoning", "#training", "#rlhf"], "emoji": "🔧", "ru": {"title": "Умное использование инструментов через энтропию рассуждений", "desc": "Исследователи предлагают фреймворк Tool-Light для улучшения интеграции внешних инструментов в рассуждения больших яз
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization"], "emoji": "🔄", "ru": {"title": "Динамический поиск экспертов: новое измерение для улучшения рассуждений LLM", "desc": "Исследование предлагает метод Dynamic Experts Search (DES), который улучшает рассуждения больших языковы
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#training", "#open_source", "#games", "#agi", "#long_context", "#multimodal", "#interpretability", "#audio", "#architecture"], "emoji": "🧠", "ru": {"title": "Единая модель для понимания и генерации речи с архитектурой мозг-рот", "desc": "MGM-Omni представляет собой унифицированную м
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#data", "#training", "#science", "#agents", "#open_source"], "emoji": "📊", "ru": {"title": "Open-source революция в автоматическом анализе данных", "desc": "DataMind представляет новый подход к созданию open-source агентов для анализа данных, которые могут 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#data", "#open_source", "#diffusion", "#multimodal", "#training"], "emoji": "🎨", "ru": {"title": "Гигантская мультимодальная модель для генерации изображений с 80 миллиардами параметров", "desc": "HunyuanImage 3.0 - это мультимодальная модель с авторегрессивной архи
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#transfer_learning", "#optimization", "#benchmark", "#small_models"], "emoji": "📡", "ru": {"title": "Маленькая модель побеждает гигантов в беспроводной математике", "desc": "Исследователи создали WirelessMathLM - компактную языковую модель, специализирующуюся на 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#multimodal", "#rl", "#alignment"], "emoji": "💬", "ru": {"title": "Обучение на реальных диалогах с пользователями", "desc": "Статья представляет новый подход RLHI для улучшения conversational моделей через обучение на реальных диалогах пользователей, а не на п
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rlhf", "#rl", "#training"], "emoji": "🎓", "ru": {"title": "Дистилляция LLM с гарантиями близости к учителю", "desc": "Исследователи предложили новый подход к дистилляции больших языковых моделей, формулируя задачу как проблему обучения с подкреплением
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#benchmark"], "emoji": "🔍", "ru": {"title": "От примеров к инсайтам: новый подход к few-shot рассуждениям", "desc": "Исследователи обнаружили, что современные reasoning LLM часто показывают худшие результаты при использовании few-sh
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#diffusion", "#reasoning", "#training", "#optimization", "#math", "#inference"], "emoji": "🎯", "ru": {"title": "Новые стратегии декодирования и RL для masked diffusion языковых моделей", "desc": "В работе исследуются masked diffusion language models (MDLM) как а
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#rl", "#open_source", "#training", "#games", "#agents"], "emoji": "🤖", "ru": {"title": "Асинхронное обучение GUI агентов через децентрализованную архитектуру", "desc": "DART представляет собой децентрализованную архитектуру для обучения с подк
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#architecture", "#diffusion"], "emoji": "🌐", "ru": {"title": "Гиперсферические ограничения для стабильной авторегрессионной генерации изображений", "desc": "В статье представлена SphereAR - авторегрессионная модель для генерации изображений, кото
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#small_models"], "emoji": "🔍", "ru": {"title": "Кооперативная самоигра для эффективного поиска и рассуждений", "desc": "В статье представлена AceSearcher — framework для кооперативной самоигры, который улучшает способности LLM к рас
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training"], "emoji": "⚡", "ru": {"title": "Революция в обучении LLM: стабильная 4-битная точность", "desc": "Исследователи разработали новый метод обучения больших языковых моделей с использованием 4-битной точности NVFP4 вместо традиционной 8-битной 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#video", "#rl", "#training", "#reasoning", "#long_context", "#optimization", "#benchmark"], "emoji": "🔍", "ru": {"title": "Адаптивное масштабирование видео для лучшего понимания длинных роликов", "desc": "Исследователи представили LOVE-R1 - модель для понимания длинных видео, котора
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#science", "#multimodal"], "emoji": "🔬", "ru": {"title": "Умная проверка научных ответов AI через междисциплинарные рассуждения", "desc": "В статье представлена система для проверки научных ответов, генерируемых большими языковыми моделями (LL
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#leakage", "#alignment", "#multimodal", "#training", "#rl", "#architecture", "#synthetic"], "emoji": "🎨", "ru": {"title": "Точная генерация изображений с множественными объектами через разделение внимания", "desc": "Исследователи представили MultiCrafter - фреймворк для генерации из
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#interpretability", "#multimodal", "#agents"], "emoji": "🔍", "ru": {"title": "Мультиагентное визуальное рассуждение высокой точности", "desc": "PixelCraft — это мультиагентная система, которая улучшает визуальное рассуждение в мультимодальных LLM п
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#alignment"], "emoji": "⚖️", "ru": {"title": "Умная балансировка данных для выравнивания LLM с человеческими предпочтениями", "desc": "В статье представлен MetaAPO - новый подход для выравнивания больших языковых моделей с человеческими предпоч
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#benchmark"], "emoji": "🧩", "ru": {"title": "Композиционное мышление - слабое место современных LLM", "desc": "Исследователи представили DafnyCOMP - бенчмарк для оценки больших языковых моделей в генерации композиционных спецификаций в языке Dafny.
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#security", "#alignment", "#rlhf"], "emoji": "⚖️", "ru": {"title": "Обучение AI самокоррекции через adversarial цепочки рассуждений", "desc": "Исследователи выявили проблему «эффекта снежного кома» в больших рассуждающих моделях (LRM), где небо
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#multimodal", "#video", "#diffusion", "#architecture", "#benchmark", "#optimization"], "emoji": "🎬", "ru": {"title": "Единая архитектура для понимания и генерации видео", "desc": "UniVid представляет унифицированную архитектуру, которая объединяет MLLM (мультимодальную большую языко
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#open_source"], "emoji": "🦜", "ru": {"title": "PARROT: реалистичный бенчмарк для перевода SQL между системами баз данных", "desc": "В статье представлен бенчмарк PARROT для оценки Cross-System SQL Translation - задачи перевода SQL-запросов между различными с
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#reasoning", "#math", "#interpretability", "#dataset"], "emoji": "📊", "ru": {"title": "Частотный анализ математического мышления в LLM", "desc": "В статье представлен MathBode - новый диагностический инструмент для анализа математических способностей бо
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#security"], "emoji": "💬", "ru": {"title": "Обман через чат-шаблоны: новая угроза для AI-агентов", "desc": "Исследователи представили ChatInject - новый тип атаки на LLM-агентов, который использует структурированные chat-шаблоны для внедрения вредоносных инструкц
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#data", "#games", "#optimization", "#multimodal", "#cv"], "emoji": "📡", "ru": {"title": "Токенизированное сжатие для эффективной мультимодальной коммуникации", "desc": "UniMIC представляет новый подход к сжатию мультимодальных данных для взаимодействия между устройствами и облачными
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#interpretability", "#reasoning", "#games", "#agents"], "emoji": "🧠", "ru": {"title": "Агент, который учится мыслить: явное рассуждение вместо скрытых весов", "desc": "В статье представлена архитектура агента CEL (Cogito, ergo ludo), которая использует большую языковую модель
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#agi", "#optimization", "#transfer_learning", "#agents"], "emoji": "🗺️", "ru": {"title": "Агенты учатся навигации, улучшая собственные демонстрации", "desc": "Статья представляет SID - подход для обучения навигационных агентов в задачах целевой навигации с языковым управление
[30.09.2025 15:11] Querying the API.
[30.09.2025 15:11] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

GRPO-MA improves the training of Chain-of-Thought reasoning in LLMs and VLMs by addressing gradient coupling, sparse rewards, and unstable advantage estimation through multi-answer generation.  					AI-generated summary 				 Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance.
[30.09.2025 15:11] Response: ```json
{
  "title": "Множественные ответы для стабильного обучения рассуждений",
  "emoji": "🧠",
  "desc": "Статья анализирует проблемы алгоритма GRPO при обучении Chain-of-Thought рассуждений в LLM и VLM: связанность градиентов между мыслями и ответами, разреженные сигналы reward и нестабильную оценку advantage. Авторы предлагают метод GRPO-MA, который генерирует несколько ответов для каждой цепочки рассуждений, что теоретически снижает дисперсию градиентов. Экспериментально показано, что увеличение количества ответов на одну мысль уменьшает скачки градиентов и повышает стабильность обучения. Метод демонстрирует улучшение производительности на задачах математики, программирования и мультимодальных задачах по сравнению с базовым GRPO."
}
```
[30.09.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GRPO-MA improves the training of Chain-of-Thought reasoning in LLMs and VLMs by addressing gradient coupling, sparse rewards, and unstable advantage estimation through multi-answer generation.  					AI-generated summary 				 Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance."

[30.09.2025 15:11] Response: ```python
["RL", "TRAINING", "MULTIMODAL"]
```
[30.09.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GRPO-MA improves the training of Chain-of-Thought reasoning in LLMs and VLMs by addressing gradient coupling, sparse rewards, and unstable advantage estimation through multi-answer generation.  					AI-generated summary 				 Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling between thoughts and answers, sparse reward signals caused by limited parallel sampling, and unstable advantage estimation. To mitigate these challenges, we propose GRPO-MA, a simple yet theoretically grounded method that leverages multi-answer generation from each thought process, enabling more robust and efficient optimization. Theoretically, we show that the variance of thought advantage decreases as the number of answers per thought increases. Empirically, our gradient analysis confirms this effect, showing that GRPO-MA reduces gradient spikes compared to GRPO. Experiments on math, code, and diverse multimodal tasks demonstrate that GRPO-MA substantially improves performance and training efficiency. Our ablation studies further reveal that increasing the number of answers per thought consistently enhances model performance."

[30.09.2025 15:11] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[30.09.2025 15:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces GRPO-MA, an enhancement to the GRPO algorithm for training Chain-of-Thought reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). It addresses key challenges such as gradient coupling, sparse rewards, and unstable advantage estimation by utilizing multi-answer generation. This approach allows for more effective optimization by reducing variance in thought advantage as the number of generated answers increases. Experimental results show that GRPO-MA significantly improves performance and training efficiency across various tasks, confirming its effectiveness over the original GRPO method.","title":"Enhancing Chain-of-Thought Reasoning with GRPO-MA"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces GRPO-MA, an enhancement to the GRPO algorithm for training Chain-of-Thought reasoning in Large Language Models (LLMs) and Vision-Language Models (VLMs). It addresses key challenges such as gradient coupling, sparse rewards, and unstable advantage estimation by utilizing multi-answer generation. This approach allows for more effective optimization by reducing variance in thought advantage as the number of generated answers increases. Experimental results show that GRPO-MA significantly improves performance and training efficiency across various tasks, confirming its effectiveness over the original GRPO method.', title='Enhancing Chain-of-Thought Reasoning with GRPO-MA'))
[30.09.2025 15:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为GRPO-MA的方法，旨在改善大语言模型（LLMs）和视觉语言模型（VLMs）中的链式思维（CoT）推理训练。我们分析了GRPO算法面临的三个主要挑战：思维与答案之间的梯度耦合、由于有限并行采样导致的稀疏奖励信号，以及不稳定的优势估计。GRPO-MA通过从每个思维过程中生成多个答案，来缓解这些问题，从而实现更稳健和高效的优化。实验结果表明，GRPO-MA在数学、代码和多模态任务上显著提高了模型的性能和训练效率。","title":"GRPO-MA：提升链式思维推理的有效方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为GRPO-MA的方法，旨在改善大语言模型（LLMs）和视觉语言模型（VLMs）中的链式思维（CoT）推理训练。我们分析了GRPO算法面临的三个主要挑战：思维与答案之间的梯度耦合、由于有限并行采样导致的稀疏奖励信号，以及不稳定的优势估计。GRPO-MA通过从每个思维过程中生成多个答案，来缓解这些问题，从而实现更稳健和高效的优化。实验结果表明，GRPO-MA在数学、代码和多模态任务上显著提高了模型的性能和训练效率。', title='GRPO-MA：提升链式思维推理的有效方法'))
[30.09.2025 15:11] Querying the API.
[30.09.2025 15:11] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Evolution strategies successfully scale to fine-tune large language models, outperforming reinforcement learning in sample efficiency and robustness.  					AI-generated summary 				 Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.
[30.09.2025 15:11] Response: ```json
{
  "desc": "Исследователи впервые успешно применили эволюционные стратегии (ES) для тонкой настройки полных параметров больших языковых моделей с миллиардами параметров. ES показали лучшие результаты по сравнению с традиционным обучением с подкреплением (RL) по нескольким критериям: sample efficiency, устойчивость к долгосрочным наградам и стабильность работы. Метод оказался более робастным к разным базовым LLM и менее подвержен reward hacking. Это открывает новое направление в fine-tuning больших языковых моделей за пределами существующих RL-техник.",
  "emoji": "🧬",
  "title": "Эволюция побеждает: ES обходит RL в тонкой настройке LLM"
}
```
[30.09.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Evolution strategies successfully scale to fine-tune large language models, outperforming reinforcement learning in sample efficiency and robustness.  					AI-generated summary 				 Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper."

[30.09.2025 15:11] Response: ```python
['TRAINING', 'RL']
```
[30.09.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Evolution strategies successfully scale to fine-tune large language models, outperforming reinforcement learning in sample efficiency and robustness.  					AI-generated summary 				 Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper."

[30.09.2025 15:11] Response: ```python
["OPTIMIZATION"]
```
[30.09.2025 15:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel approach to fine-tuning large language models (LLMs) using evolution strategies (ES), which have been traditionally overlooked in favor of reinforcement learning (RL). The authors demonstrate that ES can effectively scale to fine-tune LLMs with billions of parameters, achieving better sample efficiency and robustness compared to RL methods. They highlight that ES is less prone to issues like reward hacking and provides more stable performance across different runs. This work opens up new possibilities for LLM fine-tuning, suggesting that ES could be a viable alternative to RL techniques.","title":"Evolution Strategies: A New Frontier for Fine-Tuning Large Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel approach to fine-tuning large language models (LLMs) using evolution strategies (ES), which have been traditionally overlooked in favor of reinforcement learning (RL). The authors demonstrate that ES can effectively scale to fine-tune LLMs with billions of parameters, achieving better sample efficiency and robustness compared to RL methods. They highlight that ES is less prone to issues like reward hacking and provides more stable performance across different runs. This work opens up new possibilities for LLM fine-tuning, suggesting that ES could be a viable alternative to RL techniques.', title='Evolution Strategies: A New Frontier for Fine-Tuning Large Language Models'))
[30.09.2025 15:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"进化策略（ES）成功地扩展到大规模语言模型的微调，表现出比强化学习（RL）更高的样本效率和鲁棒性。微调预训练的大型语言模型（LLMs）是人工智能部署流程中的关键步骤。尽管强化学习是最常用的微调方法，但本研究首次成功地将进化策略扩展到全参数的LLMs，显示出其在数十亿参数中高效搜索的能力。进化策略在多个方面超越了现有的强化学习微调方法，为LLMs的微调开辟了新的方向。","title":"进化策略：超越强化学习的微调新选择"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='进化策略（ES）成功地扩展到大规模语言模型的微调，表现出比强化学习（RL）更高的样本效率和鲁棒性。微调预训练的大型语言模型（LLMs）是人工智能部署流程中的关键步骤。尽管强化学习是最常用的微调方法，但本研究首次成功地将进化策略扩展到全参数的LLMs，显示出其在数十亿参数中高效搜索的能力。进化策略在多个方面超越了现有的强化学习微调方法，为LLMs的微调开辟了新的方向。', title='进化策略：超越强化学习的微调新选择'))
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#data", "#interpretability", "#reasoning", "#multimodal"], "emoji": "🧠", "ru": {"title": "Геометрия мышления: как найти сбои в рассуждениях ИИ", "desc": "Исследователи предложили фреймворк Reasoning Manifold для анализа ошибок рассуждений в больших языковых моделях 
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#low_resource", "#data", "#machine_translation", "#multilingual"], "emoji": "⚖️", "ru": {"title": "Специализация побеждает универсальность в суммаризации юридических текстов", "desc": "Исследователи создали BOE-XSUM - датасет из 3,648 кратких резюме испанск
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#benchmark", "#reasoning", "#cv", "#multimodal"], "emoji": "🎬", "ru": {"title": "От видео к коду: новый вызов для AI в создании интерактивных веб-страниц", "desc": "В статье представлен IWR-Bench — новый бенчмарк для оценки способности больших vision
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal", "#graphs", "#optimization", "#benchmark", "#games"], "emoji": "📊", "ru": {"title": "LLM для автоматического создания и редактирования бизнес-процессов", "desc": "В статье представлен BPMN Assistant — инструмент, использующий LLM для создания и реда
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#interpretability", "#agents", "#rag", "#science", "#reasoning", "#alignment", "#benchmark"], "emoji": "🔍", "ru": {"title": "AI-помощник для поиска противоречий в Википедии", "desc": "Исследователи создали систему CLAIRE, которая объединяет рассуждения LLM с поиском информации для о
[30.09.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#data", "#optimization", "#benchmark", "#open_source", "#reasoning", "#training", "#long_context", "#dataset"], "emoji": "🏃", "ru": {"title": "Ритмы движения: предсказание мобильности через иерархическую токенизацию", "desc": "В статье представлена модель RHYTHM для
[30.09.2025 15:11] Querying the API.
[30.09.2025 15:11] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generalized Correctness Models (GCMs) improve LLM confidence estimation by leveraging historical correctness data, outperforming self-knowledge approaches and demonstrating generalizability across models and datasets.  					AI-generated summary 				 Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.
[30.09.2025 15:12] Response: ```json
{
  "desc": "Исследователи предложили Generalized Correctness Models (GCM) — модели, которые оценивают уверенность LLM в своих ответах на основе исторических данных о правильности предсказаний. Эксперименты показали, что способность модели оценивать корректность собственных ответов не превосходит оценку со стороны другой независимой модели, что опровергает подход «самопознания». GCM обучаются на данных о корректности ответов различных LLM и способны обобщать паттерны правильности на новые датасеты и модели, используя такие признаки как формулировка ответа. Подход демонстрирует, что надёжная оценка уверенности является универсальным навыком, основанным на систематическом кодировании истории корректности, а не на специфической самоинтроспекции модели.",
  "emoji": "🎯",
  "title": "История правильных ответов важнее самоанализа для оценки уверенности AI"
}
```
[30.09.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generalized Correctness Models (GCMs) improve LLM confidence estimation by leveraging historical correctness data, outperforming self-knowledge approaches and demonstrating generalizability across models and datasets.  					AI-generated summary 				 Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection."

[30.09.2025 15:12] Response: ```python
["DATASET", "DATA", "TRAINING"]
```
[30.09.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generalized Correctness Models (GCMs) improve LLM confidence estimation by leveraging historical correctness data, outperforming self-knowledge approaches and demonstrating generalizability across models and datasets.  					AI-generated summary 				 Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection."

[30.09.2025 15:12] Response: ```python
["INTERPRETABILITY", "ALIGNMENT"]
```
[30.09.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Generalized Correctness Models (GCMs) to enhance the confidence estimation of large language models (LLMs) by utilizing historical correctness data. It challenges the traditional approach of relying on a model\'s self-knowledge to assess the accuracy of its outputs, showing that this method is often ineffective. Instead, GCMs leverage patterns from the correctness data of various models, demonstrating their ability to generalize across different datasets and model architectures. The study reveals that incorporating historical correctness information significantly improves the reliability of confidence estimates, making it a crucial advancement for deploying LLMs in critical applications.","title":"Harnessing History for Better Confidence in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces Generalized Correctness Models (GCMs) to enhance the confidence estimation of large language models (LLMs) by utilizing historical correctness data. It challenges the traditional approach of relying on a model's self-knowledge to assess the accuracy of its outputs, showing that this method is often ineffective. Instead, GCMs leverage patterns from the correctness data of various models, demonstrating their ability to generalize across different datasets and model architectures. The study reveals that incorporating historical correctness information significantly improves the reliability of confidence estimates, making it a crucial advancement for deploying LLMs in critical applications.", title='Harnessing History for Better Confidence in LLMs'))
[30.09.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为广义正确性模型（GCM）的新方法，用于提高大型语言模型（LLM）的置信度估计。GCM通过利用历史正确性数据，超越了传统的自我知识方法，并在不同模型和数据集上表现出良好的泛化能力。研究表明，LLM在预测自身输出的正确性时，通常表现不佳，而历史预测数据的引入能够显著改善这一问题。通过多种方法注入历史信息，GCM能够学习到适用于多种数据集和模型的正确性预测模式。","title":"广义正确性模型：提升LLM置信度的关键"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为广义正确性模型（GCM）的新方法，用于提高大型语言模型（LLM）的置信度估计。GCM通过利用历史正确性数据，超越了传统的自我知识方法，并在不同模型和数据集上表现出良好的泛化能力。研究表明，LLM在预测自身输出的正确性时，通常表现不佳，而历史预测数据的引入能够显著改善这一问题。通过多种方法注入历史信息，GCM能够学习到适用于多种数据集和模型的正确性预测模式。', title='广义正确性模型：提升LLM置信度的关键'))
[30.09.2025 15:12] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#optimization", "#dataset", "#open_source", "#alignment", "#data"], "emoji": "🧹", "ru": {"title": "Чистые данные — качественное выравнивание LLM", "desc": "Статья представляет PrefCleanBench — первый комплексный бенчмарк для оценки 13 методов очистки данных пр
[30.09.2025 15:12] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#interpretability", "#rag"], "emoji": "🔍", "ru": {"title": "LUMINA: Обнаружение галлюцинаций через баланс внешнего контекста и внутренних знаний", "desc": "Статья представляет LUMINA — новый фреймворк для обнаружения галлюцинаций в RAG-системах, кото
[30.09.2025 15:12] Using data from previous issue: {"categories": ["#science", "#dataset", "#data", "#survey", "#multilingual"], "emoji": "🇮🇹", "ru": {"title": "Десятилетие итальянской компьютерной лингвистики: от лексики к LLM", "desc": "Исследование анализирует тренды в итальянской компьютерной лингвистике и обработке естественного языка за послед
[30.09.2025 15:12] Using data from previous issue: {"categories": ["#interpretability", "#video", "#multimodal", "#benchmark", "#optimization"], "emoji": "🎬", "ru": {"title": "Оценка видео субтитров без эталонов через фактическую обоснованность", "desc": "Исследователи представили VC-Inspector - новую систему оценки качества видео субтитров, которая
[30.09.2025 15:12] Renaming data file.
[30.09.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-09-30.json
[30.09.2025 15:12] Saving new data file.
[30.09.2025 15:12] Generating page.
[30.09.2025 15:12] Renaming previous page.
[30.09.2025 15:12] Renaming previous data. index.html to ./d/2025-09-30.html
[30.09.2025 15:12] Writing result.
[30.09.2025 15:12] Renaming log file.
[30.09.2025 15:12] Renaming previous data. log.txt to ./logs/2025-09-30_last_log.txt
