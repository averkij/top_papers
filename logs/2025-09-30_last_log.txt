[29.09.2025 23:10] Read previous papers.
[29.09.2025 23:10] Generating top page (month).
[29.09.2025 23:10] Writing top page (month).
[30.09.2025 00:52] Read previous papers.
[30.09.2025 00:52] Get feed.
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22622
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22611
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22576
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22186
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21679
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22637
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22638
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22647
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22281
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21880
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19894
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21766
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22075
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22651
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22653
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22414
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22644
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22624
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21989
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21710
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21388
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22072
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21799
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21760
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21500
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22601
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21574
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20787
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22650
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22642
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22244
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21991
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22636
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22496
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22300
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21294
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22630
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21559
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21319
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21150
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19768
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.20906
[30.09.2025 00:52] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18420
[30.09.2025 00:52] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.09.2025 00:52] No deleted papers detected.
[30.09.2025 00:52] Downloading and parsing papers (pdf, html). Total: 43.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22622.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22622.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22622.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22611.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22611.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22611.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22576.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22576.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22576.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22186.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22186.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22186.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21679.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21679.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21679.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22637.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22637.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22637.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22638.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22638.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22638.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22647.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22647.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22647.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22281.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22281.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22281.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21880.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21880.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21880.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19894.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19894.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19894.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21766.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21766.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21766.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22075.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22075.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22075.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22651.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22651.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22651.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22653.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22653.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22653.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22414.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22414.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22414.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22644.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22644.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22644.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22624.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22624.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22624.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21989.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21989.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21989.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21710.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21710.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21710.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21388.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21388.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21388.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22072.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22072.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22072.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21799.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21799.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21799.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21760.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21760.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21760.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21500.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21500.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21500.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22601.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22601.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22601.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21574.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21574.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21574.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20787.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20787.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20787.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22650.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22650.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22650.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22642.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22642.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22642.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22244.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22244.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22244.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21991.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21991.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21991.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22636.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22636.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22636.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22496.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22496.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22496.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22300.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22300.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22300.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21294.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21294.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21294.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.22630.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.22630.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.22630.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21559.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21559.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21559.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21319.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21319.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21319.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.21150.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.21150.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.21150.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.19768.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.19768.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.19768.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.20906.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.20906.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.20906.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Downloading and parsing paper https://huggingface.co/papers/2509.18420.
[30.09.2025 00:52] Extra JSON file exists (./assets/json/2509.18420.json), skip PDF parsing.
[30.09.2025 00:52] Paper image links file exists (./assets/img_data/2509.18420.json), skip HTML parsing.
[30.09.2025 00:52] Success.
[30.09.2025 00:52] Enriching papers with extra data.
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 0. LongLive is a frame-level autoregressive framework for real-time and interactive long video generation, addressing efficiency and quality challenges through causal attention, KV-recache, streaming long tuning, and short window attention.  					AI-generated summary 				 We present LongLive, a frame-l...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 1. Quantile Advantage Estimation stabilizes reinforcement learning with verifiable rewards by addressing entropy issues and improving performance on large language models.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 2. Entropy-regularized Policy Optimization (EPO) addresses exploration-exploitation challenges in multi-turn environments with sparse rewards, improving performance in tasks like ScienceWorld and ALFWorld.  					AI-generated summary 				 Training LLM agents in multi-turn environments with sparse reward...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 3. MinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.  					AI-generated summary 				 We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 4. An automated engine evaluates the factuality of review points in AI conference papers, demonstrating moderate agreement with human experts and higher accuracy at the premise level.  					AI-generated summary 				 Peer review serves as a backbone of academic research, but in most AI conferences, the ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 5. A variational reasoning framework treats thinking traces as latent variables, optimizing them through variational inference to improve language model reasoning.  					AI-generated summary 				 We introduce a variational reasoning framework for language models that treats thinking traces as latent va...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 6. Feedback-conditional policy (FCP) enables LLMs to learn from verbal feedback by treating it as a conditioning signal, improving expressiveness over scalar rewards.  					AI-generated summary 				 LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced fe...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 7. CapRL, a novel reinforcement learning framework, enhances image captioning by using a vision-free language model to evaluate caption quality through multiple-choice questions, leading to improved performance across benchmarks.  					AI-generated summary 				 Image captioning is a fundamental task th...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 8. MesaTask, an LLM-based framework with a Spatial Reasoning Chain, generates realistic tabletop scenes aligned with task descriptions using DPO algorithms.  					AI-generated summary 				 The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 9. RL-ZVP, a novel reinforcement learning algorithm, leverages zero-variance prompts to improve the accuracy and pass rate of Large Language Models in math reasoning tasks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the re...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 10. PromptCoT 2.0 uses an EM loop to generate harder and more diverse synthetic prompts, improving reasoning capabilities in large language models through self-play and supervised fine-tuning.  					AI-generated summary 				 Large language models (LLMs) are evolving from conversational systems into stro...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 11. UltraHorizon is a new benchmark that evaluates long-horizon and partially observable tasks for autonomous agents, highlighting gaps in their sustained reasoning, planning, memory, and tool use capabilities.  					AI-generated summary 				 Autonomous agents have recently achieved remarkable progress ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 12. CoSpaDi, a training-free compression framework, uses structured sparse dictionary learning to compress large language models with better accuracy and efficiency compared to low-rank methods.  					AI-generated summary 				 Post-training compression of large language models (LLMs) largely relies on l...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 13. VoiceAssistant-Eval is a benchmark for evaluating AI assistants across listening, speaking, and viewing tasks, revealing insights into model performance and identifying areas for improvement.  					AI-generated summary 				 The growing capabilities of large language models and multimodal systems hav...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 14. See, Point, Fly (SPF) is a training-free aerial vision-and-language navigation framework that treats action prediction as a 2D spatial grounding task, outperforming existing methods in both simulation and real-world evaluations.  					AI-generated summary 				 We present See, Point, Fly (SPF), a tra...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 15. LucidFlux, a caption-free UIR framework using a diffusion transformer, achieves robust image restoration through adaptive conditioning and SigLIP features without text prompts.  					AI-generated summary 				 Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 16. WebGen-Agent enhances website code generation by integrating visual feedback and GUI-agent testing with a backtracking mechanism and Step-GRPO training to improve accuracy and appearance scores.  					AI-generated summary 				 Agent systems powered by large language models (LLMs) have demonstrated i...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 17. SPARK, a synergistic policy and reward co-evolving framework, enhances LLMs and LVLMs by recycling rollouts and correctness data to train a generative reward model, reducing reliance on human preferences and external reward models.  					AI-generated summary 				 Recent Large Language Models (LLMs) ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 18. A novel method disentangles visual and semantic features from diffusion model backbones to quantify and localize visual inconsistencies in subject-driven image generation.  					AI-generated summary 				 We propose a novel approach for disentangling visual and semantic features from the backbones of...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 19. ToG-3, a novel framework, enhances LLMs with external knowledge using a dynamic, multi-agent system that evolves queries and subgraphs for precise evidence retrieval and reasoning.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important parad...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 20. TUN3D is a method for joint layout estimation and 3D object detection using multi-view images without depth sensors or ground-truth camera poses, achieving state-of-the-art performance on indoor scene understanding benchmarks.  					AI-generated summary 				 Layout estimation and 3D object detection...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 21. Restoring fine-tuning to a breadth-first pipeline with mini-batch optimization and localized tuning parameters improves its effectiveness for model editing, outperforming state-of-the-art methods.  					AI-generated summary 				 Fine-tuning, a foundational method for adapting large language models, ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 22. D-Artemis, a novel deliberative framework, enhances GUI automation by leveraging app-specific tips, proactive alignment, and reflection, achieving state-of-the-art results with general-purpose multimodal large language models.  					AI-generated summary 				 Graphical User Interface (GUI) agents aim...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 23. A pre-trained video diffusion transformer, UniVid, is fine-tuned to handle diverse vision tasks without task-specific modifications, demonstrating cross-modal and cross-source generalization.  					AI-generated summary 				 Large language models, trained on extensive corpora, successfully unify dive...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 24. Rubric-based rewards mitigate reward over-optimization in reinforcement fine-tuning by leveraging off-policy examples while maintaining reward reliability.  					AI-generated summary 				 Reinforcement fine-tuning (RFT) often suffers from reward over-optimization, where a policy model hacks the rewa...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 25. SPEAR, a curriculum-based self-imitation learning method, manages exploration-exploitation balance in reinforcement learning for LLMs by using intrinsic rewards and trajectory-level entropy control.  					AI-generated summary 				 Reinforcement learning (RL) is the dominant paradigm for sharpening s...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 26. X-Streamer is a unified multimodal framework using dual-transformer architecture for real-time, open-ended interactions across text, speech, and video, leveraging large language-speech models and autoregressive diffusion models.  					AI-generated summary 				 We introduce X-Streamer, an end-to-end ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 27. DEIMv2, an extended version of DEIM with DINOv3 features, achieves superior performance-cost trade-offs across diverse deployment scenarios, including GPU, edge, and mobile, by using Spatial Tuning Adapter and HGNetv2 with pruning.  					AI-generated summary 				 Benefiting from the simplicity and e...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 28. A new method leverages diffusion transformers' attention scores for referring segmentation without fine-tuning or additional training, improving performance through stop word filtering and attention redistribution.  					AI-generated summary 				 Most existing approaches to referring segmentation ac...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 29. WoW, a 14-billion-parameter generative world model trained on robot interactions, demonstrates improved physical intuition through SOPHIA's guidance and achieves state-of-the-art performance on physical consistency and causal reasoning in video.  					AI-generated summary 				 Humans develop an unde...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 30. FlashEdit enables real-time, high-fidelity image editing with diffusion models through efficient inversion, background preservation, and localized attention mechanisms.  					AI-generated summary 				 Text-guided image editing with diffusion models has achieved remarkable quality but suffers from pr...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 31. ERGO, a two-stage reasoning pipeline, efficiently processes high-resolution images by focusing on task-relevant regions, improving accuracy and speed compared to existing models.  					AI-generated summary 				 Efficient processing of high-resolution images is crucial for real-world vision-language ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 32. VAR, an autoregressive transformer for visual generation, is shown to be equivalent to a discrete diffusion when using a Markovian attention mask, leading to improved efficiency and generation quality.  					AI-generated summary 				 Autoregressive (AR) transformers have emerged as a powerful paradi...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 33. EAGLE is a lightweight framework that explains token generation in multimodal large language models by attributing tokens to visual regions and quantifying the influence of language and perceptual evidence.  					AI-generated summary 				 Multimodal large language models (MLLMs) have demonstrated re...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 34. A momentum-based sampling technique called history-guided sampling (HiGS) improves the quality and efficiency of diffusion models in image generation without additional computation or training.  					AI-generated summary 				 While diffusion models have made remarkable progress in image generation, ...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 35. Synthetic, culturally contextualized datasets for Indian languages improve multilingual AI performance, especially in low and medium-resource languages, through a bottom-up generation strategy using large open-source LLMs.  					AI-generated summary 				 Developing AI systems that operate effectivel...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 36. StateX is a post-training pipeline that expands the state size of pre-trained RNNs, enhancing recall and in-context learning without significant additional costs.  					AI-generated summary 				 While Transformer-based models have demonstrated remarkable language modeling performance, their high com...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 37. X-CoT, an explainable retrieval framework using LLM CoT reasoning, enhances text-to-video retrieval by providing detailed rationales and improving performance.  					AI-generated summary 				 Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute c...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 38. RLBFF combines human feedback and rule-based verification to improve reward models, achieving top performance on benchmarks with customizable principles.  					AI-generated summary 				 Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are th...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 39. CAD-Tokenizer, a multimodal tokenization framework using VQ-VAE, enhances text-guided CAD prototyping by improving instruction following and generation quality.  					AI-generated summary 				 Computer-Aided Design (CAD) is a foundational component of industrial prototyping, where models are defined...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 40. CHURRO, a 3B-parameter open-weight vision-language model, outperforms existing models in historical text recognition using the largest dataset to date, CHURRO-DS, and is more cost-effective.  					AI-generated summary 				 Accurate text recognition for historical documents can greatly advance the st...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 41. Particle filters enable 3D object localization using camera poses and image segments, suitable for drone-based surveillance with limited computational resources.  					AI-generated summary 				 3D object localisation based on a sequence of camera measurements is essential for safety-critical surveil...
[30.09.2025 00:52] ********************************************************************************
[30.09.2025 00:52] Abstract 42. IFEval-FC is a benchmark that evaluates function calling by assessing adherence to format instructions within JSON schema descriptions, revealing that even advanced models often fail to follow basic formatting rules.  					AI-generated summary 				 Function calling is a core capability of large lang...
[30.09.2025 00:52] Read previous papers.
[30.09.2025 00:52] Generating reviews via LLM API.
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#video", "#training", "#inference", "#diffusion", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "LongLive –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞–¥—Ä–æ
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#rlhf", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ö–≤–∞–Ω—Ç–∏–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ —É–º–Ω—ã–π baseline", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#games", "#optimization"], "emoji": "üåä", "ru": {"title": "–£–∫—Ä–æ—â–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ–≥–æ –∫–∞—Å–∫–∞–¥–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏ LLM –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è LLM –∞–≥–µ–Ω—Ç–æ–≤ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, –≥–¥–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ç
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#cv", "#dataset", "#data"], "emoji": "üìÑ", "ru": {"title": "–û—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ MinerU2.5 - vision-language –º–æ–¥–µ–ª—å —Å 1.2 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ú–æ
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#dataset", "#ethics", "#data"], "emoji": "üîç", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω—ã—Ö —Ä–µ—Ü–µ–Ω–∑–∏—è—Ö —Å –ø–æ–º–æ—â—å—é AI", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ—Ü–µ–Ω–∑–∏–π –Ω–∞ –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –æ–±–ª–∞—Å
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#optimization", "#rl", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ RL –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ä–∞—Å—Å–º–∞
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#alignment", "#rl", "#optimization", "#rlhf"], "emoji": "üí¨", "ru": {"title": "–û—Ç —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥ –∫ –±–æ–≥–∞—Ç–æ–π –≤–µ—Ä–±–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–±–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤–º–µ—Å—Ç–æ —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥. –ú–µ—Ç–æ–¥
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#rl", "#games", "#rlhf", "#optimization"], "emoji": "üì∏", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω CapRL ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#3d", "#synthetic", "#agents", "#reasoning", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–£–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ü–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ MesaTask ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ —Å—Ç–æ–ª–µ—à–Ω–∏—Ü–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#math", "#rlhf", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –Ω–∞–≥—Ä–∞–¥–∞—Ö: –∫–∞–∫ –∏–∑–≤–ª–µ—á—å –ø–æ–ª—å–∑—É –∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º RL-ZVP, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#data", "#dataset", "#training", "#synthetic"], "emoji": "üß†", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –¥–µ–ª–∞—é—Ç AI —É–º–Ω–µ–µ —á–µ—Ä–µ–∑ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ", "desc": "PromptCoT 2.0 ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#agents"], "emoji": "üî≠", "ru": {"title": "–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–æ–≤: –±–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏", "desc": "UltraHorizon - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Å —á–∞—Å—Ç–∏—á–Ω–æ–π –Ω–∞–±–ª
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training"], "emoji": "üìö", "ru": {"title": "–°–∂–∞—Ç–∏–µ LLM —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CoSpaDi - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ 
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#multimodal", "#audio", "#benchmark", "#open_source", "#small_models"], "emoji": "üîä", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤: –∑–≤—É–∫, —Ä–µ—á—å –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "desc": "VoiceAssistant-Eval ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#rl", "#agents", "#games", "#cv"], "emoji": "üöÅ", "ru": {"title": "–î—Ä–æ–Ω—ã –Ω–∞—É—á–∏–ª–∏—Å—å –ª–µ—Ç–∞—Ç—å –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∫–æ–º–∞–Ω–¥–∞–º —á–µ—Ä–µ–∑ 2D —Ä–∞–∑–º–µ—Ç–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SPF (See, Point, Fly) - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –¥—Ä–æ–Ω–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#open_source", "#dataset", "#diffusion", "#hallucinations"], "emoji": "üîß", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π", "desc": "LucidFlux –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#agents", "#reasoning", "#rlhf", "#optimization"], "emoji": "üåê", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–±-—Å–∞–π—Ç–æ–≤ —Å –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WebGen-Agent - –Ω–æ–≤—ã–π AI-–∞–≥–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ–±-—Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç —Å–∫
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#architecture", "#rl", "#rlhf", "#optimization", "#training", "#alignment"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∏–µ—Å—è –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Å–∏–Ω–µ—Ä–≥–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π", "desc": "SPARK - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) 
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#diffusion"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –Ω–∞—Ö–æ–¥–∏: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–ª–∏—Ç—á–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, —á—Ç–æ –ø–æ–∑
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#reasoning", "#rag", "#graphs"], "emoji": "üß†", "ru": {"title": "–î—É–º–∞–π-–Ω–∞-–≥—Ä–∞—Ñ–µ: —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π", "desc": "ToG-3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã LLM —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ –¥–∏
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#3d", "#cv", "#benchmark", "#science"], "emoji": "üè†", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ 3D —Å—Ü–µ–Ω –±–µ–∑ –≥–ª—É–±–∏–Ω—ã - —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "TUN3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∫–∏ –ø–æ–º–µ—â–µ–Ω–∏–π –∏ 3D –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üîß", "ru": {"title": "Fine-tuning –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è: –æ—Ç –Ω–µ–¥–æ–æ—Ü–µ–Ω–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∫ –ª–∏–¥–µ—Ä—É —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ fine-tuning –º–æ–∂–µ—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –µ—Å–ª–∏ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ 
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#agi", "#multimodal", "#agents", "#benchmark", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ GUI —Å —Ü–∏–∫–ª–æ–º –º—ã—à–ª–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω D-Artemis ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö, –∫–æ—Ç
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#cv", "#video", "#multimodal", "#diffusion", "#transfer_learning"], "emoji": "üé¨", "ru": {"title": "–û–¥–∏–Ω –≤–∏–¥–µ–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç UniVid - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#rl", "#alignment", "#optimization", "#rlhf", "#training"], "emoji": "üìù", "ru": {"title": "–†—É–±—Ä–∏–∫–∏ –ø—Ä–æ—Ç–∏–≤ –æ–±–º–∞–Ω–∞: –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥ –≤ fine-tuning", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥ –≤ reinforcement fine-tuning, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω–∞—á–∏
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#training", "#rl", "#games", "#rlhf", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –≤ RL —á–µ—Ä–µ–∑ —Å–∞–º–æ–∏–º–∏—Ç–∞—Ü–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ SPEAR - –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è LLM, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–∞–º–æ–∏–º–∏—Ç–∞—Ü–∏–∏ –∏ curriculum le
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#alignment", "#architecture", "#multimodal", "#agi", "#interpretability", "#games", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–¶–∏—Ñ—Ä–æ–≤–æ–π —á–µ–ª–æ–≤–µ–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –ø–æ—Ä—Ç—Ä–µ—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω X-Streamer ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–ø
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#small_models", "#training", "#inference", "#cv", "#architecture", "#optimization", "#open_source"], "emoji": "üéØ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –æ—Ç GPU –¥–æ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "DEIMv2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã DEIM –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ 
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–¥–∞—á referring segmentation, –∏—Å–ø–æ–ª—å–∑—É—è attention scores –∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –±–µ
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#dataset", "#video", "#robotics", "#agents", "#agi", "#hallucinations", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç—É–∏—Ü–∏—è AI —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∏—Ä–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ WoW ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –º–∏—Ä–∞ —Å 14 –º–∏–ª–ª–∏
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#optimization", "#cv", "#inference", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "FlashEdit ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º 
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#multimodal", "#cv", "#rl", "#optimization"], "emoji": "üîç", "ru": {"title": "–£–º–Ω–∞—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤–∞–∂–Ω—ã—Ö –¥–µ—Ç–∞–ª—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "ERGO ‚Äî —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π pipeline –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö computer vision –∏ N
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#architecture", "#cv", "#diffusion", "#optimization", "#training"], "emoji": "üîÑ", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –¥–∏—Ñ—Ñ—É–∑–∏—è - –¥–≤–∞ –ª–∏—Ü–∞ –æ–¥–Ω–æ–π –º–µ–¥–∞–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä VAR –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —ç–∫–≤–∏–≤
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#hallucinations", "#inference", "#multimodal", "#interpretability", "#open_source"], "emoji": "ü¶Ö", "ru": {"title": "–û–±—ä—è—Å–Ω—è–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω: –∫–∞–∫ MLLM –≤–∏–¥—è—Ç –∏ –≥–æ–≤–æ—Ä—è—Ç", "desc": "EAGLE - —ç—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#cv", "#inference"], "emoji": "üéØ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–∞–º—è—Ç—å –æ –ø—Ä–æ—à–ª—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è HiGS –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#synthetic", "#data", "#dataset"], "emoji": "üáÆüá≥", "ru": {"title": "–ö—É–ª—å—Ç—É—Ä–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ AI", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç Updesh –¥–ª—è –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#optimization", "#training", "#long_context", "#architecture"], "emoji": "üß†", "ru": {"title": "–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ RNN –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω StateX - –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (RNN). –ü—Ä–æ–±
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#video", "#interpretability", "#reasoning", "#dataset", "#rag", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º—ã–π –ø–æ–∏—Å–∫ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç X-CoT - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–ø–æ—á–∫–∏ 
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#open_source", "#bench", "#alignment", "#inference", "#rlhf", "#interpretability", "#dataset", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ì–∏–±–∫–∞—è –±–∏–Ω–∞—Ä–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é reward models", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ RLBFF, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é –æ
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#optimization", "#dataset", "#alignment"], "emoji": "üîß", "ru": {"title": "–£–º–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è CAD: –∫–æ–≥–¥–∞ AI –ø–æ–Ω–∏–º–∞–µ—Ç —á–µ—Ä—Ç–µ–∂–∏ –∫–∞–∫ –∏–Ω–∂–µ–Ω–µ—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CAD-Tokenizer ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ CAD –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º VQ-VA
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#science", "#dataset", "#open_source"], "emoji": "üìú", "ru": {"title": "CHURRO: AI –¥–ª—è —á—Ç–µ–Ω–∏—è –¥—Ä–µ–≤–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –Ω–∞—Å–ª–µ–¥–∏—è", "desc": "CHURRO - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è vision-language –º–æ–¥–µ–ª—å —Å 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#video", "#3d"], "emoji": "üî•", "ru": {"title": "–õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ particle filters –¥–ª—è –¥—Ä–æ–Ω–æ–≤–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ 3D –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é particle filters –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑–∏—Ü–∏–π –∫–∞–º–µ—Ä—ã –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è
[30.09.2025 00:52] Using data from previous issue: {"categories": ["#open_source", "#alignment", "#agents", "#interpretability", "#benchmark"], "emoji": "üìè", "ru": {"title": "–¢–æ—á–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –≤—ã–∑–æ–≤–µ —Ñ—É–Ω–∫—Ü–∏–π - —Å–ª–∞–±–æ–µ –º–µ—Å—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ IFEval-FC –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ—á–Ω–æ 
[30.09.2025 00:52] Renaming data file.
[30.09.2025 00:52] Renaming previous data. hf_papers.json to ./d/2025-09-30.json
[30.09.2025 00:52] Saving new data file.
[30.09.2025 00:52] Generating page.
[30.09.2025 00:52] Renaming previous page.
[30.09.2025 00:52] Renaming previous data. index.html to ./d/2025-09-30.html
[30.09.2025 00:52] Writing result.
[30.09.2025 00:52] Renaming log file.
[30.09.2025 00:52] Renaming previous data. log.txt to ./logs/2025-09-30_last_log.txt
