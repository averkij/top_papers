[30.09.2025 04:20] Read previous papers.
[30.09.2025 04:20] Generating top page (month).
[30.09.2025 04:20] Writing top page (month).
[30.09.2025 05:12] Read previous papers.
[30.09.2025 05:12] Get feed.
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24006
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23102
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25190
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23426
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25175
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24897
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23909
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25160
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24900
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24695
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24014
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22572
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25176
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24663
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22824
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22799
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25161
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24007
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24473
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23285
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24193
[30.09.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.25191
[30.09.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.25123
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23951
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25131
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24335
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23371
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23196
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21953
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25185
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24786
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23866
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22570
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25149
[30.09.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2509.25077
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25052
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24910
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24269
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23143
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22518
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24709
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23115
[30.09.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22830
[30.09.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[30.09.2025 05:12] No deleted papers detected.
[30.09.2025 05:12] Downloading and parsing papers (pdf, html). Total: 43.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24006.
[30.09.2025 05:12] Downloading paper 2509.24006 from http://arxiv.org/pdf/2509.24006v1...
[30.09.2025 05:12] Failed to download and parse paper https://huggingface.co/papers/2509.24006: 'LTChar' object is not iterable
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23102.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23102.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23102.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25190.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25190.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25190.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23426.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23426.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23426.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25175.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25175.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25175.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24897.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24897.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24897.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23909.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23909.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23909.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25160.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25160.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25160.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24900.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24900.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24900.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24695.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24695.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24695.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24014.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24014.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24014.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.22572.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.22572.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.22572.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25176.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25176.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25176.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24663.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24663.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24663.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.22824.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.22824.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.22824.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.22799.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.22799.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.22799.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25161.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25161.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25161.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24007.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24007.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24007.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24473.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24473.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24473.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23285.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23285.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23285.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24193.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24193.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24193.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25191.
[30.09.2025 05:12] Downloading paper 2509.25191 from http://arxiv.org/pdf/2509.25191v1...
[30.09.2025 05:12] Extracting affiliations from text.
[30.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 1 9 1 5 2 . 9 0 5 2 : r Preprint. Under review. VGGT-X: WHEN VGGT MEETS DENSE NOVEL VIEW SYNTHESIS Yang Liu1,2, Chuanchen Luo4, Zimo Tang3, Junran Peng5 (cid:0), & Zhaoxiang Zhang 1,2 (cid:0) 1 NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences 2 University of Chinese Academy of Sciences 3 Huazhong University of Science and Technology 4 Shandong University {liuyang2022, zhaoxiang.zhang}@ia.ac.cn, u202315173@hust.edu.cn chuanchen.luo@sdu.edu.cn, jrpeng4ever@126.com 5 University of Science and Technology Beijing Figure 1: Reconstruction and Novel View Synthesis results. In part (a), we extend VGGT to handle dense multi-view inputs and incorporate an efficient global alignment, yielding highly accurate predictions. Part (b) demonstrates that eliminating redundant VRAM usage enables inference throughput over 1000 images without compromising performance. The VGGT here denotes VGGT with the elimination of redundant intermediate features. Finally, part (c) illustrates that, with an appropriate joint pose and 3DGS optimization strategy, photorealistic rendering can be realized. "
[30.09.2025 05:12] Response: ```python
[
    "NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences",
    "University of Chinese Academy of Sciences",
    "Huazhong University of Science and Technology",
    "Shandong University",
    "University of Science and Technology Beijing"
]
```
[30.09.2025 05:12] Deleting PDF ./assets/pdf/2509.25191.pdf.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25123.
[30.09.2025 05:12] Downloading paper 2509.25123 from http://arxiv.org/pdf/2509.25123v1...
[30.09.2025 05:12] Extracting affiliations from text.
[30.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 3 2 1 5 2 . 9 0 5 2 : r a FROM (x) AND g(x) TO (g(x)): LLMS LEARN NEW SKILLS IN RL BY COMPOSING OLD ONES Lifan Yuan1, Weize Chen2, Yuchen Zhang3,4, Ganqu Cui3, Hanbin Wang4, Ziming You4, Ning Ding2,3, Zhiyuan Liu2, Maosong Sun2, Hao Peng1 1 University of Illinois Urbana-Champaign 2 Tsinghua University 4 Peking University 3 Shanghai AI Laboratory lifan4@illinois.edu chenwz21@mails.tsinghua.edu.cn "
[30.09.2025 05:12] Response: ```python
["University of Illinois Urbana-Champaign", "Tsinghua University", "Peking University", "Shanghai AI Laboratory"]
```
[30.09.2025 05:12] Deleting PDF ./assets/pdf/2509.25123.pdf.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23951.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23951.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23951.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25131.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25131.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25131.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24335.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24335.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24335.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23371.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23371.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23371.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23196.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23196.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23196.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.21953.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.21953.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.21953.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25185.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25185.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25185.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24786.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24786.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24786.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23866.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23866.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23866.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.22570.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.22570.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.22570.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25149.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25149.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25149.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25077.
[30.09.2025 05:12] Downloading paper 2509.25077 from http://arxiv.org/pdf/2509.25077v1...
[30.09.2025 05:12] Extracting affiliations from text.
[30.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 7 7 0 5 2 . 9 0 5 2 : r BRIDGE - BUILDING REINFORCEMENT-LEARNING DEPTH-TO-IMAGE DATA GENERATION ENGINE FOR MONOCULAR DEPTH ESTIMATION Dingning Liu1,2 Haoyu Guo1 Jingyi Zhou1 Tong He1 1Shanghai Artificial Intelligence Laboratory 2Fudan University Figure 1: We present BRIDGE, showcasing its RL-optimized Depth-to-Image (D2I) data generation engine which is used for generating realistic and geometrically accurate RGB images from source depth maps and Monocular Depth Estimation (MDE) model which after being trained on the massive high-quality data generated by the D2I engine, achieves superior depth prediction in complex scenes. Corresponding author. "
[30.09.2025 05:12] Response: ```python
["Shanghai Artificial Intelligence Laboratory", "Fudan University"]
```
[30.09.2025 05:12] Deleting PDF ./assets/pdf/2509.25077.pdf.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.25052.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.25052.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.25052.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24910.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24910.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24910.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24269.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24269.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24269.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23143.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23143.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23143.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.22518.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.22518.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.22518.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.24709.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.24709.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.24709.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.23115.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.23115.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.23115.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2509.22830.
[30.09.2025 05:12] Extra JSON file exists (./assets/json/2509.22830.json), skip PDF parsing.
[30.09.2025 05:12] Paper image links file exists (./assets/img_data/2509.22830.json), skip HTML parsing.
[30.09.2025 05:12] Success.
[30.09.2025 05:12] Enriching papers with extra data.
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 0. SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.  					AI-generated summary 				 In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bot...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 1. Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.  					AI-generated summary 				 Reinforcement learning from human feedback (RLHF) has emerged as the standard pa...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 2. Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.  					AI-generated summary 				 Reinforcement learning based post-training has recent...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 3. ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.  					AI-generated summary 				 AI scientists are emerging computational systems that serve as collaborative partners in dis...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 4. EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.  					AI-generated summary 				 Large language model (LLM) steering has emerged as a promising paradigm for controlling mod...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 5. RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.  					AI-generated summary 				 The integration of visual understanding and generation into uni...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 6. A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.  					AI-generated summary 				 Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 7. GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.  					AI-generated summary 				 Vision language models (VLMs) achieve unified modeling of images and text, enabling them to...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 8. OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.  					AI-generated summary 				 The performance of unified multimodal models for image generation and editing is fundamentally c...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 9. SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.  					AI-generated summary 				 We introduce SAN...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 10. SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.  					AI-generated summary 				 While diffusion language models (DLMs) offer a...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 11. Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.  					AI-generated summary 				 Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocatin...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 12. SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.  					AI-generated summary 				 We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleav...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 13. A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.  					AI-generated summary 				 Long-sequence processing is a critical capability for modern large language mode...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 14. Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.  					AI-generated summary 				 Reinforcement Learning (RL) has emerged as a popular training paradigm, pa...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 15. VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.  					AI-generated summary 				 Recent advances in text-to-video generation have produced increasingly realistic and div...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 16. Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.  					AI-generated summary 				 Streaming video generation, as one fundamental componen...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 17. Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.  					AI-generated summary 				 Diffusion language models (DLMs) have strong th...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 18. Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.  					AI-generated summary 				 Spatial intelligence spans a rich suite of abilities, including visualising and transforming sha...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 19. Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.  					AI-generated summary 				 Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 20. AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.  					AI-generated summary 				 Search-augmented LLMs often struggle with comp...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 21. VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  					AI-generated summary 				 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 22. Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  					AI-generated summary 				 Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? T...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 23. HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.  					AI-generated summary 				 We present HunyuanImage 3.0, a native mul...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 24. MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.  					AI-generated summary 				 We present MGM-Omni, a unified Omni LLM for omni-modal understanding and exp...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 25. SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.  					AI-generated summary 				 Autoregressive (AR) models are promising for image generation, ...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 26. Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.  					AI-generated summary 				 Preference optimization is crucial for aligning l...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 27. Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.  					AI-generated summary 				 Recent reasoning LLMs (RLMs), especially ...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 28. MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.  					AI-generated summary 				 Multi-subject im...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 29. PixelCraft, a multi-agent system, enhances visual reasoning in multimodal large language models by integrating high-fidelity image processing and flexible reasoning through a dynamic workflow and image memory.  					AI-generated summary 				 Structured images (e.g., charts and geometric diagrams) re...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 30. LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.  					AI-generated summary 				 Long video understanding is still challenging for recent Large Video-Language ...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 31. DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.  					AI-generated summary 				 Vision-language model (VLM) based GUI...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 32. UniMIC, a unified token-based framework, enhances multimodal communication by using compact tokenized representations and lightweight Transformer-based entropy models, achieving significant bitrate savings without compromising performance.  					AI-generated summary 				 The rapid progress of Large ...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 33. A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.  					AI-generated summary 				 Large Language Models...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 34. BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  					AI-generated summary 				 Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 35. CEL, a novel agent architecture using a Large Language Model, learns to master complex environments through explicit reasoning and planning, achieving success in diverse grid-world tasks with sparse rewards.  					AI-generated summary 				 The pursuit of artificial agents that can learn to master co...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 36. SID, a self-improving demonstration approach, enhances exploration and generalization in goal-oriented language-guided navigation tasks, achieving state-of-the-art performance.  					AI-generated summary 				 Goal-oriented language-guided navigation requires robust exploration capabilities for agent...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 37. AdvChain enhances the safety and reliability of large reasoning models by teaching them dynamic self-correction through adversarial chain-of-thought tuning.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chai...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 38. MathBode provides a diagnostic for mathematical reasoning in LLMs by analyzing frequency-resolved metrics of model outputs compared to exact solutions, revealing systematic low-pass behavior and phase lag.  					AI-generated summary 				 This paper presents MathBode, a dynamic diagnostic for mathema...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 39. The Reasoning Manifold framework quantifies and localizes reasoning failures in Large Language Models by analyzing geometric deviations in internal representations.  					AI-generated summary 				 Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms i...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 40. IWR-Bench evaluates Large Vision-Language Models in reconstructing interactive webpages from video, highlighting challenges in multi-modal reasoning and code generation.  					AI-generated summary 				 The webpage-to-code task requires models to understand visual representations of webpages and gene...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 41. RHYTHM uses hierarchical temporal tokenization and large language models to predict human mobility, capturing long-range dependencies and multi-scale periodic behaviors efficiently.  					AI-generated summary 				 Predicting human mobility is inherently challenging due to complex long-range dependen...
[30.09.2025 05:12] ********************************************************************************
[30.09.2025 05:12] Abstract 42. ChatInject, a novel attack exploiting structured chat templates and persuasive multi-turn dialogues, significantly enhances attack success rates on large language model-based agents compared to traditional methods.  					AI-generated summary 				 The growing deployment of large language model (LLM) ...
[30.09.2025 05:12] Read previous papers.
[30.09.2025 05:12] Generating reviews via LLM API.
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#video", "#training", "#optimization", "#architecture", "#diffusion"], "emoji": "⚡", "ru": {"title": "Ускорение видео-генерации через умное разделение внимания", "desc": "В статье представлен метод SLA (Sparse-Linear Attention), который ускоряет Diffusion Transformer модели для гене
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#training", "#alignment", "#optimization", "#rlhf"], "emoji": "🎮", "ru": {"title": "Многопользовательская игра для лучшего понимания предпочтений человека", "desc": "Исследователи предложили новый метод обучения языковых моделей на основе предпочтений людей, расширив подход N
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#alignment", "#multimodal", "#3d", "#cv"], "emoji": "🧩", "ru": {"title": "Собираем пазл из визуальных данных для лучшего понимания", "desc": "Visual Jigsaw - это фреймворк самообучающегося reinforcement learning для улучшения визуального понимания у
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#data", "#dataset", "#science", "#open_source", "#multimodal", "#agents"], "emoji": "🔬", "ru": {"title": "Универсальная экосистема для создания AI-ученых", "desc": "ToolUniverse - это экосистема, которая стандартизирует и интегрирует инструменты, модели и данные для AI-ученых, обесп
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#training", "#alignment", "#inference", "#optimization", "#hallucinations", "#architecture"], "emoji": "🎛️", "ru": {"title": "Высокоскоростное управление LLM без переобучения", "desc": "EasySteer — это унифицированная система для эффективного управления поведением больших языковых м
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agi", "#multimodal", "#benchmark", "#survey", "#architecture"], "emoji": "🔄", "ru": {"title": "Унификация без синергии: почему объединение понимания и генерации пока не работает", "desc": "Исследователи представляют RealUnify - новый бенчмарк для оценки с
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#data", "#training", "#optimization", "#benchmark"], "emoji": "🎨", "ru": {"title": "Высококачественная reward модель - ключ к RL в редактировании изображений", "desc": "Исследователи создали специализированную reward модель EditScore для обучения с подкреплением в задаче реда
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#survey", "#benchmark", "#reasoning", "#cv", "#dataset"], "emoji": "🧮", "ru": {"title": "Когда картинки ставят AI в тупик: визуальная математика как новый вызов для умных систем", "desc": "Исследователи создали новый бенчмарк GSM8K-V для оценки математического мышления vision-langua
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#synthetic", "#multimodal", "#dataset"], "emoji": "🎨", "ru": {"title": "Систематический подход к созданию данных — ключ к прорыву в мультимодальном AI", "desc": "Исследователи создали OpenGPT-4o-Image — крупномасштабный датасет для обучения му
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#video", "#training", "#inference", "#small_models", "#diffusion"], "emoji": "🎬", "ru": {"title": "Быстрая и экономичная генерация длинных видео высокого качества", "desc": "SANA-Video - это компактная диффузионная модель для генерации видео высокого разрешения до 720x1280 пикселей 
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#architecture", "#diffusion"], "emoji": "⚡", "ru": {"title": "Ускорение диффузионных моделей через умное разреженное внимание", "desc": "SparseD - это новый метод разреженного внимания для диффузионных языковых моделей, который решает 
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization"], "emoji": "🔄", "ru": {"title": "Динамический поиск экспертов: новое измерение для улучшения рассуждений LLM", "desc": "Исследование предлагает метод Dynamic Experts Search (DES), который улучшает рассуждения больших языковы
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#open_source", "#optimization"], "emoji": "🎯", "ru": {"title": "Умные рассуждения через сжатие и расширение контекста", "desc": "В статье представлен метод SIRI для обучения больших языковых моделей рассуждения с использованием reinforcement learnin
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#reasoning", "#training", "#long_context"], "emoji": "⚡", "ru": {"title": "Умное переключение внимания для эффективной работы с длинными текстами", "desc": "Исследователи представили InfLLM-V2 — новый подход к обработке длинных последовательностей в 
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#benchmark", "#rl", "#rlhf", "#reasoning", "#training"], "emoji": "🔍", "ru": {"title": "Критическое мышление делает AI умнее", "desc": "В статье представлен метод Critique Reinforcement Learning (CRL), который обучает LLM генерировать критическ
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#video", "#rlhf", "#interpretability", "#alignment"], "emoji": "🎬", "ru": {"title": "Умная оценка AI-видео с объяснениями", "desc": "В статье представлена VideoScore2 — многомерная и интерпретируемая система для оценки качества видео, сгенерированных из текста. Модель 
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#games", "#video"], "emoji": "🎬", "ru": {"title": "Стриминг длинных видео без накопления ошибок", "desc": "Rolling Forcing - это новая техника генерации видео, которая решает проблему накопления ошибок при создании длинных видеопотоков. Метод исполь
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#training", "#optimization"], "emoji": "🔀", "ru": {"title": "Адаптивная генерация с переменной длиной блоков", "desc": "В работе представлена Sequential Diffusion Language Model (SDLM) - новая архитектура, которая улучшает предобученные авторегрессионн
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#multimodal", "#benchmark", "#transfer_learning"], "emoji": "📐", "ru": {"title": "Геометрия как ключ к пространственному интеллекту AI", "desc": "Исследователи создали датасет Euclid30K с 30 тысячами задач по планиметрии и стереометрии для обуч
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#reasoning", "#training", "#rlhf"], "emoji": "🔧", "ru": {"title": "Умное использование инструментов через энтропию рассуждений", "desc": "Исследователи предлагают фреймворк Tool-Light для улучшения интеграции внешних инструментов в рассуждения больших яз
[30.09.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#small_models"], "emoji": "🔍", "ru": {"title": "Кооперативная самоигра для эффективного поиска и рассуждений", "desc": "В статье представлена AceSearcher — framework для кооперативной самоигры, который улучшает способности LLM к рас
[30.09.2025 05:12] Querying the API.
[30.09.2025 05:12] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  					AI-generated summary 				 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/
[30.09.2025 05:12] Response: ```json
{
  "desc": "Исследователи изучают применение 3D Foundation Models для плотного синтеза новых видов (Novel View Synthesis). Традиционные методы зависят от медленных и ненадежных алгоритмов Structure-from-Motion для получения камерных поз и облаков точек. Масштабирование 3DFM на плотные виды сталкивается с проблемами нехватки видеопамяти и низкого качества результатов. Предложенный метод VGGT-X решает эти проблемы через эффективную реализацию памяти, адаптивное выравнивание и улучшенные практики обучения 3DGS.",
  "emoji": "🎥",
  "title": "Плотный синтез видов без COLMAP с помощью 3D Foundation Models"
}
```
[30.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  					AI-generated summary 				 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/"

[30.09.2025 05:12] Response: ```python
['3D']
```
[30.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  					AI-generated summary 				 We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/"

[30.09.2025 05:12] Response: ```python
["OPTIMIZATION", "GAMES"]
```
[30.09.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VGGT-X tackles the challenges of VRAM usage and output quality when scaling 3D Foundation Models (3DFMs) for dense Novel View Synthesis (NVS) without using COLMAP. Traditional methods rely on accurate 3D attributes from Structure-from-Motion, which can be slow and unreliable in certain conditions. The paper introduces a memory-efficient implementation and adaptive techniques to enhance output quality, allowing for the processing of over 1,000 images. Experimental results demonstrate that VGGT-X significantly improves fidelity in dense NVS and pose estimation compared to existing methods, while also providing insights for future advancements in 3D modeling.","title":"VGGT-X: Scaling 3D Models for High-Quality View Synthesis Without COLMAP"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VGGT-X tackles the challenges of VRAM usage and output quality when scaling 3D Foundation Models (3DFMs) for dense Novel View Synthesis (NVS) without using COLMAP. Traditional methods rely on accurate 3D attributes from Structure-from-Motion, which can be slow and unreliable in certain conditions. The paper introduces a memory-efficient implementation and adaptive techniques to enhance output quality, allowing for the processing of over 1,000 images. Experimental results demonstrate that VGGT-X significantly improves fidelity in dense NVS and pose estimation compared to existing methods, while also providing insights for future advancements in 3D modeling.', title='VGGT-X: Scaling 3D Models for High-Quality View Synthesis Without COLMAP'))
[30.09.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VGGT-X 研究了在不依赖 COLMAP 的情况下，如何解决在密集新视图合成中扩展 3D 基础模型时的 VRAM 和输出质量问题。尽管现有的新视图合成方法取得了显著进展，但仍然依赖于从运动结构中获取的准确 3D 属性，这在低纹理或低重叠的场景中往往表现不佳。我们的研究表明，简单地将 3D 基础模型扩展到密集视图会面临 VRAM 负担增加和输出质量下降的两个主要障碍。为了解决这些问题，我们提出了 VGGT-X，采用了内存高效的实现和自适应全局对齐技术，显著提高了密集新视图合成的效果。","title":"VGGT-X：高效解决密集新视图合成中的挑战"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VGGT-X 研究了在不依赖 COLMAP 的情况下，如何解决在密集新视图合成中扩展 3D 基础模型时的 VRAM 和输出质量问题。尽管现有的新视图合成方法取得了显著进展，但仍然依赖于从运动结构中获取的准确 3D 属性，这在低纹理或低重叠的场景中往往表现不佳。我们的研究表明，简单地将 3D 基础模型扩展到密集视图会面临 VRAM 负担增加和输出质量下降的两个主要障碍。为了解决这些问题，我们提出了 VGGT-X，采用了内存高效的实现和自适应全局对齐技术，显著提高了密集新视图合成的效果。', title='VGGT-X：高效解决密集新视图合成中的挑战'))
[30.09.2025 05:12] Querying the API.
[30.09.2025 05:12] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  					AI-generated summary 				 Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.
[30.09.2025 05:13] Response: ```json
{
  "desc": "Исследование показывает, что reinforcement learning позволяет большим языковым моделям приобретать новые навыки путем композиции уже существующих способностей. Авторы доказали, что LLM могут изучать комбинации функций h(x)=g(f(x)) во время RL-обучения, даже если функции f и g были изучены ранее. Эти композиционные навыки обобщаются на более сложные задачи и переносятся между различными доменами. В отличие от обучения next-token prediction, RL фундаментально меняет рассуждения модели и способствует развитию продвинутых навыков решения сложных проблем.",
  "emoji": "🧩",
  "title": "RL учит LLM комбинировать навыки как конструктор"
}
```
[30.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  					AI-generated summary 				 Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems."

[30.09.2025 05:13] Response: ```python
["RL", "RLHF", "TRAINING"]
```
[30.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  					AI-generated summary 				 Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems."

[30.09.2025 05:13] Response: ```python
["REASONING", "TRANSFER_LEARNING", "SYNTHETIC"]
```
[30.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how reinforcement learning (RL) can help large language models (LLMs) develop new skills by combining existing ones, enhancing their reasoning capabilities. The authors investigate whether RL genuinely teaches LLMs new skills or simply reactivates learned strategies. Through a synthetic framework, they demonstrate that LLMs can learn to compose functions during RL training, allowing them to solve more complex tasks. The findings suggest that RL significantly alters the reasoning behaviors of LLMs, leading to improved performance on tasks without prior specific training.","title":"Reinforcement Learning: Unlocking New Skills in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how reinforcement learning (RL) can help large language models (LLMs) develop new skills by combining existing ones, enhancing their reasoning capabilities. The authors investigate whether RL genuinely teaches LLMs new skills or simply reactivates learned strategies. Through a synthetic framework, they demonstrate that LLMs can learn to compose functions during RL training, allowing them to solve more complex tasks. The findings suggest that RL significantly alters the reasoning behaviors of LLMs, leading to improved performance on tasks without prior specific training.', title='Reinforcement Learning: Unlocking New Skills in Language Models'))
[30.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"强化学习（RL）使大型语言模型（LLM）能够通过组合现有技能来获得新的组合技能，这些技能可以转移到不同的任务中并改善推理行为。研究表明，LLM在强化学习过程中确实可以获得真正的新技能，而不仅仅是激活已有的推理策略。我们开发了一个合成框架来控制任务复杂性，并发现LLM能够学习未见的函数组合，并且这种组合能力可以推广到更复杂的问题上。实验结果显示，LLM在源任务上获得的组合技能可以转移到不同的目标任务，即使在目标任务上没有进行组合训练。","title":"强化学习助力语言模型获得新技能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='强化学习（RL）使大型语言模型（LLM）能够通过组合现有技能来获得新的组合技能，这些技能可以转移到不同的任务中并改善推理行为。研究表明，LLM在强化学习过程中确实可以获得真正的新技能，而不仅仅是激活已有的推理策略。我们开发了一个合成框架来控制任务复杂性，并发现LLM能够学习未见的函数组合，并且这种组合能力可以推广到更复杂的问题上。实验结果显示，LLM在源任务上获得的组合技能可以转移到不同的目标任务，即使在目标任务上没有进行组合训练。', title='强化学习助力语言模型获得新技能'))
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#data", "#open_source", "#diffusion", "#multimodal", "#training"], "emoji": "🎨", "ru": {"title": "Гигантская мультимодальная модель для генерации изображений с 80 миллиардами параметров", "desc": "HunyuanImage 3.0 - это мультимодальная модель с авторегрессивной архи
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#training", "#open_source", "#games", "#agi", "#long_context", "#multimodal", "#interpretability", "#audio", "#architecture"], "emoji": "🧠", "ru": {"title": "Единая модель для понимания и генерации речи с архитектурой мозг-рот", "desc": "MGM-Omni представляет собой унифицированную м
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#architecture", "#diffusion"], "emoji": "🌐", "ru": {"title": "Гиперсферические ограничения для стабильной авторегрессионной генерации изображений", "desc": "В статье представлена SphereAR - авторегрессионная модель для генерации изображений, кото
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#alignment"], "emoji": "⚖️", "ru": {"title": "Умная балансировка данных для выравнивания LLM с человеческими предпочтениями", "desc": "В статье представлен MetaAPO - новый подход для выравнивания больших языковых моделей с человеческими предпоч
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#optimization", "#benchmark"], "emoji": "🔍", "ru": {"title": "От примеров к инсайтам: новый подход к few-shot рассуждениям", "desc": "Исследователи обнаружили, что современные reasoning LLM часто показывают худшие результаты при использовании few-sh
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#leakage", "#alignment", "#multimodal", "#training", "#rl", "#architecture", "#synthetic"], "emoji": "🎨", "ru": {"title": "Точная генерация изображений с множественными объектами через разделение внимания", "desc": "Исследователи представили MultiCrafter - фреймворк для генерации из
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#interpretability", "#multimodal", "#agents"], "emoji": "🔍", "ru": {"title": "Мультиагентное визуальное рассуждение высокой точности", "desc": "PixelCraft — это мультиагентная система, которая улучшает визуальное рассуждение в мультимодальных LLM п
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#video", "#rl", "#training", "#reasoning", "#long_context", "#optimization", "#benchmark"], "emoji": "🔍", "ru": {"title": "Адаптивное масштабирование видео для лучшего понимания длинных роликов", "desc": "Исследователи представили LOVE-R1 - модель для понимания длинных видео, котора
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#rl", "#open_source", "#training", "#games", "#agents"], "emoji": "🤖", "ru": {"title": "Асинхронное обучение GUI агентов через децентрализованную архитектуру", "desc": "DART представляет собой децентрализованную архитектуру для обучения с подк
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#data", "#games", "#optimization", "#multimodal", "#cv"], "emoji": "📡", "ru": {"title": "Токенизированное сжатие для эффективной мультимодальной коммуникации", "desc": "UniMIC представляет новый подход к сжатию мультимодальных данных для взаимодействия между устройствами и облачными
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training"], "emoji": "⚡", "ru": {"title": "Революция в обучении LLM: стабильная 4-битная точность", "desc": "Исследователи разработали новый метод обучения больших языковых моделей с использованием 4-битной точности NVFP4 вместо традиционной 8-битной 
[30.09.2025 05:13] Querying the API.
[30.09.2025 05:13] Claude request. Model: claude-sonnet-4-20250514. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  					AI-generated summary 				 Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at https://dingning-liu.github.io/bridge.github.io/.
[30.09.2025 05:13] Response: ```json
{
  "desc": "В статье представлен метод BRIDGE для улучшения моnocular depth estimation - задачи определения глубины сцены по одному изображению. Авторы используют reinforcement learning для генерации более 20 миллионов реалистичных RGB изображений с точными картами глубины из разнообразных исходных данных. Для обучения модели применяется гибридная стратегия, которая комбинирует псевдо-метки от учителя с истинными значениями глубины. Такой подход позволяет BRIDGE превосходить существующие state-of-the-art методы и лучше справляться со сложными деталями сцен.",
  "emoji": "🌉",
  "title": "Мостик к точной оценке глубины через генерацию данных"
}
```
[30.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  					AI-generated summary 				 Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at https://dingning-liu.github.io/bridge.github.io/."

[30.09.2025 05:13] Response: ```python
['DATASET', 'DATA', 'CV', 'RL', 'TRAINING']
```
[30.09.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  					AI-generated summary 				 Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at https://dingning-liu.github.io/bridge.github.io/."

[30.09.2025 05:13] Response: ```python
["SYNTHETIC", "OPTIMIZATION"]
```
[30.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BRIDGE is a novel framework that utilizes reinforcement learning (RL) to optimize the generation of depth-to-image (D2I) data, creating a vast dataset of over 20 million realistic RGB images paired with their corresponding ground truth depth maps. This approach addresses the limitations of traditional monocular depth estimation (MDE) methods, which often suffer from insufficient and low-quality data. By employing a hybrid supervision strategy that combines teacher pseudo-labels with actual depth data, BRIDGE enhances the training process for depth estimation models. As a result, BRIDGE significantly improves performance and robustness in depth estimation tasks, outperforming existing methods in both quantitative metrics and the ability to capture complex scene details.","title":"BRIDGE: Revolutionizing Depth Estimation with RL-Optimized Data Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BRIDGE is a novel framework that utilizes reinforcement learning (RL) to optimize the generation of depth-to-image (D2I) data, creating a vast dataset of over 20 million realistic RGB images paired with their corresponding ground truth depth maps. This approach addresses the limitations of traditional monocular depth estimation (MDE) methods, which often suffer from insufficient and low-quality data. By employing a hybrid supervision strategy that combines teacher pseudo-labels with actual depth data, BRIDGE enhances the training process for depth estimation models. As a result, BRIDGE significantly improves performance and robustness in depth estimation tasks, outperforming existing methods in both quantitative metrics and the ability to capture complex scene details.', title='BRIDGE: Revolutionizing Depth Estimation with RL-Optimized Data Generation'))
[30.09.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BRIDGE是一个基于强化学习优化的深度到图像生成框架，旨在创建一个大型多样化的数据集，以增强单目深度估计的鲁棒性和性能。该框架合成了超过2000万张真实且几何准确的RGB图像，并与其真实深度一一对应，解决了传统方法在数据稀缺和质量上的限制。通过采用混合监督策略，将教师伪标签与真实深度结合，BRIDGE的深度估计模型在这个数据集上进行训练，取得了显著的突破。最终，BRIDGE在规模和领域多样性上表现优异，超越了现有的最先进方法。","title":"BRIDGE：深度估计的新突破"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BRIDGE是一个基于强化学习优化的深度到图像生成框架，旨在创建一个大型多样化的数据集，以增强单目深度估计的鲁棒性和性能。该框架合成了超过2000万张真实且几何准确的RGB图像，并与其真实深度一一对应，解决了传统方法在数据稀缺和质量上的限制。通过采用混合监督策略，将教师伪标签与真实深度结合，BRIDGE的深度估计模型在这个数据集上进行训练，取得了显著的突破。最终，BRIDGE在规模和领域多样性上表现优异，超越了现有的最先进方法。', title='BRIDGE：深度估计的新突破'))
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#rl", "#interpretability", "#reasoning", "#games", "#agents"], "emoji": "🧠", "ru": {"title": "Агент, который учится мыслить: явное рассуждение вместо скрытых весов", "desc": "В статье представлена архитектура агента CEL (Cogito, ergo ludo), которая использует большую языковую модель
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#rl", "#agi", "#optimization", "#transfer_learning", "#agents"], "emoji": "🗺️", "ru": {"title": "Агенты учатся навигации, улучшая собственные демонстрации", "desc": "Статья представляет SID - подход для обучения навигационных агентов в задачах целевой навигации с языковым управление
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#security", "#alignment", "#rlhf"], "emoji": "⚖️", "ru": {"title": "Обучение AI самокоррекции через adversarial цепочки рассуждений", "desc": "Исследователи выявили проблему «эффекта снежного кома» в больших рассуждающих моделях (LRM), где небо
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#reasoning", "#math", "#interpretability", "#dataset"], "emoji": "📊", "ru": {"title": "Частотный анализ математического мышления в LLM", "desc": "В статье представлен MathBode - новый диагностический инструмент для анализа математических способностей бо
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#data", "#interpretability", "#reasoning", "#multimodal"], "emoji": "🧠", "ru": {"title": "Геометрия мышления: как найти сбои в рассуждениях ИИ", "desc": "Исследователи предложили фреймворк Reasoning Manifold для анализа ошибок рассуждений в больших языковых моделях 
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#benchmark", "#reasoning", "#cv", "#multimodal"], "emoji": "🎬", "ru": {"title": "От видео к коду: новый вызов для AI в создании интерактивных веб-страниц", "desc": "В статье представлен IWR-Bench — новый бенчмарк для оценки способности больших vision
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#architecture", "#data", "#optimization", "#benchmark", "#open_source", "#reasoning", "#training", "#long_context", "#dataset"], "emoji": "🏃", "ru": {"title": "Ритмы движения: предсказание мобильности через иерархическую токенизацию", "desc": "В статье представлена модель RHYTHM для
[30.09.2025 05:13] Using data from previous issue: {"categories": ["#agents", "#rlhf", "#security"], "emoji": "💬", "ru": {"title": "Обман через чат-шаблоны: новая угроза для AI-агентов", "desc": "Исследователи представили ChatInject - новый тип атаки на LLM-агентов, который использует структурированные chat-шаблоны для внедрения вредоносных инструкц
[30.09.2025 05:13] Renaming data file.
[30.09.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-09-30.json
[30.09.2025 05:13] Saving new data file.
[30.09.2025 05:13] Generating page.
[30.09.2025 05:13] Renaming previous page.
[30.09.2025 05:13] Renaming previous data. index.html to ./d/2025-09-30.html
[30.09.2025 05:13] Writing result.
[30.09.2025 05:13] Renaming log file.
[30.09.2025 05:13] Renaming previous data. log.txt to ./logs/2025-09-30_last_log.txt
