[11.02.2026 12:51] Read previous papers.
[11.02.2026 12:51] Generating top page (month).
[11.02.2026 12:51] Writing top page (month).
[11.02.2026 14:13] Read previous papers.
[11.02.2026 14:13] Get feed.
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05400
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09856
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09082
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10063
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08234
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09443
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10090
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08426
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07035
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10104
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09084
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07022
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04208
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09849
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10098
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08847
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10102
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06820
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08382
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09268
[11.02.2026 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2602.01244
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00268
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09823
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09439
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07153
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10116
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09662
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09024
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08344
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07839
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06161
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05435
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02464
[11.02.2026 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2602.09591
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08503
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07276
[11.02.2026 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2602.00462
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10099
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09924
[11.02.2026 14:13] Extract page data from URL. URL: https://huggingface.co/papers/2602.07670
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04802
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04521
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04908
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01725
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21235
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08519
[11.02.2026 14:13] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07755
[11.02.2026 14:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.02.2026 14:13] No deleted papers detected.
[11.02.2026 14:13] Downloading and parsing papers (pdf, html). Total: 47.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.05400.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.05400.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.05400.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09856.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09856.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09856.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09082.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09082.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09082.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.10063.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.10063.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.10063.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.08234.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.08234.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.08234.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09443.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09443.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09443.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.10090.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.10090.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.10090.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.08426.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.08426.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.08426.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.07035.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.07035.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.07035.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.10104.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.10104.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.10104.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09084.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09084.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09084.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.07022.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.07022.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.07022.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.04208.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.04208.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.04208.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09849.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09849.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09849.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.10098.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.10098.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.10098.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.08847.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.08847.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.08847.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.10102.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.10102.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.10102.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.06820.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.06820.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.06820.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.08382.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.08382.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.08382.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09268.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09268.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09268.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.01244.
[11.02.2026 14:13] Downloading paper 2602.01244 from https://arxiv.org/pdf/2602.01244v2...
[11.02.2026 14:13] Extracting affiliations from text.
[11.02.2026 14:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments Siwei Wu 1 2 3 Yizhi Li 2 3 Yuyang Song 4 Wei Zhang 5 Yang Wang 1 Riza Batista-Navarro 1 Xian Yang 1 Mingjie Tang 4 Bryan Dai 2 Jian Yang 5 Chenghua Lin "
[11.02.2026 14:13] Response: ```python
[]
```
[11.02.2026 14:13] Extracting affiliations from text.
[11.02.2026 14:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments Siwei Wu 1 2 3 Yizhi Li 2 3 Yuyang Song 4 Wei Zhang 5 Yang Wang 1 Riza Batista-Navarro 1 Xian Yang 1 Mingjie Tang 4 Bryan Dai 2 Jian Yang 5 Chenghua Lin1. Introduction 6 2 0 2 3 ] . [ 2 4 4 2 1 0 . 2 0 6 2 : r Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: Executability, since each instance requires suitable and often distinct Docker environment; and Verifiability, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20% on TB 1.0 and 10% on TB 2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30% on TB 1.0 and 22.00% on TB 2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github. com/multimodal-art-projection/ TerminalTraj. Projection Research Community 1University of Manchester 2IQuest Research 3Multimodal 4Sichuan Unito: Lin Art 5Beihang University. versity Bryan Dai <cbdai@ubiquant.com>, <chenghua.lin@manchester.ac.uk>. Correspondence Chenghua Preprint. February 4, 2026. 1 AI agents have demonstrated strong capabilities in many complex tasks, which require multi-turn, long-context reasoning (Jimenez et al., 2023; Zhou et al., 2023; Madaan et al., 2023; Yao et al., 2024; Wang et al., 2024). However, many evaluations of these capabilities rely on settings that simplify or omit execution constraints and tool-mediated interactions present in real-world workflows. To address this gap, the TerminalBench (TB) was developed to evaluate AI agents in terminal environments (Merrill et al., 2026), critical interface for real-world human-AI interaction. While current agentic models possess basic terminal capabilities, advancing their performance in complex task resolution and test-time scaling still requires large-scale agentic data. Despite recent progress in this area, existing methods still struggle to support scalable, execution-based training. Nex-N1 (Cai et al., 2025) constructs task-specific environments using rule-based designs and LLMs to simulate agent interactions and generate trajectories, and verifies these trajectories through rule-based heuristics and LLM judges, rather than grounding them in real-world execution. As result, this process remains largely decoupled from actual execution, limiting its ability to capture environment-dependent behaviors intrinsic to terminal workflows. This limitation is particularly pronounced for terminal tasks, where success often depends on concrete filesystem states, dependency resolution, and tool-mediated side effects that are only observable at runtime. In contrast, repository-level benchmarks and datasets developed for software engineering tasks, such as SWE-bench-style datasets (Jimenez et al., 2023; Yang et al., 2025), provide realistic execution environments but remain constrained in scale, as they primarily rely on filtering high-star repositories for data construction, which fundamentally limits the scalability and diversity of executable environments. As result, existing datasets either scale task diversity without strong execution grounding, or provide executable supervision at the cost of environment diversity. Together, these limitations reveal gap in existing agentic data construction methods: the lack of scalable approach for generating terminal trajectories that simultaneously support environment diversity, realistic execution grounding, Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments Figure 1. Data Curation Pipeline. The left part illustrates data source collection, the top-right part shows Docker image curation, and the bottom-right part presents instance generation. and execution-based verification. To address these challenges, we propose TerminalTraj Pipeline, large-scale terminal agentic trajectory generation pipeline that curates Docker-executable task instances from real-world GitHub repositories, synthesizes environmentaligned terminal tasks, and verifies agent rollouts through instance-specific executable validation. Specifically, to overcome environment scalability limits caused by heuristic repository filtering, we reformulate repository selection as model-based quality scoring problem, enabling fully automated construction of 32,325 Docker images across eight widely used programming languages. Beyond generalpurpose code execution, we further curate domain-specific instances spanning eight specialized domains to capture realworld task requirements involving specialized inputs, tools, and dependencies. In contrast to Nex-N1s reliance on LLMbased judges and hand-crafted rules, TerminalTraj Pipeline adopts instance-specific executable validation inspired by TerminalBench, filtering trajectories through task-specific validation code. Overall, TerminalTraj Pipeline generates 50,733 verified trajectories and supports continual, scalable synthesis of Docker-aligned agent data. We demonstrate the effectiveness of our approach along three key dimensions: 1. Large-scale execution-grounded terminal agentic trajectories enable strong agent performance. Unlike approaches that rely on heuristics or LLMgenerated data, fine-tuning Qwen2.5-Coder on TerminalTraj Pipeline yields nearly 20% and 10% absolute gains, raising the performance to 35.30% on TB 1.0 and 22.00% on TB 2.0. Consequently, TerminalTraj achieves state-of-the-art performance among models under 100B parameters, delivering capabilities comparable to the Qwen3-Coder-480B. 2. Model-based repository scoring enables continuous scaling of diverse execution environments. Unlike prior work limited by heuristic constraints, by empl"
[11.02.2026 14:13] Mistral response. {"id": "8bbb3260452b4381b3fb7f67d352060f", "created": 1770819206, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1428, "total_tokens": 1473, "completion_tokens": 45, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of Manchester\",\n    \"IQuest Research\",\n    \"Multimodal\",\n    \"Sichuan University\",\n    \"Beihang University\",\n    \"Projection Research Community\"\n]\n```"}}]}
[11.02.2026 14:13] Response: ```python
[
    "University of Manchester",
    "IQuest Research",
    "Multimodal",
    "Sichuan University",
    "Beihang University",
    "Projection Research Community"
]
```
[11.02.2026 14:13] Deleting PDF ./assets/pdf/2602.01244.pdf.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.00268.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.00268.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.00268.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09823.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09823.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09823.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09439.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09439.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09439.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.07153.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.07153.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.07153.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.10116.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.10116.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.10116.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09662.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09662.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09662.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09024.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09024.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09024.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.08344.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.08344.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.08344.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.07839.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.07839.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.07839.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.06161.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.06161.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.06161.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.05435.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.05435.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.05435.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.02464.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.02464.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.02464.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09591.
[11.02.2026 14:13] Downloading paper 2602.09591 from https://arxiv.org/pdf/2602.09591v1...
[11.02.2026 14:13] Extracting affiliations from text.
[11.02.2026 14:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 1 ] . [ 1 1 9 5 9 0 . 2 0 6 2 : r On the Optimal Reasoning Length for RL-Trained Language Models ON THE OPTIMAL REASONING LENGTH FOR RLTRAINED LANGUAGE MODELS Daisuke Nohara1, Taishi Nakamura1, Rio Yokota1 1Institute of Science Tokyo {nohara,rioyokota}@rio.scrc.iir.isct.ac.jp "
[11.02.2026 14:13] Response: ```python
["Institute of Science Tokyo"]
```
[11.02.2026 14:13] Deleting PDF ./assets/pdf/2602.09591.pdf.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.08503.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.08503.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.08503.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.07276.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.07276.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.07276.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.00462.
[11.02.2026 14:13] Downloading paper 2602.00462 from https://arxiv.org/pdf/2602.00462v2...
[11.02.2026 14:13] Extracting affiliations from text.
[11.02.2026 14:13] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LATENTLENS: Revealing Highly Interpretable Visual Tokens in LLMs Benno Krojer 1 2 Shravan Nayak 1 3 Oscar Mañas 1 3 Vaibhav Adlakha 1 2 Desmond Elliott * 4 Siva Reddy * 1 2 5 Marius Mosbach * 1 2 Abstract Transforming large language model (LLM) into vision-language model (VLM) can be achieved by mapping the visual tokens from vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LATENTLENS, novel approach for mapping latent representations to descriptions in natural language. LATENTLENS encodes large text corpus and stores contextualized token representations for each token in that corpus. Visual token representations are then compared to these contextualized representations and the top-k nearest neighbor representations serve as descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LATENTLENS instead, the majority of visual tokens are interpretable across all studied models and all layers. More broadly, our findings contribute new evidence on the alignment between vision and language representations and open up new directions for analyzing the latent representations of LLMs. LATENTLENS Demo Code 6 2 0 F 9 ] . [ 2 2 6 4 0 0 . 2 0 6 2 : r 1. Introduction Transforming large language model (LLM) into visionlanguage model (VLM) can be as simple as training linEqual senior contribution. 1Mila Quebec AI Institute, Montreal, Canada 2McGill University, Montreal, Canada 3Université de Montréal, Montreal, Canada 4University of Copenhagen, Copenhagen, Denmark 5Canada CIFAR AI Chair. Correspondence to: Benno Krojer <benno.krojer@mila.quebec>."
[11.02.2026 14:13] Response: ```python
[
    "Mila Quebec AI Institute",
    "McGill University",
    "Université de Montréal",
    "University of Copenhagen",
    "Canada CIFAR AI Chair"
]
```
[11.02.2026 14:13] Deleting PDF ./assets/pdf/2602.00462.pdf.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.10099.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.10099.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.10099.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.09924.
[11.02.2026 14:13] Extra JSON file exists (./assets/json/2602.09924.json), skip PDF parsing.
[11.02.2026 14:13] Paper image links file exists (./assets/img_data/2602.09924.json), skip HTML parsing.
[11.02.2026 14:13] Success.
[11.02.2026 14:13] Downloading and parsing paper https://huggingface.co/papers/2602.07670.
[11.02.2026 14:13] Downloading paper 2602.07670 from https://arxiv.org/pdf/2602.07670v1...
[11.02.2026 14:14] Extracting affiliations from text.
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation Jarrod Barnes 1 6 2 0 2 7 ] . [ 1 0 7 6 7 0 . 2 0 6 2 : r a "
[11.02.2026 14:14] Response: ```python
[]
```
[11.02.2026 14:14] Extracting affiliations from text.
[11.02.2026 14:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation Jarrod Barnes 1 6 2 0 2 7 ] . [ 1 0 7 6 7 0 . 2 0 6 2 : r aTest-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and 120Bparameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Bestof-N sampling achieves 90% task success (18/20 tasks) at = 64 across the full KernelBench L1 eval set while TTTs best checkpoint reaches only 30.6% (3-seed mean), with TTTs equivalent falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowestconfidence) correct sample yields 80% success vs. 50% for most-confident selection, 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zerocost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail. 1Arc Intelligence. Correspondence to: Jarrod Barnes <jarrod@arc.computer>. Preprint. February 10, 2026. 1 Figure 1. Test-time strategy comparison. Best-of-N scaling (gray) saturates at = 16. At = 64, TTT (31%, red) is 2 worse than random selection (59%); surprisal-guided (blue) matches oracle. The +30% bracket: confidence (50%) vs. surprisal (80%). 1. Introduction This paper studies compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains where deterministic evaluator provides ground-truth feedback on model outputs. GPU kernel optimization exemplifies VEG: KernelBench (Ouyang et al., 2025) evaluates 250 PyTorch ML workloads on both functional correctness and runtime speedup, with the CUDA compiler and hardware providing an unambiguous, continuous reward signal. The defining characteristic is that the evaluator provides ground-truth feedback; no human labeler or AI teacher is needed to judge output quality. Why VEG tasks are the ideal testbed. Unlike binary pass/fail benchmarks, KernelBench provides continuous speedup signals (0x to 10x+). This density enables us to detect subtle performance regressions during adaptation that binary rewards would mask. When TTT over-sharpens, we observe the decline in continuous metric; papers with sparse rewards may miss this entirely. Recent work on test-time training (TTT) has shown impressive results through extended gradient-based adaptation. TTT-Discover (Yuksekgonul et al., 2026) reports costs of Surprisal-Guided Selection few hundred dollars per problem using 50 adaptation steps on discovery tasks. This raises fundamental question: is adaptation the right strategy for dense-reward VEG tasks, or does simple search suffice? We answer this question through controlled experiments comparing Best-of-N sampling against batch test-time training under matched compute budgets. Using GPT-OSS-120B (a 120B-parameter frontier model) with LoRA adaptation, we evaluate all 20 KernelBench L1 eval tasks. The results are decisive. Best-of-N at = 64 achieves 90% task success (18/20 tasks, finding at least one fast correct kernel per task) while TTTs best checkpoint (Best-of-Adaptation) reaches only 30.6% (3-seed mean). Computing TTTs equivalent (the Best-of-N budget needed to match TTT performance) yields < 1, meaning TTT underperforms singlesample inference (K = 1). The failure mode is over-sharpening: gradient updates collapse the policy toward mediocre solutions that happened to succeed early, destroying the diversity needed to find optimal kernels in the distribution tail. Ji et al. (Ji et al., 2026) predict that RL gains arise from distribution sharpening rather than discovering new strategies; our failure mode confirms this. Our main contribution is surprisal-guided selection. Probing the relationship between model confidence (logprobability) and kernel quality reveals surprising inverse correlation: the model is least confident about its best solutions. We operationalize this as surprisal-guided selection: selecting the highest-surprisal (lowest log-probability) correct sample. This achieves 80% success (fast and correct) versus 50% for confidence-guided selection, 30% improvement with zero additional compute. Extending to surprisal-guided-top3 (evaluating the 3 highest-surprisal correct samples and selecting the fastest) matches oracle performance at 100%. Three contributions emerge from our experiments (Figure 1): 1. Search outperforms minimal adaptation (1-5 GRPO steps) for dense-reward VEG tasks. Best-of-N scaling saturates at = 16 (99.9% success on 5-task subsets; 90% on the full 20-task L1 eval), while TTT equivalent < 1. Practitioners should invest in sample diversity, not gradient updates. 2. Surprisal-guided selection recovers oracle performance. Selecting from the high-surprisal tail (solutions the model didnt expect to find) provides practical, zero-cost selection strategy. 3. Mechanistic explanation for TTT failure. Oversharpening destroys diversity, confirmed by direct correlation probing. The optimum for kernel optimization lies in the distribution tail; gradient updates collapse toward the mode, missing the tail entirely. For execution-grounded domains with dense rewards, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisalguided selection principle (that the models best solutions occupy high-surprisal regions) may generalize to other VEG domains where rare, high-quality solutions exist in lowprobability regions of the models distribution. 2. Related Work Test-Time Training vs. Search. TTT-Discover (Yuksekgonul et al., 2026) demonstrates impressive results using 50 adaptation steps on discovery tasks, reporting costs of few hundred dollars per problem. We find different dynamics for"
[11.02.2026 14:14] Mistral response. {"id": "dfb19bcb35d54c24babe79d23fb66606", "created": 1770819242, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1538, "total_tokens": 1547, "completion_tokens": 9, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Arc Intelligence\"]\n```"}}]}
[11.02.2026 14:14] Response: ```python
["Arc Intelligence"]
```
[11.02.2026 14:14] Deleting PDF ./assets/pdf/2602.07670.pdf.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Downloading and parsing paper https://huggingface.co/papers/2602.04802.
[11.02.2026 14:14] Extra JSON file exists (./assets/json/2602.04802.json), skip PDF parsing.
[11.02.2026 14:14] Paper image links file exists (./assets/img_data/2602.04802.json), skip HTML parsing.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Downloading and parsing paper https://huggingface.co/papers/2602.04521.
[11.02.2026 14:14] Extra JSON file exists (./assets/json/2602.04521.json), skip PDF parsing.
[11.02.2026 14:14] Paper image links file exists (./assets/img_data/2602.04521.json), skip HTML parsing.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Downloading and parsing paper https://huggingface.co/papers/2602.04908.
[11.02.2026 14:14] Extra JSON file exists (./assets/json/2602.04908.json), skip PDF parsing.
[11.02.2026 14:14] Paper image links file exists (./assets/img_data/2602.04908.json), skip HTML parsing.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Downloading and parsing paper https://huggingface.co/papers/2602.01725.
[11.02.2026 14:14] Extra JSON file exists (./assets/json/2602.01725.json), skip PDF parsing.
[11.02.2026 14:14] Paper image links file exists (./assets/img_data/2602.01725.json), skip HTML parsing.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Downloading and parsing paper https://huggingface.co/papers/2601.21235.
[11.02.2026 14:14] Extra JSON file exists (./assets/json/2601.21235.json), skip PDF parsing.
[11.02.2026 14:14] Paper image links file exists (./assets/img_data/2601.21235.json), skip HTML parsing.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Downloading and parsing paper https://huggingface.co/papers/2602.08519.
[11.02.2026 14:14] Extra JSON file exists (./assets/json/2602.08519.json), skip PDF parsing.
[11.02.2026 14:14] Paper image links file exists (./assets/img_data/2602.08519.json), skip HTML parsing.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Downloading and parsing paper https://huggingface.co/papers/2602.07755.
[11.02.2026 14:14] Extra JSON file exists (./assets/json/2602.07755.json), skip PDF parsing.
[11.02.2026 14:14] Paper image links file exists (./assets/img_data/2602.07755.json), skip HTML parsing.
[11.02.2026 14:14] Success.
[11.02.2026 14:14] Enriching papers with extra data.
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 0. OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.  					AI-generated summary 				 As hig...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 1. Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.  					AI-generated summary 				 Autonomous GUI agents interact with environments by perceiv...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 2. UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.  					AI-generated summary 				 GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving bo...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 3. A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.  					AI-generated summary 				 Human problem-solving is never the repetition of a ...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 4. SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.  					AI-generated summary 				 Large Language Model (LLM) agents have shown stunning results in complex t...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 5. Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.  					AI-generated summary 				 The transition from symbolic ...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 6. Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.  					AI-generated summary 				 Recent advances in large lan...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 7. Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.  					AI-generated summary 				 Block-sparse attention is promising for accelerating long-...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 8. Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.  					AI-generated summary 				 Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by ...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 9. Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.  					AI-generated summary 				 Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to ...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 10. Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.  					AI-generated summary 				 We study instruction-based image editing under prof...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 11. Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.  					AI-generated summary 				 Recent studies have explored autoregressive model...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 12. SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.  					AI-generated summary 				 Vision-Language-Action (VLA) models have e...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 13. BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.  					AI-generated summary 				 Equipping embodied agents with the ability to reason about tasks, foresee ...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 14. VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.  					AI-generated summary 				 Pretraining Vision-Language-Action (VLA) policies...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 15. Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.  					AI-generated summary 				 Multi-agent LLM systems e...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 16. VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.  					AI-generated summary 				 Lear...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 17. ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.  					AI-generated summary 				 Training generalist agents capable of adapting to diverse scenarios requires interactive environments for se...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 18. A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.  					AI-generated summary 				 Large Language Models (LLMs) face significant challenges in long-context proces...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 19. Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms.  					AI-generated summary 				 Diffusion transformers typically incorporate textual information via attention layers...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 20. A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.  					AI-generated summ...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 21. Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.  					AI-generated summary 				 Auto-regressive video generation ...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 22. Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 23. A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.  					AI-generated summary 				 High-quality and open datas...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 24. A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.  					AI-generated summary 				 End-to-end GUI agents for real desktop environments requi...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 25. SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.  					AI-generated summary 				 Real-world data collection for em...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 26. TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification.  					AI-generated summary 				 Effectively scaling GUI automation is essential fo...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 27. Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.  					AI-generated summary 				 This paper challenges the dominance of continuous pipelines in vi...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 28. Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.  					AI-generated summary 				 Parallel thinking has emerged as a new paradigm f...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 29. TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  					AI-generated summary 				 Planning has become a central capability for contemporary agent systems in navigating c...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 30. COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction.  					AI-generated summary 				 Parallel diffusion decoding can...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 31. Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.  					AI-generated summary 				 While flow matching is elegant, its reliance on single-samp...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 32. Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.  					AI-generated summary 				 Activation d...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 33. Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.  					AI-generated summary 				 Reinforcement learning substantially improves reasoning in large language models, b...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 34. Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.  					AI-generated summary 				 Self-correction is essential for solving complex reasoning problems in vision-lang...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 35. STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  					AI-generated summary 				 Activation steering has emerged...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 36. LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.  					AI-generated summary 				 Transforming a large language model (L...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 37. Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling.  					AI-generated summary 				 Leveraging representation encode...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 38. LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance.  					AI-generated summary 				 Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require add...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 39. Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.  					AI-generated summary 				 Test-time training (TTT) adapts language models through gradient-based updates at inferen...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 40. VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved impressive performance in cross-mo...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 41. Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.  					AI-generated summary 				 Modern deployments require LLMs to enforce safety policies at scale, yet many controls rel...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 42. Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.  					AI-generated summary 				 Continuous-time generative models,...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 43. SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.  					AI-generated summary 				 With the widespread deployment of Computer-using Agents (CU...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 44. Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.  					AI-generated summary 				 Large language models (LLMs) are increasingly deployed in high...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 45. PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols.  					AI-generated summary 				 Attributed Graph Clustering (AGC) is a fun...
[11.02.2026 14:14] ********************************************************************************
[11.02.2026 14:14] Abstract 46. ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains.  					AI-generated summary 				 The statelessness of foundation models bottlenecks agentic systems' ability to conti...
[11.02.2026 14:14] Read previous papers.
[11.02.2026 14:14] Generating reviews via LLM API.
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#data", "#training"], "emoji": "🎯", "ru": {"title": "Умный отбор данных через оптимизационную геометрию", "desc": "OPUS — это динамическая система отбора данных для предварительной подготовки языковых моделей, которая оценивает качество примеров на основе проекций градиентных обновл
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#agents", "#dataset", "#rlhf", "#multimodal", "#training"], "emoji": "🤖", "ru": {"title": "Предсказание визуальных состояний интерфейсов через генерацию кода", "desc": "Code2World — это модель на основе vision-language, которая предсказывает следующее визуальное состояние GUI путём 
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#rl", "#agents", "#benchmark", "#multimodal", "#training"], "emoji": "🖱️", "ru": {"title": "Унифицированный агент для автоматизации графического интерфейса через слияние моделей и обучение с подкреплением", "desc": "UI-Venus-1.5 — это унифицированный агент для автоматизации взаимоде
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#training", "#agents", "#architecture"], "emoji": "🧠", "ru": {"title": "Адаптивное переключение способов мышления для более умного рассуждения в LLM", "desc": "Предложена новая методика Chain of Mindset, которая позволяет большим языковым моделям адаптивно выбирать оптимальный стиль
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#rl", "#open_source", "#agents", "#training", "#reasoning"], "emoji": "🧠", "ru": {"title": "От опыта к навыкам: иерархическое обучение с эволюцией", "desc": "SkillRL — это фреймворк, который помогает агентам на основе LLM улучшать свою работу через автоматическое обнаружение и иерар
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#rl", "#agents", "#multimodal", "#training", "#open_source", "#cv", "#science", "#optimization"], "emoji": "🔬", "ru": {"title": "Физические законы через мультимодальное восприятие: VLM для научного рассуждения", "desc": "В работе представлена семья моделе
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#rl", "#agents", "#benchmark"], "emoji": "🌍", "ru": {"title": "Синтетические миры для обучения универсальных агентов", "desc": "В работе предлагается Agent World Model (AWM) — конвейер для автоматической генерации синтетических окружений, ко
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#optimization", "#inference", "#long_context", "#architecture", "#training"], "emoji": "🔍", "ru": {"title": "Спектральное внимание вместо слепоты: как Prism ускоряет LLM через восстановление позиционных сигналов", "desc": "Работа посвящена улучшению блочно-разреженного внимания для 
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#open_source", "#agents", "#optimization", "#inference", "#architecture", "#training", "#reasoning", "#diffusion"], "emoji": "⚡", "ru": {"title": "Параллельные рассуждения для быстрых поисковых агентов", "desc": "В статье предлагается фреймворк DLLM-Searcher для оптимизации поисковы
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#training", "#video", "#architecture"], "emoji": "🎬", "ru": {"title": "Семантическое выравнивание через наблюдаемые эффекты действий для универсального переноса управления в видео-моделях", "desc": "В работе предложен метод для обучения структурированного пространства скрытых действ
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#agents", "#dataset", "#cv", "#benchmark", "#multimodal"], "emoji": "🎨", "ru": {"title": "Профессиональное редактирование изображений через иерархического агента с памятью и послойной декомпозицией", "desc": "Agent Banana представляет иерархическую агентную архитектуру для редактиро
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "🎨", "ru": {"title": "Авторегрессивная диффузия с оптимальным транспортом превосходит классические модели", "desc": "В статье предложен подход, объединяющий авторегрессивные модели с функцией потерь диффузии для генерации изображений. Авторы д
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#inference", "#robotics", "#benchmark", "#multimodal"], "emoji": "🤖", "ru": {"title": "Адаптивное управление роботом через совместную модуляцию восприятия и действия", "desc": "В работе предложен метод SCALE для улучшения инференса Vision-Language-Action моделей, который совместно м
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#multimodal", "#training", "#reasoning"], "emoji": "🤖", "ru": {"title": "Объединение языка, зрения и действия для умной манипуляции робота", "desc": "BagelVLA — это единая модель Vision-Language-Action, которая объединяет языковое планирование, визуальн
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#training", "#robotics", "#architecture", "#cv"], "emoji": "🤖", "ru": {"title": "Предсказание без утечек: как научить робота видеть суть действий", "desc": "VLA-JEPA представляет собой фреймворк предобучения, основанный на архитектуре JEPA, который улучшает обучение политик зрительн
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#training", "#rl", "#agents", "#math", "#optimization"], "emoji": "🤖", "ru": {"title": "Стабильное обучение многоагентных LLM через агент-специфичную нормализацию", "desc": "Статья посвящена решению проблемы нестабильности обучения многоагентных систем на
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#open_source", "#agents", "#reasoning", "#video", "#robotics", "#training", "#transfer_learning", "#diffusion"], "emoji": "🎬", "ru": {"title": "От видео к знаниям: разделение динамики и внешнего вида в латентном пространстве", "desc": "VideoWorld 2 — это модель для обучения передава
[11.02.2026 14:14] Using data from previous issue: {"categories": [], "emoji": "🏗️", "ru": {"title": "Масштабирование разнообразия окружений для обобщения агентов", "desc": "ScaleEnv — это фреймворк для автоматического создания интерактивных окружений, которые помогают обучать универсальные агенты с хорошей обобщающей способностью. Фреймворк гаранти
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#inference", "#rl", "#reasoning", "#optimization", "#rag"], "emoji": "🧠", "ru": {"title": "Когнитивная память вместо сырых токенов: сжатие информации для эффективной обработки длинных контекстов", "desc": "В статье предлагается когнитивно-вдохновленный
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#multimodal", "#training"], "emoji": "🎛️", "ru": {"title": "Переосмысление модуляции: от внимания к управлению качеством генерации", "desc": "Исследование показывает, что модуляция на основе текстового embedding в диффузионных трансформерах может быть 
[11.02.2026 14:14] Querying the API.
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.  					AI-generated summary 				 Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.
[11.02.2026 14:14] Response: ```json
{
  "desc": "TerminalTraj — это масштабируемый конвейер для создания высококачественных траекторий взаимодействия агента с терминалом, необходимых для обучения agentic моделей. Система решает две ключевые проблемы: обеспечение исполняемости каждого экземпляра задачи в изолированной Docker среде и верифицируемость результатов через валидационный код. На основе отфильтрованных репозиториев авторы сгенерировали 50,733 проверенные траектории агента на 32K Docker образах и обучили языковые модели на этих данных. Модели, обученные на TerminalTraj, продемонстрировали значительные улучшения производительности на эталонах, достигая прироста до 20% на TerminalBench~1.0.",
  "emoji": "🐳",
  "title": "От репозитория к обученному агенту: конвейер генерации верифицируемых терминальных траекторий"
}
```
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.  					AI-generated summary 				 Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj."

[11.02.2026 14:14] Response: ```python
["DATASET", "DATA", "AGENTS", "PLP", "TRAINING"]
```

**Justification:**

- **DATASET**: The paper introduces TerminalTraj, a new dataset of 50,733 verified terminal trajectories across eight domains with 32K Docker images.
- **DATA**: The paper focuses on data curation, filtering, generation, and synthesis methodologies for creating high-quality terminal trajectories.
- **AGENTS**: The paper explicitly addresses training "agentic models" for terminal-based tasks and synthesizing "agent trajectories."
- **PLP**: The paper involves programming language processing, as it deals with code execution, Docker environments, and terminal-based programming tasks (using Qwen2.5-Coder backbone).
- **TRAINING**: The paper discusses training models on the curated data and improving model performance through training on this dataset.
[11.02.2026 14:14] Error. Failed to parse JSON from LLM. ["DATASET", "DATA", "AGENTS", "PLP", "TRAINING"]


**Justification:**

- **DATASET**: The paper introduces TerminalTraj, a new dataset of 50,733 verified terminal trajectories across eight domains with 32K Docker images.
- **DATA**: The paper focuses on data curation, filtering, generation, and synthesis methodologies for creating high-quality terminal trajectories.
- **AGENTS**: The paper explicitly addresses training "agentic models" for terminal-based tasks and synthesizing "agent trajectories."
- **PLP**: The paper involves programming language processing, as it deals with code execution, Docker environments, and terminal-based programming tasks (using Qwen2.5-Coder backbone).
- **TRAINING**: The paper discusses training models on the curated data and improving model performance through training on this dataset.
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.  					AI-generated summary 				 Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj."

[11.02.2026 14:14] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```

**Justification:**

1. **SYNTHETIC**: The paper explicitly discusses synthesizing agent trajectories and generating task instances artificially. The pipeline generates 50,733 verified terminal trajectories synthetically, which is a core contribution for creating artificial training data.

2. **OPEN_SOURCE**: The paper states "All code and data are available at https://github.com/Wusiwei0410/TerminalTraj," indicating the authors are releasing their models, datasets, and code to the public.
[11.02.2026 14:14] Error. Failed to parse JSON from LLM. ["SYNTHETIC", "OPEN_SOURCE"]


**Justification:**

1. **SYNTHETIC**: The paper explicitly discusses synthesizing agent trajectories and generating task instances artificially. The pipeline generates 50,733 verified terminal trajectories synthetically, which is a core contribution for creating artificial training data.

2. **OPEN_SOURCE**: The paper states "All code and data are available at https://github.com/Wusiwei0410/TerminalTraj," indicating the authors are releasing their models, datasets, and code to the public.
[11.02.2026 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces TerminalTraj, a scalable pipeline designed to create high-quality terminal trajectories for training agentic models. It addresses the challenges of executability and verifiability by filtering repositories to build Docker environments, generating task instances aligned with Docker, and synthesizing executable agent trajectories. The pipeline successfully curates 32,000 Docker images and produces over 50,000 verified terminal trajectories across eight different domains. Models trained using this data show significant performance improvements, achieving up to 20% gains on benchmark tasks, demonstrating the effectiveness of the TerminalTraj approach.","title":"Streamlining Agentic Model Training with TerminalTraj"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces TerminalTraj, a scalable pipeline designed to create high-quality terminal trajectories for training agentic models. It addresses the challenges of executability and verifiability by filtering repositories to build Docker environments, generating task instances aligned with Docker, and synthesizing executable agent trajectories. The pipeline successfully curates 32,000 Docker images and produces over 50,000 verified terminal trajectories across eight different domains. Models trained using this data show significant performance improvements, achieving up to 20% gains on benchmark tasks, demonstrating the effectiveness of the TerminalTraj approach.', title='Streamlining Agentic Model Training with TerminalTraj'))
[11.02.2026 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为TerminalTraj的可扩展管道，旨在解决为训练智能模型创建高质量终端轨迹的挑战。该管道通过过滤高质量的代码库、生成与Docker对齐的任务实例，并在多个领域合成可执行的智能体轨迹来实现。TerminalTraj能够构建Docker化的执行环境，并生成经过验证的终端轨迹，最终生成了32,000个Docker镜像和50,733个验证过的终端轨迹。使用这些数据训练的模型在TerminalBench上表现出显著的性能提升，证明了该方法的有效性。","title":"高效生成终端轨迹的可扩展管道"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为TerminalTraj的可扩展管道，旨在解决为训练智能模型创建高质量终端轨迹的挑战。该管道通过过滤高质量的代码库、生成与Docker对齐的任务实例，并在多个领域合成可执行的智能体轨迹来实现。TerminalTraj能够构建Docker化的执行环境，并生成经过验证的终端轨迹，最终生成了32,000个Docker镜像和50,733个验证过的终端轨迹。使用这些数据训练的模型在TerminalBench上表现出显著的性能提升，证明了该方法的有效性。', title='高效生成终端轨迹的可扩展管道'))
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#video", "#inference"], "emoji": "🎬", "ru": {"title": "Стабилизация авторегрессивной генерации видео через удаление нестабильных токенов", "desc": "В статье рассматривается проблема временного смещения в авторегрессивной генерации видео, которая возникает из-за накопления ошибок в т
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#open_source", "#audio", "#benchmark", "#multimodal", "#training", "#reasoning"], "emoji": "🎙️", "ru": {"title": "Конец речевому конвейеру: полнодуплексная модель для естественного диалога", "desc": "Представлен Covo-Audio — большая языковая модель для аудио с 7 миллиардами параметр
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#synthetic"], "emoji": "🎨", "ru": {"title": "Высокачественный открытый датасет для файн-тюнинга моделей текст-в-изображение", "desc": "Представлен открытый датасет Fine-T2I с более чем 6 миллионами пар текст-изображение высокого качества для файн-тюнинг
[11.02.2026 14:14] Using data from previous issue: {"categories": [], "emoji": "🌳", "ru": {"title": "Масштабируемое синтетическое расширение данных для агентов десктопного взаимодействия", "desc": "Статья представляет фреймворк Anchor для автоматического расширения траекторий взаимодействия с графическим интерфейсом на основе небольшого набора этало
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#open_source", "#agents", "#dataset", "#synthetic", "#robotics", "#3d", "#reasoning"], "emoji": "🏠", "ru": {"title": "Агентное создание реалистичных 3D сцен для обучения воплощённых агентов", "desc": "SAGE — это агентный фреймворк для автоматического создания трёхмерных сцен, готовы
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#dataset", "#rlhf", "#agents"], "emoji": "🌳", "ru": {"title": "Древовидная организация траекторий для эффективного масштабирования автоматизации интерфейсов", "desc": "TreeCUA — это система для масштабирования автоматизации графических интерфейсов, которая организует траектории взаи
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv"], "emoji": "🎨", "ru": {"title": "Дискретные токенизаторы могут превзойти непрерывные методы благодаря правильному масштабированию", "desc": "В работе предлагается новый подход к генерации изображений, основанный на дискретных токенизаторах, которые
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#reasoning", "#math"], "emoji": "🧭", "ru": {"title": "Структурированное исследование пространства решений для параллельного рассуждения", "desc": "Статья исследует применение обучения с подкреплением и проверяемыми вознаграждениями для улучшения 
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#benchmark", "#training", "#agents", "#architecture", "#rl"], "emoji": "🏗️", "ru": {"title": "Эволюция архитектур планирования через автоматический синтез и многоцелевую оптимизацию", "desc": "TodoEvolve представляет собой мета-парадигму для автоматического синтеза и динамической ад
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "⚡", "ru": {"title": "Эффективное параллельное декодирование через умную верификацию кэша", "desc": "В работе предложен метод COVER для ускорения параллельного декодирования диффузионных языковых моделей. Основная проблема заключается в том, ч
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#training", "#multimodal", "#diffusion", "#open_source", "#video", "#optimization"], "emoji": "⚡", "ru": {"title": "Стабилизация обучения flow matching через контроль дисперсии условных скоростей", "desc": "Статья представляет фреймворк Stable Velocity для решения проблемы высокой д
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#training", "#architecture"], "emoji": "🧲", "ru": {"title": "Локальная геометрия активаций: от глобальных направлений к многомерным подпространствам", "desc": "Работа предлагает Mixture of Factor Analyzers (MFA) как неконтролируемый метод для обнар
[11.02.2026 14:14] Querying the API.
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.  					AI-generated summary 				 Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.
[11.02.2026 14:14] Response: ```json
{
  "desc": "В этой работе исследуется влияние методов контроля длины выходных последовательностей на производительность моделей, обученных с помощью reinforcement learning. Авторы сравнивают различные подходы к ограничению длины на двух моделях и обнаруживают, что штрафы за длину могут препятствовать развитию способности к рассуждению. Основное открытие заключается в том, что существует оптимальная длина вывода, балансирующая между качеством рассуждений и вычислительной эффективностью. Исследование выявляет две критические проблемы: слишком длинные выходы увеличивают вариативность результатов, а чрезмерно короткие приводят к недостаточной глубине анализа.",
  "emoji": "⚖️",
  "title": "Оптимальный баланс: как контролировать длину рассуждений в языковых моделях"
}
```
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.  					AI-generated summary 				 Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking."

[11.02.2026 14:14] Response: ```python
["RL", "TRAINING", "SMALL_MODELS"]
```
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.  					AI-generated summary 				 Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking."

[11.02.2026 14:14] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[11.02.2026 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates how controlling the length of outputs in reinforcement learning (RL) trained language models impacts their reasoning abilities and computational efficiency. It highlights that while RL enhances reasoning, it often results in longer outputs that can increase computational costs. The authors evaluate various length control methods on two specific models to find an optimal balance between output length, reasoning performance, and efficiency. Their findings suggest that while length penalties can negatively affect reasoning, well-tuned length control can enhance efficiency without sacrificing reasoning quality.","title":"Balancing Output Length for Efficient Reasoning in RL Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates how controlling the length of outputs in reinforcement learning (RL) trained language models impacts their reasoning abilities and computational efficiency. It highlights that while RL enhances reasoning, it often results in longer outputs that can increase computational costs. The authors evaluate various length control methods on two specific models to find an optimal balance between output length, reasoning performance, and efficiency. Their findings suggest that while length penalties can negatively affect reasoning, well-tuned length control can enhance efficiency without sacrificing reasoning quality.', title='Balancing Output Length for Efficient Reasoning in RL Models'))
[11.02.2026 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"在强化学习训练的语言模型中，长度控制方法对推理性能和计算效率有重要影响。研究表明，最佳的输出长度可以在这两者之间取得平衡。我们比较了几种长度控制方法在不同模型上的效果，发现长度惩罚可能会妨碍推理能力的获取。适当调整的长度控制可以提高具有强推理能力模型的效率，同时也识别了两种失败模式：过长的输出会增加分散性，过短的输出则会导致思考不足。","title":"优化输出长度，提升推理与效率的平衡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='在强化学习训练的语言模型中，长度控制方法对推理性能和计算效率有重要影响。研究表明，最佳的输出长度可以在这两者之间取得平衡。我们比较了几种长度控制方法在不同模型上的效果，发现长度惩罚可能会妨碍推理能力的获取。适当调整的长度控制可以提高具有强推理能力模型的效率，同时也识别了两种失败模式：过长的输出会增加分散性，过短的输出则会导致思考不足。', title='优化输出长度，提升推理与效率的平衡'))
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#multimodal", "#open_source", "#cv", "#synthetic", "#optimization"], "emoji": "🐙", "ru": {"title": "Эффективная самокоррекция в vision-language моделях через аугментацию траекторий", "desc": "В статье предложена фреймворк Octopus для обучения с подк
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "🧭", "ru": {"title": "Гибкая адаптация LLM через композицию управляющих векторов", "desc": "STEER2ADAPT — это лёгкий фреймворк для адаптации больших языковых моделей путём комбинирования векторов активационного управления из переиспользуемых по
[11.02.2026 14:14] Querying the API.
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.  					AI-generated summary 				 Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.
[11.02.2026 14:14] Response: ```json
{
  "desc": "Статья представляет LatentLens — новый метод интерпретации визуальных токенов в мультимодальных моделях зрения и языка путём их сравнения с контекстуализированными текстовыми представлениями. Метод работает через кодирование больших текстовых корпусов и поиск k ближайших соседей для описания каждого визуального токена на естественном языке. Исследование на 10 различных VLM показывает, что предыдущие методы, такие как LogitLens, существенно недооценивали интерпретируемость визуальных токенов. Результаты демонстрируют, что большинство визуальных токенов хорошо интерпретируются на всех слоях моделей, что подтверждает глубокое выравнивание между визуальными и языковыми представлениями.",
  "emoji": "🔍",
  "title": "Раскрывая смысл визуальных токенов через текстовые представления"
}
```
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.  					AI-generated summary 				 Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations."

[11.02.2026 14:14] Response: ```python
["MULTIMODAL", "ARCHITECTURE"]
```
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.  					AI-generated summary 				 Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations."

[11.02.2026 14:14] Response: ```python
["INTERPRETABILITY"]
```
[11.02.2026 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LatentLens is a new method that helps us understand visual token representations in vision-language models by comparing them to textual representations. It shows that visual tokens are easier to interpret than we previously thought. By using a simple mapping technique, LatentLens allows us to find meaningful descriptions for visual tokens based on their closest textual counterparts. This research reveals that many visual tokens are interpretable across various models and layers, enhancing our understanding of how vision and language are connected.","title":"Unlocking the Interpretability of Visual Tokens with LatentLens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LatentLens is a new method that helps us understand visual token representations in vision-language models by comparing them to textual representations. It shows that visual tokens are easier to interpret than we previously thought. By using a simple mapping technique, LatentLens allows us to find meaningful descriptions for visual tokens based on their closest textual counterparts. This research reveals that many visual tokens are interpretable across various models and layers, enhancing our understanding of how vision and language are connected.', title='Unlocking the Interpretability of Visual Tokens with LatentLens'))
[11.02.2026 14:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LatentLens 是一种新方法，用于解释视觉-语言模型中的视觉标记表示。它通过将视觉标记与上下文化的文本表示进行比较，揭示了视觉标记的可解释性比之前认为的更高。该方法通过编码大量文本语料库，存储每个标记的上下文化表示，并利用最近邻方法为视觉标记提供描述。我们的研究表明，LatentLens 能够在多个视觉-语言模型中有效地提高视觉标记的可解释性，推动了视觉与语言表示之间的对齐研究。","title":"提升视觉标记可解释性的LatentLens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LatentLens 是一种新方法，用于解释视觉-语言模型中的视觉标记表示。它通过将视觉标记与上下文化的文本表示进行比较，揭示了视觉标记的可解释性比之前认为的更高。该方法通过编码大量文本语料库，存储每个标记的上下文化表示，并利用最近邻方法为视觉标记提供描述。我们的研究表明，LatentLens 能够在多个视觉-语言模型中有效地提高视觉标记的可解释性，推动了视觉与语言表示之间的对齐研究。', title='提升视觉标记可解释性的LatentLens'))
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#training", "#math", "#optimization", "#open_source", "#diffusion", "#architecture"], "emoji": "🌐", "ru": {"title": "Геометрическое решение: поток на многообразии вместо евклидова пространства", "desc": "Статья решает проблему обучения диффузионных трансформеров на представлениях, з
[11.02.2026 14:14] Using data from previous issue: {"categories": ["#benchmark", "#math", "#inference"], "emoji": "🧠", "ru": {"title": "Модели сами знают, насколько задача сложна для них", "desc": "Исследователи показали, что LLM кодируют информацию о сложности задачи в своих внутренних представлениях до того, как начнут генерировать ответ. Они обуч
[11.02.2026 14:14] Querying the API.
[11.02.2026 14:14] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.  					AI-generated summary 				 Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.
[11.02.2026 14:15] Response: ```json
{
  "desc": "В статье исследуется адаптация языковых моделей во время инференса для задач с верифицируемым выполнением (VEG), где детерминированный оценивающий модуль предоставляет плотные сигналы обратной связи. Авторы обнаружили, что градиентная адаптация (test-time training) приводит к избыточной концентрации выборок и неудачам, в то время как стратегия поиска с помощью Best-of-N семплирования значительно превосходит адаптацию. Главный вклад работы — метод surprisal-guided selection, который выбирает образцы с наименьшей уверенностью модели среди корректных решений, что улучшает производительность на 30 процентов. Результаты показывают, что для задач с плотной функцией вознаграждения вычислительные ресурсы следует распределять на увеличение разнообразия выборок и умный отбор, а не на градиентную адаптацию.",
  "emoji": "🎯",
  "title": "От адаптации к разнообразию: surprisal-guided отбор вместо gradient-based обучения"
}
```
[11.02.2026 14:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.  					AI-generated summary 				 Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail."

[11.02.2026 14:15] Response: ```python
["INFERENCE", "TRAINING"]
```
[11.02.2026 14:15] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.  					AI-generated summary 				 Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail."

[11.02.2026 14:15] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}, 'request_id': 'req_011CY2RT4L2faad1eAeY7X5a'}
[11.02.2026 14:15] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#dataset"], "emoji": "📖", "ru": {"title": "Модальный разрыв: как видео-языковые модели теряют способность понимать визуализированный текст", "desc": "В статье представлен VISTA-Bench — новый бенчмарк для оценки способности видео-языковых моделей п
[11.02.2026 14:15] Using data from previous issue: {"categories": ["#training", "#inference"], "emoji": "🔧", "ru": {"title": "Безопасность без цены: офлайн-редактирование моделей через локализацию нейронных цепей", "desc": "В статье предлагается метод offline-модификации больших языковых моделей для безопасного отказа в выполнении вредоносных запрос
[11.02.2026 14:15] Using data from previous issue: {"categories": ["#diffusion", "#training", "#optimization", "#architecture"], "emoji": "⚡", "ru": {"title": "Парная согласованность временных шагов для эффективного синтеза", "desc": "В работе предлагается метод Temporal Pair Consistency для снижения дисперсии в непрерывных генеративных моделях, так
[11.02.2026 14:15] Using data from previous issue: {"categories": ["#security", "#alignment"], "emoji": "🛡️", "ru": {"title": "Предсказание рисков для безопасности интеллектуальных агентов", "desc": "SafePred — это фреймворк предсказательных ограничений безопасности для компьютерных агентов, который использует предиктивную модель рисков и оптимизаци
[11.02.2026 14:15] Using data from previous issue: {"categories": ["#interpretability", "#ethics", "#benchmark"], "emoji": "⚠️", "ru": {"title": "От среднего риска к анализу катастрофических сценариев: многомерная оценка вреда больших языковых моделей", "desc": "В статье представлена новая методология SHARP для оценки социального вреда от больших яз
[11.02.2026 14:15] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#open_source", "#graphs"], "emoji": "🔗", "ru": {"title": "От исследования к практике: масштабируемая кластеризация графов для индустриальных приложений", "desc": "PyAGC представляет комплексный фреймворк для кластеризации атрибутированных г
[11.02.2026 14:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agents", "#optimization"], "emoji": "🧠", "ru": {"title": "Самообучающаяся память: от инженерии человека к автоматической оптимизации", "desc": "ALMA — это фреймворк, использующий метаобучение для автоматического открытия оптимальных архитектур памяти в аг
[11.02.2026 14:15] Renaming data file.
[11.02.2026 14:15] Renaming previous data. hf_papers.json to ./d/2026-02-11.json
[11.02.2026 14:15] Saving new data file.
[11.02.2026 14:15] Generating page.
[11.02.2026 14:15] Renaming previous page.
[11.02.2026 14:15] Renaming previous data. index.html to ./d/2026-02-11.html
[11.02.2026 14:15] Writing result.
[11.02.2026 14:15] Renaming log file.
[11.02.2026 14:15] Renaming previous data. log.txt to ./logs/2026-02-11_last_log.txt
