[11.02.2026 08:00] Read previous papers.
[11.02.2026 08:00] Generating top page (month).
[11.02.2026 08:00] Writing top page (month).
[11.02.2026 08:40] Read previous papers.
[11.02.2026 08:40] Get feed.
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05400
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09856
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09082
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08234
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09443
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10063
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10090
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08426
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07035
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10104
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09084
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04208
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07022
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08847
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06820
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10102
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09849
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09823
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07153
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10116
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10098
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08382
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08344
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09439
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09024
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02464
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05435
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08503
[11.02.2026 08:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.07839
[11.02.2026 08:40] Extract page data from URL. URL: https://huggingface.co/papers/2602.07276
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01725
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21235
[11.02.2026 08:40] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04908
[11.02.2026 08:40] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.02.2026 08:40] No deleted papers detected.
[11.02.2026 08:40] Downloading and parsing papers (pdf, html). Total: 33.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.05400.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.05400.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.05400.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09856.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09856.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09856.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09082.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09082.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09082.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.08234.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.08234.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.08234.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09443.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09443.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09443.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.10063.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.10063.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.10063.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.10090.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.10090.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.10090.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.08426.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.08426.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.08426.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.07035.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.07035.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.07035.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.10104.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.10104.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.10104.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09084.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09084.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09084.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.04208.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.04208.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.04208.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.07022.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.07022.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.07022.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.08847.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.08847.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.08847.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.06820.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.06820.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.06820.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.10102.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.10102.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.10102.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09849.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09849.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09849.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09823.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09823.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09823.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.07153.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.07153.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.07153.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.10116.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.10116.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.10116.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.10098.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.10098.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.10098.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.08382.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.08382.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.08382.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.08344.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.08344.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.08344.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09439.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09439.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09439.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.09024.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.09024.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.09024.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.02464.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.02464.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.02464.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.05435.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.05435.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.05435.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.08503.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.08503.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.08503.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.07839.
[11.02.2026 08:40] Downloading paper 2602.07839 from https://arxiv.org/pdf/2602.07839v1...
[11.02.2026 08:40] Extracting affiliations from text.
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 ] . [ 1 9 3 8 7 0 . 2 0 6 2 : r TodoEvolve: Learning to Architect Agent Planning Systems "
[11.02.2026 08:40] Response: ```python
[]
```
[11.02.2026 08:40] Extracting affiliations from text.
[11.02.2026 08:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 ] . [ 1 9 3 8 7 0 . 2 0 6 2 : r TodoEvolve: Learning to Architect Agent Planning SystemsPlanning has become central capability for contemporary agent systems in navigating complex, longhorizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, modular design space that standardizes diverse planning paradigms within unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead. Date: February 10, 2026 Code: https://github.com/EcthelionLiu/TodoEvolveWith the rapid advancement of foundation models (Team et al., 2025b,a,c), large language model (LLM)-powered agents have begun to demonstrate strong capabilities across domains such as deep research (Hu et al., 2025a; Shi et al., 2025b), complex software engineering (iQuest, 2025; Yang et al., 2024), and real-world transactions Andon (2025); Backlund and Petersson (2025). Beyond improvements in base model capacity, increasingly sophisticated agent scaffolds are equally critical (Wang et al., 2025a), equipping LLMs with essential agentic support including planning (Parmar et al., 2025; Wu et al., 2025b; Erdogan et al., 2025a), memory (Hu et al., 2026a), reflection, etc. Among these, planning stands out as central capability, enabling agents to navigate complex environments by maintaining coherent global state, preserving behavioral consistency, and coordinating actions across tasks (Cao et al., 2025). Existing planning systems developed for LLM-based agents exhibit substantial diversity. From the perspective of planning target, some are designed to support single agent, primarily addressing long-horizon execution and mitigating the risk of lost in the middle (Erdogan et al., 2025b), while others are tailored for multi-agent systems, focusing on subtask allocation and contextual coordination across agents with distinct roles (Parmar et al., 2025; Hu et al., 2025b). In terms of representational form, plans have been instantiated using wide range of structures, including linear to-do lists (LangChain, 2025), directed acyclic graphs (DAG) (Qin et al., 2025), tree-structured plans (Hu et al., 2026b), and hierarchical notes. Moreover, planning systems differ markedly across task domains, with domain-specific designs emerging for embodied action (Wang et al., 2024b), web search (Kim et al., 2024), and programming. Faced with this diversity, practitioners may naturally ask: is there single planning structure that can serve as one-size-fits-all solution that generalizes well across settings? 1 We posit that such an oracle planning system does not exist. Beyond distinct task domains require different planning priors (for instance, MCTS-based planning may be effective for mathematical reasoning yet is rarely adopted for autonomous driving agents due to the vastness of its action space (Wang et al., 2024a)), even within single task class, alternative planning priors exhibit performance disparities. For example, in web search, AOP (Li et al., 2025a) employs simple linear to-do list coupled with reward model to solve document QA in token-efficient manner, but it is substantially outperformed in more complex multimodal settings by DAG-based planning structures (Qin et al., 2025). Similarly, while linear tasks require minimal revision (Hu et al., 2025b), high-conflict environments demand continuous topological restructuring (Zhang et al., 2025), rendering single, universal planning system unrealistic. Accordingly, we contend that the central challenge is not to design one-size-fits-all planner, but to customize planning systems to the structural characteristics of each task. To this end, we propose TodoEvolve, meta-planning paradigm that synthesizes task-adaptive agentic planners and dynamically updates their planning states as execution unfolds. Concretely, we train Todo-14B using Impedance-Guided Preference Optimization (IGPO), multi-objective preference learning objective that jointly promotes high performance, stability, and token efficiency in the generated planning systems. The resulting meta-planner Todo-14B takes task instance as input and instantiates tailored planning topology, revision cadence, and navigation strategy, operationalized as task-specific to-do structure. Todo-14B integrates seamlessly with single/multi-agent execution frameworks, remains compatible with diverse LLM backbones, and generalizes across heterogeneous task domains. To ground TodoEvolve within the diverse landscape of existing planning systems, we introduce modular planning design space comprising four dimensions: Topology (the structural organization of task decomposition), Initialization (how the task topology is instantiated), Adaptation (when and how the topology is revised), and Navigation (the mechanism that issues executable directives to the acting agent). This design space provides unified abstraction capable of accommodating and localizing wide spectrum of existing planning paradigms. Building on this formulation, we decompose and re-implement ten representative planning architectures, including Plan-and-Act (Erdogan et al., 2025b), linear planning (Hu et al., 2025b), DAG-based planning (Qin et al., 2025), and parallel and dynamic planning (Zhu et al., 2025). The resulting framework, denoted as PlanFactory, serves both as (i) data synthesis engine for generating high-quality planning trajectories to train TodoEvolve and (ii) standardized codebase to facilitate future research on agentic planning capabilities. Our contributions are as follows: ‚ù∂ Unified Codebase: We introduce PlanFactory, modular "
[11.02.2026 08:40] Mistral response. {"id": "d8110c8d0e6143c791202c102dea0f30", "created": 1770799220, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1475, "total_tokens": 1481, "completion_tokens": 6, "num_cached_tokens": 1474}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[11.02.2026 08:40] Response: ```python
[]
```
[11.02.2026 08:40] Deleting PDF ./assets/pdf/2602.07839.pdf.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.07276.
[11.02.2026 08:40] Downloading paper 2602.07276 from https://arxiv.org/pdf/2602.07276v1...
[11.02.2026 08:40] Extracting affiliations from text.
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs Pengrui Han * 1 Xueqiang Xu * 1 Keyang Xuan * 1 Peiyang Song 2 Siru Ouyang 1 Runchu Tian 1 Yuqing Jiang 1 Cheng Qian 1 Pengcheng Jiang 1 Jiashuo Sun 1 Junxia Cui 1 Ming Zhong 1 Ge Liu 1 Jiawei Han 1 Jiaxuan You 1 Code: https://github.com/ulab-uiuc/Steer2Adapt 6 2 0 2 7 ] A . [ 1 6 7 2 7 0 . 2 0 6 2 : r a "
[11.02.2026 08:40] Response: ```python
[
    "University of Illinois Urbana-Champaign"
]
```
[11.02.2026 08:40] Deleting PDF ./assets/pdf/2602.07276.pdf.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.01725.
[11.02.2026 08:40] Downloading paper 2602.01725 from https://arxiv.org/pdf/2602.01725v1...
[11.02.2026 08:40] Extracting affiliations from text.
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SAFEPRED: Predictive Guardrail for Computer-Using Agents via World Models Yurun Chen 1 Zeyi Liao 2 Ping Yin 3 Taotao Xie 3 Keting Yin 1 Shengyu Zhang "
[11.02.2026 08:40] Response: ```python
[]
```
[11.02.2026 08:40] Extracting affiliations from text.
[11.02.2026 08:40] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SAFEPRED: Predictive Guardrail for Computer-Using Agents via World Models Yurun Chen 1 Zeyi Liao 2 Ping Yin 3 Taotao Xie 3 Keting Yin 1 Shengyu Zhang1. Introduction 6 2 0 2 2 ] . [ 1 5 2 7 1 0 . 2 0 6 2 : r With the widespread deployment of Computerusing Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SAFEPRED, predictive guardrail framework for CUAs that establishes risk-to-decision loop to ensure safe agent behavior. SAFEPRED supports two key abilities: (1) Shortand long-term risk prediction: by using safety policies as the basis for risk prediction, SAFEPRED leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SAFEPRED significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines. 1Zhejiang University, China 2The Ohio State University, USA 3Inspur Cloud, China. Correspondence to: Yurun Chen <yurunchen.research@gmail.com>. Code: https://github.com/YurunChen/SafePred 1 As CUAs become increasingly deployed in real-world applications, ensuring their safety has emerged as critical concern. In response, recent research (Xiang et al., 2024; Chen et al., 2025c;a) has introduced guardrails for CUAs that review inputs and constrain harmful actions prior to execution. In practice, these guardrails have demonstrated effectiveness in mitigating immediate threats, such as adversarial prompts (Liu et al., 2023; Wei et al., 2023) and environment injections (Wu et al., 2024; Liao et al., 2024; Chen et al., 2025b), thereby improving CUAs ability to handle short-term risks. We categorize these approaches, which assess safety based on the current (state, action) pair, as reactive guardrails. Despite their effectiveness, reactive guardrails are inherently limited to the pre-execution time window. At their core, they evaluate whether an action appears safe at the moment it is proposed, lacking the ability to predict how it may lead to risk consequences in the future. real scenario we observed in Figure 1 indicates the limitations of reactive guardrails. In this case, CUA is used to set up Python environment for project. When project requirements do not explicitly call for creating virtual environment, CUA with limited reasoning ability might try to modify the Python version in the base environment to get the code running. In the short term, this seems like harmless upgrade, and reactive guardrail would likely classify it as safe because it fixes an immediate problem. However, from long-term perspective, this action irreversibly breaks the dependencies of package managers and critical services on the system Python version, with recovery requiring costly rollback measures such as system reinstallation. This case reflects class of longterm risks that reactive guardrails cannot address: actions that appear reasonable initially but may lead to high-risk consequences in the future. These observations reveal that reactive guardrails lack the risk prediction capabilities that humans naturally possess. Through risk prediction, humans actively consider long-term risks, and incorporate this foresight into current decisions before taking action. Essentially, risk prediction moves safety decisions from evaluating the risk of individual actions to exploring potential future risk states. Motivated by this inSAFEPRED: Predictive Guardrail for Computer-Using Agents via World Models sue is further amplified in safety scenarios: even when the world model has predicted risks, the agent may still repeat high-risk actions due to the absence of decision guidance. To address these challenges, we propose SAFEPRED, world-model-based predictive guardrail for CUAs designed to translate predicted future risks into actionable safety decisions. SAFEPRED constructs unified policy representation to distinguish between benign and risky outcomes. It then leverages the world model to generate shortand long-term risk predictions from single-step states, using semantic state descriptions to avoid the state drift inherent in multi-step prediction. Furthermore, SAFEPRED converts these predictions into hierarchical guidance at both the step and task levels to inform safe decision-making, thereby establishing riskto-decision loop. We implemented prototype of the SAFEPRED and conducted experiments on multiple benchmarks (OSHarm (Kuntz et al., 2025) and WASP (Evtimov et al., 2025)). Extensive experiments demonstrate that SAFEPRED significantly improves agent safety, achieving over 97.6% safety performance on all benchmarks. Moreover, by decision optimization, SAFEPRED improves task performance by 21.4% compared to the reactive baselines on WASP. Furthermore, we collect 1.5K samples from the trajectories and train lightweight predictive guardrail model SafePred-8B, which achieves safety performance comparable to Deepseek-V3.2. Our contributions are summarized as follows: We introduce predictive guardrail approach, which explicitly aligns predicted future risks with current decisions, addressing the inherent limitations of existing reactive guardrails. We propose SAFEPRED, prediction guardrail for CUAs that transforms the predictive capabilities of LLM-based world models into actionable safety signals. By integrating risk prediction with decision optimization, it provides effective safety constraints and guidance for decision making. We implement prototype of S"
[11.02.2026 08:40] Mistral response. {"id": "a21090dcff3742caa13b30e1607c569a", "created": 1770799238, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1394, "total_tokens": 1427, "completion_tokens": 33, "num_cached_tokens": 1393}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Zhejiang University, China\",\n    \"The Ohio State University, USA\",\n    \"Inspur Cloud, China\"\n]\n```"}}]}
[11.02.2026 08:40] Response: ```python
[
    "Zhejiang University, China",
    "The Ohio State University, USA",
    "Inspur Cloud, China"
]
```
[11.02.2026 08:40] Deleting PDF ./assets/pdf/2602.01725.pdf.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2601.21235.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2601.21235.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2601.21235.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Downloading and parsing paper https://huggingface.co/papers/2602.04908.
[11.02.2026 08:40] Extra JSON file exists (./assets/json/2602.04908.json), skip PDF parsing.
[11.02.2026 08:40] Paper image links file exists (./assets/img_data/2602.04908.json), skip HTML parsing.
[11.02.2026 08:40] Success.
[11.02.2026 08:40] Enriching papers with extra data.
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 0. OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.  					AI-generated summary 				 As hig...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 1. Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.  					AI-generated summary 				 Autonomous GUI agents interact with environments by perceiv...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 2. UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.  					AI-generated summary 				 GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving bo...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 3. SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.  					AI-generated summary 				 Large Language Model (LLM) agents have shown stunning results in complex t...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 4. Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.  					AI-generated summary 				 The transition from symbolic ...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 5. A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.  					AI-generated summary 				 Human problem-solving is never the repetition of a ...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 6. Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.  					AI-generated summary 				 Recent advances in large lan...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 7. Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.  					AI-generated summary 				 Block-sparse attention is promising for accelerating long-...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 8. Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.  					AI-generated summary 				 Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by ...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 9. Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.  					AI-generated summary 				 Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to ...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 10. Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.  					AI-generated summary 				 We study instruction-based image editing under prof...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 11. SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.  					AI-generated summary 				 Vision-Language-Action (VLA) models have e...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 12. Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.  					AI-generated summary 				 Recent studies have explored autoregressive model...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 13. Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.  					AI-generated summary 				 Multi-agent LLM systems e...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 14. ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.  					AI-generated summary 				 Training generalist agents capable of adapting to diverse scenarios requires interactive environments for se...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 15. VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.  					AI-generated summary 				 Lear...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 16. BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.  					AI-generated summary 				 Equipping embodied agents with the ability to reason about tasks, foresee ...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 17. Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 18. A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.  					AI-generated summary 				 End-to-end GUI agents for real desktop environments requi...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 19. SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.  					AI-generated summary 				 Real-world data collection for em...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 20. VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.  					AI-generated summary 				 Pretraining Vision-Language-Action (VLA) policies...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 21. A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.  					AI-generated summary 				 Large Language Models (LLMs) face significant challenges in long-context proces...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 22. Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.  					AI-generated summary 				 Parallel thinking has emerged as a new paradigm f...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 23. A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.  					AI-generated summary 				 High-quality and open datas...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 24. Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.  					AI-generated summary 				 This paper challenges the dominance of continuous pipelines in vi...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 25. Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.  					AI-generated summary 				 Activation d...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 26. Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.  					AI-generated summary 				 While flow matching is elegant, its reliance on single-samp...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 27. Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.  					AI-generated summary 				 Self-correction is essential for solving complex reasoning problems in vision-lang...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 28. TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  					AI-generated summary 				 Planning has become a central capability for contemporary agent systems in navigating c...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 29. STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  					AI-generated summary 				 Activation steering has emerged...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 30. SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.  					AI-generated summary 				 With the widespread deployment of Computer-using Agents (CU...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 31. Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.  					AI-generated summary 				 Large language models (LLMs) are increasingly deployed in high...
[11.02.2026 08:40] ********************************************************************************
[11.02.2026 08:40] Abstract 32. Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.  					AI-generated summary 				 Continuous-time generative models,...
[11.02.2026 08:40] Read previous papers.
[11.02.2026 08:40] Generating reviews via LLM API.
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#data", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é", "desc": "OPUS ‚Äî —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–µ–∫—Ü–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –æ–±–Ω–æ–≤–ª
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#agents", "#dataset", "#rlhf", "#multimodal", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞", "desc": "Code2World ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ vision-language, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GUI –ø—É—Ç—ë–º 
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#rl", "#agents", "#benchmark", "#multimodal", "#training"], "emoji": "üñ±Ô∏è", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —á–µ—Ä–µ–∑ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "UI-Venus-1.5 ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#rl", "#open_source", "#agents", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–û—Ç –æ–ø—ã—Ç–∞ –∫ –Ω–∞–≤—ã–∫–∞–º: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —ç–≤–æ–ª—é—Ü–∏–µ–π", "desc": "SkillRL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM —É–ª—É—á—à–∞—Ç—å —Å–≤–æ—é —Ä–∞–±–æ—Ç—É —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#rl", "#agents", "#multimodal", "#training", "#open_source", "#cv", "#science", "#optimization"], "emoji": "üî¨", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ: VLM –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#training", "#agents", "#architecture"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ Chain of Mindset, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#rl", "#agents", "#benchmark"], "emoji": "üåç", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–∏—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Agent World Model (AWM) ‚Äî –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π, –∫–æ
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#optimization", "#inference", "#long_context", "#architecture", "#training"], "emoji": "üîç", "ru": {"title": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ —Å–ª–µ–ø–æ—Ç—ã: –∫–∞–∫ Prism —É—Å–∫–æ—Ä—è–µ—Ç LLM —á–µ—Ä–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –±–ª–æ—á–Ω–æ-—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è 
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#open_source", "#agents", "#optimization", "#inference", "#architecture", "#training", "#reasoning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ DLLM-Searcher –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤—ã
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#training", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö –¥–µ–π—Å—Ç–≤
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#agents", "#dataset", "#cv", "#benchmark", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ —Å –ø–∞–º—è—Ç—å—é –∏ –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π", "desc": "Agent Banana –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –∞–≥–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#inference", "#robotics", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–º —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –º–æ–¥—É–ª—è—Ü–∏—é –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SCALE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ Vision-Language-Action –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ –º
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –¥
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#training", "#rl", "#agents", "#math", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞
[11.02.2026 08:40] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "ScaleEnv ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –æ–±—É—á–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —Å —Ö–æ—Ä–æ—à–µ–π –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≥–∞—Ä–∞–Ω—Ç–∏
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#open_source", "#agents", "#reasoning", "#video", "#robotics", "#training", "#transfer_learning", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ –∫ –∑–Ω–∞–Ω–∏—è–º: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "VideoWorld 2 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥–∞–≤–∞
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#multimodal", "#training", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —è–∑—ã–∫–∞, –∑—Ä–µ–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —É–º–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞", "desc": "BagelVLA ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–∑—ã–∫–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–∏–∑—É–∞–ª—å–Ω
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#open_source", "#audio", "#benchmark", "#multimodal", "#training", "#reasoning"], "emoji": "üéôÔ∏è", "ru": {"title": "–ö–æ–Ω–µ—Ü —Ä–µ—á–µ–≤–æ–º—É –∫–æ–Ω–≤–µ–π–µ—Ä—É: –ø–æ–ª–Ω–æ–¥—É–ø–ª–µ–∫—Å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Covo-Audio ‚Äî –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞—É–¥–∏–æ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä
[11.02.2026 08:40] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Anchor –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —ç—Ç–∞–ª–æ
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#open_source", "#agents", "#dataset", "#synthetic", "#robotics", "#3d", "#reasoning"], "emoji": "üè†", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D —Å—Ü–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "SAGE ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω, –≥–æ—Ç–æ–≤—ã
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#training", "#robotics", "#architecture", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–µ–∑ —É—Ç–µ—á–µ–∫: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å —Ä–æ–±–æ—Ç–∞ –≤–∏–¥–µ—Ç—å —Å—É—Ç—å –¥–µ–π—Å—Ç–≤–∏–π", "desc": "VLA-JEPA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ JEPA, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫ –∑—Ä–∏—Ç–µ–ª—å–Ω
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#inference", "#rl", "#reasoning", "#optimization", "#rag"], "emoji": "üß†", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –≤–º–µ—Å—Ç–æ —Å—ã—Ä—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: —Å–∂–∞—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ-–≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#reasoning", "#math"], "emoji": "üß≠", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#synthetic"], "emoji": "üé®", "ru": {"title": "–í—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç Fine-T2I —Å –±–æ–ª–µ–µ —á–µ–º 6 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv"], "emoji": "üé®", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –º–æ–≥—É—Ç –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#training", "#architecture"], "emoji": "üß≤", "ru": {"title": "–õ–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π: –æ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∫ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Mixture of Factor Analyzers (MFA) –∫–∞–∫ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±–Ω–∞—Ä
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#training", "#multimodal", "#diffusion", "#open_source", "#video", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è flow matching —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —É—Å–ª–æ–≤–Ω—ã—Ö —Å–∫–æ—Ä–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Stable Velocity –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥
[11.02.2026 08:40] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#multimodal", "#open_source", "#cv", "#synthetic", "#optimization"], "emoji": "üêô", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –≤ vision-language –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Octopus –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫
[11.02.2026 08:40] Querying the API.
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  					AI-generated summary 				 Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.
[11.02.2026 08:40] Response: ```json
{
  "desc": "TodoEvolve –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ç–∞-–ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ PlanFactory ‚Äî –º–æ–¥—É–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–∏–∑–∞–π–Ω–∞, –∫–æ—Ç–æ—Ä–æ–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –µ–¥–∏–Ω–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ. –ú–æ–¥–µ–ª—å Todo-14B –æ–±—É—á–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è (IGPO), –∫–æ—Ç–æ—Ä–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ä—É—á–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –Ω–∏–∑–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö.",
  "emoji": "üèóÔ∏è",
  "title": "–≠–≤–æ–ª—é—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–Ω—Ç–µ–∑ –∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é"
}
```
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  					AI-generated summary 				 Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead."

[11.02.2026 08:40] Response: ```python
["AGENTS", "RL", "BENCHMARK", "ARCHITECTURE", "TRAINING"]
```
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  					AI-generated summary 				 Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead."

[11.02.2026 08:40] Response: ```python
['OPTIMIZATION', 'REASONING']
```

**Justification:**

- **OPTIMIZATION**: The paper discusses multi-objective reinforcement learning optimization (IGPO - Impedance-Guided Preference Optimization) for training and optimizing planning architectures, which is a core optimization method.

- **REASONING**: The paper focuses on planning capabilities and task-specific planning architectures for agent systems to navigate complex, long-horizon tasks, which relates to enhancing logical reasoning and planning capabilities.
[11.02.2026 08:40] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "REASONING"]


**Justification:**

- **OPTIMIZATION**: The paper discusses multi-objective reinforcement learning optimization (IGPO - Impedance-Guided Preference Optimization) for training and optimizing planning architectures, which is a core optimization method.

- **REASONING**: The paper focuses on planning capabilities and task-specific planning architectures for agent systems to navigate complex, long-horizon tasks, which relates to enhancing logical reasoning and planning capabilities.
[11.02.2026 08:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TodoEvolve is a new approach that helps AI systems create and improve their own planning methods for complex tasks. It uses a modular design called PlanFactory, which organizes different planning strategies into a single framework, making it easier to adapt to various problems. By applying multi-objective reinforcement learning through Impedance-Guided Preference Optimization (IGPO), TodoEvolve trains AI to develop planning systems that are efficient and effective. Tests show that TodoEvolve outperforms traditional planning methods while being cost-effective and quick to run.","title":"Empowering AI with Self-Improving Planning Systems"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TodoEvolve is a new approach that helps AI systems create and improve their own planning methods for complex tasks. It uses a modular design called PlanFactory, which organizes different planning strategies into a single framework, making it easier to adapt to various problems. By applying multi-objective reinforcement learning through Impedance-Guided Preference Optimization (IGPO), TodoEvolve trains AI to develop planning systems that are efficient and effective. Tests show that TodoEvolve outperforms traditional planning methods while being cost-effective and quick to run.', title='Empowering AI with Self-Improving Planning Systems'))
[11.02.2026 08:40] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TodoEvolve ÊòØ‰∏ÄÁßçÂÖÉËßÑÂàíËåÉÂºèÔºåËÉΩÂ§üËá™‰∏ªÂêàÊàêÂíåÂä®ÊÄÅ‰øÆËÆ¢ÁâπÂÆö‰ªªÂä°ÁöÑËßÑÂàíÊû∂ÊûÑ„ÄÇÂÆÉÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™Ê®°ÂùóÂåñËÆæËÆ°Á©∫Èó¥ PlanFactoryÔºåÊ†áÂáÜÂåñ‰∫ÜÂ§öÁßçËßÑÂàíËåÉÂºèÔºå‰ΩøÂÖ∂Âú®Áªü‰∏ÄÁöÑ‰ª£Á†ÅÂ∫ì‰∏≠ÂÆûÁé∞„ÄÇÂà©Áî®Â§öÁõÆÊ†áÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊñπÊ≥ï Impedance-Guided Preference Optimization (IGPO)ÔºåTodoEvolve ÁîüÊàêÈ´òÊïà„ÄÅÁ®≥ÂÆö‰∏îËµÑÊ∫êËäÇÁúÅÁöÑËßÑÂàíÁ≥ªÁªü„ÄÇÂÆûÈ™åËØÅÊòéÔºåTodoEvolve Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÊâãÂ∑•ËÆæËÆ°ËßÑÂàíÊ®°Âùó„ÄÇ","title":"TodoEvolveÔºöËá™‰∏ªËßÑÂàíÁöÑÊú™Êù•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TodoEvolve ÊòØ‰∏ÄÁßçÂÖÉËßÑÂàíËåÉÂºèÔºåËÉΩÂ§üËá™‰∏ªÂêàÊàêÂíåÂä®ÊÄÅ‰øÆËÆ¢ÁâπÂÆö‰ªªÂä°ÁöÑËßÑÂàíÊû∂ÊûÑ„ÄÇÂÆÉÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™Ê®°ÂùóÂåñËÆæËÆ°Á©∫Èó¥ PlanFactoryÔºåÊ†áÂáÜÂåñ‰∫ÜÂ§öÁßçËßÑÂàíËåÉÂºèÔºå‰ΩøÂÖ∂Âú®Áªü‰∏ÄÁöÑ‰ª£Á†ÅÂ∫ì‰∏≠ÂÆûÁé∞„ÄÇÂà©Áî®Â§öÁõÆÊ†áÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊñπÊ≥ï Impedance-Guided Preference Optimization (IGPO)ÔºåTodoEvolve ÁîüÊàêÈ´òÊïà„ÄÅÁ®≥ÂÆö‰∏îËµÑÊ∫êËäÇÁúÅÁöÑËßÑÂàíÁ≥ªÁªü„ÄÇÂÆûÈ™åËØÅÊòéÔºåTodoEvolve Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÊâãÂ∑•ËÆæËÆ°ËßÑÂàíÊ®°Âùó„ÄÇ', title='TodoEvolveÔºöËá™‰∏ªËßÑÂàíÁöÑÊú™Êù•'))
[11.02.2026 08:40] Querying the API.
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  					AI-generated summary 				 Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.
[11.02.2026 08:40] Response: ```json
{
  "desc": "STEER2ADAPT ‚Äî —ç—Ç–æ –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤. –í–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å –Ω—É–ª—è, –º–µ—Ç–æ–¥ –≤—ã—É—á–∏–≤–∞–µ—Ç –Ω–∏–∑–∫–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π, –æ–±—â–∏—Ö –¥–ª—è –∑–∞–¥–∞—á –≤ –æ–¥–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å). –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—É—Ç—ë–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ª–∏–Ω–µ–π–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –±–∞–∑–∏—Å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤—Å–µ–≥–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ 9 –∑–∞–¥–∞—á–∞—Ö –∏ 3 –º–æ–¥–µ–ª—è—Ö –ø–æ–∫–∞–∑–∞–ª–∞ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 8.2% –≤ —Å—Ä–µ–¥–Ω–µ–º, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –¥–∞–Ω–Ω—ã–º, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞.",
  "emoji": "üß≠",
  "title": "–ì–∏–±–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è LLM —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤"
}
```
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  					AI-generated summary 				 Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs."

[11.02.2026 08:40] Response: ```python
["TRAINING", "ARCHITECTURE"]
```
[11.02.2026 08:40] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  					AI-generated summary 				 Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs."

[11.02.2026 08:41] Response: ```python
['TRANSFER_LEARNING', 'REASONING', 'ALIGNMENT']
```

**Justification:**

1. **TRANSFER_LEARNING**: The paper explicitly discusses composing steering vectors from reusable semantic prior subspaces and adapting to new tasks by discovering linear combinations of basis vectors. This is fundamentally about transferring knowledge/capabilities across tasks.

2. **REASONING**: The paper explicitly mentions "reasoning" as one of the domains where tasks share underlying concept dimensions, and experiments are conducted across reasoning tasks.

3. **ALIGNMENT**: The paper mentions "safety" as a key domain alongside reasoning, and discusses adapting LLMs to downstream behaviors, which relates to aligning models with intended behaviors and safety considerations.
[11.02.2026 08:41] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING", "REASONING", "ALIGNMENT"]


**Justification:**

1. **TRANSFER_LEARNING**: The paper explicitly discusses composing steering vectors from reusable semantic prior subspaces and adapting to new tasks by discovering linear combinations of basis vectors. This is fundamentally about transferring knowledge/capabilities across tasks.

2. **REASONING**: The paper explicitly mentions "reasoning" as one of the domains where tasks share underlying concept dimensions, and experiments are conducted across reasoning tasks.

3. **ALIGNMENT**: The paper mentions "safety" as a key domain alongside reasoning, and discusses adapting LLMs to downstream behaviors, which relates to aligning models with intended behaviors and safety considerations.
[11.02.2026 08:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"STEER2ADAPT is a novel framework designed to enhance the adaptability of large language models (LLMs) by utilizing steering vectors derived from reusable semantic prior subspaces. This approach allows for efficient task adaptation through linear combinations of basis vectors, rather than requiring the model to learn new directions for each task. By capturing shared underlying concept dimensions, STEER2ADAPT enables the model to dynamically adjust to new tasks with minimal examples. Experimental results indicate that this method significantly improves performance across various tasks, demonstrating its data efficiency and stability during inference.","title":"Efficient Task Adaptation for LLMs with STEER2ADAPT"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='STEER2ADAPT is a novel framework designed to enhance the adaptability of large language models (LLMs) by utilizing steering vectors derived from reusable semantic prior subspaces. This approach allows for efficient task adaptation through linear combinations of basis vectors, rather than requiring the model to learn new directions for each task. By capturing shared underlying concept dimensions, STEER2ADAPT enables the model to dynamically adjust to new tasks with minimal examples. Experimental results indicate that this method significantly improves performance across various tasks, demonstrating its data efficiency and stability during inference.', title='Efficient Task Adaptation for LLMs with STEER2ADAPT'))
[11.02.2026 08:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"STEER2ADAPTÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊ°ÜÊû∂ÔºåÈÄöËøáÁªÑÂêàÂèØÈáçÁî®ÁöÑËØ≠‰πâÂÖàÈ™åÂ≠êÁ©∫Èó¥‰∏≠ÁöÑÂºïÂØºÂêëÈáèÔºåÊù•ÈÄÇÂ∫îÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÁé∞ÊúâÂºïÂØºÊñπÊ≥ïÂú®‰ªªÂä°ÂèòÂåñÊó∂ÁöÑÁÅµÊ¥ªÊÄß‰∏çË∂≥ÈóÆÈ¢òÔºåËÉΩÂ§üÈÄöËøáÁ∫øÊÄßÁªÑÂêàÂü∫Á°ÄÂêëÈáèÊù•È´òÊïàÈÄÇÂ∫îÊñ∞‰ªªÂä°„ÄÇSTEER2ADAPTÊçïÊçâÂÖ±‰∫´ÁöÑÊ¶ÇÂøµÁª¥Â∫¶ÔºåÂπ∂ÈÄöËøáÂ∞ëÈáèÁ§∫‰æãÂä®ÊÄÅÂèëÁé∞Ëøô‰∫õÁª¥Â∫¶ÁöÑÁªÑÂêàÔºå‰ªéËÄåÂÆûÁé∞‰ªªÂä°ÈÄÇÂ∫î„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSTEER2ADAPTÂú®Êé®ÁêÜÂíåÂÆâÂÖ®È¢ÜÂüüÁöÑÂ§ö‰∏™‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ≥ÂùáÊèêÂçáËææ8.2%„ÄÇ","title":"ÁÅµÊ¥ªÈÄÇÂ∫îÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑSTEER2ADAPTÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='STEER2ADAPTÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊ°ÜÊû∂ÔºåÈÄöËøáÁªÑÂêàÂèØÈáçÁî®ÁöÑËØ≠‰πâÂÖàÈ™åÂ≠êÁ©∫Èó¥‰∏≠ÁöÑÂºïÂØºÂêëÈáèÔºåÊù•ÈÄÇÂ∫îÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÁé∞ÊúâÂºïÂØºÊñπÊ≥ïÂú®‰ªªÂä°ÂèòÂåñÊó∂ÁöÑÁÅµÊ¥ªÊÄß‰∏çË∂≥ÈóÆÈ¢òÔºåËÉΩÂ§üÈÄöËøáÁ∫øÊÄßÁªÑÂêàÂü∫Á°ÄÂêëÈáèÊù•È´òÊïàÈÄÇÂ∫îÊñ∞‰ªªÂä°„ÄÇSTEER2ADAPTÊçïÊçâÂÖ±‰∫´ÁöÑÊ¶ÇÂøµÁª¥Â∫¶ÔºåÂπ∂ÈÄöËøáÂ∞ëÈáèÁ§∫‰æãÂä®ÊÄÅÂèëÁé∞Ëøô‰∫õÁª¥Â∫¶ÁöÑÁªÑÂêàÔºå‰ªéËÄåÂÆûÁé∞‰ªªÂä°ÈÄÇÂ∫î„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSTEER2ADAPTÂú®Êé®ÁêÜÂíåÂÆâÂÖ®È¢ÜÂüüÁöÑÂ§ö‰∏™‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ≥ÂùáÊèêÂçáËææ8.2%„ÄÇ', title='ÁÅµÊ¥ªÈÄÇÂ∫îÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑSTEER2ADAPTÊ°ÜÊû∂'))
[11.02.2026 08:41] Using data from previous issue: {"categories": ["#security", "#alignment"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "SafePred ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å —Ä–∏—Å–∫–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏
[11.02.2026 08:41] Using data from previous issue: {"categories": ["#interpretability", "#ethics", "#benchmark"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–û—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞ –∫ –∞–Ω–∞–ª–∏–∑—É –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤: –º–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–¥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è SHARP –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–¥–∞ –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑
[11.02.2026 08:41] Using data from previous issue: {"categories": ["#diffusion", "#training", "#optimization", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Temporal Pair Consistency –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Ç–∞–∫
[11.02.2026 08:41] Renaming data file.
[11.02.2026 08:41] Renaming previous data. hf_papers.json to ./d/2026-02-11.json
[11.02.2026 08:41] Saving new data file.
[11.02.2026 08:41] Generating page.
[11.02.2026 08:41] Renaming previous page.
[11.02.2026 08:41] Renaming previous data. index.html to ./d/2026-02-11.html
[11.02.2026 08:41] Writing result.
[11.02.2026 08:41] Renaming log file.
[11.02.2026 08:41] Renaming previous data. log.txt to ./logs/2026-02-11_last_log.txt
