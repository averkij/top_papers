[11.02.2026 11:40] Read previous papers.
[11.02.2026 11:40] Generating top page (month).
[11.02.2026 11:40] Writing top page (month).
[11.02.2026 12:50] Read previous papers.
[11.02.2026 12:50] Get feed.
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05400
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09856
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09082
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10063
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08234
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09443
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10090
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08426
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07035
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10104
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09084
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04208
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07022
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10098
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08847
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10102
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09849
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06820
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08382
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09268
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09823
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07153
[11.02.2026 12:50] Extract page data from URL. URL: https://huggingface.co/papers/2602.00268
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10116
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09662
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09024
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08344
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09439
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07839
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06161
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05435
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02464
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08503
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07276
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10099
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09924
[11.02.2026 12:50] Extract page data from URL. URL: https://huggingface.co/papers/2602.04802
[11.02.2026 12:50] Extract page data from URL. URL: https://huggingface.co/papers/2602.04521
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04908
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01725
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21235
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08519
[11.02.2026 12:50] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07755
[11.02.2026 12:50] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.02.2026 12:50] No deleted papers detected.
[11.02.2026 12:50] Downloading and parsing papers (pdf, html). Total: 43.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.05400.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.05400.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.05400.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09856.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09856.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09856.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09082.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09082.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09082.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.10063.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.10063.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.10063.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.08234.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.08234.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.08234.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09443.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09443.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09443.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.10090.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.10090.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.10090.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.08426.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.08426.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.08426.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.07035.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.07035.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.07035.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.10104.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.10104.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.10104.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09084.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09084.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09084.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.04208.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.04208.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.04208.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.07022.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.07022.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.07022.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.10098.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.10098.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.10098.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.08847.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.08847.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.08847.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.10102.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.10102.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.10102.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09849.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09849.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09849.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.06820.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.06820.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.06820.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.08382.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.08382.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.08382.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09268.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09268.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09268.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09823.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09823.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09823.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.07153.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.07153.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.07153.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.00268.
[11.02.2026 12:50] Downloading paper 2602.00268 from https://arxiv.org/pdf/2602.00268v1...
[11.02.2026 12:50] Extracting affiliations from text.
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TOKENTRIM: INFERENCE-TIME TOKEN PRUNING FOR AUTOREGRESSIVE LONG VIDEO GENERATION Ariel Shaulov School of Computer Science Tel Aviv University, Israel arielshaulov@mail.tau.ac.il Eitan Shaar Independent Researcher shaarei@biu.ac.il Amit Edenzon School of Mathematics Bar-Ilan University, Israel amit.edenzon@live.biu.ac.il Lior Wolf School of Computer Science Tel Aviv University, Israel wolf@cs.tau.ac.il 6 2 0 2 0 3 ] . [ 1 8 6 2 0 0 . 2 0 6 2 : r Figure 1: Text-to-video results before and after applying TokenTrim on Rolling Forcing [18] and Self Forcing [19]. "
[11.02.2026 12:50] Response: ```python
[
    "School of Computer Science Tel Aviv University, Israel",
    "Independent Researcher",
    "School of Mathematics Bar-Ilan University, Israel",
    "School of Computer Science Tel Aviv University, Israel"
]
```
[11.02.2026 12:50] Deleting PDF ./assets/pdf/2602.00268.pdf.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.10116.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.10116.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.10116.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09662.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09662.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09662.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09024.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09024.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09024.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.08344.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.08344.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.08344.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09439.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09439.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09439.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.07839.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.07839.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.07839.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.06161.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.06161.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.06161.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.05435.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.05435.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.05435.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.02464.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.02464.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.02464.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.08503.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.08503.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.08503.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.07276.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.07276.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.07276.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.10099.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.10099.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.10099.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.09924.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.09924.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.09924.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.04802.
[11.02.2026 12:50] Downloading paper 2602.04802 from https://arxiv.org/pdf/2602.04802v1...
[11.02.2026 12:50] Extracting affiliations from text.
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Qingan Liu * 1 Juntong Feng * 1 Yuhao Wang * 1 Xinzhe Han 1 Yujie Cheng 1 Yue Zhu 1 Haiwen Diao 2 Yunzhi Zhuge 1 Huchuan Lu 1 6 2 0 2 4 ] . [ 1 2 0 8 4 0 . 2 0 6 2 : r a "
[11.02.2026 12:50] Response: ```python
[]
```
[11.02.2026 12:50] Extracting affiliations from text.
[11.02.2026 12:50] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Qingan Liu * 1 Juntong Feng * 1 Yuhao Wang * 1 Xinzhe Han 1 Yujie Cheng 1 Yue Zhu 1 Haiwen Diao 2 Yunzhi Zhuge 1 Huchuan Lu 1 6 2 0 2 4 ] . [ 1 2 0 8 4 0 . 2 0 6 2 : r aVisionLanguage Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at VISTA-Bench. 1. Introduction Recent visionlanguage models (VLMs) (Bai et al., 2025a; Wang et al., 2025; Diao et al., 2024; 2025b;a) extend large language models (LLMs) (Yang et al., 2025; Grattafiori et al., 2024; Cai et al., 2024) to diverse tasks by treating visual tokens and text queries as standard inputs. Accordingly, existing benchmarks predominantly assume pure-text 1School of Artificial Intelligence, Dalian University of Technology, Dalian, China 2S-Lab, Nanyang Technological University, Singapore. Correspondence to: Haiwen Diao <haiwen.diao@ntu.edu.sg>, Yunzhi Zhuge <zgyz@dlut.edu.cn>. Preprint. February 5, 2026. queries. However, real-world scenarios also embed language directly within visual content in Figure 1(a), raising fundamental question: Do visionlanguage models truly understand visualized text as well as pure text? Recent explorations such as DeepSeek-OCR (Wei et al., 2025) and Glyph (Cheng et al., 2025) point to growing text-as-pixels paradigm, in which text is rendered into image to reduce token overhead in long contexts and to establish unified perceptual interface across modalities. Crucially, replacing pure-text queries with visualized text fundamentally alters the input pathway in Figure 1(b), forcing all information to be processed through unified modality. These developments expose an interesting and underexplored challenge: whether foundation models can preserve semantic fidelity and cross-modal alignment when language is represented as pixels, especially in terms of visuallanguage understanding and unified model architecture design in the future. Notably, existing VLM evaluation protocols remain largely text-centric, spanning both standard language reasoning benchmarks (Hendrycks et al., 2020) and widely used multimodal benchmarks (Liu et al., 2024b; Yue et al., 2024; Li et al., 2024a). This blind spot overlooks the perceptual challenge of reading language from pixels, leaving it unclear whether model behavior remains stable when language isnt conveyed symbolically. Fortunately, some concurrent works start to expose this gap. VTCBench (Zhao et al., 2025) reports significant performance degradation when text-only tasks are converted to visualized text. Impressively, Gemini-3-Pro (Google, 2025) exhibits tiny modality gap, highlighting the potential for modality-equivalent representations. However, existing analyses are largely confined to unimodal settings. It therefore remains unclear whether similar phenomena persist in multimodal tasks from perception to reasoning and how they manifest in such scenarios. To address this question, we uncover pronounced modality gap between pure-text and visualized-text inputs through preliminary experiments. We then introduce VISTA-Bench, benchmark of 1,500 carefully filtered samples designed for rigorous comparison under strictly matched pure-text and visualized-text conditions. VISTA-Bench is built through three-stage pipeline that curates high-quality questions, VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text? Figure 1. (a) Humans integrate visual context with embedded text, whereas standard VLM evaluation provides language as discrete tokens. (b) Presenting language as visualized text can induce behavioral deviations from pure-text inputs, revealing modality gap. renders text into diverse visual layouts, and enforces fidelity via VLM-based verification. It contains multiple-choice questions and organizes them under hierarchical capability taxonomy spanning perception, reasoning, and knowledge. Evaluating over 20 state-of-the-art VLMs, we uncover pervasive modality gap that is consistently amplified by perceptually challenging renderings. Our analysis attributes this gap primarily to limited perceptual robustness, while additional visual grounding offers only partial mitigation. Notably, MiMo-VL-7B-RL (MiMo, 2025) stands out as rare exception, exhibiting markedly stronger robustness under visualized-text inputs. Overall, VISTA-Bench provides principled testbed for diagnosing modality gaps under realistic text-as-pixels inputs and underscores the need for more robust unified representations in future VLMs. Our main contributions are summarized as follows: Exposing modality non-equivalence phenomenon. We uncover comprehensive analyses across multimodality perception, reasoning, and knowledge tasks when text is presented as pixels rather than symbols. Defining systematic visualized-text benchmark. VISTA-Bench is rigorously controlled and easy-todeploy benchmark, filling blind spot in evaluating the gap between pure-text and visualized-text inputs. Providing text-as-pixels diagnosis and outlook. Large-scale analyses identify limited perceptual robustness as the core driver of the modality gap, and VISTA-Bench elevates text-as-pixels to foundational challenge for unified visionlanguage representations. 2. Related Work 2.1. Multimodal Understanding Benchmarks The rapid advancement of Vision-Language Models (VL"
[11.02.2026 12:50] Mistral response. {"id": "8d8a8935226547f58deea8ae336dfc88", "created": 1770814227, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1438, "total_tokens": 1477, "completion_tokens": 39, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"School of Artificial Intelligence, Dalian University of Technology, Dalian, China\",\n    \"S-Lab, Nanyang Technological University, Singapore\"\n]\n```"}}]}
[11.02.2026 12:50] Response: ```python
[
    "School of Artificial Intelligence, Dalian University of Technology, Dalian, China",
    "S-Lab, Nanyang Technological University, Singapore"
]
```
[11.02.2026 12:50] Deleting PDF ./assets/pdf/2602.04802.pdf.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.04521.
[11.02.2026 12:50] Downloading paper 2602.04521 from https://arxiv.org/pdf/2602.04521v1...
[11.02.2026 12:50] Extracting affiliations from text.
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 4 ] . [ 1 1 2 5 4 0 . 2 0 6 2 : r Aditya Kasliwal, Pratinav Seth, Vinay Kumar Sankarapu Lexsi Labs "
[11.02.2026 12:50] Response: ```python
["Lexsi Labs"]
```
[11.02.2026 12:50] Deleting PDF ./assets/pdf/2602.04521.pdf.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.04908.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.04908.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.04908.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.01725.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.01725.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.01725.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2601.21235.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2601.21235.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2601.21235.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.08519.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.08519.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.08519.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Downloading and parsing paper https://huggingface.co/papers/2602.07755.
[11.02.2026 12:50] Extra JSON file exists (./assets/json/2602.07755.json), skip PDF parsing.
[11.02.2026 12:50] Paper image links file exists (./assets/img_data/2602.07755.json), skip HTML parsing.
[11.02.2026 12:50] Success.
[11.02.2026 12:50] Enriching papers with extra data.
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 0. OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.  					AI-generated summary 				 As hig...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 1. Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.  					AI-generated summary 				 Autonomous GUI agents interact with environments by perceiv...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 2. UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.  					AI-generated summary 				 GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving bo...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 3. A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.  					AI-generated summary 				 Human problem-solving is never the repetition of a ...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 4. SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.  					AI-generated summary 				 Large Language Model (LLM) agents have shown stunning results in complex t...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 5. Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.  					AI-generated summary 				 The transition from symbolic ...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 6. Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.  					AI-generated summary 				 Recent advances in large lan...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 7. Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.  					AI-generated summary 				 Block-sparse attention is promising for accelerating long-...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 8. Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.  					AI-generated summary 				 Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by ...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 9. Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.  					AI-generated summary 				 Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to ...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 10. Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.  					AI-generated summary 				 We study instruction-based image editing under prof...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 11. SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.  					AI-generated summary 				 Vision-Language-Action (VLA) models have e...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 12. Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.  					AI-generated summary 				 Recent studies have explored autoregressive model...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 13. VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.  					AI-generated summary 				 Pretraining Vision-Language-Action (VLA) policies...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 14. Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.  					AI-generated summary 				 Multi-agent LLM systems e...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 15. VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.  					AI-generated summary 				 Lear...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 16. BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.  					AI-generated summary 				 Equipping embodied agents with the ability to reason about tasks, foresee ...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 17. ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.  					AI-generated summary 				 Training generalist agents capable of adapting to diverse scenarios requires interactive environments for se...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 18. A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.  					AI-generated summary 				 Large Language Models (LLMs) face significant challenges in long-context proces...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 19. Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms.  					AI-generated summary 				 Diffusion transformers typically incorporate textual information via attention layers...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 20. Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 21. A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.  					AI-generated summary 				 End-to-end GUI agents for real desktop environments requi...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 22. Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.  					AI-generated summary 				 Auto-regressive video generation ...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 23. SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.  					AI-generated summary 				 Real-world data collection for em...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 24. TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification.  					AI-generated summary 				 Effectively scaling GUI automation is essential fo...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 25. Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.  					AI-generated summary 				 This paper challenges the dominance of continuous pipelines in vi...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 26. Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.  					AI-generated summary 				 Parallel thinking has emerged as a new paradigm f...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 27. A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.  					AI-generated summary 				 High-quality and open datas...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 28. TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  					AI-generated summary 				 Planning has become a central capability for contemporary agent systems in navigating c...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 29. COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction.  					AI-generated summary 				 Parallel diffusion decoding can...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 30. Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.  					AI-generated summary 				 While flow matching is elegant, its reliance on single-samp...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 31. Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.  					AI-generated summary 				 Activation d...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 32. Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.  					AI-generated summary 				 Self-correction is essential for solving complex reasoning problems in vision-lang...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 33. STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  					AI-generated summary 				 Activation steering has emerged...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 34. Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling.  					AI-generated summary 				 Leveraging representation encode...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 35. LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance.  					AI-generated summary 				 Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require add...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 36. VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved impressive performance in cross-mo...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 37. Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.  					AI-generated summary 				 Modern deployments require LLMs to enforce safety policies at scale, yet many controls rel...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 38. Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.  					AI-generated summary 				 Continuous-time generative models,...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 39. SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.  					AI-generated summary 				 With the widespread deployment of Computer-using Agents (CU...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 40. Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.  					AI-generated summary 				 Large language models (LLMs) are increasingly deployed in high...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 41. PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols.  					AI-generated summary 				 Attributed Graph Clustering (AGC) is a fun...
[11.02.2026 12:50] ********************************************************************************
[11.02.2026 12:50] Abstract 42. ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains.  					AI-generated summary 				 The statelessness of foundation models bottlenecks agentic systems' ability to conti...
[11.02.2026 12:50] Read previous papers.
[11.02.2026 12:50] Generating reviews via LLM API.
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#data", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é", "desc": "OPUS ‚Äî —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–µ–∫—Ü–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –æ–±–Ω–æ–≤–ª
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#agents", "#dataset", "#rlhf", "#multimodal", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞", "desc": "Code2World ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ vision-language, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GUI –ø—É—Ç—ë–º 
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#rl", "#agents", "#benchmark", "#multimodal", "#training"], "emoji": "üñ±Ô∏è", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —á–µ—Ä–µ–∑ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "UI-Venus-1.5 ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#training", "#agents", "#architecture"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ Chain of Mindset, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#rl", "#open_source", "#agents", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–û—Ç –æ–ø—ã—Ç–∞ –∫ –Ω–∞–≤—ã–∫–∞–º: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —ç–≤–æ–ª—é—Ü–∏–µ–π", "desc": "SkillRL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM —É–ª—É—á—à–∞—Ç—å —Å–≤–æ—é —Ä–∞–±–æ—Ç—É —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#rl", "#agents", "#multimodal", "#training", "#open_source", "#cv", "#science", "#optimization"], "emoji": "üî¨", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ: VLM –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#rl", "#agents", "#benchmark"], "emoji": "üåç", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–∏—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Agent World Model (AWM) ‚Äî –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π, –∫–æ
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#optimization", "#inference", "#long_context", "#architecture", "#training"], "emoji": "üîç", "ru": {"title": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ —Å–ª–µ–ø–æ—Ç—ã: –∫–∞–∫ Prism —É—Å–∫–æ—Ä—è–µ—Ç LLM —á–µ—Ä–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –±–ª–æ—á–Ω–æ-—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è 
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#open_source", "#agents", "#optimization", "#inference", "#architecture", "#training", "#reasoning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ DLLM-Searcher –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤—ã
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#training", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö –¥–µ–π—Å—Ç–≤
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#agents", "#dataset", "#cv", "#benchmark", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ —Å –ø–∞–º—è—Ç—å—é –∏ –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π", "desc": "Agent Banana –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –∞–≥–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#inference", "#robotics", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–º —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –º–æ–¥—É–ª—è—Ü–∏—é –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SCALE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ Vision-Language-Action –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ –º
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –¥
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#training", "#robotics", "#architecture", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–µ–∑ —É—Ç–µ—á–µ–∫: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å —Ä–æ–±–æ—Ç–∞ –≤–∏–¥–µ—Ç—å —Å—É—Ç—å –¥–µ–π—Å—Ç–≤–∏–π", "desc": "VLA-JEPA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ JEPA, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫ –∑—Ä–∏—Ç–µ–ª—å–Ω
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#training", "#rl", "#agents", "#math", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#open_source", "#agents", "#reasoning", "#video", "#robotics", "#training", "#transfer_learning", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ –∫ –∑–Ω–∞–Ω–∏—è–º: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "VideoWorld 2 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥–∞–≤–∞
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#multimodal", "#training", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —è–∑—ã–∫–∞, –∑—Ä–µ–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —É–º–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞", "desc": "BagelVLA ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–∑—ã–∫–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–∏–∑—É–∞–ª—å–Ω
[11.02.2026 12:50] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "ScaleEnv ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –æ–±—É—á–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —Å —Ö–æ—Ä–æ—à–µ–π –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≥–∞—Ä–∞–Ω—Ç–∏
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#inference", "#rl", "#reasoning", "#optimization", "#rag"], "emoji": "üß†", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –≤–º–µ—Å—Ç–æ —Å—ã—Ä—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: —Å–∂–∞—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ-–≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#multimodal", "#training"], "emoji": "üéõÔ∏è", "ru": {"title": "–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –º–æ–¥—É–ª—è—Ü–∏–∏: –æ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥—É–ª—è—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ embedding –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å 
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#open_source", "#audio", "#benchmark", "#multimodal", "#training", "#reasoning"], "emoji": "üéôÔ∏è", "ru": {"title": "–ö–æ–Ω–µ—Ü —Ä–µ—á–µ–≤–æ–º—É –∫–æ–Ω–≤–µ–π–µ—Ä—É: –ø–æ–ª–Ω–æ–¥—É–ø–ª–µ–∫—Å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Covo-Audio ‚Äî –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞—É–¥–∏–æ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä
[11.02.2026 12:50] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Anchor –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —ç—Ç–∞–ª–æ
[11.02.2026 12:50] Querying the API.
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.  					AI-generated summary 				 Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.
[11.02.2026 12:50] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –≤ —Ç–æ–∫–µ–Ω–∞—Ö —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—è–≤–ª—è–µ—Ç –∏ —É–¥–∞–ª—è–µ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã ‚Äî —Ç–µ, —á—å–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª–æ–Ω—è—é—Ç—Å—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤. –ü—É—Ç–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫–æ—Ä—Ä—É–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ–±—É—á–µ–Ω–∏—è.",
  "emoji": "üé¨",
  "title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"
}
```
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.  					AI-generated summary 				 Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space."

[11.02.2026 12:50] Response: ```python
["VIDEO", "INFERENCE"]
```
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.  					AI-generated summary 				 Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space."

[11.02.2026 12:50] Response: ```python
["OPTIMIZATION"]
```

The paper addresses an inference-time optimization method to improve auto-regressive video generation by mitigating temporal drift through identification and removal of unstable tokens. This falls under optimization as it presents a technique to improve model performance during inference without modifying the model architecture or training procedure.
[11.02.2026 12:50] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper addresses an inference-time optimization method to improve auto-regressive video generation by mitigating temporal drift through identification and removal of unstable tokens. This falls under optimization as it presents a technique to improve model performance during inference without modifying the model architecture or training procedure.
[11.02.2026 12:50] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the problem of temporal drift in auto-regressive video generation, where errors accumulate over time, leading to inconsistencies in long videos. The authors propose that this drift is caused by the reuse of unstable latent conditioning tokens during inference, rather than a lack of model capacity. To combat this issue, they introduce a method that identifies and removes these unstable tokens before they can affect future frame generation. By doing so, the approach enhances long-horizon consistency without altering the model\'s architecture or training process.","title":"Stabilizing Video Generation by Removing Unstable Tokens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper addresses the problem of temporal drift in auto-regressive video generation, where errors accumulate over time, leading to inconsistencies in long videos. The authors propose that this drift is caused by the reuse of unstable latent conditioning tokens during inference, rather than a lack of model capacity. To combat this issue, they introduce a method that identifies and removes these unstable tokens before they can affect future frame generation. By doing so, the approach enhances long-horizon consistency without altering the model's architecture or training process.", title='Stabilizing Video Generation by Removing Unstable Tokens'))
[11.02.2026 12:50] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Ëá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÂú®ÁîüÊàêÈïøËßÜÈ¢ëÊó∂‰ºöÂá∫Áé∞Êó∂Èó¥ÊºÇÁßªÈóÆÈ¢òÔºåËøôÊòØÁî±‰∫éÊΩúÂú®Êù°‰ª∂‰ª§ÁâåÁöÑÈîôËØØÁ¥ØÁßØÈÄ†ÊàêÁöÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºåÈÄöËøáÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËØÜÂà´Âπ∂ÁßªÈô§‰∏çÁ®≥ÂÆöÁöÑ‰ª§ÁâåÔºåÊù•ÊîπÂñÑÈïøÊó∂Èó¥ÊÆµÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÂÆö‰πâ‰∏çÁ®≥ÂÆö‰ª§Áâå‰∏∫‰∏é‰πãÂâçÁîüÊàêÁöÑÊâπÊ¨°Ë°®Á§∫ÊòæËëóÂÅèÁ¶ªÁöÑÊΩúÂú®‰ª§ÁâåÔºåËøôË°®ÊòéÂÆÉ‰ª¨ÂèØËÉΩÂ∑≤ÁªèË¢´ÊçüÂùèÊàñÂèëÁîü‰∫ÜËØ≠‰πâÊºÇÁßª„ÄÇÈÄöËøáÊòæÂºèÁßªÈô§Ëøô‰∫õ‰∏çÂèØÈù†ÁöÑÊΩúÂú®‰ø°ÊÅØÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÈïøÊó∂Èó¥ÊÆµÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÔºåËÄåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÊû∂ÊûÑÊàñËÆ≠ÁªÉËøáÁ®ã„ÄÇ","title":"Ê∂àÈô§Êó∂Èó¥ÊºÇÁßªÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàê‰∏ÄËá¥ÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Ëá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÂú®ÁîüÊàêÈïøËßÜÈ¢ëÊó∂‰ºöÂá∫Áé∞Êó∂Èó¥ÊºÇÁßªÈóÆÈ¢òÔºåËøôÊòØÁî±‰∫éÊΩúÂú®Êù°‰ª∂‰ª§ÁâåÁöÑÈîôËØØÁ¥ØÁßØÈÄ†ÊàêÁöÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºåÈÄöËøáÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËØÜÂà´Âπ∂ÁßªÈô§‰∏çÁ®≥ÂÆöÁöÑ‰ª§ÁâåÔºåÊù•ÊîπÂñÑÈïøÊó∂Èó¥ÊÆµÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÂÆö‰πâ‰∏çÁ®≥ÂÆö‰ª§Áâå‰∏∫‰∏é‰πãÂâçÁîüÊàêÁöÑÊâπÊ¨°Ë°®Á§∫ÊòæËëóÂÅèÁ¶ªÁöÑÊΩúÂú®‰ª§ÁâåÔºåËøôË°®ÊòéÂÆÉ‰ª¨ÂèØËÉΩÂ∑≤ÁªèË¢´ÊçüÂùèÊàñÂèëÁîü‰∫ÜËØ≠‰πâÊºÇÁßª„ÄÇÈÄöËøáÊòæÂºèÁßªÈô§Ëøô‰∫õ‰∏çÂèØÈù†ÁöÑÊΩúÂú®‰ø°ÊÅØÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÈïøÊó∂Èó¥ÊÆµÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÔºåËÄåÊó†ÈúÄ‰øÆÊîπÊ®°ÂûãÊû∂ÊûÑÊàñËÆ≠ÁªÉËøáÁ®ã„ÄÇ', title='Ê∂àÈô§Êó∂Èó¥ÊºÇÁßªÔºåÊèêÂçáËßÜÈ¢ëÁîüÊàê‰∏ÄËá¥ÊÄß'))
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#open_source", "#agents", "#dataset", "#synthetic", "#robotics", "#3d", "#reasoning"], "emoji": "üè†", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D —Å—Ü–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "SAGE ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω, –≥–æ—Ç–æ–≤—ã
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#dataset", "#rlhf", "#agents"], "emoji": "üå≥", "ru": {"title": "–î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "TreeCUA ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv"], "emoji": "üé®", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –º–æ–≥—É—Ç –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#reasoning", "#math"], "emoji": "üß≠", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#synthetic"], "emoji": "üé®", "ru": {"title": "–í—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç Fine-T2I —Å –±–æ–ª–µ–µ —á–µ–º 6 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#benchmark", "#training", "#agents", "#architecture", "#rl"], "emoji": "üèóÔ∏è", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–Ω—Ç–µ–∑ –∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "TodoEvolve –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ç–∞-–ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞–¥
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —É–º–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –∫—ç—à–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ COVER –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#training", "#multimodal", "#diffusion", "#open_source", "#video", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è flow matching —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —É—Å–ª–æ–≤–Ω—ã—Ö —Å–∫–æ—Ä–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Stable Velocity –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#training", "#architecture"], "emoji": "üß≤", "ru": {"title": "–õ–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π: –æ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∫ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Mixture of Factor Analyzers (MFA) –∫–∞–∫ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±–Ω–∞—Ä
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#multimodal", "#open_source", "#cv", "#synthetic", "#optimization"], "emoji": "üêô", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –≤ vision-language –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Octopus –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "üß≠", "ru": {"title": "–ì–∏–±–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è LLM —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤", "desc": "STEER2ADAPT ‚Äî —ç—Ç–æ –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø–æ
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#training", "#math", "#optimization", "#open_source", "#diffusion", "#architecture"], "emoji": "üåê", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ: –ø–æ—Ç–æ–∫ –Ω–∞ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏ –≤–º–µ—Å—Ç–æ –µ–≤–∫–ª–∏–¥–æ–≤–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö, –∑
[11.02.2026 12:50] Using data from previous issue: {"categories": ["#benchmark", "#math", "#inference"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª–∏ —Å–∞–º–∏ –∑–Ω–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∞ —Å–ª–æ–∂–Ω–∞ –¥–ª—è –Ω–∏—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM –∫–æ–¥–∏—Ä—É—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ –≤ —Å–≤–æ–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö –¥–æ —Ç–æ–≥–æ, –∫–∞–∫ –Ω–∞—á–Ω—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –û–Ω–∏ –æ–±—É—á
[11.02.2026 12:50] Querying the API.
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.
[11.02.2026 12:50] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω VISTA-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å —Ç–µ–∫—Å—Ç, –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –æ–±—ã—á–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –º–æ–¥–µ–ª–∏, —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ —Å —á–∏—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º, —á–∞—Å—Ç–æ –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–∞–∫ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ –±–æ–ª–µ–µ 20 –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω—ã—Ö VLM –∏ –≤—ã—è–≤–∏–ª–∏ –≤—ã—Å–æ–∫—É—é —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤–∞—Ä–∏–∞—Ü–∏—è–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–µ–∏–∑–º–µ–Ω–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á–∏ –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–æ —É–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –±–æ–ª–µ–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —è–∑—ã–∫–∞.",
  "emoji": "üìñ",
  "title": "–ú–æ–¥–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤: –∫–∞–∫ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ç–µ—Ä—è—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"
}
```
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench."

[11.02.2026 12:50] Response: ```python
["BENCHMARK", "MULTIMODAL", "DATASET", "CV"]
```
[11.02.2026 12:50] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench."

[11.02.2026 12:50] Response: ```python
['INTERPRETABILITY', 'OPEN_SOURCE']
```

**Justification:**

- **INTERPRETABILITY**: The paper analyzes and explains model behavior by systematically evaluating vision-language models' performance gaps between pure-text and visualized-text queries. It diagnoses limitations and reveals sensitivity to rendering variations, which falls under analyzing model behavior and explanations.

- **OPEN_SOURCE**: The paper explicitly states "The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench," indicating the authors are releasing their benchmark dataset publicly.
[11.02.2026 12:50] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "OPEN_SOURCE"]


**Justification:**

- **INTERPRETABILITY**: The paper analyzes and explains model behavior by systematically evaluating vision-language models" performance gaps between pure-text and visualized-text queries. It diagnoses limitations and reveals sensitivity to rendering variations, which falls under analyzing model behavior and explanations.

- **OPEN_SOURCE**: The paper explicitly states "The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench," indicating the authors are releasing their benchmark dataset publicly.
[11.02.2026 12:50] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VISTA-Bench is a new benchmark designed to evaluate how well vision-language models (VLMs) understand visualized text compared to pure-text queries. The study shows that while VLMs perform well with text alone, their performance drops significantly when the same information is presented as visualized text. This performance gap is influenced by the difficulty of interpreting the visualized text, which varies with rendering conditions. VISTA-Bench aims to identify these weaknesses in VLMs and promote the development of models that can better integrate and understand both text and visual information.","title":"Bridging the Gap: Evaluating Vision-Language Models with Visualized Text"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VISTA-Bench is a new benchmark designed to evaluate how well vision-language models (VLMs) understand visualized text compared to pure-text queries. The study shows that while VLMs perform well with text alone, their performance drops significantly when the same information is presented as visualized text. This performance gap is influenced by the difficulty of interpreting the visualized text, which varies with rendering conditions. VISTA-Bench aims to identify these weaknesses in VLMs and promote the development of models that can better integrate and understand both text and visual information.', title='Bridging the Gap: Evaluating Vision-Language Models with Visualized Text'))
[11.02.2026 12:51] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VISTA-Bench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁêÜËß£ÂèØËßÜÂåñÊñáÊú¨‰∏éÁ∫ØÊñáÊú¨Êü•ËØ¢ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÁöÑ VLMs Âú®Â§ÑÁêÜÁ∫ØÊñáÊú¨Êü•ËØ¢Êó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Èù¢ÂØπÂèØËßÜÂåñÊñáÊú¨Êó∂ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇËØ•Âü∫ÂáÜÈÄöËøáÂØπÊØîÁ∫ØÊñáÊú¨ÂíåÂèØËßÜÂåñÊñáÊú¨ÈóÆÈ¢òÔºåÊè≠Á§∫‰∫ÜÊ®°ÂûãÂØπÊ∏≤ÊüìÂèòÂåñÁöÑÊïèÊÑüÊÄß„ÄÇVISTA-Bench ‰∏∫ËØäÊñ≠Ëøô‰∏ÄÂ±ÄÈôêÊÄßÊèê‰æõ‰∫ÜÁ≥ªÁªüÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÊó®Âú®Êé®Âä®Êõ¥Áªü‰∏ÄÁöÑËØ≠Ë®ÄË°®Á§∫„ÄÇ","title":"VISTA-BenchÔºöÊè≠Á§∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÂ∑ÆË∑ù"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VISTA-Bench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁêÜËß£ÂèØËßÜÂåñÊñáÊú¨‰∏éÁ∫ØÊñáÊú¨Êü•ËØ¢ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÁöÑ VLMs Âú®Â§ÑÁêÜÁ∫ØÊñáÊú¨Êü•ËØ¢Êó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Èù¢ÂØπÂèØËßÜÂåñÊñáÊú¨Êó∂ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇËØ•Âü∫ÂáÜÈÄöËøáÂØπÊØîÁ∫ØÊñáÊú¨ÂíåÂèØËßÜÂåñÊñáÊú¨ÈóÆÈ¢òÔºåÊè≠Á§∫‰∫ÜÊ®°ÂûãÂØπÊ∏≤ÊüìÂèòÂåñÁöÑÊïèÊÑüÊÄß„ÄÇVISTA-Bench ‰∏∫ËØäÊñ≠Ëøô‰∏ÄÂ±ÄÈôêÊÄßÊèê‰æõ‰∫ÜÁ≥ªÁªüÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÊó®Âú®Êé®Âä®Êõ¥Áªü‰∏ÄÁöÑËØ≠Ë®ÄË°®Á§∫„ÄÇ', title='VISTA-BenchÔºöÊè≠Á§∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÂ∑ÆË∑ù'))
[11.02.2026 12:51] Querying the API.
[11.02.2026 12:51] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.  					AI-generated summary 				 Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-ŒîŒ∏: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ŒîŒ∏C supported only on that circuit (typically <5% of parameters). Applying ŒîŒ∏C yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.
[11.02.2026 12:51] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ offline-–º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –æ—Ç–∫–∞–∑–∞ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Ü–µ–ø–µ–π (circuits), –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞ –æ—Ç–∫–∞–∑, –∏ –ª–æ–∫–∞–ª–∏–∑—É—é—Ç –∏—Ö —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ EAP-IG. –ó–∞—Ç–µ–º –æ–Ω–∏ –≤—ã—á–∏—Å–ª—è—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –º–µ–Ω–µ–µ 5% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –∏ –≤–Ω–µ–¥—Ä—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ —ç—Ç–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π checkpoint –º–æ–∂–µ—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞—Ç—å—Å—è –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏—è—Ö –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø–µ—Ä–µ–º–µ—â–∞—è –≤—Å–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π offline –ø—Ä–æ—Ü–µ—Å—Å.",
  "emoji": "üîß",
  "title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–µ–∑ —Ü–µ–Ω—ã: –æ—Ñ–ª–∞–π–Ω-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Ü–µ–ø–µ–π"
}
```
[11.02.2026 12:51] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.  					AI-generated summary 				 Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-ŒîŒ∏: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ŒîŒ∏C supported only on that circuit (typically <5% of parameters). Applying ŒîŒ∏C yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks."

[11.02.2026 12:51] Response: ```python
["TRAINING", "INFERENCE"]
```
[11.02.2026 12:51] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.  					AI-generated summary 				 Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-ŒîŒ∏: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ŒîŒ∏C supported only on that circuit (typically <5% of parameters). Applying ŒîŒ∏C yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks."

[11.02.2026 12:51] Response: ```python
['SECURITY', 'INTERPRETABILITY', 'ALIGNMENT']
```

**Justification:**

- **SECURITY**: The paper addresses model security through selective refusal mechanisms and safety policy enforcement in LLMs.
- **INTERPRETABILITY**: The paper uses mechanistic understanding and circuit analysis (EAP-IG) to localize and understand refusal-causal computation, which is core to analyzing model behavior.
- **ALIGNMENT**: The paper focuses on aligning LLMs with safety policies and human values by implementing selective refusal to prevent harmful outputs.
[11.02.2026 12:51] Error. Failed to parse JSON from LLM. ["SECURITY", "INTERPRETABILITY", "ALIGNMENT"]


**Justification:**

- **SECURITY**: The paper addresses model security through selective refusal mechanisms and safety policy enforcement in LLMs.
- **INTERPRETABILITY**: The paper uses mechanistic understanding and circuit analysis (EAP-IG) to localize and understand refusal-causal computation, which is core to analyzing model behavior.
- **ALIGNMENT**: The paper focuses on aligning LLMs with safety policies and human values by implementing selective refusal to prevent harmful outputs.
[11.02.2026 12:51] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a method called C-ŒîŒ∏, which allows large language models (LLMs) to refuse certain outputs without needing to intervene during runtime. By using circuit-restricted weight updates, the authors eliminate the need for costly inference-time controls, making the process more efficient. The approach focuses on refining the model\'s parameters offline, which means that the model can be updated once and then used without additional computational costs during operation. The results show that this method maintains the model\'s performance while effectively implementing selective refusal for specific categories.","title":"Efficient Offline Selective Refusal for Large Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a method called C-ŒîŒ∏, which allows large language models (LLMs) to refuse certain outputs without needing to intervene during runtime. By using circuit-restricted weight updates, the authors eliminate the need for costly inference-time controls, making the process more efficient. The approach focuses on refining the model's parameters offline, which means that the model can be updated once and then used without additional computational costs during operation. The results show that this method maintains the model's performance while effectively implementing selective refusal for specific categories.", title='Efficient Offline Selective Refusal for Large Language Models'))
[11.02.2026 12:51] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÂÆûÁé∞Á¶ªÁ∫øÈÄâÊã©ÊÄßÊãíÁªùÔºåÈÄöËøáÁîµË∑ØÈôêÂà∂ÁöÑÊùÉÈáçÊõ¥Êñ∞Êù•Ê∂àÈô§ËøêË°åÊó∂Âπ≤È¢ÑÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫C-ŒîŒ∏ÁöÑÊú∫Âà∂ÔºåÂÆÉÂ∞ÜÊãíÁªùÁõ∏ÂÖ≥ÁöÑËÆ°ÁÆóÂ±ÄÈÉ®Âåñ‰∏∫Á®ÄÁñèÁîµË∑ØÔºåÂπ∂ËÆ°ÁÆó‰ªÖÂú®ËØ•ÁîµË∑Ø‰∏äÊîØÊåÅÁöÑÁ∫¶ÊùüÊùÉÈáçÊõ¥Êñ∞„ÄÇËøôÊ†∑ÂèØ‰ª•Â∞ÜÊàêÊú¨‰ªéÊØèÊ¨°ËØ∑Ê±ÇÁöÑÂπ≤È¢ÑËΩ¨ÁßªÂà∞‰∏ÄÊ¨°ÊÄßÁöÑÁ¶ªÁ∫øÊõ¥Êñ∞„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®ÊãíÁªùÂíåÊïàÁî®Âü∫ÂáÜÊµãËØï‰∏≠‰øùÊåÅ‰∫ÜÁ±ªÂà´ÈíàÂØπÊÄßÁöÑÈÄâÊã©ÊÄßÂíåËÉΩÂäõ„ÄÇ","title":"Á¶ªÁ∫øÈÄâÊã©ÊÄßÊãíÁªùÔºöÈôç‰ΩéÊàêÊú¨ÔºåÊèêÂçáÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÂÆûÁé∞Á¶ªÁ∫øÈÄâÊã©ÊÄßÊãíÁªùÔºåÈÄöËøáÁîµË∑ØÈôêÂà∂ÁöÑÊùÉÈáçÊõ¥Êñ∞Êù•Ê∂àÈô§ËøêË°åÊó∂Âπ≤È¢ÑÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫C-ŒîŒ∏ÁöÑÊú∫Âà∂ÔºåÂÆÉÂ∞ÜÊãíÁªùÁõ∏ÂÖ≥ÁöÑËÆ°ÁÆóÂ±ÄÈÉ®Âåñ‰∏∫Á®ÄÁñèÁîµË∑ØÔºåÂπ∂ËÆ°ÁÆó‰ªÖÂú®ËØ•ÁîµË∑Ø‰∏äÊîØÊåÅÁöÑÁ∫¶ÊùüÊùÉÈáçÊõ¥Êñ∞„ÄÇËøôÊ†∑ÂèØ‰ª•Â∞ÜÊàêÊú¨‰ªéÊØèÊ¨°ËØ∑Ê±ÇÁöÑÂπ≤È¢ÑËΩ¨ÁßªÂà∞‰∏ÄÊ¨°ÊÄßÁöÑÁ¶ªÁ∫øÊõ¥Êñ∞„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂú®ÊãíÁªùÂíåÊïàÁî®Âü∫ÂáÜÊµãËØï‰∏≠‰øùÊåÅ‰∫ÜÁ±ªÂà´ÈíàÂØπÊÄßÁöÑÈÄâÊã©ÊÄßÂíåËÉΩÂäõ„ÄÇ', title='Á¶ªÁ∫øÈÄâÊã©ÊÄßÊãíÁªùÔºöÈôç‰ΩéÊàêÊú¨ÔºåÊèêÂçáÊÄßËÉΩ'))
[11.02.2026 12:51] Using data from previous issue: {"categories": ["#diffusion", "#training", "#optimization", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Temporal Pair Consistency –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Ç–∞–∫
[11.02.2026 12:51] Using data from previous issue: {"categories": ["#security", "#alignment"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "SafePred ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å —Ä–∏—Å–∫–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏
[11.02.2026 12:51] Using data from previous issue: {"categories": ["#interpretability", "#ethics", "#benchmark"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–û—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞ –∫ –∞–Ω–∞–ª–∏–∑—É –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤: –º–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–¥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è SHARP –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–¥–∞ –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑
[11.02.2026 12:51] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#open_source", "#graphs"], "emoji": "üîó", "ru": {"title": "–û—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ: –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "PyAGC –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥
[11.02.2026 12:51] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agents", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è –ø–∞–º—è—Ç—å: –æ—Ç –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", "desc": "ALMA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–µ—Ç–∞–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–∞–º—è—Ç–∏ –≤ –∞–≥
[11.02.2026 12:51] Renaming data file.
[11.02.2026 12:51] Renaming previous data. hf_papers.json to ./d/2026-02-11.json
[11.02.2026 12:51] Saving new data file.
[11.02.2026 12:51] Generating page.
[11.02.2026 12:51] Renaming previous page.
[11.02.2026 12:51] Renaming previous data. index.html to ./d/2026-02-11.html
[11.02.2026 12:51] Writing result.
[11.02.2026 12:51] Renaming log file.
[11.02.2026 12:51] Renaming previous data. log.txt to ./logs/2026-02-11_last_log.txt
