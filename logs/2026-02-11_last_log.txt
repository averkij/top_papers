[11.02.2026 19:53] Read previous papers.
[11.02.2026 19:53] Generating top page (month).
[11.02.2026 19:53] Writing top page (month).
[11.02.2026 20:32] Read previous papers.
[11.02.2026 20:32] Get feed.
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05400
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09856
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09082
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10063
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08234
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09443
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10090
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08426
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07035
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10104
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07022
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09084
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04208
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00268
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09849
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00462
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10098
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06820
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08847
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10102
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09439
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09017
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01244
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08382
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09268
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09823
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09276
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07276
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09662
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07153
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10116
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09024
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09000
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08344
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07839
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07422
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06161
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05435
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02464
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09591
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08503
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07755
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05892
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10099
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09924
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08519
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07670
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04802
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04521
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04908
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01725
[11.02.2026 20:32] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21235
[11.02.2026 20:32] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.02.2026 20:32] No deleted papers detected.
[11.02.2026 20:32] Downloading and parsing papers (pdf, html). Total: 52.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.05400.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.05400.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.05400.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09856.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09856.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09856.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09082.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09082.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09082.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.10063.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.10063.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.10063.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.08234.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.08234.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.08234.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09443.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09443.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09443.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.10090.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.10090.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.10090.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.08426.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.08426.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.08426.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07035.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07035.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07035.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.10104.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.10104.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.10104.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07022.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07022.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07022.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09084.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09084.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09084.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.04208.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.04208.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.04208.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.00268.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.00268.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.00268.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09849.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09849.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09849.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.00462.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.00462.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.00462.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.10098.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.10098.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.10098.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.06820.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.06820.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.06820.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.08847.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.08847.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.08847.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.10102.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.10102.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.10102.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09439.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09439.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09439.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09017.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09017.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09017.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.01244.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.01244.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.01244.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.08382.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.08382.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.08382.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09268.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09268.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09268.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09823.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09823.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09823.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09276.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09276.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09276.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07276.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07276.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07276.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09662.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09662.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09662.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07153.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07153.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07153.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.10116.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.10116.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.10116.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09024.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09024.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09024.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09000.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09000.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09000.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.08344.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.08344.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.08344.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07839.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07839.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07839.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07422.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07422.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07422.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.06161.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.06161.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.06161.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.05435.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.05435.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.05435.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.02464.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.02464.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.02464.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09591.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09591.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09591.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.08503.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.08503.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.08503.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07755.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07755.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07755.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.05892.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.05892.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.05892.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.10099.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.10099.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.10099.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.09924.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.09924.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.09924.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.08519.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.08519.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.08519.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.07670.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.07670.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.07670.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.04802.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.04802.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.04802.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.04521.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.04521.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.04521.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.04908.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.04908.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.04908.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2602.01725.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2602.01725.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2602.01725.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Downloading and parsing paper https://huggingface.co/papers/2601.21235.
[11.02.2026 20:32] Extra JSON file exists (./assets/json/2601.21235.json), skip PDF parsing.
[11.02.2026 20:32] Paper image links file exists (./assets/img_data/2601.21235.json), skip HTML parsing.
[11.02.2026 20:32] Success.
[11.02.2026 20:32] Enriching papers with extra data.
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 0. OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.  					AI-generated summary 				 As hig...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 1. Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.  					AI-generated summary 				 Autonomous GUI agents interact with environments by perceiv...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 2. UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.  					AI-generated summary 				 GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving bo...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 3. A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.  					AI-generated summary 				 Human problem-solving is never the repetition of a ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 4. SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.  					AI-generated summary 				 Large Language Model (LLM) agents have shown stunning results in complex t...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 5. Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.  					AI-generated summary 				 The transition from symbolic ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 6. Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.  					AI-generated summary 				 Recent advances in large lan...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 7. Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.  					AI-generated summary 				 Block-sparse attention is promising for accelerating long-...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 8. Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.  					AI-generated summary 				 Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 9. Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.  					AI-generated summary 				 Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 10. Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.  					AI-generated summary 				 Recent studies have explored autoregressive model...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 11. Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.  					AI-generated summary 				 We study instruction-based image editing under prof...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 12. SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.  					AI-generated summary 				 Vision-Language-Action (VLA) models have e...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 13. Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.  					AI-generated summary 				 Auto-regressive video generation ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 14. BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.  					AI-generated summary 				 Equipping embodied agents with the ability to reason about tasks, foresee ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 15. LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.  					AI-generated summary 				 Transforming a large language model (L...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 16. VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.  					AI-generated summary 				 Pretraining Vision-Language-Action (VLA) policies...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 17. ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.  					AI-generated summary 				 Training generalist agents capable of adapting to diverse scenarios requires interactive environments for se...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 18. Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.  					AI-generated summary 				 Multi-agent LLM systems e...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 19. VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.  					AI-generated summary 				 Lear...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 20. A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.  					AI-generated summary 				 High-quality and open datas...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 21. Contact-Anchored Policies replace language conditioning with physical contact points and use modular utility models for robust manipulation, achieving superior zero-shot performance with minimal demonstration data through real-to-sim iteration cycles.  					AI-generated summary 				 The prevalent pa...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 22. A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.  					AI-generated summ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 23. A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.  					AI-generated summary 				 Large Language Models (LLMs) face significant challenges in long-context proces...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 24. Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms.  					AI-generated summary 				 Diffusion transformers typically incorporate textual information via attention layers...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 25. Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 26. Effective chain-of-thought reasoning strategies reduce intrinsic dimensionality, leading to better generalization by requiring fewer model parameters to achieve given accuracy thresholds.  					AI-generated summary 				 Chain-of-thought (CoT) reasoning and its variants have substantially improved th...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 27. STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.  					AI-generated summary 				 Activation steering has emerged...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 28. TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification.  					AI-generated summary 				 Effectively scaling GUI automation is essential fo...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 29. A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.  					AI-generated summary 				 End-to-end GUI agents for real desktop environments requi...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 30. SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.  					AI-generated summary 				 Real-world data collection for em...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 31. Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.  					AI-generated summary 				 This paper challenges the dominance of continuous pipelines in vi...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 32. Iterative Group Relative Policy Optimization enhances mathematical reasoning in large language models through a two-stage process combining exploratory drafting and refined iterations, achieving state-of-the-art results on competitive benchmarks.  					AI-generated summary 				 Large Language Models...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 33. Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.  					AI-generated summary 				 Parallel thinking has emerged as a new paradigm f...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 34. TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.  					AI-generated summary 				 Planning has become a central capability for contemporary agent systems in navigating c...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 35. SecCoderX uses online reinforcement learning to align large language models for secure code generation while preserving functionality, addressing the functionality-security trade-off through vulnerability detection integration and reward modeling.  					AI-generated summary 				 Large language model...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 36. COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction.  					AI-generated summary 				 Parallel diffusion decoding can...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 37. Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.  					AI-generated summary 				 While flow matching is elegant, its reliance on single-samp...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 38. Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.  					AI-generated summary 				 Activation d...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 39. Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.  					AI-generated summary 				 Reinforcement learning substantially improves reasoning in large language models, b...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 40. Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.  					AI-generated summary 				 Self-correction is essential for solving complex reasoning problems in vision-lang...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 41. ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains.  					AI-generated summary 				 The statelessness of foundation models bottlenecks agentic systems' ability to conti...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 42. ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information.  					AI-generated summary 				 LLM-based coding agents ...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 43. Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling.  					AI-generated summary 				 Leveraging representation encode...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 44. LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance.  					AI-generated summary 				 Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require add...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 45. PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols.  					AI-generated summary 				 Attributed Graph Clustering (AGC) is a fun...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 46. Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.  					AI-generated summary 				 Test-time training (TTT) adapts language models through gradient-based updates at inferen...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 47. VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.  					AI-generated summary 				 Vision-Language Models (VLMs) have achieved impressive performance in cross-mo...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 48. Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.  					AI-generated summary 				 Modern deployments require LLMs to enforce safety policies at scale, yet many controls rel...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 49. Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.  					AI-generated summary 				 Continuous-time generative models,...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 50. SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.  					AI-generated summary 				 With the widespread deployment of Computer-using Agents (CU...
[11.02.2026 20:32] ********************************************************************************
[11.02.2026 20:32] Abstract 51. Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.  					AI-generated summary 				 Large language models (LLMs) are increasingly deployed in high...
[11.02.2026 20:32] Read previous papers.
[11.02.2026 20:32] Generating reviews via LLM API.
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#data", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é", "desc": "OPUS ‚Äî —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–µ–∫—Ü–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –æ–±–Ω–æ–≤–ª
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#agents", "#dataset", "#rlhf", "#multimodal", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞", "desc": "Code2World ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ vision-language, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GUI –ø—É—Ç—ë–º 
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#rl", "#agents", "#benchmark", "#multimodal", "#training"], "emoji": "üñ±Ô∏è", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —á–µ—Ä–µ–∑ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "UI-Venus-1.5 ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#agents", "#architecture"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–∏–∫–∞ Chain of Mindset, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#rl", "#open_source", "#agents", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–û—Ç –æ–ø—ã—Ç–∞ –∫ –Ω–∞–≤—ã–∫–∞–º: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —ç–≤–æ–ª—é—Ü–∏–µ–π", "desc": "SkillRL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM —É–ª—É—á—à–∞—Ç—å —Å–≤–æ—é —Ä–∞–±–æ—Ç—É —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#rl", "#agents", "#multimodal", "#training", "#open_source", "#cv", "#science", "#optimization"], "emoji": "üî¨", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ: VLM –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ–º—å—è –º–æ–¥–µ–ª–µ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#rl", "#agents", "#benchmark"], "emoji": "üåç", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–∏—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Agent World Model (AWM) ‚Äî –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π, –∫–æ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#optimization", "#inference", "#long_context", "#architecture", "#training"], "emoji": "üîç", "ru": {"title": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ —Å–ª–µ–ø–æ—Ç—ã: –∫–∞–∫ Prism —É—Å–∫–æ—Ä—è–µ—Ç LLM —á–µ—Ä–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –±–ª–æ—á–Ω–æ-—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è 
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#open_source", "#agents", "#optimization", "#inference", "#architecture", "#training", "#reasoning", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ DLLM-Searcher –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤—ã
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö –¥–µ–π—Å—Ç–≤
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –¥
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#agents", "#dataset", "#cv", "#benchmark", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ —Å –ø–∞–º—è—Ç—å—é –∏ –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π", "desc": "Agent Banana –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –∞–≥–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#inference", "#robotics", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–º —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –º–æ–¥—É–ª—è—Ü–∏—é –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SCALE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ Vision-Language-Action –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ –º
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#video", "#inference"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –≤ —Ç
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#multimodal", "#training", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —è–∑—ã–∫–∞, –∑—Ä–µ–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —É–º–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞", "desc": "BagelVLA ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–∑—ã–∫–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–∏–∑—É–∞–ª—å–Ω
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#architecture", "#interpretability", "#multimodal"], "emoji": "üîç", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Å–º—ã—Å–ª –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LatentLens ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –ø—É—Ç—ë
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#robotics", "#architecture", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–µ–∑ —É—Ç–µ—á–µ–∫: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å —Ä–æ–±–æ—Ç–∞ –≤–∏–¥–µ—Ç—å —Å—É—Ç—å –¥–µ–π—Å—Ç–≤–∏–π", "desc": "VLA-JEPA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ JEPA, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫ –∑—Ä–∏—Ç–µ–ª—å–Ω
[11.02.2026 20:32] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "ScaleEnv ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –æ–±—É—á–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —Å —Ö–æ—Ä–æ—à–µ–π –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≥–∞—Ä–∞–Ω—Ç–∏
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#training", "#rl", "#agents", "#math", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö LLM —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#open_source", "#agents", "#reasoning", "#video", "#robotics", "#training", "#transfer_learning", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ –∫ –∑–Ω–∞–Ω–∏—è–º: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ –∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "VideoWorld 2 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥–∞–≤–∞
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#diffusion", "#open_source", "#synthetic"], "emoji": "üé®", "ru": {"title": "–í—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç Fine-T2I —Å –±–æ–ª–µ–µ —á–µ–º 6 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#dataset", "#benchmark", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —è–∑—ã–∫–∞ –∫ –∫–∞—Å–∞–Ω–∏—é: –º–æ–¥—É–ª—å–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Ç–æ—á–∫–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Contact-Anchored Policies (CAP) ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∑
[11.02.2026 20:32] Using data from previous issue: {"categories": [], "emoji": "üê≥", "ru": {"title": "–û—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∫ –æ–±—É—á–µ–Ω–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É: –∫–æ–Ω–≤–µ–π–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "TerminalTraj ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ —Å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–º, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#inference", "#rl", "#reasoning", "#optimization", "#rag"], "emoji": "üß†", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –≤–º–µ—Å—Ç–æ —Å—ã—Ä—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: —Å–∂–∞—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ-–≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#multimodal", "#training"], "emoji": "üéõÔ∏è", "ru": {"title": "–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –º–æ–¥—É–ª—è—Ü–∏–∏: –æ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥—É–ª—è—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ embedding –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å 
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#open_source", "#audio", "#benchmark", "#multimodal", "#training", "#reasoning"], "emoji": "üéôÔ∏è", "ru": {"title": "–ö–æ–Ω–µ—Ü —Ä–µ—á–µ–≤–æ–º—É –∫–æ–Ω–≤–µ–π–µ—Ä—É: –ø–æ–ª–Ω–æ–¥—É–ø–ª–µ–∫—Å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Covo-Audio ‚Äî –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞—É–¥–∏–æ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#reasoning", "#interpretability", "#math", "#small_models"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å–∂–∏–º–∞–µ—Ç –∑–∞–¥–∞—á—É: –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç—ã —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π (Chain-of-Thought) –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "üß≠", "ru": {"title": "–ì–∏–±–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è LLM —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤", "desc": "STEER2ADAPT ‚Äî —ç—Ç–æ –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø–æ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#dataset", "#rlhf", "#agents"], "emoji": "üå≥", "ru": {"title": "–î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "TreeCUA ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏
[11.02.2026 20:32] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Anchor –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —ç—Ç–∞–ª–æ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#open_source", "#agents", "#dataset", "#synthetic", "#robotics", "#3d", "#reasoning"], "emoji": "üè†", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D —Å—Ü–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "SAGE ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω, –≥–æ—Ç–æ–≤—ã
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv"], "emoji": "üé®", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –º–æ–≥—É—Ç –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#math", "#alignment", "#rlhf", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ —á–µ—Ä–Ω–æ–≤–∏–∫–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Iterative Group Relative Policy Opt
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#reasoning", "#math"], "emoji": "üß≠", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#benchmark", "#training", "#agents", "#architecture", "#rl"], "emoji": "üèóÔ∏è", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∏–Ω—Ç–µ–∑ –∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "TodoEvolve –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ç–∞-–ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞–¥
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#rl", "#security", "#open_source", "#alignment", "#plp", "#optimization"], "emoji": "üîê", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫–æ–¥ –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è LLM-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞", "desc": "SecCoderX ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#diffusion", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —É–º–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –∫—ç—à–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ COVER –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#multimodal", "#diffusion", "#open_source", "#video", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è flow matching —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —É—Å–ª–æ–≤–Ω—ã—Ö —Å–∫–æ—Ä–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Stable Velocity –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—Å–æ–∫–æ–π –¥
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#training", "#architecture"], "emoji": "üß≤", "ru": {"title": "–õ–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π: –æ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∫ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Mixture of Factor Analyzers (MFA) –∫–∞–∫ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±–Ω–∞—Ä
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#small_models", "#rl", "#optimization", "#training", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å: –∫–∞–∫ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª–∏–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#multimodal", "#open_source", "#cv", "#synthetic", "#optimization"], "emoji": "üêô", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –≤ vision-language –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Octopus –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#reasoning", "#agents", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∞—è—Å—è –ø–∞–º—è—Ç—å: –æ—Ç –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", "desc": "ALMA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–µ—Ç–∞–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–∞–º—è—Ç–∏ –≤ –∞–≥
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#plp", "#agents"], "emoji": "üîç", "ru": {"title": "–ê–Ω–∞–ª–∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –∫–æ–¥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö: —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ ContextBench ‚Äî –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ–≥–æ, –∫–∞–∫ –∞–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#math", "#optimization", "#open_source", "#diffusion", "#architecture"], "emoji": "üåê", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ: –ø–æ—Ç–æ–∫ –Ω–∞ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏ –≤–º–µ—Å—Ç–æ –µ–≤–∫–ª–∏–¥–æ–≤–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö, –∑
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#benchmark", "#math", "#inference"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª–∏ —Å–∞–º–∏ –∑–Ω–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á–∞ —Å–ª–æ–∂–Ω–∞ –¥–ª—è –Ω–∏—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM –∫–æ–¥–∏—Ä—É—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ –≤ —Å–≤–æ–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö –¥–æ —Ç–æ–≥–æ, –∫–∞–∫ –Ω–∞—á–Ω—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –û–Ω–∏ –æ–±—É—á
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#open_source", "#graphs"], "emoji": "üîó", "ru": {"title": "–û—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–∞–∫—Ç–∏–∫–µ: –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –∏–Ω–¥—É—Å—Ç—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "PyAGC –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#inference"], "emoji": "üéØ", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤—ã–±–æ—Ä–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏—é: surprisal-guided selection –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º, –≥–¥–µ –¥–æ—Å—Ç—É–ø–Ω—ã –ø–ª–æ—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#dataset"], "emoji": "üìñ", "ru": {"title": "–ú–æ–¥–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤: –∫–∞–∫ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ç–µ—Ä—è—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω VISTA-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#training", "#inference"], "emoji": "üîß", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–µ–∑ —Ü–µ–Ω—ã: –æ—Ñ–ª–∞–π–Ω-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Ü–µ–ø–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ offline-–º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –æ—Ç–∫–∞–∑–∞ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#diffusion", "#training", "#optimization", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Temporal Pair Consistency –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Ç–∞–∫
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#security", "#alignment"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "SafePred ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å —Ä–∏—Å–∫–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏
[11.02.2026 20:32] Using data from previous issue: {"categories": ["#interpretability", "#ethics", "#benchmark"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–û—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞ –∫ –∞–Ω–∞–ª–∏–∑—É –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤: –º–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–¥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è SHARP –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–¥–∞ –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑
[11.02.2026 20:32] Renaming data file.
[11.02.2026 20:32] Renaming previous data. hf_papers.json to ./d/2026-02-11.json
[11.02.2026 20:32] Saving new data file.
[11.02.2026 20:32] Generating page.
[11.02.2026 20:32] Renaming previous page.
[11.02.2026 20:32] Renaming previous data. index.html to ./d/2026-02-11.html
[11.02.2026 20:32] Writing result.
[11.02.2026 20:32] Renaming log file.
[11.02.2026 20:32] Renaming previous data. log.txt to ./logs/2026-02-11_last_log.txt
