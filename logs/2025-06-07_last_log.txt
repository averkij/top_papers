[07.06.2025 06:33] Read previous papers.
[07.06.2025 06:33] Generating top page (month).
[07.06.2025 06:33] Writing top page (month).
[07.06.2025 12:46] Read previous papers.
[07.06.2025 12:46] Get feed.
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05301
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05010
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05284
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04308
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05229
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05176
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02865
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05209
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23656
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05240
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05349
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05328
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05345
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04633
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05344
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03077
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05287
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05331
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05334
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05327
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02620
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04734
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04209
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01011
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05348
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00830
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20914
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05333
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04598
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04405
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04245
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05282
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05278
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04956
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03643
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02751
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05313
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05046
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04559
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02587
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23115
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04996
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.04462
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.03238
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.02444
[07.06.2025 12:46] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00981
[07.06.2025 12:46] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.06.2025 12:46] No deleted papers detected.
[07.06.2025 12:46] Downloading and parsing papers (pdf, html). Total: 46.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05301.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05301.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05301.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05010.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05010.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05010.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05284.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05284.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05284.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04308.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04308.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04308.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05229.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05229.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05229.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05176.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05176.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05176.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.02865.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.02865.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.02865.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05209.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05209.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05209.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2505.23656.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2505.23656.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2505.23656.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05240.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05240.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05240.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05349.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05349.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05349.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05328.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05328.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05328.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05345.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05345.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05345.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04633.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04633.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04633.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05344.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05344.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05344.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.03077.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.03077.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.03077.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05287.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05287.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05287.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05331.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05331.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05331.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05334.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05334.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05334.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05327.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05327.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05327.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.02620.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.02620.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.02620.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04734.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04734.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04734.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04209.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04209.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04209.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.01011.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.01011.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.01011.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05348.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05348.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05348.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.00830.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.00830.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.00830.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2505.20914.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2505.20914.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2505.20914.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05333.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05333.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05333.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04598.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04598.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04598.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04405.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04405.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04405.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04245.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04245.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04245.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05282.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05282.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05282.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05278.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05278.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05278.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04956.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04956.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04956.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.03643.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.03643.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.03643.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.02751.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.02751.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.02751.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05313.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05313.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05313.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.05046.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.05046.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.05046.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04559.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04559.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04559.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.02587.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.02587.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.02587.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2505.23115.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2505.23115.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2505.23115.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04996.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04996.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04996.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.04462.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.04462.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.04462.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.03238.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.03238.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.03238.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.02444.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.02444.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.02444.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Downloading and parsing paper https://huggingface.co/papers/2506.00981.
[07.06.2025 12:46] Extra JSON file exists (./assets/json/2506.00981.json), skip PDF parsing.
[07.06.2025 12:46] Paper image links file exists (./assets/img_data/2506.00981.json), skip HTML parsing.
[07.06.2025 12:46] Success.
[07.06.2025 12:46] Enriching papers with extra data.
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 0. SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.  					AI-generated summary 				 Recent advances in diffusion-based video restoratio...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 1. ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  					AI-generated summary 				 We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficie...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 2. A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.  					AI-generated summary 				 Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, am...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 3. Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 4. Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 5. In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and gene...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 6. Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.  					AI-generated summary 				 We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-d...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 7. The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.  					AI-generated summary 				 Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 8. Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the re...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 9. This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 10. VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.  					AI-generated summary 				 Mathematical reasoning in re...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 11. Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting ben...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 12. Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.  					AI-generated summary 				 Inference-time scaling trades efficiency for increa...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 13. Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We in...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 14. Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 15. StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  					AI-generated summary 				 Training language models on long sequen...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 16. The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benc...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 17. Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical ...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 18. Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  					AI-generated summary 				 Search-augmented language models combine web search with Large Language Mod...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 19. Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinu...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 20. FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  					AI-generated summary 				 Texture map production i...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 21. Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant ...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 22. Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  					AI-generated summary 				 Currently, th...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 23. A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  					AI-generated summary 				 Autoregressive (AR) image generation mod...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 24. A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.  					AI-generated summary 				 This paper addresses the challenge of reconstructing...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 25. SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  					AI-generated summary 				 The generation and...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 26. General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 27. Inference with small models is less efficient due to memory bottlenecks, leading to a new Kinetics Scaling Law emphasizing sparse attention for better test-time performance.  					AI-generated summary 				 We rethink test-time scaling laws from a practical efficiency perspective, revealing that the ...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 28. Scaling laws are derived for CLIP and MaMMUT to compare their performance and sample efficiency across different scales and datasets.  					AI-generated summary 				 In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 29. We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. ...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 30. As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent n...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 31. We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points ...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 32. A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) systems commonly suffer from Kn...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 33. Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity fr...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 34. Most existing vision encoders map images into a fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, a visually complex image (e.g., a cluttered room) inherently carries more information and thus deserves more tokens than a ...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 35. RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.  					AI-generated summary 				 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in no...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 36. MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  					AI-generated summary 				 Editing materials of objects in images based on exemplar images is an acti...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 37. FlowDirector, an inversion-free video editing framework, uses ODEs for spatiotemporal coherent editing and attention-guided masking for localized control, achieving state-of-the-art performance.  					AI-generated summary 				 Text-driven video editing aims to modify video content according to natur...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 38. A reasoning-aligned reinforcement learning strategy enhances visual representations in multi-modal large language models by optimizing captions for downstream reasoning tasks.  					AI-generated summary 				 Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demo...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 39. BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.  					AI-generated summary 				 Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 40. Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative mod...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 41. PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.  					AI-generated summary 				 Automated sports skill assessment requires capturing fundamental moveme...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 42. Watermarking techniques in LLMs can degrade truthfulness, safety, and helpfulness, and alignment resampling is proposed to restore alignment while ensuring watermark detectability.  					AI-generated summary 				 Watermarking techniques for large language models (LLMs) can significantly impact outpu...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 43. OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  					AI-generated summary 				 Automated interpretation of CT images-particularly localizin...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 44. A framework combining visual priors and dynamic constraints within a synchronized diffusion process generates HOI video and motion simultaneously, enhancing video-motion consistency and generalization.  					AI-generated summary 				 Hand-Object Interaction (HOI) generation has significant applicati...
[07.06.2025 12:46] ********************************************************************************
[07.06.2025 12:46] Abstract 45. Self-supervised Wav2Vec2 models encode Dutch linguistic features more accurately when pre-trained exclusively on Dutch data, compared to similar amounts of English or multilingual data, as shown by clustering and classification probes, and demonstrated through improved Automatic Speech Recognition p...
[07.06.2025 12:46] Read previous papers.
[07.06.2025 12:46] Generating reviews via LLM API.
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#training", "#video", "#optimization"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ –∑–∞ –æ–¥–∏–Ω —à–∞–≥", "desc": "SeedVR2 - —ç—Ç–æ –æ–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ —Ñ—É–Ω–∫
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#open_source"], "emoji": "üé®", "ru": {"title": "–ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞: ComfyUI-Copilot —É–ø—Ä–æ—â–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞", "desc": "ComfyUI-Copilot - —ç—Ç–æ –ø–ª–∞–≥–∏–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#video", "#dataset", "#3d", "#long_context"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª—è—Ö –º–∏—Ä–∞", "desc": "–ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ª—É—á—à–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –º–µ—Ö–∞–Ω–∏–∑–º –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#robotics", "#reasoning", "#dataset", "#3d", "#training", "#rl"], "emoji": "ü§ñ", "ru": {"title": "RoboRefer: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "RoboRefer - —ç—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#architecture", "#inference", "#optimization", "#training", "#long_context"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#multilingual", "#open_source", "#training", "#small_models", "#low_resource"], "emoji": "üîç", "ru": {"title": "Qwen3 Embedding: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Qwen3 Embedding, —É–ª—É—á—à–∞
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#synthetic", "#dataset", "#open_source", "#agents", "#data", "#benchmark"], "emoji": "üèÑ", "ru": {"title": "Surfer-H –∏ Holo1: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Surfer-H - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–≥–æ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#dataset", "#ethics", "#open_source", "#data"], "emoji": "üìö", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–∏—á–Ω–æ–≥–æ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Common Pile v0.1 - 8-—Ç–µ—Ä–∞–±–∞–π—Ç–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤ —Å –æ—Ç–∫—Ä—ã—Ç–æ–π –ª–∏—Ü–µ–Ω–∑–∏–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#training", "#video", "#diffusion"], "emoji": "üé•", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ–µ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞: VideoREPA —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–∑–∏–∫–∏ –≤ –º–æ–¥–µ–ª—è—Ö T2V", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ VideoREPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≤–∏
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#alignment", "#training", "#cv", "#math", "#diffusion"], "emoji": "üîÑ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –ø–æ–º–æ—â—å—é –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –æ–±—É—á–∞–µ–º—ã—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#math", "#multimodal", "#benchmark", "#transfer_learning", "#reasoning", "#video"], "emoji": "üßÆ", "ru": {"title": "VideoMathQA: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ", "desc": "VideoMathQA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –∫—Ä–æ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#reasoning", "#dataset", "#long_context", "#training", "#video", "#rl"], "emoji": "üßÆ", "ru": {"title": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Å—á–µ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CG-AV-Counting –¥–ª—è –∑
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#inference"], "emoji": "üß†", "ru": {"title": "–ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å: —Å–∂–∞—Ç–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ (Dynamic Memory Sparsification, DMS) 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "STARE: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ STARE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª—å
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#architecture", "#open_source", "#inference"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (ML
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization"], "emoji": "üöÄ", "ru": {"title": "StreamBP: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "StreamBP - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏ –≤–¥–æ–ª—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#interpretability", "#benchmark", "#games"], "emoji": "üëÅÔ∏è", "ru": {"title": "EOC-Bench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò –≤ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ EOC-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#reasoning", "#math", "#games"], "emoji": "üßÆ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Å MINT-CoT", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MINT-CoT - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#alignment", "#data", "#multilingual", "#dataset", "#open_source"], "emoji": "üîç", "ru": {"title": "–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –ò–ò-—Å–∏—Å—Ç–µ–º–∞–º–∏, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –ø–æ–∏—Å–∫–æ–º", "desc": "Search Arena - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#3d", "#architecture"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–Ω–∏—Ü –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö 3D Gaussian Splatting. –ê–≤—Ç–æ—Ä—ã 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "FlexPainter: –≥–∏–±–∫–∞—è –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç—É—Ä —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º", "desc": "FlexPainter - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—â–µ–µ —É—Å–ª–æ–≤–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –≥–∏–±–∫–æ–≥–æ 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#open_source", "#training", "#math"], "emoji": "üé¢", "ru": {"title": "–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫: –≤—ã–∑–æ–≤ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å–µ—Ä–∏–∏ Deepseek-R1-Distill –ø–æ–¥–≤–µ—Ä–∂–µ–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º –∫–æ–ª–µ–±–∞–Ω
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#long_context", "#alignment", "#cv"], "emoji": "üîç", "ru": {"title": "–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π —ç–Ω–∫–æ–¥–µ—Ä —É–ª—É—á—à–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ LIFT –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#cv", "#security", "#video"], "emoji": "üîê", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π - Lexical Bias Wat
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#3d"], "emoji": "üîÑ", "ru": {"title": "–°–≤–æ–±–æ–¥–Ω–æ–µ –≤—Ä–µ–º—è –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –≥–∞—É—Å—Å–æ–≤—ã—Ö –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω–∞—Ö", "desc": "FreeTimeGS - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ 4D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∞—É—Å—Å–æ–≤—ã–º –ø—Ä–∏–º–∏—Ç–∏–≤–∞–º –ø–æ—è–≤–ª—è—Ç—å—Å—è –≤ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#audio", "#diffusion", "#synthetic", "#video"], "emoji": "üé≠", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –≥–æ–≤–æ—Ä—è—â–∏—Ö –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤: –æ—Ç –∞—É–¥–∏–æ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–º—É –≤–∏–¥–µ–æ", "desc": "SkyReels-Audio - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#diffusion"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –≤ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) –¥–ª—è –∫–æ–º–ø–æ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#small_models"], "emoji": "üî¨", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è Kinetics, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, —Ç–∞–∫ –∏ –∑–∞—Ç
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#transfer_learning", "#training", "#dataset", "#open_source"], "emoji": "üìä", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –∑–∞–∫–æ–Ω—ã –∫–∞–∫ –∫–ª—é—á –∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –∑–∞–∫–æ–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π CLIP –∏ MaMMUT, 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#dataset", "#open_source", "#training", "#rl"], "emoji": "ü©∫", "ru": {"title": "MedAgentGYM: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "MedAgentGYM - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ä–µ–¥–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —É –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#synthetic", "#agents", "#benchmark", "#reasoning", "#dataset", "#transfer_learning", "#rl", "#leakage"], "emoji": "üîê", "ru": {"title": "–†–∞–∑—É–º–Ω–æ–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –æ–±—É—á–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ (CI) –≤ —ç–ø–æ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#dataset", "#3d", "#benchmark"], "emoji": "üß©", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏ —Å–±–æ—Ä–∫–µ —Ñ–æ—Ä–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Rectified Point Flow, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏ —Å–±–æ—Ä–∫—É –º–Ω–æ–≥–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω—ã—Ö —Ñ–æ—Ä–º –≤ –µ–¥–∏–Ω—É—é —É—Å–ª–æ–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –∑–∞–¥–∞
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "Micro-Act: —É–º–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–§—Ä–µ–π–º–≤–æ—Ä–∫ Micro-Act —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∑–Ω–∞–Ω–∏–π –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (RAG). –û–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω–æ 
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#video", "#architecture", "#open_source", "#optimization", "#training"], "emoji": "üè•", "ru": {"title": "FEAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–∏–¥–µ–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω FEAT - –Ω–æ–≤—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–∏–¥–µ–æ. FEAT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#cv", "#architecture", "#optimization", "#multimodal"], "emoji": "ü¶Ö", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "DOVE - —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–µ–∫–æ–Ω—Å
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#3d"], "emoji": "üåü", "ru": {"title": "–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ 3D-—Å—Ü–µ–Ω–∞—Ö —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤", "desc": "RobustSplat - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, –≤—ã–∑–≤–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ —Å—Ü–µ–Ω–∞—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ—Ç–ª–æ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#cv", "#games", "#optimization"], "emoji": "üé®", "ru": {"title": "–£–º–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "MARBLE - —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏—Ö —Å–≤–æ–π—Å—Ç–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ CLIP. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#games", "#video", "#multimodal", "#benchmark", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ü–ª–∞–≤–Ω–æ–µ –∏ —Ç–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –û–î–£ –∏ –º–∞—Å–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "FlowDirector - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –∏–Ω–≤–µ—Ä—Å–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω—ã–µ –¥–∏—Ñ—Ñ–µ—Ä–µ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#multimodal", "#rl", "#rag", "#cv"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ò–ò —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ RACRO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#robotics", "#cv", "#dataset", "#optimization", "#open_source"], "emoji": "üöó", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–∞–ª–∏–±—Ä–æ–≤–∫–µ LiDAR-–∫–∞–º–µ—Ä—ã —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É", "desc": "BEVCALIB - —ç—Ç–æ –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∏–¥–∞ —Å–≤–µ—Ä—Ö—É –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ LiDAR-–∫–∞–º–µ—Ä—ã –ø–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º. –ú
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#diffusion", "#agents", "#3d", "#cv"], "emoji": "üöó", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —É–ª—É—á—à–∞—é—Ç 3D-–≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –∫–∞—Ä—Ç –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–µ –∞–≤—Ç–æ–Ω
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#video"], "emoji": "üèãÔ∏è", "ru": {"title": "PATS: –£–º–Ω–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞", "desc": "PATS (Proficiency-Aware Temporal Sampling) - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤. –û–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–∞—Ö–≤–∞—Ç –ø–æ–ª–Ω—ã—Ö –ø–∞—Ç—Ç–µ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#hallucinations", "#alignment", "#rlhf", "#inference"], "emoji": "üîí", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≤–æ–¥—è–Ω—ã–º–∏ –∑–Ω–∞–∫–∞–º–∏ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –≤—ã—è–≤–ª—è—è –∏—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å, –±
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#healthcare", "#cv"], "emoji": "üî¨", "ru": {"title": "–ò–ò-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ö–¢-—Å–Ω–∏–º–∫–æ–≤", "desc": "–ú–æ–¥–µ–ª—å OminiAbnorm-CT –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ö–¢-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–ø–∏
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#architecture", "#video", "#multimodal", "#3d", "#games", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤–∑–∞–∏–º–æ
[07.06.2025 12:46] Using data from previous issue: {"categories": ["#transfer_learning", "#multilingual", "#audio", "#low_resource"], "emoji": "üó£Ô∏è", "ru": {"title": "–Ø–∑—ã–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ Wav2Vec2, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –≥–æ–ª–ª–∞–Ω–¥—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ª—É—á—à–µ –∫–æ–¥–∏—Ä—É—é—Ç –ª
[07.06.2025 12:46] Trying to get texts in Chinese.
[07.06.2025 12:46] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.  					AI-generated summary 				 Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.
[07.06.2025 12:46] Mistral response. {"id": "4598f7c0600142fca59dabffffcea6c4", "object": "chat.completion", "created": 1749300372, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a SeedVR2 \u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\u3002\u5b83\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u548c\u9002\u5e94\u6027\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u6765\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cSeedVR2 \u5728\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002\u901a\u8fc7\u7279\u5f81\u5339\u914d\u635f\u5931\uff0c\u5b83\u5728\u4e00\u6b65\u5185\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSeedVR2 \u7684\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8d8a\u73b0\u6709\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 345, "total_tokens": 503, "completion_tokens": 158}}
[07.06.2025 12:46] Response: ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ SeedVR2 ÁöÑËßÜÈ¢ë‰øÆÂ§çÊ®°Âûã„ÄÇÂÆÉ‰ΩøÁî®Âü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÂíåÈÄÇÂ∫îÊÄßÁ™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSeedVR2 Âú®ËÆ°ÁÆóÊàêÊú¨ËæÉ‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõÈ´òËßÜËßâË¥®Èáè„ÄÇÈÄöËøáÁâπÂæÅÂåπÈÖçÊçüÂ§±ÔºåÂÆÉÂú®‰∏ÄÊ≠•ÂÜÖÂÆûÁé∞‰∫ÜÁ®≥ÂÆö‰∏îÈ´òÊïàÁöÑÂØπÊäóÊÄßËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSeedVR2 ÁöÑÊÄßËÉΩÂ™≤ÁæéÊàñË∂ÖË∂äÁé∞ÊúâÁöÑËßÜÈ¢ë‰øÆÂ§çÊñπÊ≥ï„ÄÇ
[07.06.2025 12:46] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ SeedVR2 ÁöÑËßÜÈ¢ë‰øÆÂ§çÊ®°Âûã„ÄÇÂÆÉ‰ΩøÁî®Âü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÂíåÈÄÇÂ∫îÊÄßÁ™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSeedVR2 Âú®ËÆ°ÁÆóÊàêÊú¨ËæÉ‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõÈ´òËßÜËßâË¥®Èáè„ÄÇÈÄöËøáÁâπÂæÅÂåπÈÖçÊçüÂ§±ÔºåÂÆÉÂú®‰∏ÄÊ≠•ÂÜÖÂÆûÁé∞‰∫ÜÁ®≥ÂÆö‰∏îÈ´òÊïàÁöÑÂØπÊäóÊÄßËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSeedVR2 ÁöÑÊÄßËÉΩÂ™≤ÁæéÊàñË∂ÖË∂äÁé∞ÊúâÁöÑËßÜÈ¢ë‰øÆÂ§çÊñπÊ≥ï„ÄÇ
[07.06.2025 12:46] Mistral response. {"id": "7d2f387d9b714b1380271deab6fe0ea1", "object": "chat.completion", "created": 1749300374, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a SeedVR2 \u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\u3002\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng ji\u00e8 sh\u00e0o le y\u012b zh\u01d2ng m\u00edng w\u00e8i SeedVR2 de sh\u00ec p\u01d0n xi\u016b f\u00f9 m\u00f3 x\u00edng.\n\n\u5b83\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u548c\u9002\u5e94\u6027\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u6765\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u3002\nt\u0101 sh\u01d0 y\u00f2ng j\u012b y\u00fa ku\u00f2 s\u00e0n de f\u0101ng f\u01ce h\u00e9 sh\u00ec y\u00ecng x\u00ecng chu\u0101ng k\u01d2u zh\u00f9 y\u00ec l\u00ec j\u012b zh\u00ec l\u00e1i ch\u01d4 l\u01d0 g\u0101o f\u0113n bi\u00e0n l\u01dc sh\u00ec p\u01d0n.\n\n\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cSeedVR2 \u5728\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002\ny\u01d4 xi\u00e0n y\u01d2u f\u0101ng f\u01ce xi\u0101ng b\u01d0, SeedVR2 z\u00e0i j\u00ec su\u00e0n ch\u00e9ng b\u011bn ji\u00e0o d\u012b de q\u00edng ku\u00e0ng xi\u00e0 t\u00ed g\u014dng g\u0101o sh\u00ec ju\u00e9 zh\u00ec li\u00e0ng.\n\n\u901a\u8fc7\u7279\u5f81\u5339\u914d\u635f\u5931\uff0c\u5b83\u5728\u4e00\u6b65\u5185\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u3002\nt\u014dng gu\u00f2 t\u00e8 zh\u0113ng p\u01d0 p\u00e8i s\u01d4n sh\u012b, t\u0101 z\u00e0i y\u012b b\u00f9 n\u00e8i sh\u00ed xi\u00e0n le w\u011bn d\u00ecng qi\u011b g\u0101o xi\u00e0o de du\u00ec k\u00e0ng x\u00ecng x\u00f9n li\u00e0n.\n\n\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSeedVR2 \u7684\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8d8a\u73b0\u6709\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u3002\nsh\u00ed y\u00e0n ji\u00e9 gu\u01d2 xi\u01cen sh\u00ec, SeedVR2 de x\u00ecng n\u00e9ng j\u012b m\u011bi hu\u00f2 ch\u0101o yu\u00e8 xi\u00e0n y\u01d2u de sh\u00ec p\u01d0n xi\u016b f\u00f9 f\u0101ng f\u01ce."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 172, "total_tokens": 681, "completion_tokens": 509}}
[07.06.2025 12:46] Response: ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ SeedVR2 ÁöÑËßÜÈ¢ë‰øÆÂ§çÊ®°Âûã„ÄÇ
zh√® piƒÅn w√©n zhƒÅng ji√® sh√†o le yƒ´ zh«íng m√≠ng w√®i SeedVR2 de sh√¨ p«ên xi≈´ f√π m√≥ x√≠ng.

ÂÆÉ‰ΩøÁî®Âü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÂíåÈÄÇÂ∫îÊÄßÁ™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáËßÜÈ¢ë„ÄÇ
tƒÅ sh«ê y√≤ng jƒ´ y√∫ ku√≤ s√†n de fƒÅng f«é h√© sh√¨ y√¨ng x√¨ng chuƒÅng k«íu zh√π y√¨ l√¨ jƒ´ zh√¨ l√°i ch«î l«ê gƒÅo fƒìn bi√†n l«ú sh√¨ p«ên.

‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSeedVR2 Âú®ËÆ°ÁÆóÊàêÊú¨ËæÉ‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõÈ´òËßÜËßâË¥®Èáè„ÄÇ
y«î xi√†n y«íu fƒÅng f«é xiƒÅng b«ê, SeedVR2 z√†i j√¨ su√†n ch√©ng bƒõn ji√†o dƒ´ de q√≠ng ku√†ng xi√† t√≠ g≈çng gƒÅo sh√¨ ju√© zh√¨ li√†ng.

ÈÄöËøáÁâπÂæÅÂåπÈÖçÊçüÂ§±ÔºåÂÆÉÂú®‰∏ÄÊ≠•ÂÜÖÂÆûÁé∞‰∫ÜÁ®≥ÂÆö‰∏îÈ´òÊïàÁöÑÂØπÊäóÊÄßËÆ≠ÁªÉ„ÄÇ
t≈çng gu√≤ t√® zhƒìng p«ê p√®i s«în shƒ´, tƒÅ z√†i yƒ´ b√π n√®i sh√≠ xi√†n le wƒõn d√¨ng qiƒõ gƒÅo xi√†o de du√¨ k√†ng x√¨ng x√πn li√†n.

ÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSeedVR2 ÁöÑÊÄßËÉΩÂ™≤ÁæéÊàñË∂ÖË∂äÁé∞ÊúâÁöÑËßÜÈ¢ë‰øÆÂ§çÊñπÊ≥ï„ÄÇ
sh√≠ y√†n ji√© gu«í xi«én sh√¨, SeedVR2 de x√¨ng n√©ng jƒ´ mƒõi hu√≤ chƒÅo yu√® xi√†n y«íu de sh√¨ p«ên xi≈´ f√π fƒÅng f«é.
[07.06.2025 12:46] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ SeedVR2 ÁöÑËßÜÈ¢ë‰øÆÂ§çÊ®°Âûã„ÄÇÂÆÉ‰ΩøÁî®Âü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÂíåÈÄÇÂ∫îÊÄßÁ™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSeedVR2 Âú®ËÆ°ÁÆóÊàêÊú¨ËæÉ‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõÈ´òËßÜËßâË¥®Èáè„ÄÇÈÄöËøáÁâπÂæÅÂåπÈÖçÊçüÂ§±ÔºåÂÆÉÂú®‰∏ÄÊ≠•ÂÜÖÂÆûÁé∞‰∫ÜÁ®≥ÂÆö‰∏îÈ´òÊïàÁöÑÂØπÊäóÊÄßËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSeedVR2 ÁöÑÊÄßËÉΩÂ™≤ÁæéÊàñË∂ÖË∂äÁé∞ÊúâÁöÑËßÜÈ¢ë‰øÆÂ§çÊñπÊ≥ï„ÄÇ
[07.06.2025 12:46] Mistral response. {"id": "33757cc256a64b78a0ec5fab294cbea6", "object": "chat.completion", "created": 1749300380, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u6269\u6563\", \"pinyin\": \"ku\u00f2 s\u00e0n\", \"trans\": \"diffusion\"},\n    {\"word\": \"\u9002\u5e94\u6027\", \"pinyin\": \"sh\u00ec y\u00ecng x\u00ecng\", \"trans\": \"adaptive\"},\n    {\"word\": \"\u7a97\u53e3\", \"pinyin\": \"chu\u0101ng k\u01d2u\", \"trans\": \"window\"},\n    {\"word\": \"\u6ce8\u610f\u529b\", \"pinyin\": \"zh\u00f9 y\u00ec l\u00ec\", \"trans\": \"attention\"},\n    {\"word\": \"\u673a\u5236\", \"pinyin\": \"j\u012b zh\u00ec\", \"trans\": \"mechanism\"},\n    {\"word\": \"\u9ad8\u5206\u8fa8\u7387\", \"pinyin\": \"g\u0101o f\u0113n bi\u00e0n l\u01dc\", \"trans\": \"high resolution\"},\n    {\"word\": \"\u8ba1\u7b97\u6210\u672c\", \"pinyin\": \"j\u00ec su\u00e0n ch\u00e9ng b\u011bn\", \"trans\": \"computational cost\"},\n    {\"word\": \"\u89c6\u89c9\u8d28\u91cf\", \"pinyin\": \"sh\u00ec ju\u00e9 zh\u00ec li\u00e0ng\", \"trans\": \"visual quality\"},\n    {\"word\": \"\u7279\u5f81\u5339\u914d\", \"pinyin\": \"t\u00e8 zh\u0113ng p\u01d0 p\u00e8i\", \"trans\": \"feature matching\"},\n    {\"word\": \"\u635f\u5931\", \"pinyin\": \"s\u01d4n sh\u012b\", \"trans\": \"loss\"},\n    {\"word\": \"\u5bf9\u6297\u6027\", \"pinyin\": \"du\u00ec k\u00e0ng x\u00ecng\", \"trans\": \"adversarial\"},\n    {\"word\": \"\u8bad\u7ec3\", \"pinyin\": \"x\u00f9n li\u00e0n\", \"trans\": \"training\"},\n    {\"word\": \"\u7a33\u5b9a\", \"pinyin\": \"w\u011bn d\u00ecng\", \"trans\": \"stable\"},\n    {\"word\": \"\u9ad8\u6548\", \"pinyin\": \"g\u0101o xi\u00e0o\", \"trans\": \"efficient\"},\n    {\"word\": \"\u5ab2\u7f8e\", \"pinyin\": \"p\u00ec m\u011bi\", \"trans\": \"rival\"},\n    {\"word\": \"\u8d85\u8d8a\", \"pinyin\": \"ch\u0101o yu\u00e8\", \"trans\": \"surpass\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 204, "total_tokens": 713, "completion_tokens": 509}}
[07.06.2025 12:46] Response: [
    {"word": "Êâ©Êï£", "pinyin": "ku√≤ s√†n", "trans": "diffusion"},
    {"word": "ÈÄÇÂ∫îÊÄß", "pinyin": "sh√¨ y√¨ng x√¨ng", "trans": "adaptive"},
    {"word": "Á™óÂè£", "pinyin": "chuƒÅng k«íu", "trans": "window"},
    {"word": "Ê≥®ÊÑèÂäõ", "pinyin": "zh√π y√¨ l√¨", "trans": "attention"},
    {"word": "Êú∫Âà∂", "pinyin": "jƒ´ zh√¨", "trans": "mechanism"},
    {"word": "È´òÂàÜËæ®Áéá", "pinyin": "gƒÅo fƒìn bi√†n l«ú", "trans": "high resolution"},
    {"word": "ËÆ°ÁÆóÊàêÊú¨", "pinyin": "j√¨ su√†n ch√©ng bƒõn", "trans": "computational cost"},
    {"word": "ËßÜËßâË¥®Èáè", "pinyin": "sh√¨ ju√© zh√¨ li√†ng", "trans": "visual quality"},
    {"word": "ÁâπÂæÅÂåπÈÖç", "pinyin": "t√® zhƒìng p«ê p√®i", "trans": "feature matching"},
    {"word": "ÊçüÂ§±", "pinyin": "s«în shƒ´", "trans": "loss"},
    {"word": "ÂØπÊäóÊÄß", "pinyin": "du√¨ k√†ng x√¨ng", "trans": "adversarial"},
    {"word": "ËÆ≠ÁªÉ", "pinyin": "x√πn li√†n", "trans": "training"},
    {"word": "Á®≥ÂÆö", "pinyin": "wƒõn d√¨ng", "trans": "stable"},
    {"word": "È´òÊïà", "pinyin": "gƒÅo xi√†o", "trans": "efficient"},
    {"word": "Â™≤Áæé", "pinyin": "p√¨ mƒõi", "trans": "rival"},
    {"word": "Ë∂ÖË∂ä", "pinyin": "chƒÅo yu√®", "trans": "surpass"}
]
[07.06.2025 12:46] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ SeedVR2 ÁöÑËßÜÈ¢ë‰øÆÂ§çÊ®°Âûã„ÄÇÂÆÉ‰ΩøÁî®Âü∫‰∫éÊâ©Êï£ÁöÑÊñπÊ≥ïÂíåÈÄÇÂ∫îÊÄßÁ™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáËßÜÈ¢ë„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSeedVR2 Âú®ËÆ°ÁÆóÊàêÊú¨ËæÉ‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõÈ´òËßÜËßâË¥®Èáè„ÄÇÈÄöËøáÁâπÂæÅÂåπÈÖçÊçüÂ§±ÔºåÂÆÉÂú®‰∏ÄÊ≠•ÂÜÖÂÆûÁé∞‰∫ÜÁ®≥ÂÆö‰∏îÈ´òÊïàÁöÑÂØπÊäóÊÄßËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSeedVR2 ÁöÑÊÄßËÉΩÂ™≤ÁæéÊàñË∂ÖË∂äÁé∞ÊúâÁöÑËßÜÈ¢ë‰øÆÂ§çÊñπÊ≥ï„ÄÇ
[07.06.2025 12:46] Mistral response. {"id": "6fa9388ba7174c619f08312400e4bbed", "object": "chat.completion", "created": 1749300385, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a video restoration model called SeedVR2. It employs a diffusion-based method and an adaptive window attention mechanism to handle high-resolution videos. Compared to existing methods, SeedVR2 provides high visual quality at a lower computational cost. Through feature matching loss, it achieves stable and efficient adversarial training in a single step. Experimental results demonstrate that SeedVR2's performance is on par with or surpasses existing video restoration methods."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 170, "total_tokens": 275, "completion_tokens": 105}}
[07.06.2025 12:46] Response: This article introduces a video restoration model called SeedVR2. It employs a diffusion-based method and an adaptive window attention mechanism to handle high-resolution videos. Compared to existing methods, SeedVR2 provides high visual quality at a lower computational cost. Through feature matching loss, it achieves stable and efficient adversarial training in a single step. Experimental results demonstrate that SeedVR2's performance is on par with or surpasses existing video restoration methods.
[07.06.2025 12:46] Renaming data file.
[07.06.2025 12:46] Renaming previous data. hf_papers.json to ./d/2025-06-06.json
[07.06.2025 12:46] Saving new data file.
[07.06.2025 12:46] Generating page.
[07.06.2025 12:46] Renaming previous page.
[07.06.2025 12:46] Renaming previous data. index.html to ./d/2025-06-06.html
[07.06.2025 12:46] [Experimental] Generating Chinese page for reading.
[07.06.2025 12:46] Chinese vocab [{'word': 'Êâ©Êï£', 'pinyin': 'ku√≤ s√†n', 'trans': 'diffusion'}, {'word': 'ÈÄÇÂ∫îÊÄß', 'pinyin': 'sh√¨ y√¨ng x√¨ng', 'trans': 'adaptive'}, {'word': 'Á™óÂè£', 'pinyin': 'chuƒÅng k«íu', 'trans': 'window'}, {'word': 'Ê≥®ÊÑèÂäõ', 'pinyin': 'zh√π y√¨ l√¨', 'trans': 'attention'}, {'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´ zh√¨', 'trans': 'mechanism'}, {'word': 'È´òÂàÜËæ®Áéá', 'pinyin': 'gƒÅo fƒìn bi√†n l«ú', 'trans': 'high resolution'}, {'word': 'ËÆ°ÁÆóÊàêÊú¨', 'pinyin': 'j√¨ su√†n ch√©ng bƒõn', 'trans': 'computational cost'}, {'word': 'ËßÜËßâË¥®Èáè', 'pinyin': 'sh√¨ ju√© zh√¨ li√†ng', 'trans': 'visual quality'}, {'word': 'ÁâπÂæÅÂåπÈÖç', 'pinyin': 't√® zhƒìng p«ê p√®i', 'trans': 'feature matching'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'}, {'word': 'ÂØπÊäóÊÄß', 'pinyin': 'du√¨ k√†ng x√¨ng', 'trans': 'adversarial'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'training'}, {'word': 'Á®≥ÂÆö', 'pinyin': 'wƒõn d√¨ng', 'trans': 'stable'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅo xi√†o', 'trans': 'efficient'}, {'word': 'Â™≤Áæé', 'pinyin': 'p√¨ mƒõi', 'trans': 'rival'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}]
[07.06.2025 12:46] Renaming previous Chinese page.
[07.06.2025 12:46] Renaming previous data. zh.html to ./d/2025-06-06_zh_reading_task.html
[07.06.2025 12:46] Writing Chinese reading task.
[07.06.2025 12:46] Writing result.
[07.06.2025 12:46] Renaming log file.
[07.06.2025 12:46] Renaming previous data. log.txt to ./logs/2025-06-07_last_log.txt
