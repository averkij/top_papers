[04.02.2025 23:09] Read previous papers.
[04.02.2025 23:09] Generating top page (month).
[04.02.2025 23:09] Writing top page (month).
[05.02.2025 00:45] Read previous papers.
[05.02.2025 00:45] Get feed.
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01237
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01061
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01456
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18636
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01341
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01534
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01639
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.00698
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01068
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.00094
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01637
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01142
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01100
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01081
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01591
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01208
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01441
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01584
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01636
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01619
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.00314
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2501.18055
[05.02.2025 00:45] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01126
[05.02.2025 00:45] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.02.2025 00:45] No deleted papers detected.
[05.02.2025 00:45] Downloading and parsing papers (pdf, html). Total: 23.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01237.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01237.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01237.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01061.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01061.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01061.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01456.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01456.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01456.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2501.18636.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2501.18636.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2501.18636.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01341.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01341.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01341.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01534.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01534.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01534.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01639.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01639.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01639.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.00698.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.00698.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.00698.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01068.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01068.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01068.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.00094.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.00094.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.00094.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01637.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01637.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01637.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01142.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01142.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01142.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01100.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01100.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01100.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01081.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01081.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01081.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01591.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01591.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01591.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01208.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01208.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01208.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01441.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01441.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01441.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01584.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01584.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01584.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01636.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01636.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01636.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01619.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01619.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01619.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.00314.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.00314.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.00314.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2501.18055.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2501.18055.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2501.18055.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Downloading and parsing paper https://huggingface.co/papers/2502.01126.
[05.02.2025 00:45] Extra JSON file exists (./assets/json/2502.01126.json), skip PDF parsing.
[05.02.2025 00:45] Paper image links file exists (./assets/img_data/2502.01126.json), skip HTML parsing.
[05.02.2025 00:45] Success.
[05.02.2025 00:45] Enriching papers with extra data.
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 0. Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 1. End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propo...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 2. Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement lea...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 3. The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulner...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 4. Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarit...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 5. Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potentia...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 6. We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers m...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 7. IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence r...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 8. While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing ...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 9. Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narro...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 10. We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequ...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 11. Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due t...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 12. We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 13. The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI)....
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 14. We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term r...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 15. Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensure...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 16. Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 17. Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and mode...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 18. Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 19. Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors wh...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 20. The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-con...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 21. Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the wel...
[05.02.2025 00:45] ********************************************************************************
[05.02.2025 00:45] Abstract 22. Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, m...
[05.02.2025 00:45] Read previous papers.
[05.02.2025 00:45] Generating reviews via LLM API.
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#rlhf", "#training", "#alignment"], "emoji": "üéØ", "ru": {"title": "–ü—Ä—è–º–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ: –ø—Ä–æ—Å—Ç–æ–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø—Ä—è–º–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (DAA) –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#video", "#training", "#diffusion"], "emoji": "üé≠", "ru": {"title": "OmniHuman: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏", "desc": "OmniHuman - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ª—é
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "PRIME: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π —Å –Ω–µ—è–≤–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π PRIME. –û–Ω –∏—Å–ø–æ–ª—å
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#rag", "#security", "#dataset", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "SafeRAG: –æ—Ü–µ–Ω–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ SafeRAG –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (RAG). –ê–≤—Ç–æ—Ä—ã
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#alignment", "#cv", "#multimodal"], "emoji": "üîÄ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "AlignVLM - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –≤–∏–¥–µ –≤–∑
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#training", "#dataset", "#leakage"], "emoji": "üïµÔ∏è", "ru": {"title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ: LLM-—Å—É–¥—å–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–µ–¥–≤–∑—è—Ç—ã!", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É '—É—Ç–µ—á–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π' –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#dataset", "#open_source", "#multimodal", "#interpretability", "#cv"], "emoji": "üéöÔ∏è", "ru": {"title": "SliderSpace: –†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SliderSpace - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò –ø—Ä–æ–≤–∞–ª–∏–≤–∞–µ—Ç —Ç–µ—Å—Ç –Ω–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ MM-IQ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –§—Ä–µ–π–º
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#long_context", "#training", "#inference", "#optimization"], "emoji": "üöÄ", "ru": {"title": "FastKV: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FastKV - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ (KV) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#low_resource", "#multimodal", "#multilingual"], "emoji": "üåç", "ru": {"title": "AIN: –ü—Ä–æ—Ä—ã–≤ –≤ –∞—Ä–∞–±–æ—è–∑—ã—á–Ω–æ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å AIN - –¥–≤—É—è–∑—ã—á–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞—Ä–∞–±—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ 
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "SCONE - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–ª–æ—ë–≤ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#optimization", "#hallucinations", "#reasoning", "#rag", "#rl"], "emoji": "üß†", "ru": {"title": "DeepRAG: —É–º–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –ò–ò", "desc": "DeepRAG - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –º–æ–¥–µ–ª–∏—Ä—É—é—â–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–∏—Å–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–∞–∫ –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –û–Ω –∏—Ç
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#reasoning", "#training", "#inference", "#benchmark"], "emoji": "üß©", "ru": {"title": "–ü—Ä–æ–∫–ª—è—Ç–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ª–æ–≥–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –∏—Ö –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–µ–º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–≥
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agi", "#open_source", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–µ—Ä–∏–π G
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#rl", "#architecture", "#benchmark", "#games", "#training", "#reasoning", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ model-based RL: –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —á–µ–ª–æ–≤–µ–∫–∞ –≤ Craftax-classic", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏, –¥–æ—Å
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#ethics", "#rlhf", "#inference", "#alignment"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ LLM –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Å 
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#dataset", "#training", "#optimization", "#architecture", "#open_source", "#diffusion", "#video"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#inference", "#benchmark"], "emoji": "üß©", "ru": {"title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö NPR Sunday Puzz
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#training", "#interpretability", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã 
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#plp"], "emoji": "üß™", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —é–Ω–∏—Ç-—Ç–µ—Å—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–ª–∞–¥–∫–∏ –∫–æ–¥–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UTGen - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —é–Ω–∏—Ç-—Ç–µ—Å—Ç–æ–≤, –≤—ã—è–≤–ª—è—é—â–∏—Ö –æ—à–∏–±–∫
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#architecture", "#cv", "#training", "#dataset"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ–ø—É—Ö–æ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–ø—É—Ö–æ–ª–µ–π –∑–∞–±—Ä—é—à–∏–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≥
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#healthcare", "#ethics", "#security", "#dataset"], "emoji": "üî¨", "ru": {"title": "–ü—É—Ç—å –∫ –Ω–∞–¥–µ–∂–Ω—ã–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤ –ø–∞—Ç–æ–ª–æ–≥–∏–∏: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ü–µ–Ω—Ç—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–§–ú) –≤ –ø–∞—Ç–æ–ª–æ–≥–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω
[05.02.2025 00:45] Using data from previous issue: {"categories": ["#interpretability", "#rlhf", "#reasoning", "#benchmark", "#data"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≥–¥–µ –º–æ–¥–µ–ª—å —Å—Ä–∞–≤–Ω
[05.02.2025 00:45] Loading Chinese text from previous data.
[05.02.2025 00:45] Renaming data file.
[05.02.2025 00:45] Renaming previous data. hf_papers.json to ./d/2025-02-05.json
[05.02.2025 00:45] Saving new data file.
[05.02.2025 00:45] Generating page.
[05.02.2025 00:45] Renaming previous page.
[05.02.2025 00:45] Renaming previous data. index.html to ./d/2025-02-05.html
[05.02.2025 00:45] [Experimental] Generating Chinese page for reading.
[05.02.2025 00:45] Chinese vocab [{'word': 'Áõ¥Êé•ÂØπÈΩêÁÆóÊ≥ï', 'pinyin': 'zh√≠jiƒì du√¨q√≠ su√†nf«é', 'trans': 'direct alignment algorithm'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√®l√º√®', 'trans': 'strategy'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çuhu√†', 'trans': 'optimize'}, {'word': 'ÁÆÄÂåñ', 'pinyin': 'ji«énhu√†', 'trans': 'simplify'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«îy√°n m√≥x√≠ng', 'trans': 'language model'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨q√≠', 'trans': 'align'}, {'word': 'ÊéíÂêç', 'pinyin': 'p√°im√≠ng', 'trans': 'rank'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«înshƒ´', 'trans': 'loss'}, {'word': 'Â•ñÂä±', 'pinyin': 'ji«éngl√¨', 'trans': 'reward'}, {'word': 'Á±ªÂûã', 'pinyin': 'l√®ix√≠ng', 'trans': 'type'}, {'word': 'ÁõëÁù£', 'pinyin': 'ji√†nd≈´', 'trans': 'supervise'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìiti√°o', 'trans': 'fine-tune'}, {'word': 'Èò∂ÊÆµ', 'pinyin': 'jiƒìdu√†n', 'trans': 'stage'}, {'word': 'ÂçïÈò∂ÊÆµ', 'pinyin': 'dƒÅn jiƒìdu√†n', 'trans': 'single-stage'}, {'word': 'ÂèåÈò∂ÊÆµ', 'pinyin': 'shuƒÅng jiƒìdu√†n', 'trans': 'two-stage'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'ÊòæÂºè', 'pinyin': 'xi«énsh√¨', 'trans': 'explicit'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅnsh«î', 'trans': 'parameter'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√≠ngn√©ng', 'trans': 'performance'}, {'word': 'ÂàÜÊûê', 'pinyin': 'fƒìnxƒ´', 'trans': 'analysis'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'}, {'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«énji√†n', 'trans': 'key'}, {'word': 'Âõ†Á¥†', 'pinyin': 'yƒ´ns√π', 'trans': 'factor'}, {'word': 'ÊàêÂØπ', 'pinyin': 'ch√©ngdu√¨', 'trans': 'pair'}, {'word': 'ÁõÆÊ†á', 'pinyin': 'm√πbiƒÅo', 'trans': 'target'}, {'word': 'ÁÇπ', 'pinyin': 'di«én', 'trans': 'point'}, {'word': 'ÈöêÂºè', 'pinyin': 'y«ênsh√¨', 'trans': 'implicit'}, {'word': 'ÂáΩÊï∞', 'pinyin': 'h√°nsh√π', 'trans': 'function'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'}, {'word': 'Âº∫Ë∞É', 'pinyin': 'qi√°ngdi√†o', 'trans': 'emphasize'}, {'word': '‰ªîÁªÜ', 'pinyin': 'z«êx√¨', 'trans': 'careful'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': 'ÈáçË¶ÅÊÄß', 'pinyin': 'zh√≤ngy√†ox√¨ng', 'trans': 'importance'}, {'word': 'ÈÅøÂÖç', 'pinyin': 'b√¨mi«én', 'trans': 'avoid'}, {'word': 'ËøáÊó©', 'pinyin': 'gu√≤z«éo', 'trans': 'premature'}, {'word': 'Â£∞Áß∞', 'pinyin': 'shƒìngchƒìng', 'trans': 'claim'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠shƒìng', 'trans': 'improvement'}, {'word': 'Êï¥‰Ωì', 'pinyin': 'zhƒõngt«ê', 'trans': 'overall'}, {'word': '‰ºòË∂äÊÄß', 'pinyin': 'y≈çuyu√®x√¨ng', 'trans': 'superiority'}]
[05.02.2025 00:45] Renaming previous Chinese page.
[05.02.2025 00:45] Renaming previous data. zh.html to ./d/2025-02-04_zh_reading_task.html
[05.02.2025 00:45] Writing Chinese reading task.
[05.02.2025 00:45] Writing result.
[05.02.2025 00:45] Renaming log file.
[05.02.2025 00:45] Renaming previous data. log.txt to ./logs/2025-02-05_last_log.txt
