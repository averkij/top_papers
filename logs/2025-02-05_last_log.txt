[05.02.2025 02:11] Read previous papers.
[05.02.2025 02:11] Generating top page (month).
[05.02.2025 02:11] Writing top page (month).
[05.02.2025 03:14] Read previous papers.
[05.02.2025 03:14] Get feed.
[05.02.2025 03:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.02584
[05.02.2025 03:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.02508
[05.02.2025 03:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.02.2025 03:14] Downloading and parsing papers (pdf, html). Total: 2.
[05.02.2025 03:14] Downloading and parsing paper https://huggingface.co/papers/2502.02584.
[05.02.2025 03:14] Downloading paper 2502.02584 from http://arxiv.org/pdf/2502.02584v1...
[05.02.2025 03:14] Extracting affiliations from text.
[05.02.2025 03:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search Zongyu Lin 1 * Yao Tang 2 * Xingcheng Yao 1 * Da Yin 1 * Ziniu Hu 1 Yizhou Sun 1 Kai-Wei Chang "
[05.02.2025 03:14] Response: ```python
[]
```
[05.02.2025 03:14] Extracting affiliations from text.
[05.02.2025 03:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search Zongyu Lin 1 * Yao Tang 2 * Xingcheng Yao 1 * Da Yin 1 * Ziniu Hu 1 Yizhou Sun 1 Kai-Wei ChangLanguage agents have become promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in stepwise manner for open language agents. By introducing exploration tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose Q-guided generation strategy to enable language agents to better adapt to longterm value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically show that QLASS can lead to more effective decision making through qualitative analysis. 1 5 2 0 2 4 ] . [ 1 4 8 5 2 0 . 2 0 5 2 : r 1. Introduction Supervised fine-tuning (SFT) is commonly employed to make base LLMs perform effective reasoning and planning in complex agent tasks by imitating expert trajectories (Chen et al., 2023; Yin et al., 2024). However, the substantial hu- * Equal contribution. Equal advising. 1University of California, Los Angeles, USA 2Shanghai Jiaotong University, Shanghai, China. Correspondence to: Yizhou Sun <yizhou.sun@ucla.edu>, Kai-Wei Chang <kaiwei.chang@ucla.edu>. 1We will release our code and data in https://github. com/Rafa-zy/QLASS Figure 1. QLASS pipeline overview. QLASS involves mainly four stages: 1) Supervised fine-tuning (SFT) on expert data. 2) Leverage SFT agent to explore the environment and construct an exploration tree for each task. After construction, estimate the Q-value of each tree node based on Equation 7. 3) Train QNet on the estimated Q-values. 4) Use the trained QNet to provide inference guidance at each step. man annotations required to collect training data present significant bottleneck, limiting both performance and scalability. This challenge is particularly pronounced in agent tasks (Yao et al., 2022; Shridhar et al., 2021; Wang et al., 2022a), where data scarcity is critical issue due to the inherent complexity and diversity of real-world interactions. To overcome this challenge, self-improvement techniques have shown to be promising area of research, enabling LLMs to learn from self-generated data without extensive human intervention (Wang et al., 2022b; Singh et al., 2023; Hosseini et al., 2024; Zhang et al., 2024), and most recently, enable LLMs to improve their outputs by scaling up their test-time computation in more intelligent way (Wang et al., 2024; Shinn et al., 2023; Snell et al., 2024). Inspired by this, we focus on improving the inference-time search for language agents, which is crucial technique for the success of more generalizable agents in real-world environments. An essential component of inference-time scaling methods is the reward model (Snell et al., 2024), which evaluates the quality of self-explored data. Many existing works derive single outcome reward based on ground-truth (Wang et al., 2024; Shinn et al., 2023). Although this approach is straightforward, it falls short in handling complex agent tasks, since an outcome-reward model cannot accurately score each step within long trajectory in intricate scenarios. In addition, trajectory achieving high final outcome reward does not QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search necessarily indicate that every action taken was optimal; the agent may have completed the task successfully, but some actions could have been inefficient or suboptimal (Uesato et al., 2022). low reward while preserving critical decision points. This enables agents to disentangle productive reasoning from wasteful loops, even with limited supervision. To summarize, our contribution can be divided into three folds: Therefore, good process reward model is necessary to learn from the environmental feedback and provide stepwise evaluations of agent actions. This model enables the agent to fully understand and learn from the intermediate stages of complex tasks, ultimately improving performance and generalization. The key challenge lies in developing an effective process reward model for self-improvement without relying on extensive human annotations for the stepwise reward. There has been thread of work that has focused on process reward modeling (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Chen et al., 2024). However, these methods rely on either costly human annotation or computationally heavy random rollouts, rendering them inefficient for the self-improvement of language model agents. To reduce the annotation reliance on process rewards, we propose QLASS to perform effective process reward modeling to guide agent inference. Specifically, we explicitly formalize the self-generated exploratory trajectories as exploration trees and update the process rewards of all the tree nodes based on the tree structures. To better capture the future utility at each step of multi-turn reasoning process, we employ the Bellman equation (Bellman & Dreyfus, 2015) to learn Q-based process reward. Unlike simple outcome-based rewards, this Q-value captures how immediate decisions contribute to longer-term payoffs, enabling finer-grained control over the reasoning trajectory. Moreover, the Bellman update rule iteratively refines the estimated Q-values by propagating future rewards back to earlier states, reducing reliance on sparse or delayed feedback signals. This allows us to efficiently gather supervision for state-action pairs without requiring explicit annotations of full trajectories. With these values in hand, we can then train function approximator (QNet) (Watkins & Dayan, 1992) to predict the expected return of any partial solution, ultimately providing strong inductive bias to guid"
[05.02.2025 03:14] Mistral response. {"id": "e4e4be3fd4c84a95907aba79e073a2c5", "object": "chat.completion", "created": 1738725250, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"University of California, Los Angeles, USA\", \"Shanghai Jiaotong University, Shanghai, China\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1612, "total_tokens": 1643, "completion_tokens": 31}}
[05.02.2025 03:14] Response: ```python
["University of California, Los Angeles, USA", "Shanghai Jiaotong University, Shanghai, China"]
```
[05.02.2025 03:14] Deleting PDF ./assets/pdf/2502.02584.pdf.
[05.02.2025 03:14] Success.
[05.02.2025 03:14] Downloading and parsing paper https://huggingface.co/papers/2502.02508.
[05.02.2025 03:14] Downloading paper 2502.02508 from http://arxiv.org/pdf/2502.02508v1...
[05.02.2025 03:14] Extracting affiliations from text.
[05.02.2025 03:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 8 0 5 2 0 . 2 0 5 2 : r Satori: Reinforcement Learning with Chain-of-Action-Thought Maohao Shen * 1 Guangtao Zeng * 2 Zhenting Qi * 3 Zhang-Wei Hong 1 Zhenfang Chen 4 Wei Lu 2 Gregory Wornell 1 Subhro Das 4 David Cox 4 Chuang Gan "
[05.02.2025 03:14] Response: ```python
[]
```
[05.02.2025 03:14] Extracting affiliations from text.
[05.02.2025 03:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 ] . [ 1 8 0 5 2 0 . 2 0 5 2 : r Satori: Reinforcement Learning with Chain-of-Action-ThoughtMaohao Shen * 1 Guangtao Zeng * 2 Zhenting Qi * 3 Zhang-Wei Hong 1 Zhenfang Chen 4 Wei Lu 2 Gregory Wornell 1 Subhro Das 4 David Cox 4 Chuang Gan1. Introduction Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of single LLM to tackle complex tasks. Thus, we pose new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chainof-Action-Thought (COAT) reasoning and twostage training paradigm: 1) small-scale format tuning stage to internalize the COAT reasoning format and 2) large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced1. * Core contributors of Satori team, contributed equally to this work. Order determined by dice roll. 1MIT 2Singapore University of Technology and Design 3Harvard 4MIT-IBM Watson AI Lab, IBM Research 5UMass Amherst. Correspondence to: Maohao Shen <maohao@mit.edu>. Preprint, under review 1https://satori-reasoning.github.io/ 1 Large language models (LLMs) have demonstrated remarkable performance across wide range of reasoning tasks, including mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021a), programming (Chen et al., 2021; Zhuo et al., 2024) and logical reasoning (Han et al., 2024; Liu et al., 2020). One of the key techniques enabling these strong reasoning capabilities is Chain-of-Thought (CoT) prompting (Wei et al., 2022), which allows LLMs to address complex tasks by generating series of intermediate reasoning steps. As result, many early efforts focus on finetuning LLMs using large-scale, high-quality CoT reasoning chains, either through human annotation (Hendrycks et al., 2021a; Yue et al., 2024) or by distilling synthetic data from more advanced models (Yu et al., 2024; Toshniwal et al., 2024a; Ding et al., 2024). However, human annotation is extremely labor intensive, and distillation often limits the models reasoning capabilities to certain level. Apart from scaling up training resources, more recent work has focused on test-time scaling, i.e., allocating additional inference-time compute to search for more accurate solutions. This often involves extensive sampling, either by generating multiple complete solutions (Wang et al., 2023) or by sampling multiple intermediate reasoning steps (Yao et al., 2024; Wan et al., 2024). These methods typically require external feedback to guide the search process, usually through training an auxiliary reward model to rate final solutions or intermediate steps (Sun et al., 2024; Wang et al., 2024a). However, such two-player frameworks incur more model-deployment costs and do not internalize the search capabilities into single LLM. Orthogonal to the above work, our study investigates new direction that enables LLMs with autoregressive search capabilities, i.e., an extended reasoning process with selfreflection and self-exploration of new strategies. Specifically, we introduce the Chain-of-Action-Thought (COAT) mechanism, which enables LLMs to take various metaactions during problem solving. Unlike conventional posttraining consisting of large-scale supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), we propose novel two-stage training paradigm: (1) small-scale format tuning (FT) stage to internalize the COAT reasoning format and (2) large-scale selfimprovement stage that utilizes reinforcement learning with Restart and Explore (RAE) techniques. Our approach leads to the development of Satori, 7B LLM trained on open-source base models and mathematic data that achieve superior performance on both in-domain and out-of-domain tasks. To summarize, our contributions are threefold, 1. Efficiency: Satori is single LLM capable of autoregressive search without external guidance (Section 6 and Section A). Moreover, this is achieved with minimal supervision and large-scale self-improvement. 2. Effectiveness: Satori demonstrates superior performance on in-domain mathematical reasoning tasks and outperforms the instruct model built on the same base model (Section 5.1). 3. Generalizability: Unlike recent research on math reasoning, Satori exhibits strong transferability to out-ofdomain tasks and demonstrates universal capabilities for self-reflection and self-exploration (Section 5.2). 2. Related Work We summarize the literature that is closely aligned with the scope of this paper (refer to Section for more discussions). Concurrent Work. Building on the impact of OpenAIs o1 (OpenAI, 2024), significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. The most common approach relies on distilling knowledge from stronger teacher models (Huang et al., 2024a; Zhao et al., 2024; Min et al., 2024). In contrast, Satori addresses this problem from reinforcement learning (RL) perspective and requires minimal supervision (only 10K samples in the format tuning stage). The most related concurrent work is DeepSeeks recently released R1 (Guo et al., 2025), which adopts similar high-level strategy of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms. Additionally, DeepSeek-R1 focuses on training large-scale LLMs (671B), whereas our work provides insights into the development of smaller-scale LLMs (7B) for research purpose. Finally, as an industry-developed model"
[05.02.2025 03:14] Mistral response. {"id": "8a32aace118840e69541af5e0c5bfc99", "object": "chat.completion", "created": 1738725278, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "['MIT', 'Singapore University of Technology and Design', 'Harvard', 'MIT-IBM Watson AI Lab, IBM Research', 'UMass Amherst']"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1687, "total_tokens": 1722, "completion_tokens": 35}}
[05.02.2025 03:14] Response: ['MIT', 'Singapore University of Technology and Design', 'Harvard', 'MIT-IBM Watson AI Lab, IBM Research', 'UMass Amherst']
[05.02.2025 03:14] Deleting PDF ./assets/pdf/2502.02508.pdf.
[05.02.2025 03:14] Success.
[05.02.2025 03:14] Enriching papers with extra data.
[05.02.2025 03:14] ********************************************************************************
[05.02.2025 03:14] Abstract 0. Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annota...
[05.02.2025 03:14] ********************************************************************************
[05.02.2025 03:14] Abstract 1. Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verif...
[05.02.2025 03:14] Read previous papers.
[05.02.2025 03:14] Generating reviews via LLM API.
[05.02.2025 03:14] Querying the API.
[05.02.2025 03:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.
[05.02.2025 03:14] Response: {
  "desc": "Статья представляет новый метод QLASS для обучения языковых агентов. QLASS использует пошаговую оценку Q-значений для генерации аннотаций и улучшения промежуточного обучения. Метод вводит дерево рассуждений и моделирование вознаграждений процесса для эффективного пошагового руководства. QLASS позволяет языковым агентам лучше адаптироваться к долгосрочным целям, значительно улучшая производительность при решении сложных интерактивных задач.",
  "emoji": "🤖",
  "title": "QLASS: Пошаговое обучение языковых агентов для повышения эффективности"
}
[05.02.2025 03:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data."

[05.02.2025 03:14] Response: ```python
['AGENTS', 'RLHF', 'TRAINING', 'INFERENCE']
```
[05.02.2025 03:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data."

[05.02.2025 03:14] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[05.02.2025 03:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision.","title":"Enhancing Language Agents with Stepwise Q-Guidance"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces QLASS, a method designed to enhance the performance of language agents in complex tasks by providing stepwise guidance through Q-value estimation. Traditional approaches rely on outcome reward models, which can result in sub-optimal decision-making due to the lack of intermediate feedback. QLASS addresses this by creating a reasoning tree that models rewards at each step, allowing agents to learn from their interactions more effectively. The results show that QLASS can maintain strong performance with less annotated data, proving its efficiency in environments with limited supervision.', title='Enhancing Language Agents with Stepwise Q-Guidance'))
[05.02.2025 03:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为QLASS的模型，用于改进语言代理在复杂交互任务中的表现。QLASS通过逐步估计Q值来自动生成中间交互的注释，从而为语言代理提供有效的指导。与传统的结果奖励模型不同，QLASS引入了推理树和过程奖励建模，使得每一步都有明确的指导。实验结果表明，即使在标注数据减少的情况下，QLASS仍能保持良好的性能，展示了其在有限监督下的高效性。","title":"QLASS：提升语言代理的决策能力"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本文提出了一种名为QLASS的模型，用于改进语言代理在复杂交互任务中的表现。QLASS通过逐步估计Q值来自动生成中间交互的注释，从而为语言代理提供有效的指导。与传统的结果奖励模型不同，QLASS引入了推理树和过程奖励建模，使得每一步都有明确的指导。实验结果表明，即使在标注数据减少的情况下，QLASS仍能保持良好的性能，展示了其在有限监督下的高效性。', title='QLASS：提升语言代理的决策能力'))
[05.02.2025 03:14] Querying the API.
[05.02.2025 03:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.
[05.02.2025 03:14] Response: {
  "desc": "Исследователи представили новый подход к улучшению способностей больших языковых моделей (LLM) к рассуждению. Они разработали метод Chain-of-Action-Thought (COAT), который позволяет модели проводить самоанализ и исследовать новые стратегии решения задач. Процесс обучения включает два этапа: настройку формата на небольшом масштабе и масштабное самосовершенствование с использованием обучения с подкреплением. В результате была создана 7-миллиардная модель Satori, показавшая отличные результаты в задачах математического рассуждения и обобщения на новые области.",
  "emoji": "🧠",
  "title": "Satori: LLM с внутренним поиском для улучшенного рассуждения"
}
[05.02.2025 03:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced."

[05.02.2025 03:14] Response: ```python
["RL", "TRAINING", "MATH", "SMALL_MODELS"]
```
[05.02.2025 03:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced."

[05.02.2025 03:14] Response: ```python
["REASONING", "OPEN_SOURCE"]
```
[05.02.2025 03:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance.","title":"Empowering LLMs with Internalized Reasoning through COAT"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper discusses the development of Satori, a large language model (LLM) that enhances reasoning capabilities through a novel approach called Chain-of-Action-Thought (COAT). The authors propose a two-stage training process that first tunes the model to internalize COAT reasoning and then employs reinforcement learning for self-improvement. By internalizing the search capabilities typically guided by an external verifier, Satori can perform complex reasoning tasks more effectively. The results show that Satori not only excels in mathematical reasoning benchmarks but also generalizes well to other tasks, indicating its robust performance.', title='Empowering LLMs with Internalized Reasoning through COAT'))
[05.02.2025 03:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"大型语言模型（LLMs）在多个领域展示了出色的推理能力。研究表明，增加测试时的计算可以提升LLMs的推理能力，通常需要在推理时进行大量采样，并由外部LLM验证器指导。本文提出了一个新问题：能否将搜索能力内化，以根本性地增强单个LLM的推理能力？我们提出了行动思维链（COAT）推理方法，并设计了一个两阶段的训练范式，以实现LLM的自我改进和自我反思。","title":"内化搜索能力，提升推理能力！"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='大型语言模型（LLMs）在多个领域展示了出色的推理能力。研究表明，增加测试时的计算可以提升LLMs的推理能力，通常需要在推理时进行大量采样，并由外部LLM验证器指导。本文提出了一个新问题：能否将搜索能力内化，以根本性地增强单个LLM的推理能力？我们提出了行动思维链（COAT）推理方法，并设计了一个两阶段的训练范式，以实现LLM的自我改进和自我反思。', title='内化搜索能力，提升推理能力！'))
[05.02.2025 03:15] Loading Chinese text from previous data.
[05.02.2025 03:15] Renaming data file.
[05.02.2025 03:15] Renaming previous data. hf_papers.json to ./d/2025-02-05.json
[05.02.2025 03:15] Saving new data file.
[05.02.2025 03:15] Generating page.
[05.02.2025 03:15] Renaming previous page.
[05.02.2025 03:15] Renaming previous data. index.html to ./d/2025-02-05.html
[05.02.2025 03:15] [Experimental] Generating Chinese page for reading.
[05.02.2025 03:15] Chinese vocab [{'word': '直接对齐算法', 'pinyin': 'zhíjiē duìqí suànfǎ', 'trans': 'direct alignment algorithm'}, {'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'}, {'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimize'}, {'word': '简化', 'pinyin': 'jiǎnhuà', 'trans': 'simplify'}, {'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'}, {'word': '对齐', 'pinyin': 'duìqí', 'trans': 'align'}, {'word': '排名', 'pinyin': 'páimíng', 'trans': 'rank'}, {'word': '损失', 'pinyin': 'sǔnshī', 'trans': 'loss'}, {'word': '奖励', 'pinyin': 'jiǎnglì', 'trans': 'reward'}, {'word': '类型', 'pinyin': 'lèixíng', 'trans': 'type'}, {'word': '监督', 'pinyin': 'jiàndū', 'trans': 'supervise'}, {'word': '微调', 'pinyin': 'wēitiáo', 'trans': 'fine-tune'}, {'word': '阶段', 'pinyin': 'jiēduàn', 'trans': 'stage'}, {'word': '单阶段', 'pinyin': 'dān jiēduàn', 'trans': 'single-stage'}, {'word': '双阶段', 'pinyin': 'shuāng jiēduàn', 'trans': 'two-stage'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'}, {'word': '显式', 'pinyin': 'xiǎnshì', 'trans': 'explicit'}, {'word': '参数', 'pinyin': 'cānshǔ', 'trans': 'parameter'}, {'word': '性能', 'pinyin': 'xíngnéng', 'trans': 'performance'}, {'word': '分析', 'pinyin': 'fēnxī', 'trans': 'analysis'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'}, {'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key'}, {'word': '因素', 'pinyin': 'yīnsù', 'trans': 'factor'}, {'word': '成对', 'pinyin': 'chéngduì', 'trans': 'pair'}, {'word': '目标', 'pinyin': 'mùbiāo', 'trans': 'target'}, {'word': '点', 'pinyin': 'diǎn', 'trans': 'point'}, {'word': '隐式', 'pinyin': 'yǐnshì', 'trans': 'implicit'}, {'word': '函数', 'pinyin': 'hánshù', 'trans': 'function'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '强调', 'pinyin': 'qiángdiào', 'trans': 'emphasize'}, {'word': '仔细', 'pinyin': 'zǐxì', 'trans': 'careful'}, {'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'}, {'word': '重要性', 'pinyin': 'zhòngyàoxìng', 'trans': 'importance'}, {'word': '避免', 'pinyin': 'bìmiǎn', 'trans': 'avoid'}, {'word': '过早', 'pinyin': 'guòzǎo', 'trans': 'premature'}, {'word': '声称', 'pinyin': 'shēngchēng', 'trans': 'claim'}, {'word': '提升', 'pinyin': 'tíshēng', 'trans': 'improvement'}, {'word': '整体', 'pinyin': 'zhěngtǐ', 'trans': 'overall'}, {'word': '优越性', 'pinyin': 'yōuyuèxìng', 'trans': 'superiority'}]
[05.02.2025 03:15] Renaming previous Chinese page.
[05.02.2025 03:15] Renaming previous data. zh.html to ./d/2025-02-04_zh_reading_task.html
[05.02.2025 03:15] Writing Chinese reading task.
[05.02.2025 03:15] Writing result.
[05.02.2025 03:15] Renaming log file.
[05.02.2025 03:15] Renaming previous data. log.txt to ./logs/2025-02-05_last_log.txt
